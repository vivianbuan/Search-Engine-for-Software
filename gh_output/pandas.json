[{
   "collaborators_cnt": "661",
   "fork_cnt": "5478",
   "repo_description": "Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more",
   "version_cnt": "30",
   "pm_dependency_cnt": "85",
   "pm_description": "None",
   "repo_url": "https://github.com/pandas-dev/pandas",
   "private": "False",
   "watcher_cnt": "13522",
   "created_time": "2010-08-24 01:37:33 UTC",
   "last_update": "2018-03-19 21:14:00  UTC",
   "pm_name": "Pypi",
   "owner": "pandas-dev",
   "repo_dependency_cnt": "15067",
   "name": "pandas",
   "last_push": "2018-03-19 21:13:24  UTC",
   "id": "858127",
   "license": "BSD 3-Clause \"New\" or \"Revised\" License",
   "path": "1.git",
   "homepage_url": "http://pandas.pydata.org",
   "readMe": "<div align=\"center\">\n  <img src=\"https://github.com/pandas-dev/pandas/blob/master/doc/logo/pandas_logo.png\"><br>\n</div>\n\n-----------------\n\n# pandas: powerful Python data analysis toolkit\n\n<table>\n<tr>\n  <td>Latest Release</td>\n  <td><img src=\"https://img.shields.io/pypi/v/pandas.svg\" alt=\"latest release\" /></td>\n</tr>\n  <td></td>\n  <td><img src=\"https://anaconda.org/conda-forge/pandas/badges/version.svg\" alt=\"latest release\" /></td>\n</tr>\n<tr>\n  <td>Package Status</td>\n  <td><img src=\"https://img.shields.io/pypi/status/pandas.svg\" alt=\"status\" /></td>\n</tr>\n<tr>\n  <td>License</td>\n  <td><img src=\"https://img.shields.io/pypi/l/pandas.svg\" alt=\"license\" /></td>\n</tr>\n<tr>\n  <td>Build Status</td>\n  <td>\n    <a href=\"https://travis-ci.org/pandas-dev/pandas\">\n    <img src=\"https://travis-ci.org/pandas-dev/pandas.svg?branch=master\" alt=\"travis build status\" />\n    </a>\n  </td>\n</tr>\n<tr>\n  <td></td>\n  <td>\n    <a href=\"https://circleci.com/gh/pandas-dev/pandas\">\n    <img src=\"https://circleci.com/gh/circleci/mongofinil/tree/master.svg?style=shield&circle-token=223d8cafa7b02902c3e150242520af8944e34671\" alt=\"circleci build status\" />\n    </a>\n  </td>\n</tr>\n<tr>\n  <td></td>\n  <td>\n    <a href=\"https://ci.appveyor.com/project/pandas-dev/pandas\">\n    <img src=\"https://ci.appveyor.com/api/projects/status/86vn83mxgnl4xf1s/branch/master?svg=true\" alt=\"appveyor build status\" />\n    </a>\n  </td>\n</tr>\n<tr>\n  <td>Coverage</td>\n  <td><img src=\"https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=master\" alt=\"coverage\" /></td>\n</tr>\n<tr>\n  <td>Conda</td>\n  <td>\n    <a href=\"https://pandas.pydata.org\">\n    <img src=\"http://pubbadges.s3-website-us-east-1.amazonaws.com/pkgs-downloads-pandas.png\" alt=\"conda default downloads\" />\n    </a>\n  </td>\n</tr>\n<tr>\n  <td>Conda-forge</td>\n  <td>\n    <a href=\"https://pandas.pydata.org\">\n    <img src=\"https://anaconda.org/conda-forge/pandas/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n    </a>\n  </td>\n</tr>\n<tr>\n  <td>PyPI</td>\n  <td>\n    <a href=\"https://pypi.python.org/pypi/pandas/\">\n    <img src=\"https://img.shields.io/pypi/dm/pandas.svg\" alt=\"pypi downloads\" />\n    </a>\n  </td>\n</tr>\n</table>\n\n[![https://gitter.im/pydata/pandas](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/pydata/pandas?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## What is it\n\n**pandas** is a Python package providing fast, flexible, and expressive data\nstructures designed to make working with \"relational\" or \"labeled\" data both\neasy and intuitive. It aims to be the fundamental high-level building block for\ndoing practical, **real world** data analysis in Python. Additionally, it has\nthe broader goal of becoming **the most powerful and flexible open source data\nanalysis / manipulation tool available in any language**. It is already well on\nits way toward this goal.\n\n## Main Features\nHere are just a few of the things that pandas does well:\n\n  - Easy handling of [**missing data**][missing-data] (represented as\n    `NaN`) in floating point as well as non-floating point data\n  - Size mutability: columns can be [**inserted and\n    deleted**][insertion-deletion] from DataFrame and higher dimensional\n    objects\n  - Automatic and explicit [**data alignment**][alignment]: objects can\n    be explicitly aligned to a set of labels, or the user can simply\n    ignore the labels and let `Series`, `DataFrame`, etc. automatically\n    align the data for you in computations\n  - Powerful, flexible [**group by**][groupby] functionality to perform\n    split-apply-combine operations on data sets, for both aggregating\n    and transforming data\n  - Make it [**easy to convert**][conversion] ragged,\n    differently-indexed data in other Python and NumPy data structures\n    into DataFrame objects\n  - Intelligent label-based [**slicing**][slicing], [**fancy\n    indexing**][fancy-indexing], and [**subsetting**][subsetting] of\n    large data sets\n  - Intuitive [**merging**][merging] and [**joining**][joining] data\n    sets\n  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of\n    data sets\n  - [**Hierarchical**][mi] labeling of axes (possible to have multiple\n    labels per tick)\n  - Robust IO tools for loading data from [**flat files**][flat-files]\n    (CSV and delimited), [**Excel files**][excel], [**databases**][db],\n    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]\n  - [**Time series**][timeseries]-specific functionality: date range\n    generation and frequency conversion, moving window statistics,\n    moving window linear regressions, date shifting and lagging, etc.\n\n\n   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/missing_data.html#working-with-missing-data\n   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion\n   [alignment]: https://pandas.pydata.org/pandas-docs/stable/dsintro.html?highlight=alignment#intro-to-data-structures\n   [groupby]: https://pandas.pydata.org/pandas-docs/stable/groupby.html#group-by-split-apply-combine\n   [conversion]: https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe\n   [slicing]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#slicing-ranges\n   [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#advanced-indexing-with-ix\n   [subsetting]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\n   [merging]: https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging\n   [joining]: https://pandas.pydata.org/pandas-docs/stable/merging.html#joining-on-index\n   [reshape]: https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping-and-pivot-tables\n   [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/reshaping.html#pivot-tables-and-cross-tabulations\n   [mi]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#hierarchical-indexing-multiindex\n   [flat-files]: https://pandas.pydata.org/pandas-docs/stable/io.html#csv-text-files\n   [excel]: https://pandas.pydata.org/pandas-docs/stable/io.html#excel-files\n   [db]: https://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries\n   [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables\n   [timeseries]: https://pandas.pydata.org/pandas-docs/stable/timeseries.html#time-series-date-functionality\n\n## Where to get it\nThe source code is currently hosted on GitHub at:\nhttps://github.com/pandas-dev/pandas\n\nBinary installers for the latest released version are available at the [Python\npackage index](https://pypi.python.org/pypi/pandas) and on conda.\n\n```sh\n# conda\nconda install pandas\n```\n\n```sh\n# or PyPI\npip install pandas\n```\n\n## Dependencies\n- [NumPy](http://www.numpy.org): 1.9.0 or higher\n- [python-dateutil](https://labix.org/python-dateutil): 2.5.0 or higher\n- [pytz](https://pythonhosted.org/pytz): 2011k or higher\n\nSee the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies)\nfor recommended and optional dependencies.\n\n## Installation from sources\nTo install pandas from source you need Cython in addition to the normal\ndependencies above. Cython can be installed from pypi:\n\n```sh\npip install cython\n```\n\nIn the `pandas` directory (same one where you found this file after\ncloning the git repo), execute:\n\n```sh\npython setup.py install\n```\n\nor for installing in [development mode](https://pip.pypa.io/en/latest/reference/pip_install.html#editable-installs):\n\n```sh\npython setup.py develop\n```\n\nAlternatively, you can use `pip` if you want all the dependencies pulled\nin automatically (the `-e` option is for installing it in [development\nmode](https://pip.pypa.io/en/latest/reference/pip_install.html#editable-installs)):\n\n```sh\npip install -e .\n```\n\nSee the full instructions for [installing from source](https://pandas.pydata.org/pandas-docs/stable/install.html#installing-from-source).\n\n## License\n[BSD 3](LICENSE)\n\n## Documentation\nThe official documentation is hosted on PyData.org: https://pandas.pydata.org/pandas-docs/stable\n\n## Background\nWork on ``pandas`` started at AQR (a quantitative hedge fund) in 2008 and\nhas been under active development since then.\n\n## Getting Help\n\nFor usage questions, the best place to go to is [StackOverflow](https://stackoverflow.com/questions/tagged/pandas).\nFurther, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).\n\n## Discussion and Development\nMost development discussion is taking place on github in this repo. Further, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Gitter channel](https://gitter.im/pydata/pandas) is available for quick development related questions.\n\n## Contributing to pandas [![Open Source Helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)\n\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements and ideas are welcome.\n\nA detailed overview on how to contribute can be found in the **[contributing guide.](https://pandas.pydata.org/pandas-docs/stable/contributing.html)**\n\nIf you are simply looking to start working with the pandas codebase, navigate to the [GitHub \u201cissues\u201d tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. There are a number of issues listed under [Docs](https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open) and [Difficulty Novice](https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3A%22Difficulty+Novice%22) where you could start out.\n\nYou can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).\n\nOr maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking \u2018this can be improved\u2019...you can do something about it!\n\nFeel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [Gitter](https://gitter.im/pydata/pandas).\n",
   "language": "Python",
   "star_cnt": "13522",
   "size": "119957",
   "repo_keywords": "",
   "_childDocuments_": [
      {
         "up_vote_count": "511",
         "path": "2.stack",
         "body_markdown": "### Overview ###\r\n\r\nI&#39;m relatively familiar with `data.table`, not so much with `dplyr`.  I&#39;ve read through some [`dplyr` vignettes][1] and examples that have popped up on SO, and so far my conclusions are that:\r\n\r\n 1. `data.table` and `dplyr` are comparable in speed, except when there are many (i.e. &gt;10-100K) groups, and in some other circumstances (see benchmarks below)\r\n 2. `dplyr` has more accessible syntax\r\n 3. `dplyr` abstracts (or will) potential DB interactions\r\n 4. There are some minor functionality differences (see &quot;Examples/Usage&quot; below)\r\n\r\nIn my mind 2. doesn&#39;t bear much weight because I am fairly familiar with it `data.table`, though I understand that for users new to both it will be a big factor.  I would like to avoid an argument about which is more intuitive, as that is irrelevant for my specific question asked from the perspective of someone already familiar with `data.table`.  I also would like to avoid a discussion about how &quot;more intuitive&quot; leads to faster analysis (certainly true, but again, not what I&#39;m most interested about here).\r\n\r\n### Question ###\r\n\r\nWhat I want to know is:\r\n\r\n 1. Are there analytical tasks that are a lot easier to code with one or the other package for people familiar with the packages (i.e. some combination of keystrokes required vs. required level of esotericism, where less of each is a good thing).\r\n 2. Are there analytical tasks that are performed substantially (i.e. more than 2x) more efficiently in one package vs. another.\r\n\r\nOne [recent SO question][3] got me thinking about this a bit more, because up until that point I didn&#39;t think `dplyr` would offer much beyond what I can already do in `data.table`.  Here is the `dplyr` solution (data at end of Q):\r\n\r\n    dat %.%\r\n      group_by(name, job) %.%\r\n      filter(job != &quot;Boss&quot; | year == min(year)) %.%\r\n      mutate(cumu_job2 = cumsum(job2))\r\n\r\nWhich was much better than my hack attempt at a `data.table` solution.  That said, good `data.table` solutions are also pretty good (thanks Jean-Robert, Arun, and note here I favored single statement over the strictly most optimal solution):\r\n\r\n    setDT(dat)[,\r\n      .SD[job != &quot;Boss&quot; | year == min(year)][, cumjob := cumsum(job2)], \r\n      by=list(id, job)\r\n    ]\r\n\r\nThe syntax for the latter may seem very esoteric, but it actually is pretty straightforward if you&#39;re used to `data.table` (i.e. doesn&#39;t use some of the more esoteric tricks).\r\n\r\nIdeally what I&#39;d like to see is some good examples were the `dplyr` or `data.table` way is substantially more concise or performs substantially better.\r\n\r\n### Examples ###\r\n\r\n#### Usage ####\r\n\r\n - `dplyr` does not allow grouped operations that return arbitrary number of rows (from **[eddi&#39;s question][6]**, note: this looks like it will be implemented in **[dplyr 0.5](https://github.com/hadley/dplyr/issues/154)**, also, @beginneR shows a potential work-around using `do` in the answer to @eddi&#39;s question).\r\n - `data.table` supports **[rolling joins](https://stackoverflow.com/questions/12030932/rolling-joins-data-table-in-r)** (thanks @dholstius) as well as **[overlap joins](https://stackoverflow.com/questions/23371747/range-join-data-frames-specific-date-column-with-date-ranges-intervals-in-r/23377309#23377309)**\r\n - `data.table` internally optimises expressions of the form `DT[col == value]` or `DT[col %in% values]` for *speed* through *automatic indexing* which uses *binary search* while using the same base R syntax. [See here](https://gist.github.com/arunsrinivasan/dacb9d1cac301de8d9ff) for some more details and a tiny benchmark.\r\n - `dplyr` offers standard evaluation versions of functions (e.g. `regroup`, `summarize_each_`) that can simplify the programmatic use of `dplyr` (note programmatic use of `data.table` is definitely possible, just requires some careful thought, substitution/quoting, etc, at least to my knowledge)\r\n\r\n#### Benchmarks ####\r\n\r\n- I ran **[my own benchmarks](http://www.brodieg.com/?p=7)** and found both packages to be comparable in &quot;split apply combine&quot; style analysis, except when there are very large numbers of groups (&gt;100K) at which point `data.table` becomes substantially faster.\r\n- @Arun ran some **[benchmarks on joins](https://gist.github.com/arunsrinivasan/db6e1ce05227f120a2c9)**, showing that `data.table` scales better than `dplyr` as the number of groups increase (updated with recent enhancements in both packages and recent version of R).  Also, a benchmark when trying to get **[unique values](https://stackoverflow.com/questions/23668593/using-fastmatch-package-in-r/23680560?noredirect=1#comment36389040_23680560)** has `data.table` ~6x faster.\r\n- (Unverified) has `data.table` 75% faster on larger versions of a group/apply/sort while `dplyr` was 40% faster on the smaller ones (**[another SO question from comments][7]**, thanks danas).\r\n- Matt, the main author of `data.table`, has [**benchmarked grouping operations on `data.table`, `dplyr` and python `pandas` on up to 2 billion rows (~100GB in RAM)**](https://github.com/Rdatatable/data.table/wiki/Benchmarks-%3A-Grouping).\r\n- An **[older benchmark on 80K groups][2]** has `data.table` ~8x faster\r\n\r\n### Data ###\r\n\r\nThis is for the first example I showed in the question section.\r\n\r\n    dat &lt;- structure(list(id = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, \r\n    2L, 2L, 2L, 2L, 2L, 2L), name = c(&quot;Jane&quot;, &quot;Jane&quot;, &quot;Jane&quot;, &quot;Jane&quot;, \r\n    &quot;Jane&quot;, &quot;Jane&quot;, &quot;Jane&quot;, &quot;Jane&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;Bob&quot;, \r\n    &quot;Bob&quot;, &quot;Bob&quot;, &quot;Bob&quot;), year = c(1980L, 1981L, 1982L, 1983L, 1984L, \r\n    1985L, 1986L, 1987L, 1985L, 1986L, 1987L, 1988L, 1989L, 1990L, \r\n    1991L, 1992L), job = c(&quot;Manager&quot;, &quot;Manager&quot;, &quot;Manager&quot;, &quot;Manager&quot;, \r\n    &quot;Manager&quot;, &quot;Manager&quot;, &quot;Boss&quot;, &quot;Boss&quot;, &quot;Manager&quot;, &quot;Manager&quot;, &quot;Manager&quot;, \r\n    &quot;Boss&quot;, &quot;Boss&quot;, &quot;Boss&quot;, &quot;Boss&quot;, &quot;Boss&quot;), job2 = c(1L, 1L, 1L, \r\n    1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L)), .Names = c(&quot;id&quot;, \r\n    &quot;name&quot;, &quot;year&quot;, &quot;job&quot;, &quot;job2&quot;), class = &quot;data.frame&quot;, row.names = c(NA, \r\n    -16L))\r\n\r\n\r\n  [1]: http://rpubs.com/hadley/dplyr-intro\r\n  [2]: http://www.r-statistics.com/2013/09/a-speed-test-comparison-of-plyr-data-table-and-dplyr/\r\n  [3]: https://stackoverflow.com/questions/21421004/how-to-cumulatively-add-values-in-one-vector-in-r\r\n  [4]: https://stackoverflow.com/questions/21295936/can-dplyr-summarise-over-several-variables-without-listing-each-one\r\n  [5]: https://stackoverflow.com/questions/22644804/how-can-i-use-dplyr-to-apply-a-function-to-all-non-group-by-columns\r\n  [6]: https://stackoverflow.com/questions/21737815/grouped-operations-that-result-in-length-not-equal-to-1-or-length-of-group-in-dp\r\n  [7]: https://stackoverflow.com/questions/21477525/fast-frequency-and-percentage-table-with-dplyr/",
         "view_count": "70086",
         "answer_count": "3",
         "tags": "['r', 'data.table', 'dplyr']",
         "creation_date": "1391008905",
         "last_edit_date": "1516136197",
         "code_snippet": "['<code>data.table</code>', '<code>dplyr</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>dplyr</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>dat %.%\\n  group_by(name, job) %.%\\n  filter(job != \"Boss\" | year == min(year)) %.%\\n  mutate(cumu_job2 = cumsum(job2))\\n</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>setDT(dat)[,\\n  .SD[job != \"Boss\" | year == min(year)][, cumjob := cumsum(job2)], \\n  by=list(id, job)\\n]\\n</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>do</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>DT[col == value]</code>', '<code>DT[col %in% values]</code>', '<code>dplyr</code>', '<code>regroup</code>', '<code>summarize_each_</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>pandas</code>', '<code>data.table</code>', '<code>dat &lt;- structure(list(id = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, \\n2L, 2L, 2L, 2L, 2L, 2L), name = c(\"Jane\", \"Jane\", \"Jane\", \"Jane\", \\n\"Jane\", \"Jane\", \"Jane\", \"Jane\", \"Bob\", \"Bob\", \"Bob\", \"Bob\", \"Bob\", \\n\"Bob\", \"Bob\", \"Bob\"), year = c(1980L, 1981L, 1982L, 1983L, 1984L, \\n1985L, 1986L, 1987L, 1985L, 1986L, 1987L, 1988L, 1989L, 1990L, \\n1991L, 1992L), job = c(\"Manager\", \"Manager\", \"Manager\", \"Manager\", \\n\"Manager\", \"Manager\", \"Boss\", \"Boss\", \"Manager\", \"Manager\", \"Manager\", \\n\"Boss\", \"Boss\", \"Boss\", \"Boss\", \"Boss\"), job2 = c(1L, 1L, 1L, \\n1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L)), .Names = c(\"id\", \\n\"name\", \"year\", \"job\", \"job2\"), class = \"data.frame\", row.names = c(NA, \\n-16L))\\n</code>', '<code>dplyr</code>', '<code>as.data.table(dat)[, .SD[job != \"Boss\" | year == min(year)][, cumjob := cumsum(job2)], by = list(name, job)]</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>(d)plyr</code>', '<code>dplyr</code>', '<code>plyr</code>', '<code>.SD</code>', '<code>Speed</code>', '<code>Memory usage</code>', '<code>Syntax</code>', '<code>Features</code>', '<code>DT[i, j, by]</code>', '<code>i</code>', '<code>j</code>', '<code>by</code>', '<code>pandas</code>', '<code>DT[x &gt; val, sum(y), by = z]</code>', '<code>filter()</code>', '<code>slice()</code>', \"<code># sub-assign by reference, updates 'y' in-place\\nDT[x &gt;= 1L, y := NA]\\n</code>\", \"<code># copies the entire 'y' column\\nans &lt;- DF %&gt;% mutate(y = replace(y, which(x &gt;= 1L), NA))\\n</code>\", '<code>shallow()</code>', '<code>foo &lt;- function(DT) {\\n    DT = shallow(DT)          ## shallow copy DT\\n    DT[, newcol := 1L]        ## does not affect the original DT \\n    DT[x &gt; 2L, newcol := 2L]  ## no need to copy (internally), as this column exists only in shallow copied DT\\n    DT[x &gt; 2L, x := 3L]       ## have to copy (like base R / dplyr does always); otherwise original DT will \\n                              ## also get modified.\\n}\\n</code>', '<code>shallow()</code>', '<code>bar &lt;- function(DT) {\\n    DT[, newcol := 1L]        ## old behaviour, original DT gets updated by reference\\n    DT[x &gt; 2L, x := 3L]       ## old behaviour, update column x in original DT.\\n}\\n</code>', '<code>shallow()</code>', '<code>shallow()</code>', '<code>DT1 = data.table(x=c(1,1,1,1,2,2,2,2), y=c(\"a\", \"a\", \"b\", \"b\"), z=1:8, key=c(\"x\", \"y\"))\\n#    x y z\\n# 1: 1 a 1\\n# 2: 1 a 2\\n# 3: 1 b 3\\n# 4: 1 b 4\\n# 5: 2 a 5\\n# 6: 2 a 6\\n# 7: 2 b 7\\n# 8: 2 b 8\\nDT2 = data.table(x=1:2, y=c(\"a\", \"b\"), mul=4:3, key=c(\"x\", \"y\"))\\n#    x y mul\\n# 1: 1 a   4\\n# 2: 2 b   3\\n</code>', '<code>sum(z) * mul</code>', '<code>DT2</code>', '<code>x,y</code>', '<code>DT1</code>', '<code>sum(z)</code>', '<code># data.table way\\nDT1[, .(z = sum(z)), keyby = .(x,y)][DT2][, z := z*mul][]\\n\\n# dplyr equivalent\\nDF1 %&gt;% group_by(x, y) %&gt;% summarise(z = sum(z)) %&gt;% \\n    right_join(DF2) %&gt;% mutate(z = z * mul)\\n</code>', '<code>by = .EACHI</code>', '<code>DT1[DT2, list(z=sum(z) * mul), by = .EACHI]\\n</code>', '<code>j</code>', '<code>by = .EACHI</code>', '<code>dplyr</code>', '<code>DT1[DT2, col := i.mul]\\n</code>', '<code>DT1</code>', '<code>col</code>', '<code>mul</code>', '<code>DT2</code>', '<code>DT2</code>', '<code>DT1</code>', '<code>dplyr</code>', '<code>*_join</code>', '<code>DT1</code>', '<code>DT = data.table(x=1:10, y=11:20, z=rep(1:2, each=5))\\nDF = as.data.frame(DT)\\n</code>', '<code># case (a)\\nDT[, sum(y), by = z]                       ## data.table syntax\\nDF %&gt;% group_by(z) %&gt;% summarise(sum(y)) ## dplyr syntax\\nDT[, y := cumsum(y), by = z]\\nans &lt;- DF %&gt;% group_by(z) %&gt;% mutate(y = cumsum(y))\\n\\n# case (b)\\nDT[x &gt; 2, sum(y), by = z]\\nDF %&gt;% filter(x&gt;2) %&gt;% group_by(z) %&gt;% summarise(sum(y))\\nDT[x &gt; 2, y := cumsum(y), by = z]\\nans &lt;- DF %&gt;% group_by(z) %&gt;% mutate(y = replace(y, which(x &gt; 2), cumsum(y)))\\n\\n# case (c)\\nDT[, if(any(x &gt; 5L)) y[1L]-y[2L] else y[2L], by = z]\\nDF %&gt;% group_by(z) %&gt;% summarise(if (any(x &gt; 5L)) y[1L] - y[2L] else y[2L])\\nDT[, if(any(x &gt; 5L)) y[1L] - y[2L], by = z]\\nDF %&gt;% group_by(z) %&gt;% filter(any(x &gt; 5L)) %&gt;% summarise(y[1L] - y[2L])\\n</code>', '<code>filter()</code>', '<code>mutate()</code>', '<code>x &gt; 2</code>', '<code>sum(y)</code>', '<code>y</code>', '<code>DT[i, j, by]</code>', '<code>if-else</code>', '<code>if</code>', '<code>summarise()</code>', '<code>filter()</code>', '<code>summarise()</code>', '<code>filter()</code>', '<code>filter()</code>', '<code># case (a)\\nDT[, lapply(.SD, sum), by = z]                     ## data.table syntax\\nDF %&gt;% group_by(z) %&gt;% summarise_each(funs(sum)) ## dplyr syntax\\nDT[, (cols) := lapply(.SD, sum), by = z]\\nans &lt;- DF %&gt;% group_by(z) %&gt;% mutate_each(funs(sum))\\n\\n# case (b)\\nDT[, c(lapply(.SD, sum), lapply(.SD, mean)), by = z]\\nDF %&gt;% group_by(z) %&gt;% summarise_each(funs(sum, mean))\\n\\n# case (c)\\nDT[, c(.N, lapply(.SD, sum)), by = z]     \\nDF %&gt;% group_by(z) %&gt;% summarise_each(funs(n(), mean))\\n</code>', '<code>lapply()</code>', '<code>dplyr</code>', '<code>*_each()</code>', '<code>funs()</code>', '<code>:=</code>', '<code>n()</code>', '<code>j</code>', '<code>c()</code>', '<code>.N</code>', '<code>list</code>', '<code>list</code>', '<code>j</code>', '<code>c()</code>', '<code>as.list()</code>', '<code>lapply()</code>', '<code>list()</code>', '<code>.N</code>', '<code>.SD</code>', '<code>n()</code>', '<code>.</code>', '<code>DT[i, j, by]</code>', '<code>merge.data.table()</code>', '<code>setkey(DT1, x, y)\\n\\n# 1. normal join\\nDT1[DT2]            ## data.table syntax\\nleft_join(DT2, DT1) ## dplyr syntax\\n\\n# 2. select columns while join    \\nDT1[DT2, .(z, i.mul)]\\nleft_join(select(DT2, x, y, mul), select(DT1, x, y, z))\\n\\n# 3. aggregate while join\\nDT1[DT2, .(sum(z) * i.mul), by = .EACHI]\\nDF1 %&gt;% group_by(x, y) %&gt;% summarise(z = sum(z)) %&gt;% \\n    inner_join(DF2) %&gt;% mutate(z = z*mul) %&gt;% select(-mul)\\n\\n# 4. update while join\\nDT1[DT2, z := cumsum(z) * i.mul, by = .EACHI]\\n??\\n\\n# 5. rolling join\\nDT1[DT2, roll = -Inf]\\n??\\n\\n# 6. other arguments to control output\\nDT1[DT2, mult = \"first\"]\\n??\\n</code>', '<code>DT[i, j, by]</code>', '<code>merge()</code>', '<code>select()</code>', '<code>by = .EACHI</code>', '<code>mult =</code>', '<code>allow.cartesian = TRUE</code>', '<code>DT[i, j, by]</code>', '<code>do()</code>', '<code>do()</code>', '<code>DT[, list(x[1], y[1]), by = z]                 ## data.table syntax\\nDF %&gt;% group_by(z) %&gt;% summarise(x[1], y[1]) ## dplyr syntax\\nDT[, list(x[1:2], y[1]), by = z]\\nDF %&gt;% group_by(z) %&gt;% do(data.frame(.$x[1:2], .$y[1]))\\n\\nDT[, quantile(x, 0.25), by = z]\\nDF %&gt;% group_by(z) %&gt;% summarise(quantile(x, 0.25))\\nDT[, quantile(x, c(0.25, 0.75)), by = z]\\nDF %&gt;% group_by(z) %&gt;% do(data.frame(quantile(.$x, c(0.25, 0.75))))\\n\\nDT[, as.list(summary(x)), by = z]\\nDF %&gt;% group_by(z) %&gt;% do(data.frame(as.list(summary(.$x))))\\n</code>', '<code>.SD</code>', '<code>.</code>', '<code>j</code>', '<code>do()</code>', '<code>DT[i, j, by]</code>', '<code>j</code>', '<code>dplyr</code>', '<code>summarise()</code>', '<code>&lt;=, &lt;, &gt;, &gt;=</code>', '<code>setorder()</code>', '<code>data.table</code>', '<code>fsetdiff</code>', '<code>fintersect</code>', '<code>funion</code>', '<code>fsetequal</code>', '<code>all</code>', '<code>[.data.frame</code>', '<code>filter</code>', '<code>lag</code>', '<code>[</code>', '<code>OpenMP</code>', '<code>filter</code>', '<code>:=</code>', '<code>dplyr</code>', '<code>&lt;-</code>', '<code>DF &lt;- DF %&gt;% mutate...</code>', '<code>DF %&gt;% mutate...</code>', '<code>dplyr</code>', '<code>plyr</code>', '<code>data.table</code>', '<code>SQL</code>', '<code>data.table</code>', '<code>%&gt;%</code>', '<code>diamonds %&gt;%\\n  filter(cut != \"Fair\") %&gt;%\\n  group_by(cut) %&gt;%\\n  summarize(\\n    AvgPrice = mean(price),\\n    MedianPrice = as.numeric(median(price)),\\n    Count = n()\\n  ) %&gt;%\\n  arrange(desc(Count))\\n</code>', '<code>[</code>', '<code>diamondsDT &lt;- data.table(diamonds)\\ndiamondsDT[\\n  cut != \"Fair\", \\n  .(AvgPrice = mean(price),\\n    MedianPrice = as.numeric(median(price)),\\n    Count = .N\\n  ), \\n  by = cut\\n][ \\n  order(-Count) \\n]\\n</code>', '<code>[</code>', '<code>[</code>', '<code>x[y]</code>', '<code>group_by()</code>', '<code>group_by()</code>', '<code>by</code>', '<code>[.data.table</code>', '<code>%&gt;%</code>', '<code>diamonds %&gt;% \\n  data.table() %&gt;% \\n  .[cut != \"Fair\", \\n    .(AvgPrice = mean(price),\\n      MedianPrice = as.numeric(median(price)),\\n      Count = .N\\n    ), \\n    by = cut\\n  ] %&gt;% \\n  .[order(-Count)]\\n</code>', '<code>%&gt;%</code>', '<code>readr</code>', '<code>fread()</code>', '<code>data.table</code>', '<code>%&gt;%</code>', '<code>data.table</code>', '<code>[</code>', '<code>%&gt;%</code>', '<code>dplyr</code>', '<code>%&gt;%</code>', '<code>DT[\\\\n\\\\texpression\\\\n][\\\\texpression\\\\n]</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>fread()</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>data.table</code>', '<code>dplyr</code>', '<code>dplyr</code>']",
         "title": "data.table vs dplyr: can one do something well the other can&#39;t or does poorly?",
         "_childDocuments_": [
            {
               "up_vote_count": 48,
               "answer_id": 26962904,
               "last_activity_date": 1416232586,
               "path": "3.stack.answer",
               "body_markdown": "## In direct response to the _Question Title_...\r\n\r\n### `dplyr` *definitely* does things that `data.table` can not.\r\n\r\nYour point #3\r\n\r\n&gt; dplyr abstracts (or will) potential DB interactions\r\n\r\nis a direct answer to your own question but isn&#39;t elevated to a high enough level. `dplyr` is truly an extendable front-end to multiple data storage mechanisms where as `data.table` is an extension to a single one.\r\n\r\nLook at `dplyr` as a back-end agnostic interface, with all of the targets using the same grammer, where you can extend the targets and handlers at will. `data.table` is, from the `dplyr` perspective, one of those targets.\r\n\r\nYou will never (I hope) see a day that `data.table` attempts to translate your queries to create SQL statements that operate with on-disk or networked data stores.\r\n\r\n###*`dplyr` can possibly do things `data.table` will not or might not do as well.*\r\n\r\nBased on the design of working in-memory, `data.table` could have a much more difficult time extending itself into parallel processing of queries than `dplyr`.\r\n\r\n----\r\n\r\n## In response to the in-body questions...\r\n\r\n###*Usage*\r\n\r\n&gt; &lt;sub&gt;&lt;sup&gt;Are there analytical tasks that are a lot easier to code with one or the other package *for people familiar with the packages* (i.e. some combination of keystrokes required vs. required level of esotericism, where less of each is a good thing).&lt;/sup&gt;&lt;/sub&gt;\r\n\r\nThis may seem like a punt but the real answer is no. People _familiar_ with tools seem to use the either the one most familiar to them or the one that is actually the right one for the job at hand. With that being said, sometimes you want to present a particular readability, sometimes a level of performance, and when you have need for a high enough level of both you may just need another tool to go along with what you already have to make clearer abstractions.\r\n\r\n###*Performance*\r\n\r\n&gt; &lt;sup&gt;&lt;sub&gt;Are there analytical tasks that are performed substantially (i.e. more than 2x) more efficiently in one package vs. another.&lt;/sub&gt;&lt;/sup&gt;\r\n\r\nAgain, no.  `data.table` excels at being efficient in everything _it_ does where `dplyr` gets the burden of being limited in some respects to the underlying data store and registered handlers.\r\n\r\nThis means when you run into a performance issue with `data.table` you can be pretty sure it is in your query function and if it _is_ actually a bottleneck with `data.table` then you&#39;ve won yourself the joy of filing a report. This is also true when `dplyr` is using `data.table` as the back-end; you _may_ see _some_ overhead from `dplyr` but odds are it is your query.\r\n\r\nWhen `dplyr` has performance issues with back-ends you can get around them by registering a function for hybrid evaluation or (in the case of databases) manipulating the generated query prior to execution.\r\n\r\nAlso see the accepted answer to https://stackoverflow.com/questions/16153947/when-is-plyr-better-than-data-table/16154157#16154157\r\n\r\n",
               "tags": [],
               "creation_date": 1416177576,
               "last_edit_date": 1495540976,
               "is_accepted": false,
               "id": "26962904",
               "down_vote_count": 9,
               "score": 39
            },
            {
               "up_vote_count": 343,
               "answer_id": 27718317,
               "last_activity_date": 1507796130,
               "path": "3.stack.answer",
               "body_markdown": "We need to cover at least these aspects to provide a comprehensive answer/comparison (in no particular order of importance): `Speed`, `Memory usage`, `Syntax` and `Features`.\r\n\r\nMy intent is to cover each one of these as clearly as possible from data.table perspective.\r\n\r\n&gt; Note: unless explicitly mentioned otherwise, by referring to dplyr, we refer to dplyr&#39;s data.frame interface whose internals are in C++ using Rcpp.\r\n\r\n---\r\n\r\nThe data.table syntax is consistent in its form - `DT[i, j, by]`. To keep `i`, `j` and `by` together is by design. By keeping related operations together, it allows to *easily optimise* operations for *speed* and more importantly *memory usage*, and also provide some *powerful features*, all while maintaining the consistency in syntax.\r\n\r\n### 1. Speed\r\n\r\nQuite a few benchmarks (though mostly on grouping operations) have been added to the question already showing data.table gets *faster* than dplyr as the number of groups and/or rows to group by increase, including [benchmarks by Matt](https://github.com/Rdatatable/data.table/wiki/Benchmarks-%3A-Grouping) on grouping from *10 million to 2 billion rows* (100GB in RAM) on *100 - 10 million groups* and varying grouping columns, which also compares `pandas`.\r\n\r\nOn benchmarks, it would be great to cover these remaining aspects as well:\r\n\r\n* Grouping operations involving a *subset of rows* - i.e., `DT[x &gt; val, sum(y), by = z]` type operations.\r\n\r\n* Benchmark other operations such as *update* and *joins*.\r\n\r\n* Also benchmark *memory footprint* for each operation in addition to runtime.\r\n\r\n### 2. Memory usage\r\n\r\n1. Operations involving `filter()` or `slice()` in dplyr can be memory inefficient (on both data.frames and data.tables). [See this post](https://stackoverflow.com/a/27520688/559784). \r\n\r\n    &gt; Note that [Hadley&#39;s comment](https://stackoverflow.com/questions/27511604/dplyr-on-data-table-am-i-really-using-data-table/27520688#comment43492306_27520688) talks about *speed* (that dplyr is plentiful fast for him), whereas the major concern here is *memory*.\r\n\r\n2. data.table interface at the moment allows one to modify/update columns *by reference* (note that we don&#39;t need to re-assign the result back to a variable).\r\n\r\n        # sub-assign by reference, updates &#39;y&#39; in-place\r\n        DT[x &gt;= 1L, y := NA]\r\n    \r\n    But dplyr *will never* update by reference. The dplyr equivalent would be (note that the result needs to be re-assigned):\r\n\r\n        # copies the entire &#39;y&#39; column\r\n        ans &lt;- DF %&gt;% mutate(y = replace(y, which(x &gt;= 1L), NA))\r\n\r\n    A concern for this is [*referential transparency*](http://en.wikipedia.org/wiki/Referential_transparency_(computer_science)). Updating a data.table object by reference, especially within a function may not be always desirable. But this is an incredibly useful feature: see [this](https://stackoverflow.com/a/24070527/559784) and [this](https://stackoverflow.com/q/7235657/559784) posts for interesting cases. And we want to keep it.\r\n\r\n    Therefore we are working towards exporting `shallow()` function in data.table that will provide the user with *both possibilities*. For example, if it is desirable to not modify the input data.table within a function, one can then do:\r\n\r\n        foo &lt;- function(DT) {\r\n            DT = shallow(DT)          ## shallow copy DT\r\n            DT[, newcol := 1L]        ## does not affect the original DT \r\n            DT[x &gt; 2L, newcol := 2L]  ## no need to copy (internally), as this column exists only in shallow copied DT\r\n            DT[x &gt; 2L, x := 3L]       ## have to copy (like base R / dplyr does always); otherwise original DT will \r\n                                      ## also get modified.\r\n        }\r\n\r\n    By not using `shallow()`, the old functionality is retained:\r\n\r\n        bar &lt;- function(DT) {\r\n            DT[, newcol := 1L]        ## old behaviour, original DT gets updated by reference\r\n            DT[x &gt; 2L, x := 3L]       ## old behaviour, update column x in original DT.\r\n        }\r\n\r\n    By creating a *shallow copy* using `shallow()`, we understand that you don&#39;t want to modify the original object. We take care of everything internally to ensure that while also ensuring to copy columns you modify *only when it is absolutely necessary*. When implemented, this should settle the *referential transparency* issue altogether while providing the user with both possibilties.\r\n\r\n    &gt; Also, once `shallow()` is exported dplyr&#39;s data.table interface should avoid almost all copies. So those who prefer dplyr&#39;s syntax can use it with data.tables.  \r\n\r\n    &gt; But it will still lack many features that data.table provides, including (sub)-assignment by reference.\r\n\r\n3. Aggregate while joining:\r\n\r\n    Suppose you have two data.tables as follows:\r\n\r\n        DT1 = data.table(x=c(1,1,1,1,2,2,2,2), y=c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;), z=1:8, key=c(&quot;x&quot;, &quot;y&quot;))\r\n        #    x y z\r\n        # 1: 1 a 1\r\n        # 2: 1 a 2\r\n        # 3: 1 b 3\r\n        # 4: 1 b 4\r\n        # 5: 2 a 5\r\n        # 6: 2 a 6\r\n        # 7: 2 b 7\r\n        # 8: 2 b 8\r\n        DT2 = data.table(x=1:2, y=c(&quot;a&quot;, &quot;b&quot;), mul=4:3, key=c(&quot;x&quot;, &quot;y&quot;))\r\n        #    x y mul\r\n        # 1: 1 a   4\r\n        # 2: 2 b   3\r\n\r\n    And you would like to get `sum(z) * mul` for each row in `DT2` while joining by columns `x,y`. We can either:\r\n\r\n    * 1) aggregate `DT1` to get `sum(z)`, 2) perform a join and 3) multiply (or)\r\n\r\n            # data.table way\r\n            DT1[, .(z = sum(z)), keyby = .(x,y)][DT2][, z := z*mul][]\r\n\r\n            # dplyr equivalent\r\n            DF1 %&gt;% group_by(x, y) %&gt;% summarise(z = sum(z)) %&gt;% \r\n                right_join(DF2) %&gt;% mutate(z = z * mul)\r\n\r\n    * 2) do it all in one go (using `by = .EACHI` feature):\r\n\r\n            DT1[DT2, list(z=sum(z) * mul), by = .EACHI]\r\n\r\n    What is the advantage?\r\n\r\n    * We don&#39;t have to allocate memory for the intermediate result.\r\n\r\n    * We don&#39;t have to group/hash twice (one for aggregation and other for joining).\r\n\r\n    * And more importantly, the operation what we wanted to perform is clear by looking at `j` in (2).\r\n\r\n    Check [this post](https://stackoverflow.com/a/27004566/559784) for a detailed explanation of `by = .EACHI`. No intermediate results are materialised, and the join+aggregate is performed all in one go. \r\n\r\n    Have a look at [this](https://stackoverflow.com/a/27061800/559784), [this](https://stackoverflow.com/a/26453795/559784) and [this](https://stackoverflow.com/q/27466284/559784) posts for real usage scenarios.\r\n\r\n    In `dplyr` you would have to [join and aggregate or aggregate first and then join](https://gist.github.com/arunsrinivasan/b14441829e7f484fdf58), neither of which are as efficient, in terms of memory (which in turn translates to speed).\r\n\r\n4. Update and joins:\r\n\r\n    Consider the data.table code shown below:\r\n\r\n        DT1[DT2, col := i.mul]\r\n    \r\n    adds/updates `DT1`&#39;s column `col` with `mul` from `DT2` on those rows where `DT2`&#39;s key column matches `DT1`. I don&#39;t think there is an exact equivalent of this operation in `dplyr`, i.e., without avoiding a `*_join` operation, which would have to copy the entire `DT1` just to add a new column to it, which is unnecessary.\r\n\r\n    Check [this post](https://stackoverflow.com/a/26720434/559784) for a real usage scenario. \r\n\r\n&gt; To summarise, it is important to realise that every bit of optimisation matters. As [**Grace Hopper**](http://en.wikipedia.org/wiki/Grace_Hopper) would say, [**Mind your nanoseconds**](http://highscalability.com/blog/2012/3/1/grace-hopper-to-programmers-mind-your-nanoseconds.html)!\r\n\r\n### 3. Syntax\r\n\r\nLet&#39;s now look at *syntax*. Hadley commented [here](https://news.ycombinator.com/item?id=8795778):\r\n\r\n&gt; Data tables are extremely fast but I think their concision makes it *harder to learn* and *code that uses it is harder to read after you have written it* ...\r\n\r\nI find this remark pointless because it is very subjective. What we can perhaps try is to contrast *consistency in syntax*. We will compare data.table and dplyr syntax side-by-side. \r\n\r\nWe will work with the dummy data shown below:\r\n\r\n    DT = data.table(x=1:10, y=11:20, z=rep(1:2, each=5))\r\n    DF = as.data.frame(DT)\r\n\r\n1. Basic aggregation/update operations. \r\n\r\n        # case (a)\r\n        DT[, sum(y), by = z]                       ## data.table syntax\r\n        DF %&gt;% group_by(z) %&gt;% summarise(sum(y)) ## dplyr syntax\r\n        DT[, y := cumsum(y), by = z]\r\n        ans &lt;- DF %&gt;% group_by(z) %&gt;% mutate(y = cumsum(y))\r\n\r\n        # case (b)\r\n        DT[x &gt; 2, sum(y), by = z]\r\n        DF %&gt;% filter(x&gt;2) %&gt;% group_by(z) %&gt;% summarise(sum(y))\r\n        DT[x &gt; 2, y := cumsum(y), by = z]\r\n        ans &lt;- DF %&gt;% group_by(z) %&gt;% mutate(y = replace(y, which(x &gt; 2), cumsum(y)))\r\n\r\n        # case (c)\r\n        DT[, if(any(x &gt; 5L)) y[1L]-y[2L] else y[2L], by = z]\r\n        DF %&gt;% group_by(z) %&gt;% summarise(if (any(x &gt; 5L)) y[1L] - y[2L] else y[2L])\r\n        DT[, if(any(x &gt; 5L)) y[1L] - y[2L], by = z]\r\n        DF %&gt;% group_by(z) %&gt;% filter(any(x &gt; 5L)) %&gt;% summarise(y[1L] - y[2L])\r\n\r\n    * data.table syntax is compact and dplyr&#39;s quite verbose. Things are more or less equivalent in case (a). \r\n\r\n    * In case (b), we had to use `filter()` in dplyr while *summarising*. But while *updating*, we had to move the logic inside `mutate()`. In data.table however, we express both operations with the same logic - operate on rows where `x &gt; 2`, but in first case, get `sum(y)`, whereas in the second case update those rows for `y` with its cumulative sum.\r\n\r\n        This is what we mean when we say the `DT[i, j, by]` form *is consistent*.\r\n\r\n    * Similarly in case (c), when we have `if-else` condition, we are able to express the logic *&quot;as-is&quot;* in both data.table and dplyr. However, if we would like to return just those rows where the `if` condition satisfies and skip otherwise, we cannot use `summarise()` directly (AFAICT). We have to `filter()` first and then summarise because `summarise()` always expects a *single value*. \r\n\r\n        While it returns the same result, using `filter()` here makes the actual operation less obvious.\r\n\r\n        It might very well be possible to use `filter()` in the first case as well (does not seem obvious to me), but my point is that we should not have to.\r\n\r\n2. Aggregation / update on multiple columns\r\n\r\n        # case (a)\r\n        DT[, lapply(.SD, sum), by = z]                     ## data.table syntax\r\n        DF %&gt;% group_by(z) %&gt;% summarise_each(funs(sum)) ## dplyr syntax\r\n        DT[, (cols) := lapply(.SD, sum), by = z]\r\n        ans &lt;- DF %&gt;% group_by(z) %&gt;% mutate_each(funs(sum))\r\n\r\n        # case (b)\r\n        DT[, c(lapply(.SD, sum), lapply(.SD, mean)), by = z]\r\n        DF %&gt;% group_by(z) %&gt;% summarise_each(funs(sum, mean))\r\n\r\n        # case (c)\r\n        DT[, c(.N, lapply(.SD, sum)), by = z]     \r\n        DF %&gt;% group_by(z) %&gt;% summarise_each(funs(n(), mean))\r\n\r\n    * In case (a), the codes are more or less equivalent. data.table uses familiar base function `lapply()`, whereas `dplyr` introduces `*_each()` along with a bunch of functions to `funs()`. \r\n\r\n    * data.table&#39;s `:=` requires column names to be provided, whereas dplyr generates it automatically.\r\n\r\n    * In case (b), dplyr&#39;s syntax is relatively straightforward. Improving aggregations/updates on multiple functions is on data.table&#39;s list.\r\n\r\n    * In case (c) though, dplyr would return `n()` as many times as many columns, instead of just once. In data.table, all we need to do is to return a list in `j`. Each element of the list will become a column in the result. So, we can use, once again, the familiar base function `c()` to concatenate `.N` to a `list` which returns a `list`.\r\n\r\n    &gt; Note: Once again, in data.table, all we need to do is return a list in `j`. Each element of the list will become a column in result. You can use `c()`, `as.list()`, `lapply()`, `list()` etc... base functions to accomplish this, without having to learn any new functions.\r\n\r\n    &gt; You will need to learn just the special variables - `.N` and `.SD` at least. The equivalent in dplyr are `n()` and `.`\r\n\r\n3. Joins\r\n\r\n    dplyr provides separate functions for each type of join where as data.table allows joins using the same syntax `DT[i, j, by]` (and with reason). It also provides an equivalent `merge.data.table()` function as an alternative.\r\n\r\n        setkey(DT1, x, y)\r\n\r\n        # 1. normal join\r\n        DT1[DT2]            ## data.table syntax\r\n        left_join(DT2, DT1) ## dplyr syntax\r\n\r\n        # 2. select columns while join    \r\n        DT1[DT2, .(z, i.mul)]\r\n        left_join(select(DT2, x, y, mul), select(DT1, x, y, z))\r\n\r\n        # 3. aggregate while join\r\n        DT1[DT2, .(sum(z) * i.mul), by = .EACHI]\r\n        DF1 %&gt;% group_by(x, y) %&gt;% summarise(z = sum(z)) %&gt;% \r\n            inner_join(DF2) %&gt;% mutate(z = z*mul) %&gt;% select(-mul)\r\n\r\n        # 4. update while join\r\n        DT1[DT2, z := cumsum(z) * i.mul, by = .EACHI]\r\n        ??\r\n\r\n        # 5. rolling join\r\n        DT1[DT2, roll = -Inf]\r\n        ??\r\n\r\n        # 6. other arguments to control output\r\n        DT1[DT2, mult = &quot;first&quot;]\r\n        ??\r\n\r\n* Some might find a separate function for each joins much nicer (left, right, inner, anti, semi etc), whereas as others might like data.table&#39;s `DT[i, j, by]`, or `merge()` which is similar to base R. \r\n\r\n* However dplyr joins do just that. Nothing more. Nothing less. \r\n\r\n* data.tables can select columns while joining (2), and in dplyr you will need to `select()` first on both data.frames before to join as shown above. Otherwise you would materialiase the join with unnecessary columns only to remove them later and that is inefficient.\r\n\r\n* data.tables can *aggregate while joining* (3) and also *update while joining* (4), using `by = .EACHI` feature. Why materialse the entire join result to add/update just a few columns?\r\n\r\n* data.table is capable of *rolling joins* (5) - roll [forward, LOCF](https://stackoverflow.com/a/18915708/559784), [roll backward, NOCB](https://stackoverflow.com/a/23342851/559784), [nearest](https://stackoverflow.com/a/27763960/559784).\r\n\r\n* data.table also has `mult =` argument which selects *first*, *last* or *all* matches (6).\r\n\r\n* data.table has [`allow.cartesian = TRUE`](https://stackoverflow.com/a/23087759/559784) argument to protect from accidental invalid joins.\r\n\r\n&gt; Once again, the syntax is consistent with `DT[i, j, by]` with additional arguments allowing for controlling the output further.\r\n\r\n4. `do()`...\r\n\r\n    dplyr&#39;s summarise is specially designed for functions that return a single value. If your function returns multiple/unequal values, you will have to resort to `do()`. You have to know beforehand about all your functions return value. \r\n\r\n        DT[, list(x[1], y[1]), by = z]                 ## data.table syntax\r\n        DF %&gt;% group_by(z) %&gt;% summarise(x[1], y[1]) ## dplyr syntax\r\n        DT[, list(x[1:2], y[1]), by = z]\r\n        DF %&gt;% group_by(z) %&gt;% do(data.frame(.$x[1:2], .$y[1]))\r\n\r\n        DT[, quantile(x, 0.25), by = z]\r\n        DF %&gt;% group_by(z) %&gt;% summarise(quantile(x, 0.25))\r\n        DT[, quantile(x, c(0.25, 0.75)), by = z]\r\n        DF %&gt;% group_by(z) %&gt;% do(data.frame(quantile(.$x, c(0.25, 0.75))))\r\n\r\n        DT[, as.list(summary(x)), by = z]\r\n        DF %&gt;% group_by(z) %&gt;% do(data.frame(as.list(summary(.$x))))\r\n\r\n* `.SD`&#39;s equivalent is `.`\r\n\r\n* In data.table, you can throw pretty much anything in `j` - the only thing to remember is for it to return a list so that each element of the list gets converted to a column.\r\n\r\n* In dplyr, cannot do that. Have to resort to `do()` depending on how sure you are as to whether your function would always return a single value. And it is quite slow. \r\n\r\n&gt; Once again, data.table&#39;s syntax is consistent with `DT[i, j, by]`. We can just keep throwing expressions in `j` without having to worry about these things.\r\n\r\nHave a look at [this SO question](https://stackoverflow.com/q/27695278/559784) and [this one](https://stackoverflow.com/q/16878905/559784). I wonder if it would be possible to express the answer as straightforward using dplyr&#39;s syntax...\r\n\r\n&gt; To summarise, I have particularly highlighted *several* instances where dplyr&#39;s syntax is either inefficient, limited or fails to make operations straightforward. This is particularly because data.table gets quite a bit of backlash about &quot;harder to read/learn&quot; syntax (like the one pasted/linked above). Most posts that cover dplyr talk about most straightforward operations. And that is great. But it is important to realise its syntax and feature limitations as well, and I am yet to see a post on it.\r\n\r\n&gt; data.table has its quirks as well (some of which I have pointed out that we are attempting to fix). We are also attempting to improve data.table&#39;s joins as I have highlighted [here](https://stackoverflow.com/a/27615377/559784). \r\n\r\n&gt; But one should also consider the number of features that dplyr lacks in comparison to data.table.\r\n\r\n### 4. Features\r\n\r\nI have pointed out most of the features [here](https://stackoverflow.com/a/27520688/559784) and also in this post. In addition:\r\n\r\n* **fread** - fast file reader has been available for a long time now.\r\n\r\n* **fwrite** - NEW in the current devel, v1.9.7, a *parallelised* fast file writer is now available. See [this post](http://blog.h2o.ai/2016/04/fast-csv-writing-for-r/) for a detailed explanation on the implementation and [#1664](https://github.com/Rdatatable/data.table/issues/1664) for keeping track of further developments.\r\n\r\n* [Automatic indexing](https://gist.github.com/arunsrinivasan/dacb9d1cac301de8d9ff) - another handy feature to optimise base R syntax as is, internally.\r\n\r\n* **Ad-hoc grouping**: `dplyr` automatically sorts the results by grouping variables during `summarise()`, which may not be always desirable.\r\n\r\n* Numerous advantages in data.table joins (for speed / memory efficiency and syntax) mentioned above.\r\n\r\n* **Non-equi joins**: is a NEW feature available from v1.9.7+. It allows joins using other operators `&lt;=, &lt;, &gt;, &gt;=` along with all other advantages of data.table joins.\r\n\r\n* [Overlapping range joins](https://github.com/Rdatatable/data.table/wiki/talks/EARL2014_OverlapRangeJoin_Arun.pdf) was implemented in data.table recently. Check [this post](https://stackoverflow.com/a/25655497/559784) for an overview with benchmarks. \r\n\r\n* `setorder()` function in data.table that allows really fast reordering of data.tables by reference.\r\n\r\n* dplyr provides [interface to databases](http://cran.r-project.org/web/packages/dplyr/vignettes/databases.html) using the same syntax, which data.table does not at the moment.\r\n\r\n* `data.table` provides faster equivalents of *set operations* from v1.9.7+ (written by Jan Gorecki) - `fsetdiff`, `fintersect`, `funion` and `fsetequal` with additional `all` argument (as in SQL).\r\n\r\n* data.table loads cleanly with no masking warnings and has a mechanism described [here](https://stackoverflow.com/a/10529888/403310) for `[.data.frame` compatibility when passed to any R package. dplyr changes base functions `filter`, `lag` and `[` which can cause problems; e.g. [here](https://stackoverflow.com/a/35057612/403310) and [here](https://stackoverflow.com/a/35329418/403310).\r\n\r\n---\r\n\r\nFinally:\r\n\r\n* On databases - there is no reason why data.table cannot provide similar interface, but this is not a priority now. It might get bumped up if users would very much like that feature.. not sure.\r\n\r\n* On parallelism - Everything is difficult, until someone goes ahead and does it. Of course it will take effort (being thread safe). \r\n\r\n    * Progress is being made currently (in v1.9.7 devel) towards parallelising known time consuming parts for incremental performance gains using `OpenMP`.\r\n",
               "tags": [],
               "creation_date": 1420014420,
               "last_edit_date": 1507796130,
               "is_accepted": true,
               "id": "27718317",
               "down_vote_count": 6,
               "score": 337
            },
            {
               "up_vote_count": 284,
               "answer_id": 27840349,
               "last_activity_date": 1514932520,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s my attempt at a comprehensive answer from the dplyr perspective,\r\nfollowing the broad outline of Arun&#39;s answer (but somewhat rearranged\r\nbased on differing priorities).\r\n\r\nSyntax\r\n------\r\n\r\nThere is some subjectivity to syntax, but I stand by my statement that\r\nthe concision of data.table makes it harder to learn and harder to read.\r\nThis is partly because dplyr is solving a much easier problem!\r\n\r\nOne really important thing that dplyr does for you is that it\r\n*constrains* your options. I claim that most single table problems can\r\nbe solved with just five key verbs filter, select, mutate, arrange and\r\nsummarise, along with a &quot;by group&quot; adverb. That constraint is a big help\r\nwhen you&#39;re learning data manipulation, because it helps order your\r\nthinking about the problem. In dplyr, each of these verbs is mapped to a\r\nsingle function. Each function does one job, and is easy to understand\r\nin isolation.\r\n\r\nYou create complexity by piping these simple operations together with\r\n`%&gt;%`. Here&#39;s an example from one of the posts Arun [linked\r\nto](https://stackoverflow.com/questions/27511604/):\r\n\r\n    diamonds %&gt;%\r\n      filter(cut != &quot;Fair&quot;) %&gt;%\r\n      group_by(cut) %&gt;%\r\n      summarize(\r\n        AvgPrice = mean(price),\r\n        MedianPrice = as.numeric(median(price)),\r\n        Count = n()\r\n      ) %&gt;%\r\n      arrange(desc(Count))\r\n\r\nEven if you&#39;ve never seen dplyr before (or even R!), you can still get\r\nthe gist of what&#39;s happening because the functions are all English\r\nverbs. The disadvantage of English verbs is that they require more typing than\r\n`[`, but I think that can be largely mitigated by better autocomplete.\r\n\r\nHere&#39;s the equivalent data.table code:\r\n\r\n    diamondsDT &lt;- data.table(diamonds)\r\n    diamondsDT[\r\n      cut != &quot;Fair&quot;, \r\n      .(AvgPrice = mean(price),\r\n        MedianPrice = as.numeric(median(price)),\r\n        Count = .N\r\n      ), \r\n      by = cut\r\n    ][ \r\n      order(-Count) \r\n    ]\r\n\r\nIt&#39;s harder to follow this code unless you&#39;re already familiar with\r\ndata.table. (I also couldn&#39;t figure out how to indent the repeated `[`\r\nin a way that looks good to my eye). Personally, when I look at code I\r\nwrote 6 months ago, it&#39;s like looking at a code written by a stranger,\r\nso I&#39;ve come to prefer straightforward, if verbose, code.\r\n\r\nTwo other minor factors that I think slightly decrease readability:\r\n\r\n-   Since almost every data table operation uses `[` you need additional\r\n    context to figure out what&#39;s happening. For example, is `x[y]`\r\n    joining two data tables or extracting columns from a data frame?\r\n    This is only a small issue, because in well-written code the\r\n    variable names should suggest what&#39;s happening.\r\n\r\n-   I like that `group_by()` is a separate operation in dplyr. It\r\n    fundamentally changes the computation so I think should be obvious\r\n    when skimming the code, and it&#39;s easier to spot `group_by()` than\r\n    the `by` argument to `[.data.table`.\r\n\r\nI also like that the [the pipe](https://github.com/smbache/magrittr)\r\nisn&#39;t just limited to just one package. You can start by tidying your\r\ndata with\r\n[tidyr](http://blog.rstudio.org/2014/07/22/introducing-tidyr/), and\r\nfinish up with a plot in [ggvis](http://ggvis.rstudio.com). And you&#39;re\r\nnot limited to the packages that I write - anyone can write a function\r\nthat forms a seamless part of a data manipulation pipe. In fact, I\r\nrather prefer the previous data.table code rewritten with `%&gt;%`:\r\n\r\n    diamonds %&gt;% \r\n      data.table() %&gt;% \r\n      .[cut != &quot;Fair&quot;, \r\n        .(AvgPrice = mean(price),\r\n          MedianPrice = as.numeric(median(price)),\r\n          Count = .N\r\n        ), \r\n        by = cut\r\n      ] %&gt;% \r\n      .[order(-Count)]\r\n\r\nAnd the idea of piping with `%&gt;%` is not limited to just data frames and\r\nis easily generalised to other contexts: [interactive web\r\ngraphics](http://rstudio.github.io/dygraphs/), [web\r\nscraping](https://github.com/hadley/rvest),\r\n[gists](https://github.com/ropensci/gistr), [run-time\r\ncontracts](https://github.com/smbache/ensurer), ...)\r\n\r\nMemory and performance\r\n----------------------\r\n\r\nI&#39;ve lumped these together, because, to me, they&#39;re not that important.\r\nMost R users work with well under 1 million rows of data, and dplyr is\r\nsufficiently fast enough for that size of data that you&#39;re not aware of\r\nprocessing time. We optimise dplyr for expressiveness on medium data;\r\nfeel free to use data.table for raw speed on bigger data.\r\n\r\nThe flexibility of dplyr also means that you can easily tweak performance\r\ncharacteristics using the same syntax. If the performance of dplyr with\r\nthe data frame backend is not good enough for you, you can use the\r\ndata.table backend (albeit with a somewhat restricted set of functionality).\r\nIf the data you&#39;re working with doesn&#39;t fit in memory, then you can use\r\na database backend.\r\n\r\nAll that said, dplyr performance will get better in the long-term. We&#39;ll\r\ndefinitely implement some of the great ideas of data.table like radix\r\nordering and using the same index for joins &amp; filters. We&#39;re also\r\nworking on parallelisation so we can take advantage of multiple cores.\r\n\r\nFeatures\r\n--------\r\n\r\nA few things that we&#39;re planning to work on in 2015:\r\n\r\n-   the `readr` package, to make it easy to get files off disk and in\r\n    to memory, analogous to `fread()`.\r\n\r\n-   More flexible joins, including support for non-equi-joins.\r\n\r\n-   More flexible grouping like bootstrap samples, rollups and more\r\n\r\nI&#39;m also investing time into improving R&#39;s [database\r\nconnectors](https://github.com/rstats-db/DBI), the ability to talk to\r\n[web apis](https://github.com/hadley/httr), and making it easier to\r\n[scrape html pages](https://github.com/hadley/rvest).\r\n",
               "tags": [],
               "creation_date": 1420720742,
               "last_edit_date": 1514932520,
               "is_accepted": false,
               "id": "27840349",
               "down_vote_count": 5,
               "score": 279
            }
         ],
         "link": "https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly",
         "id": "858127-2244"
      },
      {
         "up_vote_count": "1415",
         "path": "2.stack",
         "body_markdown": "I have a list of arbitrary length, and I need to split it up into equal size chunks and operate on it. There are some obvious ways to do this, like keeping a counter and two lists, and when the second list fills up, add it to the first list and empty the second list for the next round of data, but this is potentially extremely expensive.\r\n\r\nI was wondering if anyone had a good solution to this for lists of any length, e.g. using generators.\r\n\r\nI was looking for something useful in `itertools` but I couldn&#39;t find anything obviously useful. Might&#39;ve missed it, though.\r\n\r\nRelated question: [What is the most \u201cpythonic\u201d way to iterate over a list in chunks?][1]\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks",
         "view_count": "547428",
         "answer_count": "56",
         "tags": "['python', 'list', 'split', 'chunks']",
         "creation_date": "1227442552",
         "last_edit_date": "1495540511",
         "code_snippet": "['<code>itertools</code>', '<code>more_itertools</code>', '<code>chunked</code>', '<code>def chunks(l, n):\\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\\n    for i in range(0, len(l), n):\\n        yield l[i:i + n]\\n</code>', '<code>import pprint\\npprint.pprint(list(chunks(range(10, 75), 10)))\\n[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\\n [70, 71, 72, 73, 74]]\\n</code>', '<code>xrange()</code>', '<code>range()</code>', '<code>def chunks(l, n):\\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\\n    for i in xrange(0, len(l), n):\\n        yield l[i:i + n]\\n</code>', '<code>[l[i:i + n] for i in range(0, len(l), n)]\\n</code>', '<code>[l[i:i + n] for i in xrange(0, len(l), n)]\\n</code>', '<code>range</code>', '<code>xrange</code>', '<code>range</code>', '<code>tuple(l[i:i+n] for i in xrange(0, len(l), n))</code>', '<code>def chunks(l, n):\\n    n = max(1, n)\\n    return (l[i:i+n] for i in xrange(0, len(l), n))\\n</code>', '<code>from itertools import izip, chain, repeat\\n\\ndef grouper(n, iterable, padvalue=None):\\n    \"grouper(3, \\'abcdefg\\', \\'x\\') --&gt; (\\'a\\',\\'b\\',\\'c\\'), (\\'d\\',\\'e\\',\\'f\\'), (\\'g\\',\\'x\\',\\'x\\')\"\\n    return izip(*[chain(iterable, repeat(padvalue, n-1))]*n)\\n</code>', '<code>#from itertools import izip_longest as zip_longest # for Python 2.x\\nfrom itertools import zip_longest # for Python 3.x\\n#from six.moves import zip_longest # for both (uses the six compat library)\\n\\ndef grouper(n, iterable, padvalue=None):\\n    \"grouper(3, \\'abcdefg\\', \\'x\\') --&gt; (\\'a\\',\\'b\\',\\'c\\'), (\\'d\\',\\'e\\',\\'f\\'), (\\'g\\',\\'x\\',\\'x\\')\"\\n    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)\\n</code>', '<code>[iter(iterable)]*n</code>', '<code>n</code>', '<code>izip_longest</code>', '<code>n</code>', '<code>izip_longest(*[iter(iterable)]*n, fillvalue=fillvalue)</code>', '<code>zip(*[iter(yourList)]*n)</code>', '<code>izip_longest</code>', '<code>itertools</code>', '<code>numpy.array_split</code>', '<code>lst = range(50)\\nIn [26]: np.array_split(lst,5)\\nOut[26]: \\n[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\\n array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\\n array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\\n array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\\n array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])]\\n</code>', '<code>def split_seq(iterable, size):\\n    it = iter(iterable)\\n    item = list(itertools.islice(it, size))\\n    while item:\\n        yield item\\n        item = list(itertools.islice(it, size))\\n</code>', '<code>&gt;&gt;&gt; import pprint\\n&gt;&gt;&gt; pprint.pprint(list(split_seq(xrange(75), 10)))\\n[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\\n [70, 71, 72, 73, 74]]\\n</code>', '<code>iter</code>', '<code>from itertools import islice\\n\\ndef chunk(it, size):\\n    it = iter(it)\\n    return iter(lambda: tuple(islice(it, size)), ())\\n</code>', '<code>&gt;&gt;&gt; list(chunk(range(14), 3))\\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]\\n</code>', '<code>from itertools import islice, chain, repeat\\n\\ndef chunk_pad(it, size, padval=None):\\n    it = chain(iter(it), repeat(padval))\\n    return iter(lambda: tuple(islice(it, size)), (padval,) * size)\\n</code>', \"<code>&gt;&gt;&gt; list(chunk_pad(range(14), 3))\\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]\\n&gt;&gt;&gt; list(chunk_pad(range(14), 3, 'a'))\\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 'a')]\\n</code>\", '<code>izip_longest</code>', '<code>_no_padding = object()\\n\\ndef chunk(it, size, padval=_no_padding):\\n    if padval == _no_padding:\\n        it = iter(it)\\n        sentinel = ()\\n    else:\\n        it = chain(iter(it), repeat(padval))\\n        sentinel = (padval,) * size\\n    return iter(lambda: tuple(islice(it, size)), sentinel)\\n</code>', \"<code>&gt;&gt;&gt; list(chunk(range(14), 3))\\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]\\n&gt;&gt;&gt; list(chunk(range(14), 3, None))\\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]\\n&gt;&gt;&gt; list(chunk(range(14), 3, 'a'))\\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 'a')]\\n</code>\", '<code>islice(it, size)</code>', '<code>iter()</code>', '<code>iter</code>', '<code>len()</code>', '<code>def chunk(input, size):\\n    return map(None, *([iter(input)] * size))\\n</code>', '<code>map(None, iter)</code>', '<code>izip_longest(iter)</code>', '<code>*</code>', '<code>*</code>', '<code>l = range(1, 1000)\\nprint [l[x:x+10] for x in xrange(0, len(l), 10)]\\n</code>', '<code>chunks = lambda l, n: [l[x: x+n] for x in xrange(0, len(l), n)]\\nchunks(l, 10)\\n</code>', '<code>1</code>', '<code>l</code>', '<code>0</code>', '<code>O</code>', '<code>I</code>', '<code>1</code>', '<code>print [l[x:x+10] for x in xrange(1, len(l), 10)]</code>', '<code>range</code>', '<code>from itertools import zip_longest\\n\\na = range(1, 16)\\ni = iter(a)\\nr = list(zip_longest(i, i, i))\\n&gt;&gt;&gt; print(r)\\n[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, 15)]\\n</code>', '<code>a = range(1, 15)</code>', '<code>[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, None)]\\n</code>', '<code>zip_longest</code>', '<code>zip</code>', '<code>(13, 14, None)</code>', '<code>izip_longest</code>', '<code>zip(i, i, i, ... i)</code>', '<code>zip(*[i]*chunk_size)</code>', '<code>zip_longest</code>', '<code>range(1, 15)</code>', '<code>range(1, 15)</code>', '<code>[60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\\n[70, 71, 72, 73, 74]]\\n</code>', '<code>list(grouper(3, xrange(7)))</code>', '<code>chunk(xrange(7), 3)</code>', '<code>[(0, 1, 2), (3, 4, 5), (6, None, None)]</code>', '<code>None</code>', '<code>xrange</code>', '<code>range</code>', '<code>def baskets_from(items, maxbaskets=25):\\n    baskets = [[] for _ in xrange(maxbaskets)] # in Python 3 use range\\n    for i, item in enumerate(items):\\n        baskets[i % maxbaskets].append(item)\\n    return filter(None, baskets) \\n</code>', \"<code>def iter_baskets_from(items, maxbaskets=3):\\n    '''generates evenly balanced baskets from indexable iterable'''\\n    item_count = len(items)\\n    baskets = min(item_count, maxbaskets)\\n    for x_i in xrange(baskets):\\n        yield [items[y_i] for y_i in xrange(x_i, item_count, baskets)]\\n</code>\", \"<code>def iter_baskets_contiguous(items, maxbaskets=3, item_count=None):\\n    '''\\n    generates balanced baskets from iterable, contiguous contents\\n    provide item_count if providing a iterator that doesn't support len()\\n    '''\\n    item_count = item_count or len(items)\\n    baskets = min(item_count, maxbaskets)\\n    items = iter(items)\\n    floor = item_count // baskets \\n    ceiling = floor + 1\\n    stepdown = item_count % baskets\\n    for x_i in xrange(baskets):\\n        length = ceiling if x_i &lt; stepdown else floor\\n        yield [items.next() for _ in xrange(length)]\\n</code>\", \"<code>print(baskets_from(xrange(6), 8))\\nprint(list(iter_baskets_from(xrange(6), 8)))\\nprint(list(iter_baskets_contiguous(xrange(6), 8)))\\nprint(baskets_from(xrange(22), 8))\\nprint(list(iter_baskets_from(xrange(22), 8)))\\nprint(list(iter_baskets_contiguous(xrange(22), 8)))\\nprint(baskets_from('ABCDEFG', 3))\\nprint(list(iter_baskets_from('ABCDEFG', 3)))\\nprint(list(iter_baskets_contiguous('ABCDEFG', 3)))\\nprint(baskets_from(xrange(26), 5))\\nprint(list(iter_baskets_from(xrange(26), 5)))\\nprint(list(iter_baskets_contiguous(xrange(26), 5)))\\n</code>\", \"<code>[[0], [1], [2], [3], [4], [5]]\\n[[0], [1], [2], [3], [4], [5]]\\n[[0], [1], [2], [3], [4], [5]]\\n[[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]\\n[[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]\\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19], [20, 21]]\\n[['A', 'D', 'G'], ['B', 'E'], ['C', 'F']]\\n[['A', 'D', 'G'], ['B', 'E'], ['C', 'F']]\\n[['A', 'B', 'C'], ['D', 'E'], ['F', 'G']]\\n[[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]\\n[[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]\\n[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]\\n</code>\", '<code>list(grouper(3, xrange(7)))</code>', '<code>chunk(xrange(7), 3)</code>', '<code>[(0, 1, 2), (3, 4, 5), (6, None, None)]</code>', '<code>None</code>', '<code>import pandas as pd; [pd.DataFrame(np.arange(7))[i::3] for i in xrange(3)]</code>', '<code>grouped</code>', '<code>sliding</code>', '<code>grouped(N)</code>', '<code>sliding(N, N)</code>', '<code>zip(*[iterable[i::3] for i in range(3)]) \\n</code>', '<code>def SplitList(list, chunk_size):\\n    return [list[offs:offs+chunk_size] for offs in range(0, len(list), chunk_size)]\\n</code>', '<code>def IterChunks(sequence, chunk_size):\\n    res = []\\n    for item in sequence:\\n        res.append(item)\\n        if len(res) &gt;= chunk_size:\\n            yield res\\n            res = []\\n    if res:\\n        yield res  # yield the last, incomplete, portion\\n</code>', '<code>def chunks(seq, n):\\n    return (seq[i:i+n] for i in xrange(0, len(seq), n))\\n</code>', '<code>print list(chunks(range(1, 1000), 10))\\n</code>', '<code>from itertools import islice\\n\\ndef chunks(n, iterable):\\n    iterable = iter(iterable)\\n    while True:\\n        yield tuple(islice(iterable, n)) or iterable.next()\\n</code>', '<code>from itertools import chain, islice\\n\\ndef chunks(n, iterable):\\n   iterable = iter(iterable)\\n   while True:\\n       yield chain([next(iterable)], islice(iterable, n-1))\\n</code>', '<code>partition</code>', '<code>from toolz.itertoolz.core import partition\\n\\nlist(partition(2, [1, 2, 3, 4]))\\n[(1, 2), (3, 4)]\\n</code>', '<code>def chunks(li, n):\\n    if li == []:\\n        return\\n    yield li[:n]\\n    for e in chunks(li[n:], n):\\n        yield e\\n</code>', '<code>def chunks(li, n):\\n    if li == []:\\n        return\\n    yield li[:n]\\n    yield from chunks(li[n:], n)\\n</code>', '<code>def dec(gen):\\n    def new_gen(li, n):\\n        for e in gen(li, n):\\n            if e == []:\\n                return\\n            yield e\\n    return new_gen\\n\\n@dec\\ndef chunks(li, n):\\n    yield li[:n]\\n    for e in chunks(li[n:], n):\\n        yield e\\n</code>', '<code>get_chunks</code>', '<code>utilspie</code>', '<code>&gt;&gt;&gt; from utilspie import iterutils\\n&gt;&gt;&gt; a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\\n\\n&gt;&gt;&gt; list(iterutils.get_chunks(a, 5))\\n[[1, 2, 3, 4, 5], [6, 7, 8, 9]]\\n</code>', '<code>utilspie</code>', '<code>sudo pip install utilspie\\n</code>', '<code>[AA[i:i+SS] for i in range(len(AA))[::SS]]\\n</code>', '<code>&gt;&gt;&gt; AA=range(10,21);SS=3\\n&gt;&gt;&gt; [AA[i:i+SS] for i in range(len(AA))[::SS]]\\n[[10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]\\n# or [range(10, 13), range(13, 16), range(16, 19), range(19, 21)] in py3\\n</code>', '<code>def split_seq(seq, num_pieces):\\n    start = 0\\n    for i in xrange(num_pieces):\\n        stop = start + len(seq[i::num_pieces])\\n        yield seq[start:stop]\\n        start = stop\\n</code>', '<code>seq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n\\nfor seq in split_seq(seq, 3):\\n    print seq\\n</code>', '<code>def split_list(the_list, chunk_size):\\n    result_list = []\\n    while the_list:\\n        result_list.append(the_list[:chunk_size])\\n        the_list = the_list[chunk_size:]\\n    return result_list\\n\\na_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n\\nprint split_list(a_list, 3)\\n</code>', '<code>[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]\\n</code>', '<code>list</code>', '<code>In [48]: chunk = lambda ulist, step:  map(lambda i: ulist[i:i+step],  xrange(0, len(ulist), step))\\n\\nIn [49]: chunk(range(1,100), 10)\\nOut[49]: \\n[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\n [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\\n [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\\n [31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\\n [41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\\n [51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\\n [61, 62, 63, 64, 65, 66, 67, 68, 69, 70],\\n [71, 72, 73, 74, 75, 76, 77, 78, 79, 80],\\n [81, 82, 83, 84, 85, 86, 87, 88, 89, 90],\\n [91, 92, 93, 94, 95, 96, 97, 98, 99]]\\n</code>', '<code>def chunk</code>', '<code>chunk=lambda</code>', '<code>&lt;lamba&gt;</code>', '<code>def chunkList(initialList, chunkSize):\\n    \"\"\"\\n    This function chunks a list into sub lists \\n    that have a length equals to chunkSize.\\n\\n    Example:\\n    lst = [3, 4, 9, 7, 1, 1, 2, 3]\\n    print(chunkList(lst, 3)) \\n    returns\\n    [[3, 4, 9], [7, 1, 1], [2, 3]]\\n    \"\"\"\\n    finalList = []\\n    for i in range(0, len(initialList), chunkSize):\\n        finalList.append(initialList[i:i+chunkSize])\\n    return finalList\\n</code>', '<code>import matplotlib.cbook as cbook\\nsegments = cbook.pieces(np.arange(20), 3)\\nfor s in segments:\\n     print s\\n</code>', '<code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\\nCHUNK = 4\\n[a[i*CHUNK:(i+1)*CHUNK] for i in xrange((len(a) + CHUNK - 1) / CHUNK )]\\n</code>', '<code>def splitter(l, n):\\n    i = 0\\n    chunk = l[:n]\\n    while chunk:\\n        yield chunk\\n        i += n\\n        chunk = l[i:i+n]\\n</code>', '<code>def isplitter(l, n):\\n    l = iter(l)\\n    chunk = list(islice(l, n))\\n    while chunk:\\n        yield chunk\\n        chunk = list(islice(l, n))\\n</code>', '<code>def isplitter2(l, n):\\n    return takewhile(bool,\\n                     (tuple(islice(start, n))\\n                            for start in repeat(iter(l))))\\n</code>', '<code>def chunks_gen_sentinel(n, seq):\\n    continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))\\n    return iter(imap(tuple, continuous_slices).next,())\\n</code>', '<code>def chunks_gen_filter(n, seq):\\n    continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))\\n    return takewhile(bool,imap(tuple, continuous_slices))\\n</code>', '<code>len()</code>', '<code>def chunks(iterable,n):\\n    \"\"\"assumes n is an integer&gt;0\\n    \"\"\"\\n    iterable=iter(iterable)\\n    while True:\\n        result=[]\\n        for i in range(n):\\n            try:\\n                a=next(iterable)\\n            except StopIteration:\\n                break\\n            else:\\n                result.append(a)\\n        if result:\\n            yield result\\n        else:\\n            break\\n\\ng1=(i*i for i in range(10))\\ng2=chunks(g1,3)\\nprint g2\\n\\'&lt;generator object chunks at 0x0337B9B8&gt;\\'\\nprint list(g2)\\n\\'[[0, 1, 4], [9, 16, 25], [36, 49, 64], [81]]\\'\\n</code>', '<code>def chunker(iterable, chunksize):\\n    for i,c in enumerate(iterable[::chunksize]):\\n        yield iterable[i*chunksize:(i+1)*chunksize]\\n\\n&gt;&gt;&gt; for chunk in chunker(range(0,100), 10):\\n...     print list(chunk)\\n... \\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\\n[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\\n... etc ...\\n</code>', '<code>&gt;&gt;&gt; orange = range(1, 1001)\\n&gt;&gt;&gt; otuples = list( zip(*[iter(orange)]*10))\\n&gt;&gt;&gt; print(otuples)\\n[(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), ... (991, 992, 993, 994, 995, 996, 997, 998, 999, 1000)]\\n&gt;&gt;&gt; olist = [list(i) for i in otuples]\\n&gt;&gt;&gt; print(olist)\\n[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ..., [991, 992, 993, 994, 995, 996, 997, 998, 999, 1000]]\\n&gt;&gt;&gt; \\n</code>', '<code>zip(*[iter(range(7))]*3)</code>', '<code>[(0, 1, 2), (3, 4, 5)]</code>', '<code>6</code>', '<code>Y = lambda f: (lambda x: x(x))(lambda y: f(lambda *args: y(y)(*args)))\\nchunks = Y(lambda f: lambda n: [n[0][:n[1]]] + f((n[0][n[1]:], n[1])) if len(n[0]) &gt; 0 else [])\\n</code>']",
         "title": "How do you split a list into evenly sized chunks?",
         "_childDocuments_": [
            {
               "up_vote_count": 1887,
               "answer_id": 312464,
               "last_activity_date": 1503167006,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s a generator that yields the chunks you want:\r\n\r\n\tdef chunks(l, n):\r\n\t\t&quot;&quot;&quot;Yield successive n-sized chunks from l.&quot;&quot;&quot;\r\n\t\tfor i in range(0, len(l), n):\r\n\t\t\tyield l[i:i + n]\r\n\r\n_____________\r\n\r\n\timport pprint\r\n\tpprint.pprint(list(chunks(range(10, 75), 10)))\r\n\t[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\r\n\t [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\r\n\t [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\r\n\t [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\r\n\t [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\r\n\t [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\r\n\t [70, 71, 72, 73, 74]]\r\n\r\n\r\n_____________\r\n\r\nIf you&#39;re using Python 2, you should use `xrange()` instead of `range()`:\r\n\r\n\tdef chunks(l, n):\r\n\t\t&quot;&quot;&quot;Yield successive n-sized chunks from l.&quot;&quot;&quot;\r\n\t\tfor i in xrange(0, len(l), n):\r\n\t\t\tyield l[i:i + n]\r\n\t\t\t\r\n_____________\r\n\r\nAlso you can simply use list comprehension instead of writing a function. Python 3:\r\n\r\n\t[l[i:i + n] for i in range(0, len(l), n)]\r\n\t\r\nPython 2 version:\r\n\r\n\t[l[i:i + n] for i in xrange(0, len(l), n)]",
               "tags": [],
               "creation_date": 1227443633,
               "last_edit_date": 1503167006,
               "is_accepted": true,
               "id": "312464",
               "down_vote_count": 8,
               "score": 1879
            },
            {
               "up_vote_count": 16,
               "answer_id": 312466,
               "last_activity_date": 1347916985,
               "path": "3.stack.answer",
               "body_markdown": "If you know list size:\r\n\r\n    def SplitList(list, chunk_size):\r\n        return [list[offs:offs+chunk_size] for offs in range(0, len(list), chunk_size)]\r\n\r\nIf you don&#39;t (an iterator):\r\n\r\n    def IterChunks(sequence, chunk_size):\r\n        res = []\r\n        for item in sequence:\r\n            res.append(item)\r\n            if len(res) &gt;= chunk_size:\r\n                yield res\r\n                res = []\r\n        if res:\r\n            yield res  # yield the last, incomplete, portion\r\n\r\nIn the latter case, it can be rephrased in a more beautiful way if you can be sure that the sequence always contains a whole number of chunks of given size (i.e. there is no incomplete last chunk).",
               "tags": [],
               "creation_date": 1227444039,
               "last_edit_date": 1347916985,
               "is_accepted": false,
               "id": "312466",
               "down_vote_count": 1,
               "score": 15
            },
            {
               "up_vote_count": 73,
               "answer_id": 312467,
               "last_activity_date": 1347916923,
               "path": "3.stack.answer",
               "body_markdown": "Here is a generator that work on arbitrary iterables:\r\n\r\n    def split_seq(iterable, size):\r\n        it = iter(iterable)\r\n        item = list(itertools.islice(it, size))\r\n        while item:\r\n            yield item\r\n            item = list(itertools.islice(it, size))\r\n\r\nExample:\r\n\r\n    &gt;&gt;&gt; import pprint\r\n    &gt;&gt;&gt; pprint.pprint(list(split_seq(xrange(75), 10)))\r\n    [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n     [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\r\n     [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\r\n     [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\r\n     [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\r\n     [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\r\n     [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\r\n     [70, 71, 72, 73, 74]]\r\n",
               "tags": [],
               "creation_date": 1227444097,
               "last_edit_date": 1347916923,
               "is_accepted": false,
               "id": "312467",
               "down_vote_count": 2,
               "score": 71
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 312472,
               "is_accepted": false,
               "last_activity_date": 1227444676,
               "body_markdown": "heh, one line version\r\n\r\n    In [48]: chunk = lambda ulist, step:  map(lambda i: ulist[i:i+step],  xrange(0, len(ulist), step))\r\n    \r\n    In [49]: chunk(range(1,100), 10)\r\n    Out[49]: \r\n    [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\r\n     [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\r\n     [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\r\n     [31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\r\n     [41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\r\n     [51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\r\n     [61, 62, 63, 64, 65, 66, 67, 68, 69, 70],\r\n     [71, 72, 73, 74, 75, 76, 77, 78, 79, 80],\r\n     [81, 82, 83, 84, 85, 86, 87, 88, 89, 90],\r\n     [91, 92, 93, 94, 95, 96, 97, 98, 99]]\r\n\r\n\r\n",
               "id": "312472",
               "tags": [],
               "down_vote_count": 4,
               "creation_date": 1227444676,
               "score": 5
            },
            {
               "up_vote_count": 241,
               "answer_id": 312644,
               "last_activity_date": 1505987249,
               "path": "3.stack.answer",
               "body_markdown": "Directly from the (old) Python documentation (recipes for itertools):\r\n\r\n    from itertools import izip, chain, repeat\r\n\r\n    def grouper(n, iterable, padvalue=None):\r\n        &quot;grouper(3, &#39;abcdefg&#39;, &#39;x&#39;) --&gt; (&#39;a&#39;,&#39;b&#39;,&#39;c&#39;), (&#39;d&#39;,&#39;e&#39;,&#39;f&#39;), (&#39;g&#39;,&#39;x&#39;,&#39;x&#39;)&quot;\r\n        return izip(*[chain(iterable, repeat(padvalue, n-1))]*n)\r\n\r\nThe current version, as suggested by J.F.Sebastian:\r\n\r\n    #from itertools import izip_longest as zip_longest # for Python 2.x\r\n    from itertools import zip_longest # for Python 3.x\r\n    #from six.moves import zip_longest # for both (uses the six compat library)\r\n\r\n    def grouper(n, iterable, padvalue=None):\r\n        &quot;grouper(3, &#39;abcdefg&#39;, &#39;x&#39;) --&gt; (&#39;a&#39;,&#39;b&#39;,&#39;c&#39;), (&#39;d&#39;,&#39;e&#39;,&#39;f&#39;), (&#39;g&#39;,&#39;x&#39;,&#39;x&#39;)&quot;\r\n        return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)\r\n\r\nI guess Guido&#39;s time machine works\u2014worked\u2014will work\u2014will have worked\u2014was working again.\r\n\r\nThese solutions work because `[iter(iterable)]*n` (or the equivalent in the earlier version) creates *one* iterator, repeated `n` times in the list. `izip_longest` then effectively performs a round-robin of &quot;each&quot; iterator; because this is the same iterator, it is advanced by each such call, resulting in each such zip-roundrobin generating one tuple of `n` items.",
               "tags": [],
               "creation_date": 1227455333,
               "last_edit_date": 1505987249,
               "is_accepted": false,
               "id": "312644",
               "down_vote_count": 4,
               "score": 237
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 314771,
               "is_accepted": false,
               "last_activity_date": 1227545817,
               "body_markdown": "    def split_seq(seq, num_pieces):\r\n        start = 0\r\n        for i in xrange(num_pieces):\r\n            stop = start + len(seq[i::num_pieces])\r\n            yield seq[start:stop]\r\n            start = stop\r\n\r\n\r\nusage:\r\n\r\n    seq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n    \r\n    for seq in split_seq(seq, 3):\r\n        print seq\r\n\r\n",
               "id": "314771",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1227545817,
               "score": 6
            },
            {
               "up_vote_count": 2,
               "answer_id": 319970,
               "last_activity_date": 1340464772,
               "path": "3.stack.answer",
               "body_markdown": "    def chunk(lst):\r\n        out = []\r\n        for x in xrange(2, len(lst) + 1):\r\n            if not len(lst) % x:\r\n                factor = len(lst) / x\r\n                break\r\n        while lst:\r\n            out.append([lst.pop(0) for x in xrange(factor)])\r\n        return out",
               "tags": [],
               "creation_date": 1227684289,
               "last_edit_date": 1340464772,
               "is_accepted": false,
               "id": "319970",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 6,
               "answer_id": 1668586,
               "last_activity_date": 1257268208,
               "path": "3.stack.answer",
               "body_markdown": "    &gt;&gt;&gt; f = lambda x, n, acc=[]: f(x[n:], n, acc+[(x[:n])]) if x else acc\r\n    &gt;&gt;&gt; f(&quot;Hallo Welt&quot;, 3)\r\n    [&#39;Hal&#39;, &#39;lo &#39;, &#39;Wel&#39;, &#39;t&#39;]\r\n    &gt;&gt;&gt; \r\n\r\nIf you are into brackets - I picked up a book on Erlang :)\r\n",
               "tags": [],
               "creation_date": 1257266731,
               "last_edit_date": 1257268208,
               "is_accepted": false,
               "id": "1668586",
               "down_vote_count": 4,
               "score": 2
            },
            {
               "up_vote_count": 466,
               "answer_id": 1751478,
               "last_activity_date": 1473676672,
               "path": "3.stack.answer",
               "body_markdown": "If you want something super simple:\r\n  \r\n    def chunks(l, n):\r\n        n = max(1, n)\r\n        return (l[i:i+n] for i in xrange(0, len(l), n))",
               "tags": [],
               "creation_date": 1258489036,
               "last_edit_date": 1473676672,
               "is_accepted": false,
               "id": "1751478",
               "down_vote_count": 3,
               "score": 463
            },
            {
               "up_vote_count": 5,
               "answer_id": 2270932,
               "last_activity_date": 1463380152,
               "path": "3.stack.answer",
               "body_markdown": "Without calling len() which is good for large lists:\r\n\r\n    def splitter(l, n):\r\n        i = 0\r\n        chunk = l[:n]\r\n        while chunk:\r\n            yield chunk\r\n            i += n\r\n            chunk = l[i:i+n]\r\n\r\nAnd this is for iterables:\r\n\r\n    def isplitter(l, n):\r\n        l = iter(l)\r\n        chunk = list(islice(l, n))\r\n        while chunk:\r\n            yield chunk\r\n            chunk = list(islice(l, n))\r\n\r\nThe functional flavour of the above:\r\n\r\n    def isplitter2(l, n):\r\n        return takewhile(bool,\r\n                         (tuple(islice(start, n))\r\n                                for start in repeat(iter(l))))\r\nOR:\r\n\r\n    def chunks_gen_sentinel(n, seq):\r\n        continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))\r\n        return iter(imap(tuple, continuous_slices).next,())\r\n\r\nOR:\r\n\r\n    def chunks_gen_filter(n, seq):\r\n        continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))\r\n        return takewhile(bool,imap(tuple, continuous_slices))\r\n",
               "tags": [],
               "creation_date": 1266299387,
               "last_edit_date": 1463380152,
               "is_accepted": false,
               "id": "2270932",
               "down_vote_count": 1,
               "score": 4
            },
            {
               "up_vote_count": 48,
               "answer_id": 3125186,
               "last_activity_date": 1347916945,
               "path": "3.stack.answer",
               "body_markdown": "    def chunk(input, size):\r\n        return map(None, *([iter(input)] * size))",
               "tags": [],
               "creation_date": 1277579407,
               "last_edit_date": 1347916945,
               "is_accepted": false,
               "id": "3125186",
               "down_vote_count": 1,
               "score": 47
            },
            {
               "up_vote_count": 35,
               "answer_id": 3226719,
               "last_activity_date": 1423564059,
               "path": "3.stack.answer",
               "body_markdown": "Simple yet elegant\r\n\r\n    l = range(1, 1000)\r\n    print [l[x:x+10] for x in xrange(0, len(l), 10)]\r\n\r\nor if you prefer:\r\n\r\n    chunks = lambda l, n: [l[x: x+n] for x in xrange(0, len(l), n)]\r\n    chunks(l, 10)\r\n\r\n \r\n",
               "tags": [],
               "creation_date": 1278921523,
               "last_edit_date": 1423564059,
               "is_accepted": false,
               "id": "3226719",
               "down_vote_count": 0,
               "score": 35
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 5711993,
               "is_accepted": false,
               "last_activity_date": 1303190839,
               "body_markdown": "If you had a chunk size of 3 for example, you could do:\r\n\r\n    zip(*[iterable[i::3] for i in range(3)]) \r\n\r\nsource:\r\nhttp://code.activestate.com/recipes/303060-group-a-list-into-sequential-n-tuples/\r\n\r\nI would use this when my chunk size is fixed number I can type, e.g. &#39;3&#39;, and would never change.",
               "id": "5711993",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1303190839,
               "score": 15
            },
            {
               "up_vote_count": 7,
               "answer_id": 5872632,
               "last_activity_date": 1331231235,
               "path": "3.stack.answer",
               "body_markdown": "Consider using [matplotlib.cbook][1] pieces\r\n\r\nfor example:\r\n\r\n    import matplotlib.cbook as cbook\r\n    segments = cbook.pieces(np.arange(20), 3)\r\n    for s in segments:\r\n         print s\r\n\r\n\r\n  [1]: http://matplotlib.sourceforge.net/",
               "tags": [],
               "creation_date": 1304440057,
               "last_edit_date": 1331231235,
               "is_accepted": false,
               "id": "5872632",
               "down_vote_count": 3,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 9255750,
               "is_accepted": false,
               "last_activity_date": 1329108638,
               "body_markdown": "    def chunks(iterable,n):\r\n        &quot;&quot;&quot;assumes n is an integer&gt;0\r\n        &quot;&quot;&quot;\r\n        iterable=iter(iterable)\r\n        while True:\r\n            result=[]\r\n            for i in range(n):\r\n                try:\r\n                    a=next(iterable)\r\n                except StopIteration:\r\n                    break\r\n                else:\r\n                    result.append(a)\r\n            if result:\r\n                yield result\r\n            else:\r\n                break\r\n    \r\n    g1=(i*i for i in range(10))\r\n    g2=chunks(g1,3)\r\n    print g2\r\n    &#39;&lt;generator object chunks at 0x0337B9B8&gt;&#39;\r\n    print list(g2)\r\n    &#39;[[0, 1, 4], [9, 16, 25], [36, 49, 64], [81]]&#39;\r\n\r\n",
               "id": "9255750",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1329108638,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 12150728,
               "is_accepted": false,
               "last_activity_date": 1346108285,
               "body_markdown": "I realise this question is old (stumbled over it on Google), but surely something like the following is far simpler and clearer than any of the huge complex suggestions and only uses slicing:\r\n\r\n    def chunker(iterable, chunksize):\r\n        for i,c in enumerate(iterable[::chunksize]):\r\n            yield iterable[i*chunksize:(i+1)*chunksize]\r\n\r\n    &gt;&gt;&gt; for chunk in chunker(range(0,100), 10):\r\n    ... \tprint list(chunk)\r\n    ... \r\n    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n    [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\r\n    [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\r\n    ... etc ...",
               "id": "12150728",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1346108285,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 13756402,
               "is_accepted": false,
               "last_activity_date": 1354850211,
               "body_markdown": "No one use tee() function under itertools ?\r\n\r\nhttp://docs.python.org/2/library/itertools.html#itertools.tee\r\n\r\n    &gt;&gt;&gt; import itertools\r\n    &gt;&gt;&gt; itertools.tee([1,2,3,4,5,6],3)\r\n    (&lt;itertools.tee object at 0x02932DF0&gt;, &lt;itertools.tee object at 0x02932EB8&gt;, &lt;itertools.tee object at 0x02932EE0&gt;)\r\n\r\nThis will split list to 3 iterator , loop the iterator will get the sublist with equal length",
               "id": "13756402",
               "tags": [],
               "down_vote_count": 6,
               "creation_date": 1354850211,
               "score": -3
            },
            {
               "up_vote_count": 5,
               "answer_id": 14937534,
               "last_activity_date": 1415958522,
               "path": "3.stack.answer",
               "body_markdown": "See [this reference](http://docs.python.org/3.3/library/functions.html?highlight=zip#zip)\r\n\r\n\t&gt;&gt;&gt; orange = range(1, 1001)\r\n\t&gt;&gt;&gt; otuples = list( zip(*[iter(orange)]*10))\r\n\t&gt;&gt;&gt; print(otuples)\r\n\t[(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), ... (991, 992, 993, 994, 995, 996, 997, 998, 999, 1000)]\r\n\t&gt;&gt;&gt; olist = [list(i) for i in otuples]\r\n\t&gt;&gt;&gt; print(olist)\r\n\t[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ..., [991, 992, 993, 994, 995, 996, 997, 998, 999, 1000]]\r\n\t&gt;&gt;&gt; \r\n\r\nPython3",
               "tags": [],
               "creation_date": 1361194311,
               "last_edit_date": 1415958522,
               "is_accepted": false,
               "id": "14937534",
               "down_vote_count": 2,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 15,
               "answer_id": 16004505,
               "is_accepted": false,
               "last_activity_date": 1365974766,
               "body_markdown": "A generator expression:\r\n\r\n    def chunks(seq, n):\r\n        return (seq[i:i+n] for i in xrange(0, len(seq), n))\r\n\r\neg.\r\n\r\n    print list(chunks(range(1, 1000), 10))",
               "id": "16004505",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1365974766,
               "score": 14
            },
            {
               "up_vote_count": 23,
               "answer_id": 16315158,
               "last_activity_date": 1499701690,
               "path": "3.stack.answer",
               "body_markdown": "[more-itertools has a chunks iterator.](http://more-itertools.readthedocs.io/en/latest/api.html#more_itertools.chunked)\r\n\r\nIt also has a lot more things, including all the recipes in the itertools documentation.",
               "tags": [],
               "creation_date": 1367397741,
               "last_edit_date": 1499701690,
               "is_accepted": false,
               "id": "16315158",
               "down_vote_count": 1,
               "score": 22
            },
            {
               "up_vote_count": 0,
               "answer_id": 16760646,
               "last_activity_date": 1415958448,
               "path": "3.stack.answer",
               "body_markdown": "using List Comprehensions of python\r\n\r\n    [range(t,t+10) for t in range(1,1000,10)]\r\n\r\n    [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\r\n     [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\r\n     [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],....\r\n     ....[981, 982, 983, 984, 985, 986, 987, 988, 989, 990],\r\n     [991, 992, 993, 994, 995, 996, 997, 998, 999, 1000]]\r\n\r\nvisit [this link](http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions) to know about List Comprehensions\r\n",
               "tags": [],
               "creation_date": 1369581522,
               "last_edit_date": 1415958448,
               "is_accepted": false,
               "id": "16760646",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 82,
               "answer_id": 16935535,
               "last_activity_date": 1502200118,
               "path": "3.stack.answer",
               "body_markdown": "I know this is kind of old but I don&#39;t why nobody mentioned `numpy.array_split`:\r\n\r\n    lst = range(50)\r\n    In [26]: np.array_split(lst,5)\r\n    Out[26]: \r\n    [array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\r\n     array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\r\n     array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\r\n     array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\r\n     array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])]\r\n\r\n",
               "tags": [],
               "creation_date": 1370422466,
               "last_edit_date": 1502200118,
               "is_accepted": false,
               "id": "16935535",
               "down_vote_count": 2,
               "score": 80
            },
            {
               "up_vote_count": 4,
               "answer_id": 16970117,
               "last_activity_date": 1421153937,
               "path": "3.stack.answer",
               "body_markdown": "Not exactly the same but still nice\r\n\r\n    def chunks(l, chunks):\r\n        return zip(*[iter(l)]*chunks)\r\n    \r\n    l = range(1, 1000)\r\n    print chunks(l, 10) -&gt; [ ( 1..10 ), ( 11..20 ), .., ( 991..999 ) ]\r\n\r\n",
               "tags": [],
               "creation_date": 1370545311,
               "last_edit_date": 1421153937,
               "is_accepted": false,
               "id": "16970117",
               "down_vote_count": 2,
               "score": 2
            },
            {
               "up_vote_count": 3,
               "answer_id": 18793562,
               "last_activity_date": 1379099828,
               "path": "3.stack.answer",
               "body_markdown": "* Works with any iterable\r\n* Inner data is generator object (not a list)\r\n* One liner\r\n&lt;pre&gt;\r\nIn [259]: get_in_chunks = lambda itr,n: ( (v for _,v in g) for _,g in itertools.groupby(enumerate(itr),lambda (ind,_): ind/n))\r\n\r\nIn [260]: list(list(x) for x in get_in_chunks(range(30),7))\r\nOut[260]:\r\n[[0, 1, 2, 3, 4, 5, 6],\r\n [7, 8, 9, 10, 11, 12, 13],\r\n [14, 15, 16, 17, 18, 19, 20],\r\n [21, 22, 23, 24, 25, 26, 27],\r\n [28, 29]]\r\n&lt;/pre&gt;",
               "tags": [],
               "creation_date": 1379099504,
               "last_edit_date": 1379099828,
               "is_accepted": false,
               "id": "18793562",
               "down_vote_count": 1,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 18970810,
               "is_accepted": false,
               "last_activity_date": 1379980663,
               "body_markdown": "    def chunked(iterable, size):\r\n        chunk = ()\r\n\r\n        for item in iterable:\r\n            chunk += (item,)\r\n            if len(chunk) % size == 0:\r\n                yield chunk\r\n                chunk = ()\r\n    \r\n        if chunk:\r\n            yield chunk",
               "id": "18970810",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1379980663,
               "score": 0
            },
            {
               "up_vote_count": 14,
               "answer_id": 19264525,
               "last_activity_date": 1383985283,
               "path": "3.stack.answer",
               "body_markdown": "I like the Python doc&#39;s version proposed by tzot and J.F.Sebastian a lot,\r\n but it has two shortcomings:\r\n\r\n - it is not very explicit\r\n - I usually don&#39;t want a fill value in the last chunk\r\n\r\nI&#39;m using this one a lot in my code:\r\n\r\n    from itertools import islice\r\n\r\n    def chunks(n, iterable):\r\n        iterable = iter(iterable)\r\n        while True:\r\n            yield tuple(islice(iterable, n)) or iterable.next()\r\n\r\nUPDATE: A lazy chunks version:\r\n\r\n    from itertools import chain, islice\r\n\r\n    def chunks(n, iterable):\r\n       iterable = iter(iterable)\r\n       while True:\r\n           yield chain([next(iterable)], islice(iterable, n-1))\r\n",
               "tags": [],
               "creation_date": 1381299449,
               "last_edit_date": 1383985283,
               "is_accepted": false,
               "id": "19264525",
               "down_vote_count": 1,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 20106816,
               "is_accepted": false,
               "last_activity_date": 1384980922,
               "body_markdown": "The [toolz][1] library has the `partition` function for this:\r\n\r\n    from toolz.itertoolz.core import partition\r\n    \r\n    list(partition(2, [1, 2, 3, 4]))\r\n    [(1, 2), (3, 4)]\r\n  [1]: https://github.com/pytoolz/toolz",
               "id": "20106816",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1384980922,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 20228836,
               "is_accepted": false,
               "last_activity_date": 1385503083,
               "body_markdown": "Yes, it is an old question, but I had to post this one, because it is even a little shorter than the similar ones.\r\nYes, the result looks scrambled, but if it is just about even length...\r\n\r\n    &gt;&gt;&gt; n = 3 # number of groups\r\n    &gt;&gt;&gt; biglist = range(30)\r\n    &gt;&gt;&gt;\r\n    &gt;&gt;&gt; [ biglist[i::n] for i in xrange(n) ]\r\n    [[0, 3, 6, 9, 12, 15, 18, 21, 24, 27],\r\n     [1, 4, 7, 10, 13, 16, 19, 22, 25, 28],\r\n     [2, 5, 8, 11, 14, 17, 20, 23, 26, 29]]",
               "id": "20228836",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1385503083,
               "score": -1
            },
            {
               "up_vote_count": 23,
               "answer_id": 21767522,
               "last_activity_date": 1393431060,
               "path": "3.stack.answer",
               "body_markdown": "Critique of other answers here:\r\n-------------------------------\r\n\r\nNone of these answers are evenly sized chunks, they all leave a runt chunk at the end, so they&#39;re not completely balanced. If you were using these functions to distribute work, you&#39;ve built-in the prospect of one likely finishing well before the others, so it would sit around doing nothing while the others continued working hard.\r\n\r\nFor example, the current top answer ends with:\r\n\r\n    [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\r\n    [70, 71, 72, 73, 74]]\r\n\r\nI just hate that runt at the end!\r\n\r\nOthers, like `list(grouper(3, xrange(7)))`, and `chunk(xrange(7), 3)` both return: `[(0, 1, 2), (3, 4, 5), (6, None, None)]`. The `None`&#39;s are just padding, and rather inelegant in my opinion. They are NOT evenly chunking the iterables.\r\n\r\nWhy can&#39;t we divide these better?\r\n\r\nMy Solution(s)\r\n--------------\r\n\r\nHere&#39;s a balanced solution, adapted from a function I&#39;ve used in production (Note in Python 3 to replace `xrange` with `range`):\r\n\r\n    def baskets_from(items, maxbaskets=25):\r\n        baskets = [[] for _ in xrange(maxbaskets)] # in Python 3 use range\r\n        for i, item in enumerate(items):\r\n            baskets[i % maxbaskets].append(item)\r\n        return filter(None, baskets) \r\n\r\nAnd I created a generator that does the same if you put it into a list:\r\n\r\n    def iter_baskets_from(items, maxbaskets=3):\r\n        &#39;&#39;&#39;generates evenly balanced baskets from indexable iterable&#39;&#39;&#39;\r\n        item_count = len(items)\r\n        baskets = min(item_count, maxbaskets)\r\n        for x_i in xrange(baskets):\r\n            yield [items[y_i] for y_i in xrange(x_i, item_count, baskets)]\r\n        \r\nAnd finally, since I see that all of the above functions return elements in a contiguous order (as they were given):\r\n\r\n    def iter_baskets_contiguous(items, maxbaskets=3, item_count=None):\r\n        &#39;&#39;&#39;\r\n        generates balanced baskets from iterable, contiguous contents\r\n        provide item_count if providing a iterator that doesn&#39;t support len()\r\n        &#39;&#39;&#39;\r\n        item_count = item_count or len(items)\r\n        baskets = min(item_count, maxbaskets)\r\n        items = iter(items)\r\n        floor = item_count // baskets \r\n        ceiling = floor + 1\r\n        stepdown = item_count % baskets\r\n        for x_i in xrange(baskets):\r\n            length = ceiling if x_i &lt; stepdown else floor\r\n            yield [items.next() for _ in xrange(length)]\r\n\r\nOutput\r\n------\r\n\r\nTo test them out:\r\n\r\n    print(baskets_from(xrange(6), 8))\r\n    print(list(iter_baskets_from(xrange(6), 8)))\r\n    print(list(iter_baskets_contiguous(xrange(6), 8)))\r\n    print(baskets_from(xrange(22), 8))\r\n    print(list(iter_baskets_from(xrange(22), 8)))\r\n    print(list(iter_baskets_contiguous(xrange(22), 8)))\r\n    print(baskets_from(&#39;ABCDEFG&#39;, 3))\r\n    print(list(iter_baskets_from(&#39;ABCDEFG&#39;, 3)))\r\n    print(list(iter_baskets_contiguous(&#39;ABCDEFG&#39;, 3)))\r\n    print(baskets_from(xrange(26), 5))\r\n    print(list(iter_baskets_from(xrange(26), 5)))\r\n    print(list(iter_baskets_contiguous(xrange(26), 5)))\r\n\r\nWhich prints out:\r\n\r\n\r\n    [[0], [1], [2], [3], [4], [5]]\r\n    [[0], [1], [2], [3], [4], [5]]\r\n    [[0], [1], [2], [3], [4], [5]]\r\n    [[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]\r\n    [[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]\r\n    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19], [20, 21]]\r\n    [[&#39;A&#39;, &#39;D&#39;, &#39;G&#39;], [&#39;B&#39;, &#39;E&#39;], [&#39;C&#39;, &#39;F&#39;]]\r\n    [[&#39;A&#39;, &#39;D&#39;, &#39;G&#39;], [&#39;B&#39;, &#39;E&#39;], [&#39;C&#39;, &#39;F&#39;]]\r\n    [[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;], [&#39;D&#39;, &#39;E&#39;], [&#39;F&#39;, &#39;G&#39;]]\r\n    [[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]\r\n    [[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]\r\n    [[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]\r\n\r\nNotice that the contiguous generator provide chunks in the same length patterns as the other two, but the items are all in order, and they are as evenly divided as one may divide a list of discrete elements.",
               "tags": [],
               "creation_date": 1392332837,
               "last_edit_date": 1393431060,
               "is_accepted": false,
               "id": "21767522",
               "down_vote_count": 1,
               "score": 22
            },
            {
               "up_vote_count": 54,
               "answer_id": 22045226,
               "last_activity_date": 1393427417,
               "path": "3.stack.answer",
               "body_markdown": "I&#39;m surprised nobody has thought of using `iter`&#39;s [two-argument form](http://docs.python.org/2/library/functions.html#iter):\r\n\r\n    from itertools import islice\r\n    \r\n    def chunk(it, size):\r\n        it = iter(it)\r\n        return iter(lambda: tuple(islice(it, size)), ())\r\n\r\nDemo:\r\n\r\n    &gt;&gt;&gt; list(chunk(range(14), 3))\r\n    [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]\r\n\r\nThis works with any iterable and produces output lazily. It returns tuples rather than iterators, but I think it has a certain elegance nonetheless. It also doesn&#39;t pad; if you want padding, a simple variation on the above will suffice:\r\n\r\n    from itertools import islice, chain, repeat\r\n    \r\n    def chunk_pad(it, size, padval=None):\r\n        it = chain(iter(it), repeat(padval))\r\n        return iter(lambda: tuple(islice(it, size)), (padval,) * size)\r\n\r\nDemo:\r\n\r\n    &gt;&gt;&gt; list(chunk_pad(range(14), 3))\r\n    [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]\r\n    &gt;&gt;&gt; list(chunk_pad(range(14), 3, &#39;a&#39;))\r\n    [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, &#39;a&#39;)]\r\n\r\nLike the `izip_longest`-based solutions, the above _always_ pads. As far as I know, there&#39;s no one- or two-line itertools recipe for a function that _optionally_ pads. By combining the above two approaches, this one comes pretty close:\r\n\r\n    _no_padding = object()\r\n    \r\n    def chunk(it, size, padval=_no_padding):\r\n        if padval == _no_padding:\r\n            it = iter(it)\r\n            sentinel = ()\r\n        else:\r\n            it = chain(iter(it), repeat(padval))\r\n            sentinel = (padval,) * size\r\n        return iter(lambda: tuple(islice(it, size)), sentinel)\r\n\r\nDemo:\r\n\r\n    &gt;&gt;&gt; list(chunk(range(14), 3))\r\n    [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]\r\n    &gt;&gt;&gt; list(chunk(range(14), 3, None))\r\n    [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]\r\n    &gt;&gt;&gt; list(chunk(range(14), 3, &#39;a&#39;))\r\n    [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, &#39;a&#39;)]\r\n\r\nI believe this is the shortest chunker proposed that offers optional padding. \r\n",
               "tags": [],
               "creation_date": 1393426920,
               "last_edit_date": 1393427417,
               "is_accepted": false,
               "id": "22045226",
               "down_vote_count": 1,
               "score": 53
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 22138685,
               "is_accepted": false,
               "last_activity_date": 1393821024,
               "body_markdown": "I wrote a small library expressly for this purpose, available [here][1]. The library&#39;s `chunked` function is particularly efficient because it&#39;s implemented as a [generator][2], so a substantial amount of memory can be saved in certain situations. It also doesn&#39;t rely on the slice notation, so any arbitrary iterator can be used.\r\n\r\n    import iterlib\r\n\r\n    print list(iterlib.chunked(xrange(1, 1000), 10))\r\n    # prints [(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), (11, 12, 13, 14, 15, 16, 17, 18, 19, 20), ...]\r\n\r\n\r\n  [1]: https://github.com/rectangletangle/iterlib\r\n  [2]: https://wiki.python.org/moin/Generators",
               "id": "22138685",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1393821024,
               "score": 2
            },
            {
               "up_vote_count": 1,
               "answer_id": 25650543,
               "last_activity_date": 1409766195,
               "path": "3.stack.answer",
               "body_markdown": "Like @AaronHall I got here looking for roughly evenly sized chunks. There are different interpretations of that. In my case, if the desired size is N, I would like each group to be of size&gt;=N.\r\nThus, the orphans which are created in most of the above should be redistributed to other groups.\r\n\r\nThis can be done using:\r\n\r\n    def nChunks(l, n):\r\n        &quot;&quot;&quot; Yield n successive chunks from l.\r\n        Works for lists,  pandas dataframes, etc\r\n        &quot;&quot;&quot;\r\n        newn = int(1.0 * len(l) / n + 0.5)\r\n        for i in xrange(0, n-1):\r\n            yield l[i*newn:i*newn+newn]\r\n        yield l[n*newn-newn:]\r\n\r\n(from https://stackoverflow.com/questions/2130016/splitting-a-list-of-arbitrary-size-into-only-roughly-n-equal-parts) by simply calling it as nChunks(l,l/n)  or nChunks(l,floor(l/n))\r\n",
               "tags": [],
               "creation_date": 1409766195,
               "last_edit_date": 1495540979,
               "is_accepted": false,
               "id": "25650543",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 27371167,
               "is_accepted": false,
               "last_activity_date": 1418097289,
               "body_markdown": "letting r be the chunk size  and L be the initial list, you can do. \r\n\r\n    chunkL = [ [i for i in L[r*k:r*(k+1)] ] for k in range(len(L)/r)] ",
               "id": "27371167",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1418097289,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 28756559,
               "is_accepted": false,
               "last_activity_date": 1425004415,
               "body_markdown": "Use list comprehensions:\r\n\r\n    l = [1,2,3,4,5,6,7,8,9,10,11,12]\r\n    k = 5 #chunk size\r\n    print [tuple(l[x:y]) for (x, y) in [(x, x+k) for x in range(0, len(l), k)]]",
               "id": "28756559",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1425004415,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 28786255,
               "is_accepted": false,
               "last_activity_date": 1425153903,
               "body_markdown": "Another more explicit version.\r\n\r\n    def chunkList(initialList, chunkSize):\r\n        &quot;&quot;&quot;\r\n        This function chunks a list into sub lists \r\n        that have a length equals to chunkSize.\r\n    \r\n        Example:\r\n        lst = [3, 4, 9, 7, 1, 1, 2, 3]\r\n        print(chunkList(lst, 3)) \r\n        returns\r\n        [[3, 4, 9], [7, 1, 1], [2, 3]]\r\n        &quot;&quot;&quot;\r\n        finalList = []\r\n        for i in range(0, len(initialList), chunkSize):\r\n            finalList.append(initialList[i:i+chunkSize])\r\n        return finalList\r\n\r\n",
               "id": "28786255",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1425153903,
               "score": 5
            },
            {
               "up_vote_count": 27,
               "answer_id": 29009933,
               "last_activity_date": 1498052201,
               "path": "3.stack.answer",
               "body_markdown": "I saw the most awesome Python-ish answer in a [duplicate][1] of this question:\r\n\r\n    from itertools import zip_longest\r\n    \r\n    a = range(1, 16)\r\n    i = iter(a)\r\n    r = list(zip_longest(i, i, i))\r\n    &gt;&gt;&gt; print(r)\r\n    [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, 15)]\r\n\r\nYou can create n-tuple for any n. If `a = range(1, 15)`, then the result will be:\r\n\r\n    [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, None)]\r\n\r\nIf the list is divided evenly, then you can replace `zip_longest` with `zip`, otherwise the triplet `(13, 14, None)` would be lost. Python 3 is used above. For Python 2, use `izip_longest`.\r\n\r\n  [1]: https://stackoverflow.com/questions/23286254/convert-list-to-a-list-of-tuples-python",
               "tags": [],
               "creation_date": 1426163770,
               "last_edit_date": 1498052201,
               "is_accepted": false,
               "id": "29009933",
               "down_vote_count": 0,
               "score": 27
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 29707187,
               "is_accepted": false,
               "last_activity_date": 1429296515,
               "body_markdown": "The answer above (by koffein) has a little problem: the list is always split into an equal number of splits, not equal number of items per partition. This is my version. The &quot;// chs + 1&quot; takes into account that the number of items may not be divideable exactly by the partition size, so the last partition will only be partially filled.\r\n\r\n    # Given &#39;l&#39; is your list\r\n\r\n    chs = 12 # Your chunksize\r\n    partitioned = [ l[i*chs:(i*chs)+chs] for i in range((len(l) // chs)+1) ]",
               "id": "29707187",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1429296515,
               "score": 2
            },
            {
               "up_vote_count": 6,
               "answer_id": 31178232,
               "last_activity_date": 1472664654,
               "path": "3.stack.answer",
               "body_markdown": "code:\r\n\r\n    def split_list(the_list, chunk_size):\r\n        result_list = []\r\n        while the_list:\r\n            result_list.append(the_list[:chunk_size])\r\n            the_list = the_list[chunk_size:]\r\n        return result_list\r\n\r\n    a_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n\r\n    print split_list(a_list, 3)\r\nresult:\r\n\r\n    [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]",
               "tags": [],
               "creation_date": 1435822369,
               "last_edit_date": 1472664654,
               "is_accepted": false,
               "id": "31178232",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 31442939,
               "is_accepted": false,
               "last_activity_date": 1437002839,
               "body_markdown": "    a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n    CHUNK = 4\r\n    [a[i*CHUNK:(i+1)*CHUNK] for i in xrange((len(a) + CHUNK - 1) / CHUNK )]",
               "id": "31442939",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1437002839,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 32658232,
               "is_accepted": false,
               "last_activity_date": 1442598879,
               "body_markdown": "I have come up to following solution without creation temorary list object, which should work with any iterable object. Please note that this version for Python 2.x:\r\n\r\n    def chunked(iterable, size):\r\n        stop = []\r\n        it = iter(iterable)\r\n        def _next_chunk():\r\n            try:\r\n                for _ in xrange(size):\r\n                    yield next(it)\r\n            except StopIteration:\r\n                stop.append(True)\r\n                return\r\n\r\n        while not stop:\r\n            yield _next_chunk()\r\n\r\n    for it in chunked(xrange(16), 4):\r\n       print list(it)\r\n\r\nOutput:\r\n\r\n    [0, 1, 2, 3]\r\n    [4, 5, 6, 7]\r\n    [8, 9, 10, 11]\r\n    [12, 13, 14, 15] \r\n    []\r\nAs you can see if len(iterable) % size == 0 then we have additional empty iterator object. But I do not think that it is big problem.",
               "id": "32658232",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1442598879,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 33180285,
               "is_accepted": false,
               "last_activity_date": 1445033369,
               "body_markdown": "Since I had to do something like this, here&#39;s my solution given a generator and a batch size:\r\n\r\n    def pop_n_elems_from_generator(g, n):\r\n        elems = []\r\n        try:\r\n            for idx in xrange(0, n):\r\n                elems.append(g.next())\r\n            return elems\r\n        except StopIteration:\r\n            return elems\r\n\r\n\r\n",
               "id": "33180285",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1445033369,
               "score": 1
            },
            {
               "up_vote_count": 9,
               "answer_id": 33510840,
               "last_activity_date": 1446594178,
               "path": "3.stack.answer",
               "body_markdown": "At this point, I think we need a **recursive generator**, just in case...\r\n\r\nIn python 2:\r\n\r\n    def chunks(li, n):\r\n        if li == []:\r\n            return\r\n        yield li[:n]\r\n        for e in chunks(li[n:], n):\r\n            yield e\r\n\r\n\r\nIn python 3:\r\n\r\n    def chunks(li, n):\r\n        if li == []:\r\n            return\r\n        yield li[:n]\r\n        yield from chunks(li[n:], n)\r\n\r\nAlso, in case of massive Alien invasion, a **decorated recursive generator** might become handy:\r\n\r\n\r\n    def dec(gen):\r\n        def new_gen(li, n):\r\n            for e in gen(li, n):\r\n                if e == []:\r\n                    return\r\n                yield e\r\n        return new_gen\r\n\r\n    @dec\r\n    def chunks(li, n):\r\n        yield li[:n]\r\n        for e in chunks(li[n:], n):\r\n            yield e",
               "tags": [],
               "creation_date": 1446592250,
               "last_edit_date": 1446594178,
               "is_accepted": false,
               "id": "33510840",
               "down_vote_count": 0,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 33517774,
               "is_accepted": false,
               "last_activity_date": 1446628347,
               "body_markdown": "At this point, I think we need the obligatory anonymous-recursive function.\r\n\r\n\r\n    Y = lambda f: (lambda x: x(x))(lambda y: f(lambda *args: y(y)(*args)))\r\n    chunks = Y(lambda f: lambda n: [n[0][:n[1]]] + f((n[0][n[1]:], n[1])) if len(n[0]) &gt; 0 else [])\r\n\r\n",
               "id": "33517774",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1446628347,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 34322647,
               "is_accepted": false,
               "last_activity_date": 1450302176,
               "body_markdown": "    [AA[i:i+SS] for i in range(len(AA))[::SS]]\r\n\r\nWhere AA is array, SS is chunk size. For example:\r\n\r\n    &gt;&gt;&gt; AA=range(10,21);SS=3\r\n    &gt;&gt;&gt; [AA[i:i+SS] for i in range(len(AA))[::SS]]\r\n    [[10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]\r\n    # or [range(10, 13), range(13, 16), range(16, 19), range(19, 21)] in py3\r\n\r\n\r\n\r\n\r\n ",
               "id": "34322647",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1450302176,
               "score": 7
            },
            {
               "up_vote_count": 2,
               "answer_id": 38808533,
               "last_activity_date": 1473096331,
               "path": "3.stack.answer",
               "body_markdown": "As per [this answer](https://stackoverflow.com/a/21767522/15055), the top-voted answer leaves a &#39;runt&#39; at the end. Here&#39;s my solution to really get about as evenly-sized chunks as you can, with no runts. It basically tries to pick exactly the fractional spot where it should split the list, but just rounds it off to the nearest integer:\r\n\r\n    from __future__ import division  # not needed in Python 3\r\n    def n_even_chunks(l, n):\r\n        &quot;&quot;&quot;Yield n as even chunks as possible from l.&quot;&quot;&quot;\r\n        last = 0\r\n        for i in range(1, n+1):\r\n            cur = int(round(i * (len(l) / n)))\r\n            yield l[last:cur]\r\n            last = cur\r\n\r\nDemonstration:\r\n\r\n    &gt;&gt;&gt; pprint.pprint(list(n_even_chunks(list(range(100)), 9)))\r\n    [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\r\n     [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\r\n     [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\r\n     [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\r\n     [44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55],\r\n     [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66],\r\n     [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77],\r\n     [78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88],\r\n     [89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\r\n    &gt;&gt;&gt; pprint.pprint(list(n_even_chunks(list(range(100)), 11)))\r\n    [[0, 1, 2, 3, 4, 5, 6, 7, 8],\r\n     [9, 10, 11, 12, 13, 14, 15, 16, 17],\r\n     [18, 19, 20, 21, 22, 23, 24, 25, 26],\r\n     [27, 28, 29, 30, 31, 32, 33, 34, 35],\r\n     [36, 37, 38, 39, 40, 41, 42, 43, 44],\r\n     [45, 46, 47, 48, 49, 50, 51, 52, 53, 54],\r\n     [55, 56, 57, 58, 59, 60, 61, 62, 63],\r\n     [64, 65, 66, 67, 68, 69, 70, 71, 72],\r\n     [73, 74, 75, 76, 77, 78, 79, 80, 81],\r\n     [82, 83, 84, 85, 86, 87, 88, 89, 90],\r\n     [91, 92, 93, 94, 95, 96, 97, 98, 99]]\r\n\r\nCompare to the top-voted `chunks` answer:\r\n\r\n    &gt;&gt;&gt; pprint.pprint(list(chunks(list(range(100)), 100//9)))\r\n    [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\r\n     [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\r\n     [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\r\n     [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\r\n     [44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54],\r\n     [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65],\r\n     [66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76],\r\n     [77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87],\r\n     [88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98],\r\n     [99]]\r\n    &gt;&gt;&gt; pprint.pprint(list(chunks(list(range(100)), 100//11)))\r\n    [[0, 1, 2, 3, 4, 5, 6, 7, 8],\r\n     [9, 10, 11, 12, 13, 14, 15, 16, 17],\r\n     [18, 19, 20, 21, 22, 23, 24, 25, 26],\r\n     [27, 28, 29, 30, 31, 32, 33, 34, 35],\r\n     [36, 37, 38, 39, 40, 41, 42, 43, 44],\r\n     [45, 46, 47, 48, 49, 50, 51, 52, 53],\r\n     [54, 55, 56, 57, 58, 59, 60, 61, 62],\r\n     [63, 64, 65, 66, 67, 68, 69, 70, 71],\r\n     [72, 73, 74, 75, 76, 77, 78, 79, 80],\r\n     [81, 82, 83, 84, 85, 86, 87, 88, 89],\r\n     [90, 91, 92, 93, 94, 95, 96, 97, 98],\r\n     [99]]\r\n\r\n",
               "tags": [],
               "creation_date": 1470516247,
               "last_edit_date": 1495541909,
               "is_accepted": false,
               "id": "38808533",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 40409475,
               "is_accepted": false,
               "last_activity_date": 1478200245,
               "body_markdown": "Since everybody here talking about iterators. [`boltons`](https://boltons.readthedocs.io/) has perfect method for that, called [`iterutils.chunked_iter`](https://boltons.readthedocs.io/en/latest/iterutils.html#boltons.iterutils.chunked_iter).\r\n\r\n    from boltons import iterutils\r\n\r\n    list(iterutils.chunked_iter(list(range(50)), 11))\r\n\r\nOutput:\r\n\r\n    [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\r\n     [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\r\n     [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\r\n     [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\r\n     [44, 45, 46, 47, 48, 49]]\r\n\r\nBut if you don&#39;t want to be mercy on memory, you can use old-way and store the full `list` in the first place with [`iterutils.chunked`](https://boltons.readthedocs.io/en/latest/iterutils.html#boltons.iterutils.chunked).",
               "id": "40409475",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1478200245,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 40700737,
               "is_accepted": false,
               "last_activity_date": 1479616349,
               "body_markdown": "You could use numpy&#39;s array_split function e.g., `np.array_split(np.array(data), 20)` to split into 20 nearly equal size chunks.\r\n\r\nTo make sure chunks are exactly equal in size use `np.split`.",
               "id": "40700737",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1479616349,
               "score": 3
            },
            {
               "up_vote_count": 3,
               "answer_id": 41586849,
               "last_activity_date": 1513681474,
               "path": "3.stack.answer",
               "body_markdown": "I have one solution below which does work but more important than that solution is a few comments on other approaches.  First, a good solution shouldn&#39;t require that one loop through the sub-iterators in order.  If I run\r\n\r\n    g = paged_iter(list(range(50)), 11))\r\n    i0 = next(g)\r\n    i1 = next(g)\r\n    list(i1)\r\n    list(i0)\r\n\r\nThe appropriate output for the last command is   \r\n\r\n     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\nnot\r\n\r\n     []\r\n\r\nAs most of the itertools based solutions here return.  This isn&#39;t just the usual boring restriction about accessing iterators in order.  Imagine a consumer trying to clean up poorly entered data which reversed the appropriate order of blocks of 5, i.e., the data looks like [B5, A5, D5, C5] and should look like [A5, B5, C5, D5] (where A5 is just five elements not a sublist).  This consumer would look at the claimed behavior of the grouping function and not hesitate to write a loop like\r\n\r\n    i = 0\r\n    out = []\r\n    for it in paged_iter(data,5)\r\n        if (i % 2 == 0):\r\n             swapped = it\r\n        else: \r\n             out += list(it)\r\n             out += list(swapped)\r\n        i = i + 1\r\n\r\nThis will produce mysteriously wrong results if you sneakily assume that sub-iterators are always fully used in order.  It gets even worse if you want to interleave elements from the chunks. \r\n        \r\n\r\nSecond, a decent number of the suggested solutions implicitly rely on the fact that iterators have a deterministic order (they don&#39;t e.g. set) and while some of the solutions using islice may be ok it worries me.\r\n\r\nThird, the itertools grouper approach works but the recipe relies on internal behavior of the zip_longest (or zip) functions that isn&#39;t part of their published behavior.  In particular, the grouper function only works because in zip_longest(i0...in) the next function is always called in order next(i0), next(i1), ... next(in) before starting over.  As grouper passes n copies of the same iterator object it relies on this behavior.\r\n\r\nFinally, while the solution below can be improved if you make the assumption criticized above that sub-iterators are accessed in order and fully perused without this assumption one MUST implicitly (via call chain) or explicitly (via deques or other data structure) store elements for each subiterator somewhere.  So don&#39;t bother wasting time (as I did) assuming one could get around this with some clever trick.\r\n         \r\n    def paged_iter(iterat, n):\r\n        itr = iter(iterat)\r\n        deq = None\r\n        try:\r\n            while(True):\r\n                deq = collections.deque(maxlen=n)\r\n                for q in range(n):\r\n                    deq.append(next(itr))\r\n                yield (i for i in deq)\r\n        except StopIteration:\r\n            yield (i for i in deq)\r\n\r\n",
               "tags": [],
               "creation_date": 1484126333,
               "last_edit_date": 1513681474,
               "is_accepted": false,
               "id": "41586849",
               "down_vote_count": 1,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 41904532,
               "is_accepted": false,
               "last_activity_date": 1485558727,
               "body_markdown": "You may also use [`get_chunks`][1] function of [`utilspie`][2] library as:\r\n\r\n    &gt;&gt;&gt; from utilspie import iterutils\r\n    &gt;&gt;&gt; a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n    \r\n    &gt;&gt;&gt; list(iterutils.get_chunks(a, 5))\r\n    [[1, 2, 3, 4, 5], [6, 7, 8, 9]]\r\n\r\nYou can install [`utilspie`][3] via pip:\r\n\r\n    sudo pip install utilspie\r\n\r\n*Disclaimer: I am the creator of [utilspie][4] library*.\r\n\r\n\r\n  [1]: http://utilspie.readthedocs.io/en/latest/#get-chunks\r\n  [2]: http://utilspie.readthedocs.io\r\n  [3]: https://pypi.python.org/pypi/utilspie\r\n  [4]: https://github.com/moin18/utilspie",
               "id": "41904532",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1485558727,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 42677465,
               "is_accepted": false,
               "last_activity_date": 1488992626,
               "body_markdown": "Here&#39;s an idea using itertools.groupby:\r\n\r\n    def chunks(l, n):\r\n        c = itertools.count()\r\n        return (it for _, it in itertools.groupby(l, lambda x: next(c)//n))\r\n\r\nThis returns a generator of generators. If you want a list of lists, just replace the last line with\r\n\r\n        return [list(it) for _, it in itertools.groupby(l, lambda x: next(c)//n)]\r\n\r\nExample returning list of lists:\r\n\r\n    &gt;&gt;&gt; chunks(&#39;abcdefghij&#39;, 4)\r\n    [[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;], [&#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;], [&#39;i&#39;, &#39;j&#39;]]\r\n\r\n(So yes, this suffers form the &quot;runt problem&quot;, which may or may not be a problem in a given situation.)",
               "id": "42677465",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1488992626,
               "score": 1
            },
            {
               "up_vote_count": 3,
               "answer_id": 43454601,
               "last_activity_date": 1513679238,
               "path": "3.stack.answer",
               "body_markdown": "One more solution\r\n\r\n    def make_chunks(data, chunk_size): \r\n        while data:\r\n            chunk, data = data[:chunk_size], data[chunk_size:]\r\n            yield chunk\r\n\r\n    &gt;&gt;&gt; for chunk in make_chunks([1, 2, 3, 4, 5, 6, 7], 2):\r\n    ...     print chunk\r\n    ... \r\n    [1, 2]\r\n    [3, 4]\r\n    [5, 6]\r\n    [7]\r\n    &gt;&gt;&gt; \r\n",
               "tags": [],
               "creation_date": 1492443536,
               "last_edit_date": 1513679238,
               "is_accepted": false,
               "id": "43454601",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 44959796,
               "is_accepted": false,
               "last_activity_date": 1499379854,
               "body_markdown": "This works in v2/v3, is inlineable, generator-based and uses only the standard library:\r\n\r\n    import itertools\r\n    def split_groups(iter_in, group_size):\r\n        return ((x for _, x in item) for _, item in itertools.groupby(enumerate(iter_in), key=lambda x: x[0] // group_size))\r\n\r\n",
               "id": "44959796",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1499379854,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47010604,
               "is_accepted": false,
               "last_activity_date": 1509351226,
               "body_markdown": "No magic, but simple and correct:\r\n\r\n    def chunks(iterable, n):\r\n        &quot;&quot;&quot;Yield successive n-sized chunks from iterable.&quot;&quot;&quot;\r\n        values = []\r\n        for i, item in enumerate(iterable, 1):\r\n            values.append(item)\r\n            if i % n == 0:\r\n                yield values\r\n                values = []\r\n        if values:\r\n            yield values\r\n",
               "id": "47010604",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1509351226,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 47096024,
               "is_accepted": false,
               "last_activity_date": 1509712736,
               "body_markdown": "I don&#39;t think I saw this option, so just to add another one :)) :\r\n\r\n    def chunks(iterable, chunk_size):\r\n      i = 0;\r\n      while i &lt; len(iterable):\r\n        yield iterable[i:i+chunk_size]\r\n        i += chunk_size",
               "id": "47096024",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1509712736,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 48135727,
               "is_accepted": false,
               "last_activity_date": 1515315534,
               "body_markdown": "I was curious about the performance of different approaches and here it is:\r\n\r\nTested on Python 3.5.1\r\n\r\n    import time\r\n    batch_size = 7\r\n    arr_len = 298937\r\n    \r\n    #---------slice-------------\r\n    \r\n    print(&quot;\\r\\nslice&quot;)\r\n    start = time.time()\r\n    arr = [i for i in range(0, arr_len)]\r\n    while True:\r\n        if not arr:\r\n            break\r\n    \r\n        tmp = arr[0:batch_size]\r\n        arr = arr[batch_size:-1]\r\n    print(time.time() - start)\r\n    \r\n    #-----------index-----------\r\n    \r\n    print(&quot;\\r\\nindex&quot;)\r\n    arr = [i for i in range(0, arr_len)]\r\n    start = time.time()\r\n    for i in range(0, round(len(arr) / batch_size + 1)):\r\n        tmp = arr[batch_size * i : batch_size * (i + 1)]\r\n    print(time.time() - start)\r\n    \r\n    #----------batches 1------------\r\n    \r\n    def batch(iterable, n=1):\r\n        l = len(iterable)\r\n        for ndx in range(0, l, n):\r\n            yield iterable[ndx:min(ndx + n, l)]\r\n    \r\n    print(&quot;\\r\\nbatches 1&quot;)\r\n    arr = [i for i in range(0, arr_len)]\r\n    start = time.time()\r\n    for x in batch(arr, batch_size):\r\n        tmp = x\r\n    print(time.time() - start)\r\n    \r\n    #----------batches 2------------\r\n    \r\n    from itertools import islice, chain\r\n    \r\n    def batch(iterable, size):\r\n        sourceiter = iter(iterable)\r\n        while True:\r\n            batchiter = islice(sourceiter, size)\r\n            yield chain([next(batchiter)], batchiter)\r\n    \r\n    \r\n    print(&quot;\\r\\nbatches 2&quot;)\r\n    arr = [i for i in range(0, arr_len)]\r\n    start = time.time()\r\n    for x in batch(arr, batch_size):\r\n        tmp = x\r\n    print(time.time() - start)\r\n    \r\n    #---------chunks-------------\r\n    def chunks(l, n):\r\n        &quot;&quot;&quot;Yield successive n-sized chunks from l.&quot;&quot;&quot;\r\n        for i in range(0, len(l), n):\r\n            yield l[i:i + n]\r\n    print(&quot;\\r\\nchunks&quot;)\r\n    arr = [i for i in range(0, arr_len)]\r\n    start = time.time()\r\n    for x in chunks(arr, batch_size):\r\n        tmp = x\r\n    print(time.time() - start)\r\n    \r\n    #-----------grouper-----------\r\n    \r\n    from itertools import zip_longest # for Python 3.x\r\n    #from six.moves import zip_longest # for both (uses the six compat library)\r\n    \r\n    def grouper(iterable, n, padvalue=None):\r\n        &quot;grouper(3, &#39;abcdefg&#39;, &#39;x&#39;) --&gt; (&#39;a&#39;,&#39;b&#39;,&#39;c&#39;), (&#39;d&#39;,&#39;e&#39;,&#39;f&#39;), (&#39;g&#39;,&#39;x&#39;,&#39;x&#39;)&quot;\r\n        return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)\r\n    \r\n    arr = [i for i in range(0, arr_len)]\r\n    print(&quot;\\r\\ngrouper&quot;)\r\n    start = time.time()\r\n    for x in grouper(arr, batch_size):\r\n        tmp = x\r\n    print(time.time() - start)\r\n\r\n\r\n**Results:**\r\n\r\n    slice\r\n    31.18285083770752\r\n    \r\n    index\r\n    0.02184295654296875\r\n    \r\n    batches 1\r\n    0.03503894805908203\r\n    \r\n    batches 2\r\n    0.22681021690368652\r\n    \r\n    chunks\r\n    0.019841909408569336\r\n    \r\n    grouper\r\n    0.006506919860839844",
               "id": "48135727",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1515315534,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48920317,
               "is_accepted": false,
               "last_activity_date": 1519277653,
               "body_markdown": "one more solution \r\n\r\n    def chunk(xs, n):\r\n        &#39;&#39;&#39;Split the list, xs, into n evenly sized chunks&#39;&#39;&#39;\r\n        L = len(xs)\r\n        assert 0 &lt; n &lt;= L\r\n        s, r = divmod(L, n)\r\n        t = s + 1\r\n        return ([xs[p:p+t] for p in range(0, r*t, t)] +\r\n                [xs[p:p+s] for p in range(r*t, L, s)])\r\n\r\n",
               "id": "48920317",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519277653,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks",
         "id": "858127-2245"
      },
      {
         "up_vote_count": "440",
         "path": "2.stack",
         "body_markdown": "I have two lists in Python, like these:\r\n\r\n    temp1 = [&#39;One&#39;, &#39;Two&#39;, &#39;Three&#39;, &#39;Four&#39;]\r\n    temp2 = [&#39;One&#39;, &#39;Two&#39;]\r\n\r\nI need to create a third list with items from the first list which aren&#39;t present in the second one. From the example I have to get:\r\n\r\n    temp3 = [&#39;Three&#39;, &#39;Four&#39;]\r\n\r\nAre there any fast ways without cycles and checking?",
         "view_count": "401100",
         "answer_count": "22",
         "tags": "['python', 'performance', 'list', 'set', 'set-difference']",
         "creation_date": "1281555490",
         "last_edit_date": "1493471527",
         "code_snippet": "[\"<code>temp1 = ['One', 'Two', 'Three', 'Four']\\ntemp2 = ['One', 'Two']\\n</code>\", \"<code>temp3 = ['Three', 'Four']\\n</code>\", \"<code>temp1 = ['One', 'One', 'One']</code>\", \"<code>temp2 = ['One']</code>\", \"<code>['One', 'One']</code>\", '<code>[]</code>', \"<code>In [5]: list(set(temp1) - set(temp2))\\nOut[5]: ['Four', 'Three']\\n</code>\", '<code>In [5]: set([1, 2]) - set([2, 3])\\nOut[5]: set([1]) \\n</code>', '<code>set([1, 3])</code>', '<code>set([1, 3])</code>', '<code>set([1, 2]).symmetric_difference(set([2, 3]))</code>', '<code>s = set(temp2)\\ntemp3 = [x for x in temp1 if x not in s]\\n</code>', \"<code>import timeit\\ninit = 'temp1 = list(range(100)); temp2 = [i * 2 for i in range(50)]'\\nprint timeit.timeit('list(set(temp1) - set(temp2))', init, number = 100000)\\nprint timeit.timeit('s = set(temp2);[x for x in temp1 if x not in s]', init, number = 100000)\\nprint timeit.timeit('[item for item in temp1 if item not in temp2]', init, number = 100000)\\n</code>\", \"<code>4.34620224079 # ars' answer\\n4.2770634955  # This answer\\n30.7715615392 # matt b's answer\\n</code>\", \"<code>init = '''\\ntemp1 = [str(i) for i in range(100000)]\\ntemp2 = [str(i * 2) for i in range(50)]\\n'''\\n</code>\", \"<code>11.3836875916 # ars' answer\\n3.63890368748 # this answer (3 times faster!)\\n37.7445402279 # matt b's answer\\n</code>\", '<code>temp3 = [item for item in temp1 if item not in temp2]\\n</code>', '<code>temp2</code>', '<code>item not in temp2</code>', '<code>item not in set(temp2)</code>', '<code>temp2</code>', '<code>pip install deepdiff\\n</code>', '<code>&gt;&gt;&gt; from deepdiff import DeepDiff\\n&gt;&gt;&gt; from pprint import pprint\\n&gt;&gt;&gt; from __future__ import print_function # In case running on Python 2\\n</code>', '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3}\\n&gt;&gt;&gt; t2 = t1\\n&gt;&gt;&gt; print(DeepDiff(t1, t2))\\n{}\\n</code>', '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3}\\n&gt;&gt;&gt; t2 = {1:1, 2:\"2\", 3:3}\\n&gt;&gt;&gt; pprint(DeepDiff(t1, t2), indent=2)\\n{ \\'type_changes\\': { \\'root[2]\\': { \\'newtype\\': &lt;class \\'str\\'&gt;,\\n                                 \\'newvalue\\': \\'2\\',\\n                                 \\'oldtype\\': &lt;class \\'int\\'&gt;,\\n                                 \\'oldvalue\\': 2}}}\\n</code>', \"<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3}\\n&gt;&gt;&gt; t2 = {1:1, 2:4, 3:3}\\n&gt;&gt;&gt; pprint(DeepDiff(t1, t2), indent=2)\\n{'values_changed': {'root[2]': {'newvalue': 4, 'oldvalue': 2}}}\\n</code>\", \"<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:4}\\n&gt;&gt;&gt; t2 = {1:1, 2:4, 3:3, 5:5, 6:6}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\\n&gt;&gt;&gt; pprint (ddiff)\\n{'dic_item_added': ['root[5]', 'root[6]'],\\n 'dic_item_removed': ['root[4]'],\\n 'values_changed': {'root[2]': {'newvalue': 4, 'oldvalue': 2}}}\\n</code>\", '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":\"world\"}}\\n&gt;&gt;&gt; t2 = {1:1, 2:4, 3:3, 4:{\"a\":\"hello\", \"b\":\"world!\"}}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\\n&gt;&gt;&gt; pprint (ddiff, indent = 2)\\n{ \\'values_changed\\': { \\'root[2]\\': {\\'newvalue\\': 4, \\'oldvalue\\': 2},\\n                      \"root[4][\\'b\\']\": { \\'newvalue\\': \\'world!\\',\\n                                        \\'oldvalue\\': \\'world\\'}}}\\n</code>', '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":\"world!\\\\nGoodbye!\\\\n1\\\\n2\\\\nEnd\"}}\\n&gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":\"world\\\\n1\\\\n2\\\\nEnd\"}}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\\n&gt;&gt;&gt; pprint (ddiff, indent = 2)\\n{ \\'values_changed\\': { \"root[4][\\'b\\']\": { \\'diff\\': \\'--- \\\\n\\'\\n                                                \\'+++ \\\\n\\'\\n                                                \\'@@ -1,5 +1,4 @@\\\\n\\'\\n                                                \\'-world!\\\\n\\'\\n                                                \\'-Goodbye!\\\\n\\'\\n                                                \\'+world\\\\n\\'\\n                                                \\' 1\\\\n\\'\\n                                                \\' 2\\\\n\\'\\n                                                \\' End\\',\\n                                        \\'newvalue\\': \\'world\\\\n1\\\\n2\\\\nEnd\\',\\n                                        \\'oldvalue\\': \\'world!\\\\n\\'\\n                                                    \\'Goodbye!\\\\n\\'\\n                                                    \\'1\\\\n\\'\\n                                                    \\'2\\\\n\\'\\n                                                    \\'End\\'}}}\\n\\n&gt;&gt;&gt; \\n&gt;&gt;&gt; print (ddiff[\\'values_changed\\'][\"root[4][\\'b\\']\"][\"diff\"])\\n--- \\n+++ \\n@@ -1,5 +1,4 @@\\n-world!\\n-Goodbye!\\n+world\\n 1\\n 2\\n End\\n</code>', '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 2, 3]}}\\n&gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":\"world\\\\n\\\\n\\\\nEnd\"}}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\\n&gt;&gt;&gt; pprint (ddiff, indent = 2)\\n{ \\'type_changes\\': { \"root[4][\\'b\\']\": { \\'newtype\\': &lt;class \\'str\\'&gt;,\\n                                      \\'newvalue\\': \\'world\\\\n\\\\n\\\\nEnd\\',\\n                                      \\'oldtype\\': &lt;class \\'list\\'&gt;,\\n                                      \\'oldvalue\\': [1, 2, 3]}}}\\n</code>', '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 2, 3, 4]}}\\n&gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 2]}}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\\n&gt;&gt;&gt; pprint (ddiff, indent = 2)\\n{\\'iterable_item_removed\\': {\"root[4][\\'b\\'][2]\": 3, \"root[4][\\'b\\'][3]\": 4}}\\n</code>', '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 2, 3]}}\\n&gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 3, 2, 3]}}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\\n&gt;&gt;&gt; pprint (ddiff, indent = 2)\\n{ \\'iterable_item_added\\': {\"root[4][\\'b\\'][3]\": 3},\\n  \\'values_changed\\': { \"root[4][\\'b\\'][1]\": {\\'newvalue\\': 3, \\'oldvalue\\': 2},\\n                      \"root[4][\\'b\\'][2]\": {\\'newvalue\\': 2, \\'oldvalue\\': 3}}}\\n</code>', '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 2, 3]}}\\n&gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 3, 2, 3]}}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2, ignore_order=True)\\n&gt;&gt;&gt; print (ddiff)\\n{}\\n</code>', '<code>&gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 2, {1:1, 2:2}]}}\\n&gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{\"a\":\"hello\", \"b\":[1, 2, {1:3}]}}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\\n&gt;&gt;&gt; pprint (ddiff, indent = 2)\\n{ \\'dic_item_removed\\': [\"root[4][\\'b\\'][2][2]\"],\\n  \\'values_changed\\': {\"root[4][\\'b\\'][2][1]\": {\\'newvalue\\': 3, \\'oldvalue\\': 1}}}\\n</code>', \"<code>&gt;&gt;&gt; t1 = {1, 2, 8}\\n&gt;&gt;&gt; t2 = {1, 2, 3, 5}\\n&gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\\n&gt;&gt;&gt; pprint (DeepDiff(t1, t2))\\n{'set_item_added': ['root[3]', 'root[5]'], 'set_item_removed': ['root[8]']}\\n</code>\", \"<code>&gt;&gt;&gt; from collections import namedtuple\\n&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])\\n&gt;&gt;&gt; t1 = Point(x=11, y=22)\\n&gt;&gt;&gt; t2 = Point(x=11, y=23)\\n&gt;&gt;&gt; pprint (DeepDiff(t1, t2))\\n{'values_changed': {'root.y': {'newvalue': 23, 'oldvalue': 22}}}\\n</code>\", \"<code>&gt;&gt;&gt; class ClassA(object):\\n...     a = 1\\n...     def __init__(self, b):\\n...         self.b = b\\n... \\n&gt;&gt;&gt; t1 = ClassA(1)\\n&gt;&gt;&gt; t2 = ClassA(2)\\n&gt;&gt;&gt; \\n&gt;&gt;&gt; pprint(DeepDiff(t1, t2))\\n{'values_changed': {'root.b': {'newvalue': 2, 'oldvalue': 1}}}\\n</code>\", '<code>&gt;&gt;&gt; t2.c = \"new attribute\"\\n&gt;&gt;&gt; pprint(DeepDiff(t1, t2))\\n{\\'attribute_added\\': [\\'root.c\\'],\\n \\'values_changed\\': {\\'root.b\\': {\\'newvalue\\': 2, \\'oldvalue\\': 1}}}\\n</code>', '<code>def diff(list1, list2):\\n    c = set(list1).union(set(list2))  # or c = set(list1) | set(list2)\\n    d = set(list1).intersection(set(list2))  # or d = set(list1) &amp; set(list2)\\n    return list(c - d)\\n</code>', '<code>def diff(list1, list2):\\n    return list(set(list1).symmetric_difference(set(list2)))  # or return list(set(list1) ^ set(list2))\\n</code>', '<code>diff(temp2, temp1)</code>', '<code>diff(temp1, temp2)</code>', \"<code>['Four', 'Three']</code>\", '<code>temp3 = tuple(set(temp1) - set(temp2))\\n</code>', '<code>#edited using @Mark Byers idea. If you accept this one as answer, just accept his instead.\\ntemp3 = tuple(x for x in temp1 if x not in set(temp2))\\n</code>', '<code>set(temp1) ^ set(temp2)\\n</code>', \"<code>temp1 = ['1','2','3'], temp2 =['2','3','4'] , answer : ['3','2']</code>\", '<code>temp3 = set(temp1) - set(temp2)\\n</code>', '<code>filterfalse(set(temp2).__contains__, temp1)\\n</code>', '<code>from itertools import filterfalse</code>', '<code>diff</code>', '<code>bash</code>', '<code>SequenceMather</code>', '<code>difflib</code>', '<code>diff</code>', \"<code>a = 'A quick fox jumps the lazy dog'.split()\\nb = 'A quick brown mouse jumps over the dog'.split()\\n\\nfrom difflib import SequenceMatcher\\n\\nfor tag, i, j, k, l in SequenceMatcher(None, a, b).get_opcodes():\\n  if tag == 'equal': print('both have', a[i:j])\\n  if tag in ('delete', 'replace'): print('  1st has', a[i:j])\\n  if tag in ('insert', 'replace'): print('  2nd has', b[k:l])\\n</code>\", \"<code>both have ['A', 'quick']\\n  1st has ['fox']\\n  2nd has ['brown', 'mouse']\\nboth have ['jumps']\\n  2nd has ['over']\\nboth have ['the']\\n  1st has ['lazy']\\nboth have ['dog']\\n</code>\", '<code>diff</code>', '<code>a = [1,2,3,4,5]\\nb = [5,4,3,2,1]\\n</code>', '<code>  2nd has [5, 4, 3, 2]\\nboth have [1]\\n  1st has [2, 3, 4, 5]\\n</code>', '<code>list1=[1,2,3,4,5]\\nlist2=[1,2,3]\\n\\nprint list1[len(list2):]\\n</code>', '<code>subset=set(list1).difference(list2)\\n\\nprint subset\\n\\nimport timeit\\ninit = \\'temp1 = list(range(100)); temp2 = [i * 2 for i in range(50)]\\'\\nprint \"Naive solution: \", timeit.timeit(\\'temp1[len(temp2):]\\', init, number = 100000)\\nprint \"Native set solution: \", timeit.timeit(\\'set(temp1).difference(temp2)\\', init, number = 100000)\\n</code>', \"<code>TypeError: unhashable type: 'list'</code>\", '<code>set(map(tuple, list_of_lists1)).symmetric_difference(set(map(tuple, list_of_lists2)))\\n</code>', '<code>Counter</code>', \"<code>from collections import Counter\\n\\nlst1 = ['One', 'Two', 'Three', 'Four']\\nlst2 = ['One', 'Two']\\n\\nc1 = Counter(lst1)\\nc2 = Counter(lst2)\\ndiff = list((c1 - c2).elements())\\n</code>\", '<code>diff = list((Counter(lst1) - Counter(lst2)).elements())\\n</code>', \"<code>['Three', 'Four']\\n</code>\", '<code>list(...)</code>', \"<code>lst1 = ['One', 'Two', 'Two', 'Two', 'Three', 'Three', 'Four']\\nlst2 = ['One', 'Two']\\n</code>\", \"<code>['Two', 'Two', 'Three', 'Three', 'Four']\\n</code>\", '<code>def diff(a, b):\\n    xa = [i for i in set(a) if i not in b]\\n    xb = [i for i in set(b) if i not in a]\\n    return xa + xb\\n</code>', '<code>def diff(listA, listB):\\n    return set(listA) - set(listB) | set(listA) -set(listB)\\n</code>', '<code>from collections import Counter\\n\\ndef diff(a, b):\\n  \"\"\" more verbose than needs to be, for clarity \"\"\"\\n  ca, cb = Counter(a), Counter(b)\\n  to_add = cb - ca\\n  to_remove = ca - cb\\n  changes = Counter(to_add)\\n  changes.subtract(to_remove)\\n  return changes\\n\\nlista = [\\'one\\', \\'three\\', \\'four\\', \\'four\\', \\'one\\']\\nlistb = [\\'one\\', \\'two\\', \\'three\\']\\n\\nIn [127]: diff(lista, listb)\\nOut[127]: Counter({\\'two\\': 1, \\'one\\': -1, \\'four\\': -2})\\n# in order to go from lista to list b, you need to add a \"two\", remove a \"one\", and remove two \"four\"s\\n\\nIn [128]: diff(listb, lista)\\nOut[128]: Counter({\\'four\\': 2, \\'one\\': 1, \\'two\\': -1})\\n# in order to go from listb to lista, you must add two \"four\"s, add a \"one\", and remove a \"two\"\\n</code>', \"<code>temp1 = ['One', 'Two', 'Three', 'Four']\\ntemp2 = ['One', 'Two', 'Five']\\n\\nset(temp1+temp2)-(set(temp1)&amp;set(temp2))\\n\\nOut: set(['Four', 'Five', 'Three']) \\n</code>\", '<code>temp3 = list(set(temp1).difference(set(temp2)))\\n</code>', '<code>list_a = [1,2,3]\\nlist_b = [2,3]\\nprint set(list_a).difference(set(list_b))\\n</code>', '<code>set([1])</code>', '<code>print list(set(list_a).difference(set(list_b)))\\n</code>', '<code>(list(set(a)-set(b))+list(set(b)-set(a)))\\n</code>', '<code>list(set(x).symmetric_difference(set(y)))\\nlist(set(x) ^ set(y))\\n</code>', '<code>import time\\nimport random\\nfrom itertools import filterfalse\\n\\n# 1 - performance (time taken)\\n# 2 - correctness (answer - 1,4,5,6)\\n# set performance\\nperformance = 1\\nnumberoftests = 7\\n\\ndef answer(x,y,z):\\n    if z == 0:\\n        start = time.clock()\\n        lists = (str(list(set(x)-set(y))+list(set(y)-set(y))))\\n        times = (\"1 = \" + str(time.clock() - start))\\n        return (lists,times)\\n\\n    elif z == 1:\\n        start = time.clock()\\n        lists = (str(list(set(x).symmetric_difference(set(y)))))\\n        times = (\"2 = \" + str(time.clock() - start))\\n        return (lists,times)\\n\\n    elif z == 2:\\n        start = time.clock()\\n        lists = (str(list(set(x) ^ set(y))))\\n        times = (\"3 = \" + str(time.clock() - start))\\n        return (lists,times)\\n\\n    elif z == 3:\\n        start = time.clock()\\n        lists = (filterfalse(set(y).__contains__, x))\\n        times = (\"4 = \" + str(time.clock() - start))\\n        return (lists,times)\\n\\n    elif z == 4:\\n        start = time.clock()\\n        lists = (tuple(set(x) - set(y)))\\n        times = (\"5 = \" + str(time.clock() - start))\\n        return (lists,times)\\n\\n    elif z == 5:\\n        start = time.clock()\\n        lists = ([tt for tt in x if tt not in y])\\n        times = (\"6 = \" + str(time.clock() - start))\\n        return (lists,times)\\n\\n    else:    \\n        start = time.clock()\\n        Xarray = [iDa for iDa in x if iDa not in y]\\n        Yarray = [iDb for iDb in y if iDb not in x]\\n        lists = (str(Xarray + Yarray))\\n        times = (\"7 = \" + str(time.clock() - start))\\n        return (lists,times)\\n\\nn = numberoftests\\n\\nif performance == 2:\\n    a = [1,2,3,4,5]\\n    b = [3,2,6]\\n    for c in range(0,n):\\n        d = answer(a,b,c)\\n        print(d[0])\\n\\nelif performance == 1:\\n    for tests in range(0,10):\\n        print(\"Test Number\" + str(tests + 1))\\n        a = random.sample(range(1, 900000), 9999)\\n        b = random.sample(range(1, 900000), 9999)\\n        for c in range(0,n):\\n            #if c not in (1,4,5,6):\\n            d = answer(a,b,c)\\n            print(d[1])\\n</code>']",
         "title": "Get difference between two lists",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 3462159,
               "is_accepted": false,
               "last_activity_date": 1281555599,
               "body_markdown": "Try this:\r\n\r\n    temp3 = set(temp1) - set(temp2)",
               "id": "3462159",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1281555599,
               "score": 7
            },
            {
               "up_vote_count": 682,
               "answer_id": 3462160,
               "last_activity_date": 1495194655,
               "path": "3.stack.answer",
               "body_markdown": "    In [5]: list(set(temp1) - set(temp2))\r\n    Out[5]: [&#39;Four&#39;, &#39;Three&#39;]\r\n\r\nBeware that \r\n\r\n    In [5]: set([1, 2]) - set([2, 3])\r\n    Out[5]: set([1]) \r\n\r\nwhere you might expect/want it to equal `set([1, 3])`. If you do want `set([1, 3])` as your answer, you&#39;ll need to use `set([1, 2]).symmetric_difference(set([2, 3]))`.",
               "tags": [],
               "creation_date": 1281555600,
               "last_edit_date": 1495194655,
               "is_accepted": true,
               "id": "3462160",
               "down_vote_count": 2,
               "score": 680
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 40,
               "answer_id": 3462164,
               "is_accepted": false,
               "last_activity_date": 1281555627,
               "body_markdown": "    temp3 = [item for item in temp1 if item not in temp2]",
               "id": "3462164",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1281555627,
               "score": 39
            },
            {
               "up_vote_count": 11,
               "answer_id": 3462181,
               "last_activity_date": 1281560238,
               "path": "3.stack.answer",
               "body_markdown": "i&#39;ll toss in since none of the present solutions yield a tuple:\r\n\r\n    temp3 = tuple(set(temp1) - set(temp2))\r\n\r\nalternatively:\r\n\r\n    #edited using @Mark Byers idea. If you accept this one as answer, just accept his instead.\r\n    temp3 = tuple(x for x in temp1 if x not in set(temp2))\r\n\r\nLike the other non-tuple yielding answers in this direction, it preserves order\r\n\r\n",
               "tags": [],
               "creation_date": 1281555726,
               "last_edit_date": 1281560238,
               "is_accepted": false,
               "id": "3462181",
               "down_vote_count": 0,
               "score": 11
            },
            {
               "up_vote_count": 360,
               "answer_id": 3462202,
               "last_activity_date": 1281558925,
               "path": "3.stack.answer",
               "body_markdown": "The existing solutions all offer either one or the other of:\r\n\r\n * Faster than O(n*m) performance.\r\n * Preserve order of input list.\r\n\r\nBut so far no solution has both. If you want both, try this:\r\n\r\n    s = set(temp2)\r\n    temp3 = [x for x in temp1 if x not in s]\r\n\r\n**Performance test**\r\n\r\n    import timeit\r\n    init = &#39;temp1 = list(range(100)); temp2 = [i * 2 for i in range(50)]&#39;\r\n    print timeit.timeit(&#39;list(set(temp1) - set(temp2))&#39;, init, number = 100000)\r\n    print timeit.timeit(&#39;s = set(temp2);[x for x in temp1 if x not in s]&#39;, init, number = 100000)\r\n    print timeit.timeit(&#39;[item for item in temp1 if item not in temp2]&#39;, init, number = 100000)\r\n\r\nResults:\r\n\r\n    4.34620224079 # ars&#39; answer\r\n    4.2770634955  # This answer\r\n    30.7715615392 # matt b&#39;s answer\r\n\r\nThe method I presented as well as preserving order is also (slightly) faster than the set subtraction because it doesn&#39;t require construction of an unnecessary set. The performance difference would be more noticable if the first list is considerably longer than the second and if hashing is expensive. Here&#39;s a second test demonstrating this:\r\n\r\n    init = &#39;&#39;&#39;\r\n    temp1 = [str(i) for i in range(100000)]\r\n    temp2 = [str(i * 2) for i in range(50)]\r\n    &#39;&#39;&#39;\r\n\r\nResults:\r\n\r\n    11.3836875916 # ars&#39; answer\r\n    3.63890368748 # this answer (3 times faster!)\r\n    37.7445402279 # matt b&#39;s answer\r\n\r\n",
               "tags": [],
               "creation_date": 1281555887,
               "last_edit_date": 1281558925,
               "is_accepted": false,
               "id": "3462202",
               "down_vote_count": 0,
               "score": 360
            },
            {
               "up_vote_count": 7,
               "answer_id": 7103048,
               "last_activity_date": 1316291188,
               "path": "3.stack.answer",
               "body_markdown": "this could be even faster than Mark&#39;s list comprehension:\r\n\r\n    filterfalse(set(temp2).__contains__, temp1)\r\n\r\n",
               "tags": [],
               "creation_date": 1313647288,
               "last_edit_date": 1316291188,
               "is_accepted": false,
               "id": "7103048",
               "down_vote_count": 0,
               "score": 7
            },
            {
               "up_vote_count": 14,
               "answer_id": 12005040,
               "last_activity_date": 1499256437,
               "path": "3.stack.answer",
               "body_markdown": "The difference between two lists (say list1 and list2) can be found using the following simple function.\r\n\r\n    def diff(list1, list2):\r\n        c = set(list1).union(set(list2))  # or c = set(list1) | set(list2)\r\n        d = set(list1).intersection(set(list2))  # or d = set(list1) &amp; set(list2)\r\n        return list(c - d)\r\n\r\nor\r\n\r\n    def diff(list1, list2):\r\n        return list(set(list1).symmetric_difference(set(list2)))  # or return list(set(list1) ^ set(list2))\r\n\r\nBy Using the above function, the difference can be found using `diff(temp2, temp1)` or `diff(temp1, temp2)`. Both will give the result `[&#39;Four&#39;, &#39;Three&#39;]`. You don&#39;t have to worry about the order of the list or which list is to be given first.\r\n\r\n[Python doc reference](https://docs.python.org/2/library/sets.html#set-objects)",
               "tags": [],
               "creation_date": 1345203480,
               "last_edit_date": 1499256437,
               "is_accepted": false,
               "id": "12005040",
               "down_vote_count": 1,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 22726310,
               "is_accepted": false,
               "last_activity_date": 1396060591,
               "body_markdown": "This is another solution:\r\n\r\n    def diff(a, b):\r\n        xa = [i for i in set(a) if i not in b]\r\n        xb = [i for i in set(b) if i not in a]\r\n        return xa + xb",
               "id": "22726310",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1396060591,
               "score": 2
            },
            {
               "up_vote_count": 3,
               "answer_id": 23934118,
               "last_activity_date": 1401369684,
               "path": "3.stack.answer",
               "body_markdown": "You could use a naive method if the elements of the difflist are sorted and sets.\r\n\r\n    list1=[1,2,3,4,5]\r\n    list2=[1,2,3]\r\n\r\n    print list1[len(list2):]\r\n\r\n\r\nor with native set methods:\r\n\r\n    subset=set(list1).difference(list2)\r\n\r\n    print subset\r\n\r\n    import timeit\r\n    init = &#39;temp1 = list(range(100)); temp2 = [i * 2 for i in range(50)]&#39;\r\n    print &quot;Naive solution: &quot;, timeit.timeit(&#39;temp1[len(temp2):]&#39;, init, number = 100000)\r\n    print &quot;Native set solution: &quot;, timeit.timeit(&#39;set(temp1).difference(temp2)&#39;, init, number = 100000)\r\n\r\nNaive solution:  0.0787101593292\r\n\r\nNative set solution:  0.998837615564\r\n\r\n",
               "tags": [],
               "creation_date": 1401368891,
               "last_edit_date": 1401369684,
               "is_accepted": false,
               "id": "23934118",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 2,
               "answer_id": 24429858,
               "last_activity_date": 1403784164,
               "path": "3.stack.answer",
               "body_markdown": "single line version of **arulmr** solution\r\n\r\n    def diff(listA, listB):\r\n        return set(listA) - set(listB) | set(listA) -set(listB)",
               "tags": [],
               "creation_date": 1403783699,
               "last_edit_date": 1403784164,
               "is_accepted": false,
               "id": "24429858",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 3,
               "answer_id": 24674161,
               "last_activity_date": 1404988001,
               "path": "3.stack.answer",
               "body_markdown": "If you run into `TypeError: unhashable type: &#39;list&#39;` you need to turn lists or sets into tuples, e.g.\r\n\r\n    set(map(tuple, list_of_lists1)).symmetric_difference(set(map(tuple, list_of_lists2)))\r\n\r\nSee also https://stackoverflow.com/questions/6105777/how-to-compare-a-list-of-lists-sets-in-python",
               "tags": [],
               "creation_date": 1404988001,
               "last_edit_date": 1495541441,
               "is_accepted": false,
               "id": "24674161",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 13,
               "answer_id": 26079411,
               "last_activity_date": 1457647055,
               "path": "3.stack.answer",
               "body_markdown": "In case you want the difference recursively, I have written a package for python:\r\nhttps://github.com/seperman/deepdiff\r\n\r\n##Installation\r\n\r\nInstall from PyPi:\r\n\r\n    pip install deepdiff\r\n\r\n##Example usage\r\n\r\nImporting\r\n\r\n    &gt;&gt;&gt; from deepdiff import DeepDiff\r\n    &gt;&gt;&gt; from pprint import pprint\r\n    &gt;&gt;&gt; from __future__ import print_function # In case running on Python 2\r\n\r\nSame object returns empty\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3}\r\n    &gt;&gt;&gt; t2 = t1\r\n    &gt;&gt;&gt; print(DeepDiff(t1, t2))\r\n    {}\r\n\r\nType of an item has changed\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:&quot;2&quot;, 3:3}\r\n    &gt;&gt;&gt; pprint(DeepDiff(t1, t2), indent=2)\r\n    { &#39;type_changes&#39;: { &#39;root[2]&#39;: { &#39;newtype&#39;: &lt;class &#39;str&#39;&gt;,\r\n                                     &#39;newvalue&#39;: &#39;2&#39;,\r\n                                     &#39;oldtype&#39;: &lt;class &#39;int&#39;&gt;,\r\n                                     &#39;oldvalue&#39;: 2}}}\r\n\r\nValue of an item has changed\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:4, 3:3}\r\n    &gt;&gt;&gt; pprint(DeepDiff(t1, t2), indent=2)\r\n    {&#39;values_changed&#39;: {&#39;root[2]&#39;: {&#39;newvalue&#39;: 4, &#39;oldvalue&#39;: 2}}}\r\n\r\nItem added and/or removed\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:4}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:4, 3:3, 5:5, 6:6}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\r\n    &gt;&gt;&gt; pprint (ddiff)\r\n    {&#39;dic_item_added&#39;: [&#39;root[5]&#39;, &#39;root[6]&#39;],\r\n     &#39;dic_item_removed&#39;: [&#39;root[4]&#39;],\r\n     &#39;values_changed&#39;: {&#39;root[2]&#39;: {&#39;newvalue&#39;: 4, &#39;oldvalue&#39;: 2}}}\r\n\r\nString difference\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:&quot;world&quot;}}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:4, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:&quot;world!&quot;}}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\r\n    &gt;&gt;&gt; pprint (ddiff, indent = 2)\r\n    { &#39;values_changed&#39;: { &#39;root[2]&#39;: {&#39;newvalue&#39;: 4, &#39;oldvalue&#39;: 2},\r\n                          &quot;root[4][&#39;b&#39;]&quot;: { &#39;newvalue&#39;: &#39;world!&#39;,\r\n                                            &#39;oldvalue&#39;: &#39;world&#39;}}}\r\n\r\n\r\nString difference 2\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:&quot;world!\\nGoodbye!\\n1\\n2\\nEnd&quot;}}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:&quot;world\\n1\\n2\\nEnd&quot;}}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\r\n    &gt;&gt;&gt; pprint (ddiff, indent = 2)\r\n    { &#39;values_changed&#39;: { &quot;root[4][&#39;b&#39;]&quot;: { &#39;diff&#39;: &#39;--- \\n&#39;\r\n                                                    &#39;+++ \\n&#39;\r\n                                                    &#39;@@ -1,5 +1,4 @@\\n&#39;\r\n                                                    &#39;-world!\\n&#39;\r\n                                                    &#39;-Goodbye!\\n&#39;\r\n                                                    &#39;+world\\n&#39;\r\n                                                    &#39; 1\\n&#39;\r\n                                                    &#39; 2\\n&#39;\r\n                                                    &#39; End&#39;,\r\n                                            &#39;newvalue&#39;: &#39;world\\n1\\n2\\nEnd&#39;,\r\n                                            &#39;oldvalue&#39;: &#39;world!\\n&#39;\r\n                                                        &#39;Goodbye!\\n&#39;\r\n                                                        &#39;1\\n&#39;\r\n                                                        &#39;2\\n&#39;\r\n                                                        &#39;End&#39;}}}\r\n\r\n    &gt;&gt;&gt; \r\n    &gt;&gt;&gt; print (ddiff[&#39;values_changed&#39;][&quot;root[4][&#39;b&#39;]&quot;][&quot;diff&quot;])\r\n    --- \r\n    +++ \r\n    @@ -1,5 +1,4 @@\r\n    -world!\r\n    -Goodbye!\r\n    +world\r\n     1\r\n     2\r\n     End\r\n\r\nType change\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 2, 3]}}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:&quot;world\\n\\n\\nEnd&quot;}}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\r\n    &gt;&gt;&gt; pprint (ddiff, indent = 2)\r\n    { &#39;type_changes&#39;: { &quot;root[4][&#39;b&#39;]&quot;: { &#39;newtype&#39;: &lt;class &#39;str&#39;&gt;,\r\n                                          &#39;newvalue&#39;: &#39;world\\n\\n\\nEnd&#39;,\r\n                                          &#39;oldtype&#39;: &lt;class &#39;list&#39;&gt;,\r\n                                          &#39;oldvalue&#39;: [1, 2, 3]}}}\r\n\r\nList difference\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 2, 3, 4]}}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 2]}}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\r\n    &gt;&gt;&gt; pprint (ddiff, indent = 2)\r\n    {&#39;iterable_item_removed&#39;: {&quot;root[4][&#39;b&#39;][2]&quot;: 3, &quot;root[4][&#39;b&#39;][3]&quot;: 4}}\r\n\r\nList difference 2:\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 2, 3]}}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 3, 2, 3]}}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\r\n    &gt;&gt;&gt; pprint (ddiff, indent = 2)\r\n    { &#39;iterable_item_added&#39;: {&quot;root[4][&#39;b&#39;][3]&quot;: 3},\r\n      &#39;values_changed&#39;: { &quot;root[4][&#39;b&#39;][1]&quot;: {&#39;newvalue&#39;: 3, &#39;oldvalue&#39;: 2},\r\n                          &quot;root[4][&#39;b&#39;][2]&quot;: {&#39;newvalue&#39;: 2, &#39;oldvalue&#39;: 3}}}\r\n\r\nList difference ignoring order or duplicates: (with the same dictionaries as above)\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 2, 3]}}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 3, 2, 3]}}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2, ignore_order=True)\r\n    &gt;&gt;&gt; print (ddiff)\r\n    {}\r\n\r\nList that contains dictionary:\r\n\r\n    &gt;&gt;&gt; t1 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 2, {1:1, 2:2}]}}\r\n    &gt;&gt;&gt; t2 = {1:1, 2:2, 3:3, 4:{&quot;a&quot;:&quot;hello&quot;, &quot;b&quot;:[1, 2, {1:3}]}}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\r\n    &gt;&gt;&gt; pprint (ddiff, indent = 2)\r\n    { &#39;dic_item_removed&#39;: [&quot;root[4][&#39;b&#39;][2][2]&quot;],\r\n      &#39;values_changed&#39;: {&quot;root[4][&#39;b&#39;][2][1]&quot;: {&#39;newvalue&#39;: 3, &#39;oldvalue&#39;: 1}}}\r\n\r\nSets:\r\n\r\n    &gt;&gt;&gt; t1 = {1, 2, 8}\r\n    &gt;&gt;&gt; t2 = {1, 2, 3, 5}\r\n    &gt;&gt;&gt; ddiff = DeepDiff(t1, t2)\r\n    &gt;&gt;&gt; pprint (DeepDiff(t1, t2))\r\n    {&#39;set_item_added&#39;: [&#39;root[3]&#39;, &#39;root[5]&#39;], &#39;set_item_removed&#39;: [&#39;root[8]&#39;]}\r\n\r\nNamed Tuples:\r\n\r\n    &gt;&gt;&gt; from collections import namedtuple\r\n    &gt;&gt;&gt; Point = namedtuple(&#39;Point&#39;, [&#39;x&#39;, &#39;y&#39;])\r\n    &gt;&gt;&gt; t1 = Point(x=11, y=22)\r\n    &gt;&gt;&gt; t2 = Point(x=11, y=23)\r\n    &gt;&gt;&gt; pprint (DeepDiff(t1, t2))\r\n    {&#39;values_changed&#39;: {&#39;root.y&#39;: {&#39;newvalue&#39;: 23, &#39;oldvalue&#39;: 22}}}\r\n\r\nCustom objects:\r\n\r\n    &gt;&gt;&gt; class ClassA(object):\r\n    ...     a = 1\r\n    ...     def __init__(self, b):\r\n    ...         self.b = b\r\n    ... \r\n    &gt;&gt;&gt; t1 = ClassA(1)\r\n    &gt;&gt;&gt; t2 = ClassA(2)\r\n    &gt;&gt;&gt; \r\n    &gt;&gt;&gt; pprint(DeepDiff(t1, t2))\r\n    {&#39;values_changed&#39;: {&#39;root.b&#39;: {&#39;newvalue&#39;: 2, &#39;oldvalue&#39;: 1}}}\r\n\r\nObject attribute added:\r\n\r\n    &gt;&gt;&gt; t2.c = &quot;new attribute&quot;\r\n    &gt;&gt;&gt; pprint(DeepDiff(t1, t2))\r\n    {&#39;attribute_added&#39;: [&#39;root.c&#39;],\r\n     &#39;values_changed&#39;: {&#39;root.b&#39;: {&#39;newvalue&#39;: 2, &#39;oldvalue&#39;: 1}}}\r\n\r\n",
               "tags": [],
               "creation_date": 1411853422,
               "last_edit_date": 1457647055,
               "is_accepted": false,
               "id": "26079411",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 12,
               "answer_id": 31881491,
               "last_activity_date": 1493449452,
               "path": "3.stack.answer",
               "body_markdown": "If you are really looking into performance, then use numpy!\r\n\r\nHere is the full notebook as a gist on github with comparison between list, numpy, and pandas.\r\n\r\nhttps://gist.github.com/denfromufa/2821ff59b02e9482be15d27f2bbd4451\r\n\r\n\r\n \r\n   [![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/lhT55.png",
               "tags": [],
               "creation_date": 1438961458,
               "last_edit_date": 1493449452,
               "is_accepted": false,
               "id": "31881491",
               "down_vote_count": 0,
               "score": 12
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 34345375,
               "is_accepted": false,
               "last_activity_date": 1450390850,
               "body_markdown": "if you want something more like a changeset... could use Counter\r\n\r\n    from collections import Counter\r\n    \r\n    def diff(a, b):\r\n      &quot;&quot;&quot; more verbose than needs to be, for clarity &quot;&quot;&quot;\r\n      ca, cb = Counter(a), Counter(b)\r\n      to_add = cb - ca\r\n      to_remove = ca - cb\r\n      changes = Counter(to_add)\r\n      changes.subtract(to_remove)\r\n      return changes\r\n\r\n    lista = [&#39;one&#39;, &#39;three&#39;, &#39;four&#39;, &#39;four&#39;, &#39;one&#39;]\r\n    listb = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]\r\n\r\n    In [127]: diff(lista, listb)\r\n    Out[127]: Counter({&#39;two&#39;: 1, &#39;one&#39;: -1, &#39;four&#39;: -2})\r\n    # in order to go from lista to list b, you need to add a &quot;two&quot;, remove a &quot;one&quot;, and remove two &quot;four&quot;s\r\n\r\n    In [128]: diff(listb, lista)\r\n    Out[128]: Counter({&#39;four&#39;: 2, &#39;one&#39;: 1, &#39;two&#39;: -1})\r\n    # in order to go from listb to lista, you must add two &quot;four&quot;s, add a &quot;one&quot;, and remove a &quot;two&quot;",
               "id": "34345375",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1450390850,
               "score": 2
            },
            {
               "up_vote_count": 6,
               "answer_id": 35855314,
               "last_activity_date": 1457390127,
               "path": "3.stack.answer",
               "body_markdown": "I wanted something that would take two lists and could do what `diff` in `bash` does. Since this question pops up first when you search for &quot;python diff two lists&quot; and is not very specific, I will post what I came up with.\r\n\r\nUsing [`SequenceMather`][1] from `difflib` you can compare two lists like `diff` does. None of the other answers will tell you the position where the difference occurs, but this one does. Some answers give the difference in only one direction. Some reorder the elements. Some don&#39;t handle duplicates. But this solution gives you a true difference between two lists:\r\n\r\n    a = &#39;A quick fox jumps the lazy dog&#39;.split()\r\n    b = &#39;A quick brown mouse jumps over the dog&#39;.split()\r\n    \r\n    from difflib import SequenceMatcher\r\n    \r\n    for tag, i, j, k, l in SequenceMatcher(None, a, b).get_opcodes():\r\n      if tag == &#39;equal&#39;: print(&#39;both have&#39;, a[i:j])\r\n      if tag in (&#39;delete&#39;, &#39;replace&#39;): print(&#39;  1st has&#39;, a[i:j])\r\n      if tag in (&#39;insert&#39;, &#39;replace&#39;): print(&#39;  2nd has&#39;, b[k:l])\r\n\r\nThis outputs:\r\n\r\n    both have [&#39;A&#39;, &#39;quick&#39;]\r\n      1st has [&#39;fox&#39;]\r\n      2nd has [&#39;brown&#39;, &#39;mouse&#39;]\r\n    both have [&#39;jumps&#39;]\r\n      2nd has [&#39;over&#39;]\r\n    both have [&#39;the&#39;]\r\n      1st has [&#39;lazy&#39;]\r\n    both have [&#39;dog&#39;]\r\n\r\nOf course, if your application makes the same assumptions the other answers make, you will benefit from them the most. But if you are looking for a true `diff` functionality, then this is the only way to go.\r\n\r\nFor example, none of the other answers could handle:\r\n\r\n    a = [1,2,3,4,5]\r\n    b = [5,4,3,2,1]\r\n\r\nBut this one does:\r\n\r\n      2nd has [5, 4, 3, 2]\r\n    both have [1]\r\n      1st has [2, 3, 4, 5]\r\n\r\n  [1]: https://docs.python.org/3.5/library/difflib.html#difflib.SequenceMatcher",
               "tags": [],
               "creation_date": 1457389410,
               "last_edit_date": 1457390127,
               "is_accepted": false,
               "id": "35855314",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "up_vote_count": 10,
               "answer_id": 38240169,
               "last_activity_date": 1474262716,
               "path": "3.stack.answer",
               "body_markdown": "Can be done using python XOR operator.\r\n\r\n- This will remove the duplicates in each list\r\n- This will show difference of temp1 from temp2 and temp2 from temp1.\r\n\r\n-----------------\r\n\r\n\r\n\r\n    set(temp1) ^ set(temp2)",
               "tags": [],
               "creation_date": 1467877804,
               "last_edit_date": 1474262716,
               "is_accepted": false,
               "id": "38240169",
               "down_vote_count": 0,
               "score": 10
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 42081195,
               "is_accepted": false,
               "last_activity_date": 1486437987,
               "body_markdown": "Here&#39;s a `Counter` answer for the simplest case.\r\n\r\nThis is shorter than the one above that does two-way diffs because it only does exactly what the question asks: generate a list of what&#39;s in the first list but not the second.\r\n\r\n    from collections import Counter\r\n    \r\n    lst1 = [&#39;One&#39;, &#39;Two&#39;, &#39;Three&#39;, &#39;Four&#39;]\r\n    lst2 = [&#39;One&#39;, &#39;Two&#39;]\r\n    \r\n    c1 = Counter(lst1)\r\n    c2 = Counter(lst2)\r\n    diff = list((c1 - c2).elements())\r\n\r\nAlternatively, depending on your readability preferences, it makes for a decent one-liner:\r\n\r\n    diff = list((Counter(lst1) - Counter(lst2)).elements())\r\n\r\nOutput:\r\n\r\n    [&#39;Three&#39;, &#39;Four&#39;]\r\n\r\n*Note that you can remove the `list(...)` call if you are just iterating over it.*\r\n\r\nBecause this solution uses counters, it handles quantities properly vs the many set-based answers.  For example on this input:\r\n\r\n    lst1 = [&#39;One&#39;, &#39;Two&#39;, &#39;Two&#39;, &#39;Two&#39;, &#39;Three&#39;, &#39;Three&#39;, &#39;Four&#39;]\r\n    lst2 = [&#39;One&#39;, &#39;Two&#39;]\r\n\r\nThe output is:\r\n\r\n    [&#39;Two&#39;, &#39;Two&#39;, &#39;Three&#39;, &#39;Three&#39;, &#39;Four&#39;]\r\n",
               "id": "42081195",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1486437987,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 43541692,
               "is_accepted": false,
               "last_activity_date": 1492774464,
               "body_markdown": "We can calculate intersection minus union of lists:\r\n\r\n    temp1 = [&#39;One&#39;, &#39;Two&#39;, &#39;Three&#39;, &#39;Four&#39;]\r\n    temp2 = [&#39;One&#39;, &#39;Two&#39;, &#39;Five&#39;]\r\n\r\n    set(temp1+temp2)-(set(temp1)&amp;set(temp2))\r\n\r\n    Out: set([&#39;Four&#39;, &#39;Five&#39;, &#39;Three&#39;]) \r\n",
               "id": "43541692",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1492774464,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 44461877,
               "is_accepted": false,
               "last_activity_date": 1497023042,
               "body_markdown": "This can be solved with one line.\r\nThe question is given two lists (temp1 and temp2) return their difference in a third list (temp3).\r\n\r\n    temp3 = list(set(temp1).difference(set(temp2)))",
               "id": "44461877",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1497023042,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 44954900,
               "is_accepted": false,
               "last_activity_date": 1499360098,
               "body_markdown": "    (list(set(a)-set(b))+list(set(b)-set(a)))",
               "id": "44954900",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1499360098,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 46816178,
               "is_accepted": false,
               "last_activity_date": 1508348698,
               "body_markdown": "I am little too late in the game for this but you can do a comparison of performance of some of the above mentioned code with this, two of the fastest contenders are,\r\n\r\n    list(set(x).symmetric_difference(set(y)))\r\n    list(set(x) ^ set(y))\r\n\r\nI apologize for the elementary level of coding.\r\n\r\n    import time\r\n    import random\r\n    from itertools import filterfalse\r\n\r\n    # 1 - performance (time taken)\r\n    # 2 - correctness (answer - 1,4,5,6)\r\n    # set performance\r\n    performance = 1\r\n    numberoftests = 7\r\n    \r\n    def answer(x,y,z):\r\n        if z == 0:\r\n            start = time.clock()\r\n            lists = (str(list(set(x)-set(y))+list(set(y)-set(y))))\r\n            times = (&quot;1 = &quot; + str(time.clock() - start))\r\n            return (lists,times)\r\n    \r\n        elif z == 1:\r\n            start = time.clock()\r\n            lists = (str(list(set(x).symmetric_difference(set(y)))))\r\n            times = (&quot;2 = &quot; + str(time.clock() - start))\r\n            return (lists,times)\r\n        \r\n        elif z == 2:\r\n            start = time.clock()\r\n            lists = (str(list(set(x) ^ set(y))))\r\n            times = (&quot;3 = &quot; + str(time.clock() - start))\r\n            return (lists,times)\r\n            \r\n        elif z == 3:\r\n            start = time.clock()\r\n            lists = (filterfalse(set(y).__contains__, x))\r\n            times = (&quot;4 = &quot; + str(time.clock() - start))\r\n            return (lists,times)\r\n                \r\n        elif z == 4:\r\n            start = time.clock()\r\n            lists = (tuple(set(x) - set(y)))\r\n            times = (&quot;5 = &quot; + str(time.clock() - start))\r\n            return (lists,times)\r\n                    \r\n        elif z == 5:\r\n            start = time.clock()\r\n            lists = ([tt for tt in x if tt not in y])\r\n            times = (&quot;6 = &quot; + str(time.clock() - start))\r\n            return (lists,times)\r\n    \r\n        else:    \r\n            start = time.clock()\r\n            Xarray = [iDa for iDa in x if iDa not in y]\r\n            Yarray = [iDb for iDb in y if iDb not in x]\r\n            lists = (str(Xarray + Yarray))\r\n            times = (&quot;7 = &quot; + str(time.clock() - start))\r\n            return (lists,times)\r\n    \r\n    n = numberoftests\r\n    \r\n    if performance == 2:\r\n        a = [1,2,3,4,5]\r\n        b = [3,2,6]\r\n        for c in range(0,n):\r\n            d = answer(a,b,c)\r\n            print(d[0])\r\n    \r\n    elif performance == 1:\r\n        for tests in range(0,10):\r\n            print(&quot;Test Number&quot; + str(tests + 1))\r\n            a = random.sample(range(1, 900000), 9999)\r\n            b = random.sample(range(1, 900000), 9999)\r\n            for c in range(0,n):\r\n                #if c not in (1,4,5,6):\r\n                d = answer(a,b,c)\r\n                print(d[1])",
               "id": "46816178",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1508348698,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47574120,
               "is_accepted": false,
               "last_activity_date": 1512046740,
               "body_markdown": "most simple way,\r\n\r\nuse **set().difference(set())**\r\n\r\n    list_a = [1,2,3]\r\n    list_b = [2,3]\r\n    print set(list_a).difference(set(list_b))\r\n\r\nanswer is `set([1])`\r\n\r\ncan print as a list,\r\n\r\n    print list(set(list_a).difference(set(list_b)))\r\n\r\n",
               "id": "47574120",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512046740,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/3462143/get-difference-between-two-lists",
         "id": "858127-2246"
      },
      {
         "up_vote_count": "520",
         "path": "2.stack",
         "body_markdown": "I have a string representing a unix timestamp (i.e. &quot;1284101485&quot;) in Python, and I&#39;d like to convert it to a readable date. When I use `time.strftime`, I get a `TypeError`:\r\n\r\n    &gt;&gt;&gt;import time\r\n    &gt;&gt;&gt;print time.strftime(&quot;%B %d %Y&quot;, &quot;1284101485&quot;)\r\n\r\n    Traceback (most recent call last):\r\n      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\n    TypeError: argument must be 9-item sequence, not str\r\n\r\n",
         "view_count": "586006",
         "answer_count": "12",
         "tags": "['python', 'datetime', 'unix-timestamp', 'strftime']",
         "creation_date": "1284101818",
         "last_edit_date": "1411492403",
         "code_snippet": "['<code>time.strftime</code>', '<code>TypeError</code>', '<code>&gt;&gt;&gt;import time\\n&gt;&gt;&gt;print time.strftime(\"%B %d %Y\", \"1284101485\")\\n\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nTypeError: argument must be 9-item sequence, not str\\n</code>', '<code>datetime</code>', '<code>import datetime\\nprint(\\n    datetime.datetime.fromtimestamp(\\n        int(\"1284101485\")\\n    ).strftime(\\'%Y-%m-%d %H:%M:%S\\')\\n)\\n</code>', '<code>datetime.datetime</code>', '<code>datetime</code>', '<code>datetime.datetime.fromtimestamp()</code>', '<code>fromtimestamp()</code>', '<code>datetime</code>', '<code>datetime</code>', '<code>.fromtimestamp()</code>', '<code>pytz</code>', '<code>.utcfromtimestamp()</code>', '<code>pytz</code>', '<code>pytz</code>', '<code>&gt;&gt;&gt; from datetime import datetime\\n&gt;&gt;&gt; datetime.fromtimestamp(1172969203.1)\\ndatetime.datetime(2007, 3, 4, 0, 46, 43, 100000)\\n</code>', \"<code>datetime.datetime.utcfromtimestamp(posix_time).strftime('%Y-%m-%dT%H:%M:%SZ')\\n</code>\", '<code>&gt;&gt;&gt; import time\\n&gt;&gt;&gt; time.ctime(int(\"1284101485\"))\\n\\'Fri Sep 10 16:51:25 2010\\'\\n&gt;&gt;&gt; time.strftime(\"%D %H:%M\", time.localtime(int(\"1284101485\")))\\n\\'09/10/10 16:51\\'\\n</code>', '<code>time.ctime()</code>', '<code>time.localtime()</code>', '<code>pytz</code>', '<code>time.gmtime()</code>', '<code>datetime</code>', '<code>datetime.utcfromtimestamp()</code>', '<code>time</code>', '<code>import os, datetime\\n\\ndatetime.datetime.fromtimestamp(float(os.path.getmtime(\"FILE\"))).strftime(\"%B %d, %Y\")\\n</code>', '<code>.fromtimestamp()</code>', '<code>.strftime()</code>', '<code>getmtime()</code>', '<code>pytz</code>', '<code>#!/usr/bin/env python\\nfrom datetime import datetime\\nimport tzlocal  # $ pip install tzlocal\\n\\nunix_timestamp = float(\"1284101485\")\\nlocal_timezone = tzlocal.get_localzone() # get pytz timezone\\nlocal_time = datetime.fromtimestamp(unix_timestamp, local_timezone)\\n</code>', '<code>print(local_time.strftime(\"%Y-%m-%d %H:%M:%S.%f%z (%Z)\"))\\nprint(local_time.strftime(\"%B %d %Y\"))  # print date in your format\\n</code>', '<code>utc_time = datetime.utcfromtimestamp(unix_timestamp)\\nprint(utc_time.strftime(\"%Y-%m-%d %H:%M:%S.%f+00:00 (UTC)\"))\\n</code>', '<code>local_time = datetime.fromtimestamp(unix_timestamp)\\nprint(local_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\\n</code>', '<code>#!/usr/bin/env python3\\nfrom datetime import datetime, timezone\\n\\nutc_time = datetime.fromtimestamp(unix_timestamp, timezone.utc)\\nlocal_time = utc_time.astimezone()\\nprint(local_time.strftime(\"%Y-%m-%d %H:%M:%S.%f%z (%Z)\"))\\n</code>', '<code>time</code>', '<code>datetime</code>', '<code>#!/usr/bin/env python\\nimport time\\n\\nunix_timestamp  = int(\"1284101485\")\\nutc_time = time.gmtime(unix_timestamp)\\nlocal_time = time.localtime(unix_timestamp)\\nprint(time.strftime(\"%Y-%m-%d %H:%M:%S\", local_time)) \\nprint(time.strftime(\"%Y-%m-%d %H:%M:%S+00:00 (UTC)\", utc_time))  \\n</code>', \"<code>t=datetime.fromtimestamp(time.time())\\nt.strftime('%Y-%m-%d')\\n'2012-03-07'\\n</code>\", '<code>import datetime,time\\n\\ndef createDateObject(str_date,strFormat=\"%Y-%m-%d\"):    \\n    timeStamp = time.mktime(time.strptime(str_date,strFormat))\\n    return datetime.datetime.fromtimestamp(timeStamp)\\n\\ndef FormatDate(objectDate,strFormat=\"%Y-%m-%d\"):\\n    return objectDate.strftime(strFormat)\\n\\nUsage\\n=====\\no=createDateObject(\\'2013-03-03\\')\\nprint FormatDate(o,\\'%d-%m-%Y\\')\\n\\nOutput 03-03-2013\\n</code>', \"<code>import datetime\\ntemp = datetime.datetime.fromtimestamp(1386181800).strftime('%Y-%m-%d %H:%M:%S')\\nprint temp\\n</code>\", '<code>&gt;&gt;&gt; type(tstamp)\\npandas.tslib.Timestamp\\n&gt;&gt;&gt; newDt = tstamp.date()\\n&gt;&gt;&gt; type(newDt)\\ndatetime.date\\n</code>', \"<code>'-'.join(str(x) for x in list(tuple(datetime.datetime.now().timetuple())[:6]))\\n</code>\", '<code>import date_converter\\nmy_date_string = date_converter.timestamp_to_string(1284101485, \"%B %d, %Y\")\\n</code>', '<code>strptime</code>', '<code>strftime</code>']",
         "title": "Converting unix timestamp string to readable date in Python",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 3682758,
               "is_accepted": false,
               "last_activity_date": 1284101922,
               "body_markdown": "Did you look at the datetime package? I believe it has a fromUnixTimestamp method.",
               "id": "3682758",
               "tags": [],
               "down_vote_count": 16,
               "creation_date": 1284101922,
               "score": -9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 165,
               "answer_id": 3682766,
               "is_accepted": false,
               "last_activity_date": 1284101997,
               "body_markdown": "    &gt;&gt;&gt; from datetime import datetime\r\n    &gt;&gt;&gt; datetime.fromtimestamp(1172969203.1)\r\n    datetime.datetime(2007, 3, 4, 0, 46, 43, 100000)\r\nTaken from http://seehuhn.de/pages/pdate",
               "id": "3682766",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1284101997,
               "score": 163
            },
            {
               "up_vote_count": 764,
               "answer_id": 3682808,
               "last_activity_date": 1403705433,
               "path": "3.stack.answer",
               "body_markdown": "Use `datetime` module:\r\n\r\n    import datetime\r\n    print(\r\n        datetime.datetime.fromtimestamp(\r\n            int(&quot;1284101485&quot;)\r\n        ).strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)\r\n    )\r\n\r\nIn this code `datetime.datetime` can look strange, but 1st `datetime` is module name and 2nd is class name. So `datetime.datetime.fromtimestamp()` is `fromtimestamp()` method of `datetime` class from `datetime` module.\r\n",
               "tags": [],
               "creation_date": 1284102565,
               "last_edit_date": 1403705433,
               "is_accepted": false,
               "id": "3682808",
               "down_vote_count": 1,
               "score": 763
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 63,
               "answer_id": 3683166,
               "is_accepted": false,
               "last_activity_date": 1284107286,
               "body_markdown": "    &gt;&gt;&gt; import time\r\n    &gt;&gt;&gt; time.ctime(int(&quot;1284101485&quot;))\r\n    &#39;Fri Sep 10 16:51:25 2010&#39;\r\n    &gt;&gt;&gt; time.strftime(&quot;%D %H:%M&quot;, time.localtime(int(&quot;1284101485&quot;)))\r\n    &#39;09/10/10 16:51&#39;\r\n\r\n",
               "id": "3683166",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1284107286,
               "score": 62
            },
            {
               "up_vote_count": 25,
               "answer_id": 9597396,
               "last_activity_date": 1362742606,
               "path": "3.stack.answer",
               "body_markdown": "You can convert the current time like this\r\n\r\n    t=datetime.fromtimestamp(time.time())\r\n    t.strftime(&#39;%Y-%m-%d&#39;)\r\n    &#39;2012-03-07&#39;\r\n\r\nTo convert a date in string to different formats.\r\n\r\n    import datetime,time\r\n\r\n    def createDateObject(str_date,strFormat=&quot;%Y-%m-%d&quot;):    \r\n        timeStamp = time.mktime(time.strptime(str_date,strFormat))\r\n        return datetime.datetime.fromtimestamp(timeStamp)\r\n\r\n    def FormatDate(objectDate,strFormat=&quot;%Y-%m-%d&quot;):\r\n        return objectDate.strftime(strFormat)\r\n    \r\n    Usage\r\n    =====\r\n    o=createDateObject(&#39;2013-03-03&#39;)\r\n    print FormatDate(o,&#39;%d-%m-%Y&#39;)\r\n \r\n    Output 03-03-2013\r\n\r\n    \r\n",
               "tags": [],
               "creation_date": 1331105439,
               "last_edit_date": 1362742606,
               "is_accepted": false,
               "id": "9597396",
               "down_vote_count": 3,
               "score": 22
            },
            {
               "up_vote_count": 37,
               "answer_id": 14048587,
               "last_activity_date": 1423234920,
               "path": "3.stack.answer",
               "body_markdown": "For a human readable timestamp from a UNIX timestamp, I have used this in scripts before:\r\n\r\n    import os, datetime\r\n\r\n    datetime.datetime.fromtimestamp(float(os.path.getmtime(&quot;FILE&quot;))).strftime(&quot;%B %d, %Y&quot;)\r\n\r\nOutput:\r\n\r\n&#39;December 26, 2012&#39;",
               "tags": [],
               "creation_date": 1356574260,
               "last_edit_date": 1423234920,
               "is_accepted": false,
               "id": "14048587",
               "down_vote_count": 7,
               "score": 30
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 16381605,
               "is_accepted": false,
               "last_activity_date": 1367730600,
               "body_markdown": "quick and dirty one liner:\r\n\r\n    &#39;-&#39;.join(str(x) for x in list(tuple(datetime.datetime.now().timetuple())[:6]))\r\n\r\n&#39;2013-5-5-1-9-43&#39;",
               "id": "16381605",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1367730600,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 21578729,
               "is_accepted": false,
               "last_activity_date": 1391606947,
               "body_markdown": "    import datetime\r\n    temp = datetime.datetime.fromtimestamp(1386181800).strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)\r\n    print temp\r\n",
               "id": "21578729",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1391606947,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 27975613,
               "is_accepted": false,
               "last_activity_date": 1421369057,
               "body_markdown": "i just successfully used:\r\n\r\n    &gt;&gt;&gt; type(tstamp)\r\n    pandas.tslib.Timestamp\r\n    &gt;&gt;&gt; newDt = tstamp.date()\r\n    &gt;&gt;&gt; type(newDt)\r\n    datetime.date\r\n",
               "id": "27975613",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1421369057,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 30252835,
               "is_accepted": false,
               "last_activity_date": 1431670919,
               "body_markdown": "You can use [easy_date][1] to make it easy:\r\n\r\n    import date_converter\r\n    my_date_string = date_converter.timestamp_to_string(1284101485, &quot;%B %d, %Y&quot;)\r\n\r\n\r\n  [1]: https://github.com/ralphavalon/easy_date",
               "id": "30252835",
               "tags": [],
               "down_vote_count": 4,
               "creation_date": 1431670919,
               "score": 0
            },
            {
               "up_vote_count": 100,
               "answer_id": 37188257,
               "last_activity_date": 1468395797,
               "path": "3.stack.answer",
               "body_markdown": "The most voted answer suggests using fromtimestamp which is error prone since it uses the local timezone. To avoid issues a better approach is to use UTC:\r\n\r\n    datetime.datetime.utcfromtimestamp(posix_time).strftime(&#39;%Y-%m-%dT%H:%M:%SZ&#39;)\r\n\r\nWhere posix_time is the Posix epoch time you want to convert",
               "tags": [],
               "creation_date": 1463059380,
               "last_edit_date": 1468395797,
               "is_accepted": false,
               "id": "37188257",
               "down_vote_count": 0,
               "score": 100
            },
            {
               "up_vote_count": 28,
               "answer_id": 40769643,
               "last_activity_date": 1484308490,
               "path": "3.stack.answer",
               "body_markdown": "There are two parts:\r\n\r\n1. Convert the unix timestamp (&quot;seconds since epoch&quot;) to the local time\r\n2. Display the local time in the desired format.\r\n\r\nA portable way to get the local time that works even if the local time zone had a different utc offset in the past and python has no access to the tz database is to use a `pytz` timezone:\r\n\r\n    #!/usr/bin/env python\r\n    from datetime import datetime\r\n    import tzlocal  # $ pip install tzlocal\r\n    \r\n    unix_timestamp = float(&quot;1284101485&quot;)\r\n    local_timezone = tzlocal.get_localzone() # get pytz timezone\r\n    local_time = datetime.fromtimestamp(unix_timestamp, local_timezone)\r\n\r\nTo display it, you could use any time format that is supported by your system e.g.:\r\n\r\n    print(local_time.strftime(&quot;%Y-%m-%d %H:%M:%S.%f%z (%Z)&quot;))\r\n    print(local_time.strftime(&quot;%B %d %Y&quot;))  # print date in your format\r\n\r\n---\r\n\r\nIf you do not need a local time, to get a readable UTC time instead:\r\n\r\n    utc_time = datetime.utcfromtimestamp(unix_timestamp)\r\n    print(utc_time.strftime(&quot;%Y-%m-%d %H:%M:%S.%f+00:00 (UTC)&quot;))\r\n\r\n---\r\n\r\nIf you don&#39;t care about the timezone issues that might affect *what date is returned* or if python has access to the tz database on your system:\r\n\r\n    local_time = datetime.fromtimestamp(unix_timestamp)\r\n    print(local_time.strftime(&quot;%Y-%m-%d %H:%M:%S.%f&quot;))\r\n\r\nOn Python 3, you could get a timezone-aware datetime using only stdlib (the UTC offset may be wrong if python has no access to the tz database on your system e.g., on Windows):\r\n\r\n    #!/usr/bin/env python3\r\n    from datetime import datetime, timezone\r\n\r\n    utc_time = datetime.fromtimestamp(unix_timestamp, timezone.utc)\r\n    local_time = utc_time.astimezone()\r\n    print(local_time.strftime(&quot;%Y-%m-%d %H:%M:%S.%f%z (%Z)&quot;))\r\n\r\nFunctions from the `time` module are thin wrappers around the corresponding C API and therefore they may be less portable than the corresponding `datetime` methods otherwise you could use them too:\r\n\r\n    #!/usr/bin/env python\r\n    import time\r\n    \r\n    unix_timestamp  = int(&quot;1284101485&quot;)\r\n    utc_time = time.gmtime(unix_timestamp)\r\n    local_time = time.localtime(unix_timestamp)\r\n    print(time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, local_time)) \r\n    print(time.strftime(&quot;%Y-%m-%d %H:%M:%S+00:00 (UTC)&quot;, utc_time))  ",
               "tags": [],
               "creation_date": 1479918625,
               "last_edit_date": 1484308490,
               "is_accepted": false,
               "id": "40769643",
               "down_vote_count": 0,
               "score": 28
            }
         ],
         "link": "https://stackoverflow.com/questions/3682748/converting-unix-timestamp-string-to-readable-date-in-python",
         "id": "858127-2247"
      },
      {
         "up_vote_count": "2665",
         "path": "2.stack",
         "body_markdown": "I have a dictionary of values read from two fields in a database: a string field and a numeric field. The string field is unique, so that is the key of the dictionary.\r\n\r\nI can sort on the keys, but how can I sort based on the values?\r\n\r\nNote: I have read Stack Overflow question *https://stackoverflow.com/questions/72899* and probably could change my code to have a list of dictionaries, but since I do not really need a list of dictionaries I wanted to know if there is a simpler solution.\r\n",
         "view_count": "1606273",
         "answer_count": "37",
         "tags": "['python', 'sorting', 'dictionary']",
         "creation_date": "1236214145",
         "last_edit_date": "1511876586",
         "code_snippet": "['<code>Series</code>', '<code>pandas.Series.order</code>', '<code>import operator\\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\\nsorted_x = sorted(x.items(), key=operator.itemgetter(1))\\n</code>', '<code>sorted_x</code>', '<code>dict(sorted_x) == x</code>', '<code>import operator\\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\\nsorted_x = sorted(x.items(), key=operator.itemgetter(0))\\n</code>', '<code>sorted_x.reverse()</code>', '<code>sorted()</code>', '<code>reverse=True</code>', '<code>sorted(d.items(), key=lambda x: x[1])</code>', '<code>sorted(dict1, key=dict1.get)</code>', '<code>from collections import defaultdict\\nd = defaultdict(int)\\nfor w in text.split():\\n  d[w] += 1\\n</code>', '<code>sorted(d, key=d.get)</code>', '<code>for w in sorted(d, key=d.get, reverse=True):\\n  print w, d[w]\\n</code>', '<code>key=operator.itemgetter(1)</code>', '<code>key=d.get</code>', '<code>from operator import itemgetter  d = {\"a\":7, \"b\":1, \"c\":5, \"d\":3}  sorted_keys = sorted(d, key=itemgetter, reverse=True)  for key in sorted_keys:      print \"%s: %d\" % (key, d[key])</code>', '<code>sorted_keys = sorted(d.items(), key=itemgetter(1), reverse=True)</code>', '<code>for key, val in sorted_keys: print \"%s: %d\" % (key, val)</code>', '<code>collections.Counter</code>', '<code>most_common</code>', '<code>sorted(d.items(), key=lambda x: x[1])</code>', '<code>(key, value)</code>', '<code>key=lambda (k, v): v</code>', '<code>(k, v)</code>', '<code>reverse=True</code>', '<code>sorted(a.items(), key=lambda x: x[1], reverse=True)</code>', '<code>sorted(d.values())\\n</code>', '<code>from operator import itemgetter\\nsorted(d.items(), key=itemgetter(1))\\n</code>', '<code>sorted(d.values())</code>', '<code>sorted(d, key=d.get)</code>', '<code>&gt;&gt;&gt; d = {\"third\": 3, \"first\": 1, \"fourth\": 4, \"second\": 2}\\n\\n&gt;&gt;&gt; for k, v in d.items():\\n...     print \"%s: %s\" % (k, v)\\n...\\nsecond: 2\\nfourth: 4\\nthird: 3\\nfirst: 1\\n\\n&gt;&gt;&gt; d\\n{\\'second\\': 2, \\'fourth\\': 4, \\'third\\': 3, \\'first\\': 1}\\n</code>', '<code>&gt;&gt;&gt; from collections import OrderedDict\\n&gt;&gt;&gt; d_sorted_by_value = OrderedDict(sorted(d.items(), key=lambda x: x[1]))\\n</code>', '<code>&gt;&gt;&gt; for k, v in d_sorted_by_value.items():\\n...     print \"%s: %s\" % (k, v)\\n...\\nfirst: 1\\nsecond: 2\\nthird: 3\\nfourth: 4\\n\\n&gt;&gt;&gt; d_sorted_by_value\\nOrderedDict([(\\'first\\': 1), (\\'second\\': 2), (\\'third\\': 3), (\\'fourth\\': 4)])\\n</code>', '<code>sorted(d.items(), key=lambda x: x[1])</code>', '<code>x</code>', '<code>x[1]</code>', '<code>x[0]</code>', '<code>d.items()</code>', '<code>x</code>', '<code>x[0]</code>', '<code>x[1]</code>', '<code>x[1]</code>', '<code>x: x[1]</code>', '<code>1</code>', '<code>0</code>', '<code>from operator import itemgetter\\nfrom collections import OrderedDict\\n\\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\\nsorted_x = OrderedDict(sorted(x.items(), key=itemgetter(1)))\\n# OrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\\n</code>', \"<code># regular unsorted dictionary\\nd = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}\\n\\n# dictionary sorted by value\\nOrderedDict(sorted(d.items(), key=lambda t: t[1]))\\n# OrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])\\n</code>\", \"<code>import collections\\nPlayer = collections.namedtuple('Player', 'score name')\\nd = {'John':5, 'Alex':10, 'Richard': 7}\\n</code>\", '<code>worst = sorted(Player(v,k) for (k,v) in d.items())\\n</code>', '<code>best = sorted([Player(v,k) for (k,v) in d.items()], reverse=True)\\n</code>', \"<code>player = best[1]\\nplayer.name\\n    'Richard'\\nplayer.score\\n    7\\n</code>\", '<code>[(key, value) for (value, key) in sorted_list_of_tuples]</code>', '<code>sorted</code>', '<code>([])</code>', '<code>SELECT a_key, a_value FROM a_table ORDER BY a_value;\\n</code>', \"<code>k_seq = ('foo', 'bar', 'baz')\\nv_seq = (0, 1, 42)\\nordered_map = dict(zip(k_seq, v_seq))\\n</code>\", '<code>for k, v in ordered_map.items():\\n    print(k, v)\\n</code>', '<code>foo 0\\nbar 1\\nbaz 42\\n</code>', '<code>bar 1\\nfoo 0\\nbaz 42\\n</code>', '<code>dict</code>', '<code>collections.OrderedDict</code>', '<code>dict</code>', '<code>OrderedDict</code>', '<code>OrderedDict</code>', '<code>ImportError</code>', '<code>OrderedDict</code>', '<code>e = {1:39, 4:34, 7:110, 2:87}\\n</code>', '<code>sred = sorted(e.items(), key=lambda value: value[1])\\n</code>', '<code>[(4, 34), (1, 39), (2, 87), (7, 110)]\\n</code>', \"<code>from collections import OrderedDict\\n# regular unsorted dictionary\\nd = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}\\n\\n# dictionary sorted by key\\nOrderedDict(sorted(d.items(), key=lambda t: t[0]))\\nOrderedDict([('apple', 4), ('banana', 3), ('orange', 2), ('pear', 1)])\\n\\n# dictionary sorted by value\\nOrderedDict(sorted(d.items(), key=lambda t: t[1]))\\nOrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])\\n</code>\", '<code>WantedOutput = sorted(MyDict, key=lambda x : MyDict[x]) \\n</code>', \"<code>from collections import Counter\\n\\nx={'hello':1,'python':5, 'world':3}\\nc=Counter(x)\\nprint c.most_common()\\n\\n\\n&gt;&gt; [('python', 5), ('world', 3), ('hello', 1)]    \\n</code>\", \"<code>Counter({'hello':1, 'python':5, 'world':300}).most_common()</code>\", \"<code>[('world', 300), ('python', 5), ('hello', 1)]</code>\", '<code>import operator\\norigin_list = [\\n    {\"name\": \"foo\", \"rank\": 0, \"rofl\": 20000},\\n    {\"name\": \"Silly\", \"rank\": 15, \"rofl\": 1000},\\n    {\"name\": \"Baa\", \"rank\": 300, \"rofl\": 20},\\n    {\"name\": \"Zoo\", \"rank\": 10, \"rofl\": 200},\\n    {\"name\": \"Penguin\", \"rank\": -1, \"rofl\": 10000}\\n]\\nprint \"&gt;&gt; Original &gt;&gt;\"\\nfor foo in origin_list:\\n    print foo\\n\\nprint \"\\\\n&gt;&gt; Rofl sort &gt;&gt;\"\\nfor foo in sorted(origin_list, key=operator.itemgetter(\"rofl\")):\\n    print foo\\n\\nprint \"\\\\n&gt;&gt; Rank sort &gt;&gt;\"\\nfor foo in sorted(origin_list, key=operator.itemgetter(\"rank\")):\\n    print foo\\n</code>', \"<code>{'name': 'foo', 'rank': 0, 'rofl': 20000}\\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\\n{'name': 'Baa', 'rank': 300, 'rofl': 20}\\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\\n{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\\n</code>\", \"<code>{'name': 'Baa', 'rank': 300, 'rofl': 20}\\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\\n{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\\n{'name': 'foo', 'rank': 0, 'rofl': 20000}\\n</code>\", \"<code>{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\\n{'name': 'foo', 'rank': 0, 'rofl': 20000}\\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\\n{'name': 'Baa', 'rank': 300, 'rofl': 20}\\n</code>\", '<code>from collections import defaultdict\\ninverse= defaultdict( list )\\nfor k, v in originalDict.items():\\n    inverse[v].append( k )\\n</code>', '<code>for k in sorted(inverse):\\n    print k, inverse[k]\\n</code>', '<code>&gt;&gt;&gt; x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\\n&gt;&gt;&gt; from collections import Counter\\n&gt;&gt;&gt; #To sort in reverse order\\n&gt;&gt;&gt; Counter(x).most_common()\\n[(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\\n&gt;&gt;&gt; #To sort in ascending order\\n&gt;&gt;&gt; Counter(x).most_common()[::-1]\\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\\n&gt;&gt;&gt; #To get a dictionary sorted by values\\n&gt;&gt;&gt; from collections import OrderedDict\\n&gt;&gt;&gt; OrderedDict(Counter(x).most_common()[::-1])\\nOrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\\n</code>', '<code>sorted(a_dictionary.values())\\n</code>', \"<code>mydict = {'carl':40,\\n          'alan':2,\\n          'bob':1,\\n          'danny':3}\\n</code>\", '<code>for key in sorted(mydict.iterkeys()):\\n    print \"%s: %s\" % (key, mydict[key])\\n</code>', '<code>alan: 2\\nbob: 1\\ncarl: 40\\ndanny: 3\\n</code>', '<code>for key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k)):\\n    print \"%s: %s\" % (key, value)\\n</code>', '<code>bob: 1\\nalan: 2\\ndanny: 3\\ncarl: 40\\n</code>', '<code>for key, value in sorted(mydict.iteritems(), key=lambda (k,v): v[\"score\"]):</code>', '<code>from django.utils.datastructures import SortedDict\\n\\ndef sortedDictByKey(self,data):\\n    \"\"\"Sorted dictionary order by key\"\"\"\\n    sortedDict = SortedDict()\\n    if data:\\n        if isinstance(data, dict):\\n            sortedKey = sorted(data.keys())\\n            for k in sortedKey:\\n                sortedDict[k] = data[k]\\n    return sortedDict\\n</code>', '<code>&gt;&gt;&gt; data = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\\n&gt;&gt;&gt; SkipDict(data)\\n{0: 0.0, 2: 1.0, 1: 2.0, 4: 3.0, 3: 4.0}\\n</code>', '<code>keys()</code>', '<code>values()</code>', '<code>items()</code>', '<code>sorted(d.items(), key=lambda x: x[1], reverse=True)\\n</code>', '<code>sorted(d.items(), reverse=True)\\n</code>', '<code>sorted(d.items(), reverse=True)</code>', '<code>from dicts.sorteddict import ValueSortedDict\\nd = {1: 2, 3: 4, 4:3, 2:1, 0:0}\\nsorted_dict = ValueSortedDict(d)\\nprint sorted_dict.items() \\n\\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\\n</code>', '<code>def dict_val(x):\\n    return x[1]\\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\\nsorted_x = sorted(x.items(), key=dict_val)\\n</code>', '<code>x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\\nsorted_x = sorted(x.items(), key=lambda t: t[1])\\n</code>', '<code>$ python --version\\nPython 3.2.2\\n\\n$ cat sort_dict_by_val_desc.py \\ndictionary = dict(siis = 1, sana = 2, joka = 3, tuli = 4, aina = 5)\\nfor word in sorted(dictionary, key=dictionary.get, reverse=True):\\n  print(word, dictionary[word])\\n\\n$ python sort_dict_by_val_desc.py \\naina 5\\ntuli 4\\njoka 3\\nsana 2\\nsiis 1\\n</code>', '<code>collections.Counter</code>', '<code>dict</code>', '<code>most_common</code>', '<code>import operator\\nslovar_sorted=sorted(slovar.items(), key=operator.itemgetter(1), reverse=True)\\nprint(slovar_sorted)\\n</code>', '<code>import operator    \\nx = {1: 2, 3: 4, 4:3, 2:1, 0:0}\\nsorted_x = {k[0]:k[1] for k in sorted(x.items(), key=operator.itemgetter(1))}\\n</code>', '<code>x.items()</code>', '<code>iteritems()</code>', '<code>&gt;&gt;&gt; sorted_x\\n{0: 0, 1: 2, 2: 1, 3: 4, 4: 3}\\n</code>', '<code>collections.OrderedDict</code>', '<code>x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\\nfrom collections import OrderedDict\\n\\nod1 = OrderedDict(sorted(x.items(), key=lambda t: t[1]))\\n</code>', '<code>{key: value for key, value in ...}</code>', '<code>d.values()</code>', '<code>d.keys()</code>', \"<code>d = {'key1': 874.7, 'key2': 5, 'key3': 8.1}\\n\\nd_sorted = sorted(zip(d.values(), d.keys()))\\n\\nprint d_sorted \\n# prints: [(5, 'key2'), (8.1, 'key3'), (874.7, 'key1')]\\n</code>\", '<code>sorted(iterable[, cmp[, key[, reverse]]])</code>', '<code>sorted(dictionary.items(),key = lambda x :x[1])</code>', '<code>&gt;&gt;&gt; x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\\n&gt;&gt;&gt; x_items = x.items()\\n&gt;&gt;&gt; heapq.heapify(x_items)\\n&gt;&gt;&gt; #To sort in reverse order\\n&gt;&gt;&gt; heapq.nlargest(len(x_items),x_items, operator.itemgetter(1))\\n[(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\\n&gt;&gt;&gt; #To sort in ascending order\\n&gt;&gt;&gt; heapq.nsmallest(len(x_items),x_items, operator.itemgetter(1))\\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\\n</code>']",
         "title": "How do I sort a dictionary by value?",
         "_childDocuments_": [
            {
               "up_vote_count": 15,
               "answer_id": 613207,
               "last_activity_date": 1511876627,
               "path": "3.stack.answer",
               "body_markdown": "Technically, dictionaries aren&#39;t sequences, and therefore can&#39;t be sorted. You can do something like\r\n\r\n    sorted(a_dictionary.values())\r\n\r\nassuming performance isn&#39;t a huge deal.\r\n",
               "tags": [],
               "creation_date": 1236214572,
               "last_edit_date": 1511876627,
               "is_accepted": false,
               "id": "613207",
               "down_vote_count": 1,
               "score": 14
            },
            {
               "up_vote_count": 3214,
               "answer_id": 613218,
               "last_activity_date": 1513647171,
               "path": "3.stack.answer",
               "body_markdown": "It is not possible to sort a dict, only to get a representation of a dict that is sorted. Dicts are inherently orderless, but other types, such as lists and tuples, are not. So you need a ordered data type to represent sorted values, which will be a list\u2014probably a list of tuples.\r\n\r\nFor instance,\r\n\r\n    import operator\r\n    x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\r\n    sorted_x = sorted(x.items(), key=operator.itemgetter(1))\r\n\r\n`sorted_x` will be a list of tuples sorted by the second element in each tuple. `dict(sorted_x) == x`.\r\n\r\nAnd for those wishing to sort on keys instead of values:\r\n\r\n    import operator\r\n    x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\r\n    sorted_x = sorted(x.items(), key=operator.itemgetter(0))\r\n",
               "tags": [],
               "creation_date": 1236214774,
               "last_edit_date": 1513647171,
               "is_accepted": true,
               "id": "613218",
               "down_vote_count": 11,
               "score": 3203
            },
            {
               "up_vote_count": 147,
               "answer_id": 613228,
               "last_activity_date": 1410888370,
               "path": "3.stack.answer",
               "body_markdown": "Dicts can&#39;t be sorted, but you can build a sorted list from them.\r\n\r\nA sorted list of dict values:\r\n\r\n    sorted(d.values())\r\n\r\nA list of (key, value) pairs, sorted by value:\r\n\r\n    from operator import itemgetter\r\n    sorted(d.items(), key=itemgetter(1))",
               "tags": [],
               "creation_date": 1236215122,
               "last_edit_date": 1410888370,
               "is_accepted": false,
               "id": "613228",
               "down_vote_count": 0,
               "score": 147
            },
            {
               "up_vote_count": 50,
               "answer_id": 613230,
               "last_activity_date": 1510238178,
               "path": "3.stack.answer",
               "body_markdown": "Pretty much the same as Hank Gay&#39;s answer;\r\n\r\n&lt;pre&gt;\r\n\r\n    sorted([(value,key) for (key,value) in mydict.items()])\r\n\r\n&lt;/pre&gt;\r\n\r\nOr optimized a bit as suggested by  John Fouhy;\r\n\r\n&lt;pre&gt;\r\n\r\n    sorted((value,key) for (key,value) in mydict.items())\r\n\r\n&lt;/pre&gt;\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1236215164,
               "last_edit_date": 1510238178,
               "is_accepted": false,
               "id": "613230",
               "down_vote_count": 1,
               "score": 49
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 18,
               "answer_id": 613326,
               "is_accepted": false,
               "last_activity_date": 1236217938,
               "body_markdown": "You can create an &quot;inverted index&quot;, also\r\n\r\n    from collections import defaultdict\r\n    inverse= defaultdict( list )\r\n    for k, v in originalDict.items():\r\n        inverse[v].append( k )\r\n\r\nNow your inverse has the values; each value has a list of applicable keys.\r\n\r\n    for k in sorted(inverse):\r\n        print k, inverse[k]",
               "id": "613326",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1236217938,
               "score": 16
            },
            {
               "up_vote_count": 544,
               "answer_id": 2258273,
               "last_activity_date": 1278317168,
               "path": "3.stack.answer",
               "body_markdown": "You could use:\r\n\r\n`sorted(d.items(), key=lambda x: x[1])`\r\n\r\nThis will sort the dictionary by the values of each entry within the dictionary from smallest to largest.",
               "tags": [],
               "creation_date": 1266078831,
               "last_edit_date": 1278317168,
               "is_accepted": false,
               "id": "2258273",
               "down_vote_count": 2,
               "score": 542
            },
            {
               "up_vote_count": 117,
               "answer_id": 3177025,
               "last_activity_date": 1396544379,
               "path": "3.stack.answer",
               "body_markdown": "In recent Python 2.7, we have the new [OrderedDict][1] type, which remembers the order in which the items were added.\r\n\r\n    &gt;&gt;&gt; d = {&quot;third&quot;: 3, &quot;first&quot;: 1, &quot;fourth&quot;: 4, &quot;second&quot;: 2}\r\n    \r\n    &gt;&gt;&gt; for k, v in d.items():\r\n    ...     print &quot;%s: %s&quot; % (k, v)\r\n    ...\r\n    second: 2\r\n    fourth: 4\r\n    third: 3\r\n    first: 1\r\n    \r\n    &gt;&gt;&gt; d\r\n    {&#39;second&#39;: 2, &#39;fourth&#39;: 4, &#39;third&#39;: 3, &#39;first&#39;: 1}\r\n\r\nTo make a new ordered dictionary from the original, sorting by the values:\r\n    \r\n    &gt;&gt;&gt; from collections import OrderedDict\r\n    &gt;&gt;&gt; d_sorted_by_value = OrderedDict(sorted(d.items(), key=lambda x: x[1]))\r\n    \r\nThe OrderedDict behaves like a normal dict:\r\n\r\n    &gt;&gt;&gt; for k, v in d_sorted_by_value.items():\r\n    ...     print &quot;%s: %s&quot; % (k, v)\r\n    ...\r\n    first: 1\r\n    second: 2\r\n    third: 3\r\n    fourth: 4\r\n    \r\n    &gt;&gt;&gt; d_sorted_by_value\r\n    OrderedDict([(&#39;first&#39;: 1), (&#39;second&#39;: 2), (&#39;third&#39;: 3), (&#39;fourth&#39;: 4)])\r\n\r\n  [1]: http://docs.python.org/dev/whatsnew/2.7.html#pep-372-adding-an-ordered-dictionary-to-collections\r\n",
               "tags": [],
               "creation_date": 1278298241,
               "last_edit_date": 1396544379,
               "is_accepted": false,
               "id": "3177025",
               "down_vote_count": 3,
               "score": 114
            },
            {
               "up_vote_count": 906,
               "answer_id": 3177911,
               "last_activity_date": 1487829356,
               "path": "3.stack.answer",
               "body_markdown": "## As simple as: `sorted(dict1, key=dict1.get)` ##\r\n\r\nWell, it is actually possible to do a &quot;sort by dictionary values&quot;. Recently I had to do that in a Code Golf (Stack Overflow question *[Code golf: Word frequency chart][1]*). Abridged, the problem was of the kind: given a text, count how often each word is encountered and display a list of the top words, sorted by decreasing frequency. \r\n\r\nIf you construct a dictionary with the words as keys and the number of occurrences of each word as value, simplified here as:\r\n\r\n    from collections import defaultdict\r\n    d = defaultdict(int)\r\n    for w in text.split():\r\n      d[w] += 1\r\n\r\nthen you can get a list of the words, ordered by frequency of use with `sorted(d, key=d.get)` - the sort iterates over the dictionary keys, using the number of word occurrences as a sort key . \r\n\r\n    for w in sorted(d, key=d.get, reverse=True):\r\n      print w, d[w]\r\n\r\nI am writing this detailed explanation to illustrate what people often mean by &quot;I can easily sort a dictionary by key, but how do I sort by value&quot; - and I think the OP was trying to address such an issue. And the solution is to do sort of list of the keys, based on the values, as shown above.\r\n\r\n  [1]: https://stackoverflow.com/questions/3169051/code-golf-word-frequency-chart#3170549\r\n",
               "tags": [],
               "creation_date": 1278316876,
               "last_edit_date": 1495542396,
               "is_accepted": false,
               "id": "3177911",
               "down_vote_count": 5,
               "score": 901
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 4068769,
               "is_accepted": false,
               "last_activity_date": 1288613801,
               "body_markdown": "    from django.utils.datastructures import SortedDict\r\n\r\n    def sortedDictByKey(self,data):\r\n        &quot;&quot;&quot;Sorted dictionary order by key&quot;&quot;&quot;\r\n        sortedDict = SortedDict()\r\n        if data:\r\n            if isinstance(data, dict):\r\n                sortedKey = sorted(data.keys())\r\n                for k in sortedKey:\r\n                    sortedDict[k] = data[k]\r\n        return sortedDict",
               "id": "4068769",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1288613801,
               "score": 11
            },
            {
               "up_vote_count": 28,
               "answer_id": 4215710,
               "last_activity_date": 1511876664,
               "path": "3.stack.answer",
               "body_markdown": "I had the same problem, and I solved it like this:\r\n\r\n    WantedOutput = sorted(MyDict, key=lambda x : MyDict[x]) \r\n\r\n(People who answer &quot;It is not possible to sort a dict&quot; did not read the question! In fact, &quot;I can sort on the keys, but how can I sort based on the values?&quot; clearly means that he wants a list of the keys sorted according to the value of their values.)\r\n\r\nPlease notice that the order is not well defined (keys with the same value will be in an arbitrary order in the output list).\r\n",
               "tags": [],
               "creation_date": 1290089997,
               "last_edit_date": 1511876664,
               "is_accepted": false,
               "id": "4215710",
               "down_vote_count": 0,
               "score": 28
            },
            {
               "up_vote_count": 19,
               "answer_id": 5227519,
               "last_activity_date": 1456904546,
               "path": "3.stack.answer",
               "body_markdown": "This is the code:\r\n\r\n    import operator\r\n    origin_list = [\r\n    \t{&quot;name&quot;: &quot;foo&quot;, &quot;rank&quot;: 0, &quot;rofl&quot;: 20000},\r\n    \t{&quot;name&quot;: &quot;Silly&quot;, &quot;rank&quot;: 15, &quot;rofl&quot;: 1000},\r\n    \t{&quot;name&quot;: &quot;Baa&quot;, &quot;rank&quot;: 300, &quot;rofl&quot;: 20},\r\n    \t{&quot;name&quot;: &quot;Zoo&quot;, &quot;rank&quot;: 10, &quot;rofl&quot;: 200},\r\n    \t{&quot;name&quot;: &quot;Penguin&quot;, &quot;rank&quot;: -1, &quot;rofl&quot;: 10000}\r\n    ]\r\n    print &quot;&gt;&gt; Original &gt;&gt;&quot;\r\n    for foo in origin_list:\r\n    \tprint foo\r\n    \t\r\n    print &quot;\\n&gt;&gt; Rofl sort &gt;&gt;&quot;\r\n    for foo in sorted(origin_list, key=operator.itemgetter(&quot;rofl&quot;)):\r\n    \tprint foo\r\n    \r\n    print &quot;\\n&gt;&gt; Rank sort &gt;&gt;&quot;\r\n    for foo in sorted(origin_list, key=operator.itemgetter(&quot;rank&quot;)):\r\n    \tprint foo\r\n\r\nHere are the results:\r\n\r\n**Original**\r\n\r\n    {&#39;name&#39;: &#39;foo&#39;, &#39;rank&#39;: 0, &#39;rofl&#39;: 20000}\r\n    {&#39;name&#39;: &#39;Silly&#39;, &#39;rank&#39;: 15, &#39;rofl&#39;: 1000}\r\n    {&#39;name&#39;: &#39;Baa&#39;, &#39;rank&#39;: 300, &#39;rofl&#39;: 20}\r\n    {&#39;name&#39;: &#39;Zoo&#39;, &#39;rank&#39;: 10, &#39;rofl&#39;: 200}\r\n    {&#39;name&#39;: &#39;Penguin&#39;, &#39;rank&#39;: -1, &#39;rofl&#39;: 10000}\r\n\r\n**Rofl**\r\n\r\n    {&#39;name&#39;: &#39;Baa&#39;, &#39;rank&#39;: 300, &#39;rofl&#39;: 20}\r\n    {&#39;name&#39;: &#39;Zoo&#39;, &#39;rank&#39;: 10, &#39;rofl&#39;: 200}\r\n    {&#39;name&#39;: &#39;Silly&#39;, &#39;rank&#39;: 15, &#39;rofl&#39;: 1000}\r\n    {&#39;name&#39;: &#39;Penguin&#39;, &#39;rank&#39;: -1, &#39;rofl&#39;: 10000}\r\n    {&#39;name&#39;: &#39;foo&#39;, &#39;rank&#39;: 0, &#39;rofl&#39;: 20000}\r\n\r\n**Rank** \r\n\r\n    {&#39;name&#39;: &#39;Penguin&#39;, &#39;rank&#39;: -1, &#39;rofl&#39;: 10000}\r\n    {&#39;name&#39;: &#39;foo&#39;, &#39;rank&#39;: 0, &#39;rofl&#39;: 20000}\r\n    {&#39;name&#39;: &#39;Zoo&#39;, &#39;rank&#39;: 10, &#39;rofl&#39;: 200}\r\n    {&#39;name&#39;: &#39;Silly&#39;, &#39;rank&#39;: 15, &#39;rofl&#39;: 1000}\r\n    {&#39;name&#39;: &#39;Baa&#39;, &#39;rank&#39;: 300, &#39;rofl&#39;: 20}",
               "tags": [],
               "creation_date": 1299549985,
               "last_edit_date": 1456904546,
               "is_accepted": false,
               "id": "5227519",
               "down_vote_count": 0,
               "score": 19
            },
            {
               "up_vote_count": 57,
               "answer_id": 7237524,
               "last_activity_date": 1492999919,
               "path": "3.stack.answer",
               "body_markdown": "\r\nIt can often be very handy to use &lt;b&gt;[namedtuple](http://docs.python.org/library/collections.html#collections.namedtuple)&lt;/b&gt;. For example, you have a dictionary of &#39;name&#39; as keys and &#39;score&#39; as values and you want to sort on &#39;score&#39;:\r\n\r\n    import collections\r\n    Player = collections.namedtuple(&#39;Player&#39;, &#39;score name&#39;)\r\n    d = {&#39;John&#39;:5, &#39;Alex&#39;:10, &#39;Richard&#39;: 7}\r\n\r\nsorting with lowest score first:\r\n\r\n    worst = sorted(Player(v,k) for (k,v) in d.items())\r\n\r\nsorting with highest score first:\r\n\r\n    best = sorted([Player(v,k) for (k,v) in d.items()], reverse=True)\r\n\r\n\r\nNow you can get the name and score of, let&#39;s say the second-best player (index=1) very Pythonically like this:\r\n    \r\n    player = best[1]\r\n    player.name\r\n        &#39;Richard&#39;\r\n    player.score\r\n        7\r\n",
               "tags": [],
               "creation_date": 1314664215,
               "last_edit_date": 1492999919,
               "is_accepted": false,
               "id": "7237524",
               "down_vote_count": 0,
               "score": 57
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 7817348,
               "is_accepted": false,
               "last_activity_date": 1319005541,
               "body_markdown": "Use **ValueSortedDict** from [dicts](http://pypi.python.org/pypi/dicts):\r\n\r\n\r\n    from dicts.sorteddict import ValueSortedDict\r\n    d = {1: 2, 3: 4, 4:3, 2:1, 0:0}\r\n    sorted_dict = ValueSortedDict(d)\r\n    print sorted_dict.items() \r\n\r\n    [(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]",
               "id": "7817348",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1319005541,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 7947321,
               "is_accepted": false,
               "last_activity_date": 1320003726,
               "body_markdown": "Iterate through a dict and sort it by its values in descending order:\r\n\r\n    $ python --version\r\n    Python 3.2.2\r\n\r\n    $ cat sort_dict_by_val_desc.py \r\n    dictionary = dict(siis = 1, sana = 2, joka = 3, tuli = 4, aina = 5)\r\n    for word in sorted(dictionary, key=dictionary.get, reverse=True):\r\n      print(word, dictionary[word])\r\n\r\n    $ python sort_dict_by_val_desc.py \r\n    aina 5\r\n    tuli 4\r\n    joka 3\r\n    sana 2\r\n    siis 1\r\n\r\n\r\n    \r\n\r\n",
               "id": "7947321",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1320003726,
               "score": 5
            },
            {
               "up_vote_count": 5,
               "answer_id": 8148132,
               "last_activity_date": 1352230051,
               "path": "3.stack.answer",
               "body_markdown": "This works in 3.1.x:\r\n\r\n    import operator\r\n    slovar_sorted=sorted(slovar.items(), key=operator.itemgetter(1), reverse=True)\r\n    print(slovar_sorted)\r\n\r\n",
               "tags": [],
               "creation_date": 1321428721,
               "last_edit_date": 1352230051,
               "is_accepted": false,
               "id": "8148132",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "up_vote_count": 5,
               "answer_id": 8992838,
               "last_activity_date": 1327434643,
               "path": "3.stack.answer",
               "body_markdown": "If your values are integers, and you use Python 2.7 or newer, you can use [`collections.Counter`](http://docs.python.org/py3k/library/collections.html#collections.Counter) instead of `dict`. The `most_common` method will give you all items, sorted by the value.\r\n\r\n",
               "tags": [],
               "creation_date": 1327433316,
               "last_edit_date": 1327434643,
               "is_accepted": false,
               "id": "8992838",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "up_vote_count": 20,
               "answer_id": 11230132,
               "last_activity_date": 1340812172,
               "path": "3.stack.answer",
               "body_markdown": "If values are numeric you may also use Counter from collections\r\n\r\n    from collections import Counter\r\n    \r\n    x={&#39;hello&#39;:1,&#39;python&#39;:5, &#39;world&#39;:3}\r\n    c=Counter(x)\r\n    print c.most_common()\r\n\r\n\r\n    &gt;&gt; [(&#39;python&#39;, 5), (&#39;world&#39;, 3), (&#39;hello&#39;, 1)]    ",
               "tags": [],
               "creation_date": 1340811825,
               "last_edit_date": 1340812172,
               "is_accepted": false,
               "id": "11230132",
               "down_vote_count": 1,
               "score": 19
            },
            {
               "up_vote_count": 1,
               "answer_id": 13208582,
               "last_activity_date": 1396544631,
               "path": "3.stack.answer",
               "body_markdown": "Using Python 3.2:\r\n   \r\n    x = {&quot;b&quot;:4, &quot;a&quot;:3, &quot;c&quot;:1}\r\n    for i in sorted(x.values()):\r\n        print(list(x.keys())[list(x.values()).index(i)])\r\n",
               "tags": [],
               "creation_date": 1351940841,
               "last_edit_date": 1396544631,
               "is_accepted": false,
               "id": "13208582",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 20,
               "answer_id": 15310681,
               "last_activity_date": 1396544698,
               "path": "3.stack.answer",
               "body_markdown": "You can use the [collections.Counter][1]. Note, this will work for both numeric and non-numeric values.\r\n\r\n    &gt;&gt;&gt; x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\r\n    &gt;&gt;&gt; from collections import Counter\r\n    &gt;&gt;&gt; #To sort in reverse order\r\n    &gt;&gt;&gt; Counter(x).most_common()\r\n    [(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\r\n    &gt;&gt;&gt; #To sort in ascending order\r\n    &gt;&gt;&gt; Counter(x).most_common()[::-1]\r\n    [(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\r\n    &gt;&gt;&gt; #To get a dictionary sorted by values\r\n    &gt;&gt;&gt; from collections import OrderedDict\r\n    &gt;&gt;&gt; OrderedDict(Counter(x).most_common()[::-1])\r\n    OrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\r\n\r\n  [1]: http://docs.python.org/2/library/collections.html#collections.Counter\r\n",
               "tags": [],
               "creation_date": 1362832222,
               "last_edit_date": 1396544698,
               "is_accepted": false,
               "id": "15310681",
               "down_vote_count": 4,
               "score": 16
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 15587800,
               "is_accepted": false,
               "last_activity_date": 1364048393,
               "body_markdown": "For the sake of completeness, I am posting a solution using [heapq](http://docs.python.org/2/library/heapq.html). Note, this method will work for both numeric and non-numeric values\r\n\r\n\r\n    &gt;&gt;&gt; x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\r\n    &gt;&gt;&gt; x_items = x.items()\r\n    &gt;&gt;&gt; heapq.heapify(x_items)\r\n    &gt;&gt;&gt; #To sort in reverse order\r\n    &gt;&gt;&gt; heapq.nlargest(len(x_items),x_items, operator.itemgetter(1))\r\n    [(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\r\n    &gt;&gt;&gt; #To sort in ascending order\r\n    &gt;&gt;&gt; heapq.nsmallest(len(x_items),x_items, operator.itemgetter(1))\r\n    [(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]",
               "id": "15587800",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1364048393,
               "score": 3
            },
            {
               "up_vote_count": 6,
               "answer_id": 16435785,
               "last_activity_date": 1396544879,
               "path": "3.stack.answer",
               "body_markdown": "I came up with this one, \r\n\r\n    import operator    \r\n    x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\r\n    sorted_x = {k[0]:k[1] for k in sorted(x.items(), key=operator.itemgetter(1))}\r\n\r\nFor Python 3.x: `x.items()` replacing `iteritems()`.\r\n\r\n    &gt;&gt;&gt; sorted_x\r\n    {0: 0, 1: 2, 2: 1, 3: 4, 4: 3}\r\n\r\nOr try with `collections.OrderedDict`!\r\n\r\n    x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\r\n    from collections import OrderedDict\r\n    \r\n    od1 = OrderedDict(sorted(x.items(), key=lambda t: t[1]))\r\n",
               "tags": [],
               "creation_date": 1368001075,
               "last_edit_date": 1396544879,
               "is_accepted": false,
               "id": "16435785",
               "down_vote_count": 1,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 29,
               "answer_id": 18375444,
               "is_accepted": false,
               "last_activity_date": 1377160728,
               "body_markdown": "In Python 2.7, simply do:\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    from collections import OrderedDict\r\n    # regular unsorted dictionary\r\n    d = {&#39;banana&#39;: 3, &#39;apple&#39;:4, &#39;pear&#39;: 1, &#39;orange&#39;: 2}\r\n\r\n    # dictionary sorted by key\r\n    OrderedDict(sorted(d.items(), key=lambda t: t[0]))\r\n    OrderedDict([(&#39;apple&#39;, 4), (&#39;banana&#39;, 3), (&#39;orange&#39;, 2), (&#39;pear&#39;, 1)])\r\n   \r\n    # dictionary sorted by value\r\n    OrderedDict(sorted(d.items(), key=lambda t: t[1]))\r\n    OrderedDict([(&#39;pear&#39;, 1), (&#39;orange&#39;, 2), (&#39;banana&#39;, 3), (&#39;apple&#39;, 4)])\r\n\r\ncopy-paste from : http://docs.python.org/dev/library/collections.html#ordereddict-examples-and-recipes\r\n\r\nEnjoy ;-)",
               "id": "18375444",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1377160728,
               "score": 29
            },
            {
               "up_vote_count": 9,
               "answer_id": 21738569,
               "last_activity_date": 1447109779,
               "path": "3.stack.answer",
               "body_markdown": "This returns the list of key-value pairs in the dictionary, sorted by value from highest to lowest:\r\n\r\n    sorted(d.items(), key=lambda x: x[1], reverse=True)\r\n\r\nFor the dictionary sorted by key, use the following:\r\n\r\n    sorted(d.items(), reverse=True)\r\n\r\nThe return is a list of tuples because dictionaries themselves can&#39;t be sorted.\r\n\r\nThis can be both printed or sent into further computation.",
               "tags": [],
               "creation_date": 1392235846,
               "last_edit_date": 1447109779,
               "is_accepted": false,
               "id": "21738569",
               "down_vote_count": 0,
               "score": 9
            },
            {
               "up_vote_count": 2,
               "answer_id": 21767696,
               "last_activity_date": 1392335995,
               "path": "3.stack.answer",
               "body_markdown": "    months = {&quot;January&quot;: 31, &quot;February&quot;: 28, &quot;March&quot;: 31, &quot;April&quot;: 30, &quot;May&quot;: 31,\r\n              &quot;June&quot;: 30, &quot;July&quot;: 31, &quot;August&quot;: 31, &quot;September&quot;: 30, &quot;October&quot;: 31,\r\n              &quot;November&quot;: 30, &quot;December&quot;: 31}\r\n\r\n    def mykey(t):\r\n        &quot;&quot;&quot; Customize your sorting logic using this function.  The parameter to\r\n        this function is a tuple.  Comment/uncomment the return statements to test\r\n        different logics.\r\n        &quot;&quot;&quot;\r\n        return t[1]              # sort by number of days in the month\r\n        #return t[1], t[0]       # sort by number of days, then by month name\r\n        #return len(t[0])        # sort by length of month name\r\n        #return t[0][-1]         # sort by last character of month name\r\n\r\n\r\n    # Since a dictionary can&#39;t be sorted by value, what you can do is to convert\r\n    # it into a list of tuples with tuple length 2.\r\n    # You can then do custom sorts by passing your own function to sorted().\r\n    months_as_list = sorted(months.items(), key=mykey, reverse=False)\r\n\r\n    for month in months_as_list:\r\n        print month\r\n",
               "tags": [],
               "creation_date": 1392333500,
               "last_edit_date": 1392335995,
               "is_accepted": false,
               "id": "21767696",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 21784867,
               "is_accepted": false,
               "last_activity_date": 1392396274,
               "body_markdown": "    &gt;&gt;&gt; import collections\r\n    &gt;&gt;&gt; x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\r\n    &gt;&gt;&gt; sorted_x = collections.OrderedDict(sorted(x.items(), key=lambda t:t[1]))\r\n    &gt;&gt;&gt; OrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\r\n\r\n`OrderedDict` is subclass of `dict`",
               "id": "21784867",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1392396274,
               "score": 0
            },
            {
               "up_vote_count": 1,
               "answer_id": 22150003,
               "last_activity_date": 1396545013,
               "path": "3.stack.answer",
               "body_markdown": "Because of requirements to retain backward compatability with older versions of [Python][1] I think the OrderedDict solution is very unwise. You want something that works with Python 2.7 and older versions.\r\n\r\nBut the collections solution mentioned in another answer is absolutely superb, because you retrain a connection between the key and value which in the case of dictionaries is extremely important.\r\n\r\nI don&#39;t agree with the number one choice presented in another answer, because it throws away the keys.\r\n\r\nI used the solution mentioned above (code shown below) and retained access to both keys and values and in my case the ordering was on the values, but the importance was the ordering of the keys after ordering the values.\r\n\r\n    from collections import Counter\r\n\r\n    x = {&#39;hello&#39;:1, &#39;python&#39;:5, &#39;world&#39;:3}\r\n    c=Counter(x)\r\n    print c.most_common()\r\n\r\n\r\n    &gt;&gt; [(&#39;python&#39;, 5), (&#39;world&#39;, 3), (&#39;hello&#39;, 1)]\r\n\r\n  [1]: http://en.wikipedia.org/wiki/Python_%28programming_language%29\r\n",
               "tags": [],
               "creation_date": 1393858710,
               "last_edit_date": 1396545013,
               "is_accepted": false,
               "id": "22150003",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 22903797,
               "is_accepted": false,
               "last_activity_date": 1396846004,
               "body_markdown": "Why not try this approach. Let us define a dictionary called mydict with the following data:\r\n\r\n    mydict = {&#39;carl&#39;:40,\r\n              &#39;alan&#39;:2,\r\n              &#39;bob&#39;:1,\r\n              &#39;danny&#39;:3}\r\n\r\nIf one wanted to sort the dictionary by keys, one could do something like:\r\n\r\n    for key in sorted(mydict.iterkeys()):\r\n        print &quot;%s: %s&quot; % (key, mydict[key])\r\n\r\nThis should return the following output:\r\n\r\n    alan: 2\r\n    bob: 1\r\n    carl: 40\r\n    danny: 3\r\n\r\nOn the other hand, if one wanted to sort a dictionary by value (as is asked in the question), one could do the following:\r\n\r\n    for key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k)):\r\n        print &quot;%s: %s&quot; % (key, value)\r\n\r\nThe result of this command (sorting the dictionary by value) should return the following:\r\n\r\n    bob: 1\r\n    alan: 2\r\n    danny: 3\r\n    carl: 40\r\n",
               "id": "22903797",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1396846004,
               "score": 12
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 11,
               "answer_id": 26049456,
               "is_accepted": false,
               "last_activity_date": 1411685815,
               "body_markdown": "You can use a [skip dict](https://pypi.python.org/pypi/skipdict/1.0) which is a dictionary that&#39;s permanently sorted by value.\r\n\r\n    &gt;&gt;&gt; data = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\r\n    &gt;&gt;&gt; SkipDict(data)\r\n    {0: 0.0, 2: 1.0, 1: 2.0, 4: 3.0, 3: 4.0}\r\n\r\nIf you use ``keys()``, ``values()`` or ``items()`` then you&#39;ll iterate in sorted order by value.\r\n\r\nIt&#39;s implemented using the [skip list](http://en.wikipedia.org/wiki/Skip_list) datastructure.",
               "id": "26049456",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1411685815,
               "score": 11
            },
            {
               "up_vote_count": 4,
               "answer_id": 27064308,
               "last_activity_date": 1432151667,
               "path": "3.stack.answer",
               "body_markdown": "You can use the sorted function of Python\r\n\r\n`sorted(iterable[, cmp[, key[, reverse]]])`\r\n\r\nThus you can use:\r\n\r\n`sorted(dictionary.items(),key = lambda x :x[1])`\r\n\r\nVisit this link for more information on sorted function: https://docs.python.org/2/library/functions.html#sorted",
               "tags": [],
               "creation_date": 1416582249,
               "last_edit_date": 1432151667,
               "is_accepted": false,
               "id": "27064308",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 30949456,
               "is_accepted": false,
               "last_activity_date": 1434764698,
               "body_markdown": "Here is a solution using zip on [`d.values()` and `d.keys()`][1].  A few lines down this link (on Dictionary view objects) is:\r\n\r\n&gt;This allows the creation of (value, key) pairs using zip(): pairs = zip(d.values(), d.keys()).\r\n\r\nSo we can do the following:\r\n\r\n    d = {&#39;key1&#39;: 874.7, &#39;key2&#39;: 5, &#39;key3&#39;: 8.1}\r\n\r\n    d_sorted = sorted(zip(d.values(), d.keys()))\r\n\r\n    print d_sorted \r\n    # prints: [(5, &#39;key2&#39;), (8.1, &#39;key3&#39;), (874.7, &#39;key1&#39;)]\r\n\r\n\r\n  [1]: https://docs.python.org/2/library/stdtypes.html#dictionary-view-objects",
               "id": "30949456",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1434764698,
               "score": 5
            },
            {
               "up_vote_count": 1,
               "answer_id": 31689638,
               "last_activity_date": 1439493498,
               "path": "3.stack.answer",
               "body_markdown": "I&#39;ve found that the following function performs well compared to other solutions posted, even on large dictionaries.\r\n\r\n    vsort = lambda d: sorted(d.iteritems(), key=lambda (k, v): v)\r\n    \r\nExample:\r\n\r\n    data = {}\r\n    for i in range(10):\r\n        data[i] = i if i % 2  else -i\r\n    \r\n    print &#39;Original&#39;\r\n    for k, v in data.items():\r\n        print &quot;k: %s v: %s&quot; % (k, v)\r\n    print &#39;&#39;\r\n    \r\n    print &#39;Value-sorted&#39;\r\n    for k, v in vsort(data):\r\n        print &quot;k: %s v: %s&quot; % (k, v)\r\n    print &#39;&#39;\r\n\r\nOutput:\r\n\r\n    Original\r\n    k: 0 v: 0\r\n    k: 1 v: 1\r\n    k: 2 v: -2\r\n    k: 3 v: 3\r\n    k: 4 v: -4\r\n    k: 5 v: 5\r\n    k: 6 v: -6\r\n    k: 7 v: 7\r\n    k: 8 v: -8\r\n    k: 9 v: 9\r\n    \r\n    Value-sorted\r\n    k: 8 v: -8\r\n    k: 6 v: -6\r\n    k: 4 v: -4\r\n    k: 2 v: -2\r\n    k: 0 v: 0\r\n    k: 1 v: 1\r\n    k: 3 v: 3\r\n    k: 5 v: 5\r\n    k: 7 v: 7\r\n    k: 9 v: 9\r\n\r\nSample timing code:\r\n\r\n    import numpy as np\r\n    from time import time\r\n    import operator\r\n    np.random.seed(0)\r\n\r\n    N = int(1e6)\r\n    x = {i: np.random.random() for i in xrange(N)}\r\n\r\n    t0 = -time()\r\n    sorted_0 = sorted(x.items(), key=operator.itemgetter(1))\r\n    t0 += time()\r\n\r\n    t1 = -time()\r\n    sorted_1 = vsort(x)\r\n    t1 += time()\r\n\r\n    print &#39;operator-sort: %f vsort: %f&#39; % (t0, t1)\r\n    print sorted_0[:3]\r\n    print sorted_1[:3]\r\n\r\nOutput:\r\n\r\n    operator-sort: 2.041510 vsort: 1.692324\r\n    [(661553, 7.071203171893359e-07), (529124, 1.333679640169727e-06), (263972, 2.9504162779581122e-06)]\r\n    [(661553, 7.071203171893359e-07), (529124, 1.333679640169727e-06), (263972, 2.9504162779581122e-06)]\r\n",
               "tags": [],
               "creation_date": 1438132093,
               "last_edit_date": 1439493498,
               "is_accepted": false,
               "id": "31689638",
               "down_vote_count": 3,
               "score": -2
            },
            {
               "up_vote_count": 3,
               "answer_id": 31741215,
               "last_activity_date": 1438331705,
               "path": "3.stack.answer",
               "body_markdown": "Of course, remember, you need to use `OrderedDict` because regular Python dictionaries don&#39;t keep the original order. \r\n\r\n    from collections import OrderedDict\r\n    a = OrderedDict(sorted(originalDict.items(), key = lambda x: x[1]))\r\n\r\n________________________________________________________________________________\r\nIf you do not have Python 2.7 or higher, the best you can do is iterate over the values in a generator function. (There is an OrderedDict for 2.4 and 2.6  [here](https://pypi.python.org/pypi/ordereddict), but \r\n\r\n    a) I don&#39;t know about how well it works \r\n\r\nand \r\n\r\n    b) You have to download and install it of course. If you do not have administrative access, then I&#39;m afraid the option&#39;s out.)\r\n\r\n________________________________________________________________________________\r\n\r\n    def gen(originalDict):\r\n        for x,y in sorted(zip(originalDict.keys(), originalDict.values()), key = lambda z: z[1]):\r\n            yield (x, y)\r\n        #Yields as a tuple with (key, value). You can iterate with conditional clauses to get what you want. \r\n\r\n    for bleh, meh in gen(myDict):\r\n        if bleh == &quot;foo&quot;:\r\n            print(myDict[bleh])\r\n________________________________________________________________________________\r\nYou can also print out every value\r\n    \r\n    for bleh, meh in gen(myDict):\r\n        print(bleh,meh)\r\n\r\nPlease remember to remove the parentheses after print if not using Python 3.0 or above",
               "tags": [],
               "creation_date": 1438330129,
               "last_edit_date": 1438331705,
               "is_accepted": false,
               "id": "31741215",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 63,
               "answer_id": 34103440,
               "last_activity_date": 1450158893,
               "path": "3.stack.answer",
               "body_markdown": "**UPDATE: 5 DECEMBER 2015 using Python 3.5**\r\n\r\nWhilst I found the accepted answer useful, I was also surprised that it hasn&#39;t been updated to reference **[OrderedDict][1]** from the standard library **collections** module as a viable, modern alternative - designed to solve exactly this type of problem.\r\n\r\n    from operator import itemgetter\r\n    from collections import OrderedDict\r\n\r\n    x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\r\n    sorted_x = OrderedDict(sorted(x.items(), key=itemgetter(1)))\r\n    # OrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\r\n\r\n\r\nThe official **[OrderedDict][1]** documentation offers a very similar example too, but using a lambda for the sort function:\r\n\r\n    # regular unsorted dictionary\r\n    d = {&#39;banana&#39;: 3, &#39;apple&#39;:4, &#39;pear&#39;: 1, &#39;orange&#39;: 2}\r\n    \r\n    # dictionary sorted by value\r\n    OrderedDict(sorted(d.items(), key=lambda t: t[1]))\r\n    # OrderedDict([(&#39;pear&#39;, 1), (&#39;orange&#39;, 2), (&#39;banana&#39;, 3), (&#39;apple&#39;, 4)])\r\n\r\n\r\n  [1]: https://docs.python.org/3/library/collections.html#collections.OrderedDict",
               "tags": [],
               "creation_date": 1449308766,
               "last_edit_date": 1450158893,
               "is_accepted": false,
               "id": "34103440",
               "down_vote_count": 0,
               "score": 63
            },
            {
               "up_vote_count": 33,
               "answer_id": 34995529,
               "last_activity_date": 1501415361,
               "path": "3.stack.answer",
               "body_markdown": "Given dictionary\r\n\r\n    e = {1:39, 4:34, 7:110, 2:87}\r\n\r\nSorting\r\n\r\n    sred = sorted(e.items(), key=lambda value: value[1])\r\n\r\nResult \r\n\r\n    [(4, 34), (1, 39), (2, 87), (7, 110)]\r\n\r\nYou can use a lambda function to sort things up by value and store them processed inside a variable, in this case **sred** with **e** the original dictionary.\r\n\r\nHope that helps!",
               "tags": [],
               "creation_date": 1453733683,
               "last_edit_date": 1501415361,
               "is_accepted": false,
               "id": "34995529",
               "down_vote_count": 1,
               "score": 32
            },
            {
               "up_vote_count": 36,
               "answer_id": 39424969,
               "last_activity_date": 1513439275,
               "path": "3.stack.answer",
               "body_markdown": "# As of [Python 3.6][whatsnew36] the built-in dict will be ordered\r\n\r\nGood news, so the OP&#39;s original use case of mapping pairs retrieved from a database with unique string ids as keys and numeric values as values into a built-in Python v3.6+ dict, should now respect the insert order.\r\n\r\nIf say the resulting two column table expressions from a database query like:\r\n\r\n    SELECT a_key, a_value FROM a_table ORDER BY a_value;\r\n\r\nwould be stored in two Python tuples, k_seq and v_seq (aligned by numerical index and with the same length of course), then:\r\n\r\n    k_seq = (&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;)\r\n    v_seq = (0, 1, 42)\r\n    ordered_map = dict(zip(k_seq, v_seq))\r\n\r\nAllow to output later as:\r\n\r\n    for k, v in ordered_map.items():\r\n        print(k, v)\r\n\r\nyielding in this case (for the new Python 3.6+ built-in dict!):\r\n\r\n    foo 0\r\n    bar 1\r\n    baz 42\r\n\r\nin the same ordering per value of v.\r\n\r\nWhere in the Python 3.5 install on my machine it currently yields:\r\n\r\n    bar 1\r\n    foo 0\r\n    baz 42\r\n\r\n## Details:\r\n\r\nAs proposed in 2012 by Raymond Hettinger (cf. mail on python-dev with subject [&quot;More compact dictionaries with faster iteration&quot;][initial]) and now (in 2016) announced in a mail by Victor Stinner to python-dev with subject [&quot;Python 3.6 dict becomes compact and gets a private version; and keywords become ordered&quot;][announce] due to the fix/implementation of issue 27350 [&quot;Compact and ordered dict&quot;][issue27350] in Python 3.6 we will now be able, to use a built-in dict to maintain insert order!!\r\n\r\nHopefully this will lead to a thin layer OrderedDict implementation as a first step. As @JimFasarakis-Hilliard indicated, some see use cases for the OrderedDict type also in the future. I think the Python community at large will carefully inspect, if this will stand the test of time, and what the next steps will be.\r\n\r\nTime to rethink our coding habits to not miss the possibilities opened by stable ordering of:\r\n\r\n* Keyword arguments and\r\n* (intermediate) dict storage\r\n\r\nThe first because it eases dispatch in the implementation of functions and methods in some cases.\r\n\r\nThe second as it encourages to more easily use `dict`s as intermediate storage in processing pipelines.\r\n\r\nRaymond Hettinger kindly provided documentation explaining &quot;[The Tech Behind Python 3.6 Dictionaries][sfmurh2016]&quot; - from his San Francisco Python Meetup Group presentation 2016-DEC-08.\r\n\r\nAnd maybe quite some Stack Overflow high decorated question and answer pages will receive variants of this information and many high quality answers will require a per version update too.\r\n\r\n### Caveat Emptor (but also see below update 2017-12-15):\r\n\r\nAs @ajcr rightfully notes: &quot;The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon.&quot; (from the [whatsnew36]) not nit picking, **but** the citation was cut a bit pessimistic ;-). It continues as &quot; (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5).&quot;\r\n\r\nSo as in some human languages (e.g. German), usage shapes the language, and the will now has been declared ... in [whatsnew36].\r\n\r\n[whatsnew36]: https://docs.python.org/3.6/whatsnew/3.6.html\r\n[initial]: https://mail.python.org/pipermail/python-dev/2012-December/123028.html\r\n[announce]: https://mail.python.org/pipermail/python-dev/2016-September/146327.html\r\n[issue27350]: http://bugs.python.org/issue27350\r\n[sfmurh2016]: https://dl.dropboxusercontent.com/u/3967849/sfmu2/_build/html/goal.html &quot;The Tech Behind Python 3.6 Dictionaries&quot;\r\n\r\n### Update 2017-12-15: \r\nIn a [mail to the python-dev list](https://mail.python.org/pipermail/python-dev/2017-December/151283.html), Guido van Rossum declared:\r\n\r\n&gt; Make it so. &quot;Dict keeps insertion order&quot; is the ruling. Thanks! \r\n\r\nSo, the version 3.6 CPython side-effect of dict insertion ordering is now becoming part of the language spec (and not anymore only an implementation detail). That mail thread also surfaced some distinguishing design goals for `collections.OrderedDict` as reminded by Raymond Hettinger during discussion.",
               "tags": [],
               "creation_date": 1473501936,
               "last_edit_date": 1513439275,
               "is_accepted": false,
               "id": "39424969",
               "down_vote_count": 3,
               "score": 33
            },
            {
               "up_vote_count": 2,
               "answer_id": 44148366,
               "last_activity_date": 1511877031,
               "path": "3.stack.answer",
               "body_markdown": "This method will not use lambda and works well on Python 3.6:\r\n\r\n     # sort dictionary by value\r\n    d = {&#39;a1&#39;: &#39;fsdfds&#39;, &#39;g5&#39;: &#39;aa3432ff&#39;, &#39;ca&#39;:&#39;zz23432&#39;}\r\n    def getkeybyvalue(d,i):\r\n        for k, v in d.items():\r\n            if v == i:\r\n                return (k)\r\n        \r\n    sortvaluelist = sorted(d.values())\r\n    sortresult ={}\r\n    for i1 in sortvaluelist:   \r\n        key = getkeybyvalue(d,i1)\r\n        sortresult[key] = i1\r\n    print (&#39;=====sort by value=====&#39;)\r\n    print (sortresult)\r\n    print (&#39;=======================&#39;)",
               "tags": [],
               "creation_date": 1495595944,
               "last_edit_date": 1511877031,
               "is_accepted": false,
               "id": "44148366",
               "down_vote_count": 2,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 44187197,
               "is_accepted": false,
               "last_activity_date": 1495735991,
               "body_markdown": "You can also use custom function that can be passed to key.\r\n\r\n    def dict_val(x):\r\n        return x[1]\r\n    x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\r\n    sorted_x = sorted(x.items(), key=dict_val)\r\n\r\nOne more way to do is to use labmda function\r\n\r\n    x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\r\n    sorted_x = sorted(x.items(), key=lambda t: t[1])\r\n\r\n",
               "id": "44187197",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1495735991,
               "score": 6
            }
         ],
         "link": "https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value",
         "id": "858127-2248"
      },
      {
         "up_vote_count": "198",
         "path": "2.stack",
         "body_markdown": "I am curious as to why `df[2]` is not supported, while `df.ix[2]` and `df[2:3]` both work. \r\n\r\n    In [26]: df.ix[2]\r\n    Out[26]: \r\n    A    1.027680\r\n    B    1.514210\r\n    C   -1.466963\r\n    D   -0.162339\r\n    Name: 2000-01-03 00:00:00\r\n    \r\n    In [27]: df[2:3]\r\n    Out[27]: \r\n                      A        B         C         D\r\n    2000-01-03  1.02768  1.51421 -1.466963 -0.162339\r\n\r\nI would expect `df[2]` to work the same way as `df[2:3]` to be consistent with Python indexing convention. Is there a design reason for not supporting indexing row by single integer?",
         "view_count": "321213",
         "answer_count": "6",
         "tags": "['python', 'pandas', 'dataframe', 'indexing']",
         "creation_date": "1366341240",
         "last_edit_date": "1513326749",
         "code_snippet": "['<code>df[2]</code>', '<code>df.ix[2]</code>', '<code>df[2:3]</code>', '<code>In [26]: df.ix[2]\\nOut[26]: \\nA    1.027680\\nB    1.514210\\nC   -1.466963\\nD   -0.162339\\nName: 2000-01-03 00:00:00\\n\\nIn [27]: df[2:3]\\nOut[27]: \\n                  A        B         C         D\\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\\n</code>', '<code>df[2]</code>', '<code>df[2:3]</code>', '<code>df.ix[2]</code>', \"<code>pandas version  '0.19.2'</code>\", '<code>[]</code>', '<code>.ix</code>', '<code>.iloc</code>', '<code>.loc</code>', \"<code>In [1]: df = DataFrame(randn(5,2),index=range(0,10,2),columns=list('AB'))\\n\\nIn [2]: df\\nOut[2]: \\n          A         B\\n0  1.068932 -0.794307\\n2 -0.470056  1.192211\\n4 -0.284561  0.756029\\n6  1.037563 -0.267820\\n8 -0.538478 -0.800654\\n\\nIn [5]: df.iloc[[2]]\\nOut[5]: \\n          A         B\\n4 -0.284561  0.756029\\n\\nIn [6]: df.loc[[2]]\\nOut[6]: \\n          A         B\\n2 -0.470056  1.192211\\n</code>\", '<code>[]</code>', '<code>iloc</code>', '<code>loc</code>', '<code>.iloc</code>', '<code>.iloc[[2]]</code>', '<code>df</code>', '<code>4</code>', '<code>.loc</code>', '<code>A[i]</code>', '<code>df.loc[-1] = df.iloc[[0]]</code>', '<code>ValueError: cannot set a row with mismatched columns</code>', '<code>df[key]</code>', '<code>key</code>', '<code>np_df = df.as_matrix()\\n</code>', '<code>np_df[i] \\n</code>', '<code>[]</code>', '<code>df[2]</code>', '<code>2</code>', '<code>KeyError</code>', '<code>df[2:3]\\n</code>', '<code>df[6:20:3]\\n</code>', '<code>.loc/.iloc</code>', '<code>DataFrame</code>', '<code>_slice()</code>', '<code>DataFrame</code>', '<code>axis</code>', '<code>__getitem__()</code>', '<code>DataFrame</code>', '<code>_slice()</code>', '<code>_slice()</code>', '<code>print df._slice(slice(0, 2))\\nprint df._slice(slice(0, 2), 0)\\nprint df._slice(slice(0, 2), 1)\\n</code>', '<code>for ad in range(1,dataframe_c.size):\\n    print(dataframe_c.values[ad])\\n</code>']",
         "title": "Selecting a row of pandas series/dataframe by integer index",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 14,
               "answer_id": 16099579,
               "is_accepted": false,
               "last_activity_date": 1366356812,
               "body_markdown": "You can think DataFrame as a dict of Series. `df[key]` try to select the column index by `key` and returns a Series object.\r\n\r\nHowever slicing inside of [] slices the rows, because it&#39;s a very common operation.\r\n\r\nYou can read the document for detail:\r\n\r\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#basics",
               "id": "16099579",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1366356812,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 16102866,
               "is_accepted": false,
               "last_activity_date": 1366368445,
               "body_markdown": "You can take a look at the [source code](https://github.com/pydata/pandas/blob/master/pandas/core/frame.py#L1890) .\r\n\r\n`DataFrame` has a private function `_slice()` to slice the `DataFrame`, and it allows the parameter `axis` to determine which axis to slice. The `__getitem__()` for `DataFrame` doesn&#39;t set the axis while invoking `_slice()`. So the `_slice()` slice it by default axis 0.\r\n\r\nYou can take a simple experiment, that might help you:\r\n\r\n    print df._slice(slice(0, 2))\r\n    print df._slice(slice(0, 2), 0)\r\n    print df._slice(slice(0, 2), 1)",
               "id": "16102866",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1366368445,
               "score": 2
            },
            {
               "up_vote_count": 302,
               "answer_id": 16104482,
               "last_activity_date": 1445373667,
               "path": "3.stack.answer",
               "body_markdown": "echoing @HYRY, see the new docs in 0.11\r\n\r\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html\r\n\r\nHere we have new operators, ``.iloc`` to explicity support only integer indexing, and ``.loc`` to explicity support only label indexing\r\n\r\ne.g. imagine this scenario\r\n\r\n    In [1]: df = DataFrame(randn(5,2),index=range(0,10,2),columns=list(&#39;AB&#39;))\r\n    \r\n    In [2]: df\r\n    Out[2]: \r\n              A         B\r\n    0  1.068932 -0.794307\r\n    2 -0.470056  1.192211\r\n    4 -0.284561  0.756029\r\n    6  1.037563 -0.267820\r\n    8 -0.538478 -0.800654\r\n    \r\n    In [5]: df.iloc[[2]]\r\n    Out[5]: \r\n              A         B\r\n    4 -0.284561  0.756029\r\n    \r\n    In [6]: df.loc[[2]]\r\n    Out[6]: \r\n              A         B\r\n    2 -0.470056  1.192211\r\n\r\n``[]`` slices the rows (by label location) only",
               "tags": [],
               "creation_date": 1366374025,
               "last_edit_date": 1445373667,
               "is_accepted": true,
               "id": "16104482",
               "down_vote_count": 2,
               "score": 300
            },
            {
               "up_vote_count": 2,
               "answer_id": 36099473,
               "last_activity_date": 1492080964,
               "path": "3.stack.answer",
               "body_markdown": "you can loop through the data frame like this .\r\n\r\n    for ad in range(1,dataframe_c.size):\r\n        print(dataframe_c.values[ad])\r\n\r\n",
               "tags": [],
               "creation_date": 1458375305,
               "last_edit_date": 1492080964,
               "is_accepted": false,
               "id": "36099473",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 37384347,
               "is_accepted": false,
               "last_activity_date": 1463986386,
               "body_markdown": "To index-based access to the pandas table, one can also consider *numpy.as_array* option to convert the table to Numpy array as\r\n\r\n    np_df = df.as_matrix()\r\n\r\nand then\r\n\r\n    np_df[i] \r\nwould work.\r\n",
               "id": "37384347",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1463986386,
               "score": 7
            },
            {
               "up_vote_count": 7,
               "answer_id": 46920450,
               "last_activity_date": 1509910900,
               "path": "3.stack.answer",
               "body_markdown": "### The primary purpose of the DataFrame indexing operator, `[]` is to select columns.\r\nWhen the indexing operator is passed a string or integer, it attempts to find a column with that particular name and return it as a Series.\r\n\r\nSo, in the question above: `df[2]` searches for a column name matching the integer value `2`. This column does not exist and a `KeyError` is raised.\r\n\r\n___\r\n### The DataFrame indexing operator completely changes behavior to select rows when slice notation is used\r\n\r\nStrangely, when given a slice, the DataFrame indexing operator selects rows and can do so by integer location or by index label. \r\n\r\n    df[2:3]\r\n\r\nThis will slice beginning from the row with integer location 2 up to 3, exclusive of the last element. So, just a single row. The following selects rows beginning at integer location 6 up to but not including 20 by every third row.\r\n\r\n    df[6:20:3]\r\n\r\nYou can also use slices consisting of string labels if your DataFrame index has strings in it. For more details, see [this solution on .iloc vs .loc](https://stackoverflow.com/questions/31593201/pandas-iloc-vs-ix-vs-loc-explanation/46915810#46915810).\r\n\r\nI almost never use this slice notation with the indexing operator as its not explicit and hardly ever used. When slicing by rows, stick with `.loc/.iloc`.\r\n\r\n",
               "tags": [],
               "creation_date": 1508881027,
               "last_edit_date": 1509910900,
               "is_accepted": false,
               "id": "46920450",
               "down_vote_count": 0,
               "score": 7
            }
         ],
         "link": "https://stackoverflow.com/questions/16096627/selecting-a-row-of-pandas-series-dataframe-by-integer-index",
         "id": "858127-2249"
      },
      {
         "up_vote_count": "300",
         "path": "2.stack",
         "body_markdown": "Can someone explain how these three methods of slicing are different?  \r\nI&#39;ve seen [the docs](http://pandas.pydata.org/pandas-docs/stable/indexing.html), \r\nand I&#39;ve seen [these](https://stackoverflow.com/questions/28757389/loc-vs-iloc-vs-ix-vs-at-vs-iat) [answers](https://stackoverflow.com/questions/27667759/is-ix-always-better-than-loc-and-iloc-since-it-is-faster-and-supports-i), but I still find myself unable to explain how the three are different.  To me, they seem interchangeable in large part, because they are at the lower levels of slicing.\r\n\r\nFor example, say we want to get the first five rows of a `DataFrame`.  How is it that all three of these work?\r\n\r\n    df.loc[:5]\r\n    df.ix[:5]\r\n    df.iloc[:5]\r\n\r\nCan someone present three cases where the distinction in uses are clearer?\r\n",
         "view_count": "189752",
         "answer_count": "3",
         "tags": "['python', 'pandas', 'indexing', 'dataframe']",
         "creation_date": "1437669250",
         "last_edit_date": "1519285624",
         "code_snippet": "['<code>DataFrame</code>', '<code>df.loc[:5]\\ndf.ix[:5]\\ndf.iloc[:5]\\n</code>', '<code>ix</code>', '<code>loc</code>', '<code>iloc</code>', '<code>ix</code>', '<code>ix</code>', '<code>loc</code>', '<code>iloc</code>', '<code>ix</code>', '<code>loc</code>', '<code>iloc</code>', '<code>ix</code>', '<code>ix</code>', '<code>ix</code>', '<code>ix</code>', '<code>&gt;&gt;&gt; s = pd.Series(np.nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])\\n&gt;&gt;&gt; s\\n49   NaN\\n48   NaN\\n47   NaN\\n46   NaN\\n45   NaN\\n1    NaN\\n2    NaN\\n3    NaN\\n4    NaN\\n5    NaN\\n</code>', '<code>3</code>', '<code>s.iloc[:3]</code>', '<code>s.loc[:3]</code>', '<code>&gt;&gt;&gt; s.iloc[:3] # slice the first three rows\\n49   NaN\\n48   NaN\\n47   NaN\\n\\n&gt;&gt;&gt; s.loc[:3] # slice up to and including label 3\\n49   NaN\\n48   NaN\\n47   NaN\\n46   NaN\\n45   NaN\\n1    NaN\\n2    NaN\\n3    NaN\\n\\n&gt;&gt;&gt; s.ix[:3] # the integer is in the index so s.ix[:3] works like loc\\n49   NaN\\n48   NaN\\n47   NaN\\n46   NaN\\n45   NaN\\n1    NaN\\n2    NaN\\n3    NaN\\n</code>', '<code>s.ix[:3]</code>', '<code>s.loc[:3]</code>', '<code>s</code>', '<code>6</code>', '<code>s.iloc[:6]</code>', '<code>s.loc[:6]</code>', '<code>6</code>', '<code>&gt;&gt;&gt; s.iloc[:6]\\n49   NaN\\n48   NaN\\n47   NaN\\n46   NaN\\n45   NaN\\n1    NaN\\n\\n&gt;&gt;&gt; s.loc[:6]\\nKeyError: 6\\n\\n&gt;&gt;&gt; s.ix[:6]\\nKeyError: 6\\n</code>', '<code>s.ix[:6]</code>', '<code>loc</code>', '<code>6</code>', '<code>ix</code>', '<code>iloc</code>', '<code>ix</code>', '<code>iloc</code>', \"<code>&gt;&gt;&gt; s2 = pd.Series(np.nan, index=['a','b','c','d','e', 1, 2, 3, 4, 5])\\n&gt;&gt;&gt; s2.index.is_mixed() # index is mix of different types\\nTrue\\n&gt;&gt;&gt; s2.ix[:6] # now behaves like iloc given integer\\na   NaN\\nb   NaN\\nc   NaN\\nd   NaN\\ne   NaN\\n1   NaN\\n</code>\", '<code>ix</code>', '<code>loc</code>', \"<code>&gt;&gt;&gt; s2.ix[:'c'] # behaves like loc given non-integer\\na   NaN\\nb   NaN\\nc   NaN\\n</code>\", '<code>loc</code>', '<code>iloc</code>', '<code>ix</code>', \"<code>&gt;&gt;&gt; df = pd.DataFrame(np.nan, \\n                      index=list('abcde'),\\n                      columns=['x','y','z', 8, 9])\\n&gt;&gt;&gt; df\\n    x   y   z   8   9\\na NaN NaN NaN NaN NaN\\nb NaN NaN NaN NaN NaN\\nc NaN NaN NaN NaN NaN\\nd NaN NaN NaN NaN NaN\\ne NaN NaN NaN NaN NaN\\n</code>\", '<code>ix</code>', '<code>ix</code>', '<code>4</code>', \"<code>&gt;&gt;&gt; df.ix[:'c', :4]\\n    x   y   z   8\\na NaN NaN NaN NaN\\nb NaN NaN NaN NaN\\nc NaN NaN NaN NaN\\n</code>\", '<code>iloc</code>', \"<code>&gt;&gt;&gt; df.iloc[:df.index.get_loc('c') + 1, :4]\\n    x   y   z   8\\na NaN NaN NaN NaN\\nb NaN NaN NaN NaN\\nc NaN NaN NaN NaN\\n</code>\", '<code>get_loc()</code>', '<code>iloc</code>', '<code>loc</code>', '<code>iloc</code>', '<code>ix</code>', \"<code>dfmi.loc[:, 'one'].loc[:, 'second']</code>\", \"<code>dfmi['one']['second']</code>\", \"<code>df.ix[1:3, :'b']</code>\", '<code>iloc</code>', '<code>df.iloc[0]\\n</code>', '<code>df.iloc[-5:]\\n</code>', '<code>df.iloc[:, 2]    # the : in the first position indicates all rows\\n</code>', '<code>df.iloc[:3, :3] # The upper-left 3 X 3 entries (assuming df has 3+ rows and columns)\\n</code>', '<code>.loc</code>', \"<code>df = pd.DataFrame(index=['a', 'b', 'c'], columns=['time', 'date', 'name'])\\n</code>\", \"<code>df.loc['a']     # equivalent to df.iloc[0]\\n</code>\", \"<code>'date'</code>\", \"<code>df.loc['b':, 'date']   # equivalent to df.iloc[1:, 1]\\n</code>\", '<code>DataFrame</code>', '<code>iloc</code>', '<code>loc</code>', '<code>df.loc[:5]</code>', '<code>__getitem__</code>', \"<code>df['time']    # equivalent to df.loc[:, 'time']\\n</code>\", '<code>.ix</code>', \"<code>df.ix[:2, 'time']    # the first two rows of the 'time' column\\n</code>\", '<code>loc</code>', '<code> b = [True, False, True]\\n df.loc[b] \\n</code>', '<code>df</code>', '<code>df[b]</code>', \"<code>df.loc[b, 'name'] = 'Mary', 'John'\\n</code>\", '<code>df.loc[:, :]</code>', '<code>DataFrame</code>', '<code>.iloc</code>', '<code>.iloc</code>', '<code>.iloc</code>', '<code>.ix</code>', '<code>.loc</code>', '<code>.iloc</code>', \"<code>df = pd.DataFrame({'age':[30, 2, 12, 4, 32, 33, 69],\\n                   'color':['blue', 'green', 'red', 'white', 'gray', 'black', 'red'],\\n                   'food':['Steak', 'Lamb', 'Mango', 'Apple', 'Cheese', 'Melon', 'Beans'],\\n                   'height':[165, 70, 120, 80, 180, 172, 150],\\n                   'score':[4.6, 8.3, 9.0, 3.3, 1.8, 9.5, 2.2],\\n                   'state':['NY', 'TX', 'FL', 'AL', 'AK', 'TX', 'TX']\\n                   },\\n                  index=['Jane', 'Nick', 'Aaron', 'Penelope', 'Dean', 'Christina', 'Cornelia'])\\n</code>\", '<code>age</code>', '<code>color</code>', '<code>food</code>', '<code>height</code>', '<code>score</code>', '<code>state</code>', '<code>Jane</code>', '<code>Nick</code>', '<code>Aaron</code>', '<code>Penelope</code>', '<code>Dean</code>', '<code>Christina</code>', '<code>Cornelia</code>', '<code>.loc</code>', '<code>.iloc</code>', '<code>.loc</code>', '<code>.loc</code>', '<code>.loc</code>', \"<code>df.loc['Penelope']\\n</code>\", '<code>age           4\\ncolor     white\\nfood      Apple\\nheight       80\\nscore       3.3\\nstate        AL\\nName: Penelope, dtype: object\\n</code>', \"<code>df.loc[['Cornelia', 'Jane', 'Dean']]\\n</code>\", \"<code>df.loc['Aaron':'Dean']\\n</code>\", '<code>.iloc</code>', '<code>.iloc</code>', '<code>df.iloc[4]\\n</code>', '<code>age           32\\ncolor       gray\\nfood      Cheese\\nheight       180\\nscore        1.8\\nstate         AK\\nName: Dean, dtype: object\\n</code>', '<code>df.iloc[[2, -2]]\\n</code>', '<code>df.iloc[:5:3]\\n</code>', '<code>.loc/.iloc</code>', \"<code>df.loc[['Jane', 'Dean'], 'height':]\\n</code>\", '<code>.iloc</code>', '<code>df.iloc[[1,4], 2]\\nNick      Lamb\\nDean    Cheese\\nName: food, dtype: object\\n</code>', '<code>.ix</code>', '<code>Nick</code>', '<code>Cornelia</code>', '<code>.loc</code>', \"<code>col_names = df.columns[[2, 4]]\\ndf.loc[['Nick', 'Cornelia'], col_names] \\n</code>\", '<code>get_loc</code>', \"<code>labels = ['Nick', 'Cornelia']\\nindex_ints = [df.index.get_loc(label) for label in labels]\\ndf.iloc[index_ints, [2, 4]]\\n</code>\", '<code>food</code>', '<code>score</code>', \"<code>df.loc[df['age'] &gt; 30, ['food', 'score']] \\n</code>\", '<code>.iloc</code>', \"<code>df.iloc[(df['age'] &gt; 30).values, [2, 4]] \\n</code>\", '<code>.loc/.iloc</code>', \"<code>df.loc[:, 'color':'score':2]\\n</code>\", '<code>[]</code>', \"<code>df['food']\\n\\nJane          Steak\\nNick           Lamb\\nAaron         Mango\\nPenelope      Apple\\nDean         Cheese\\nChristina     Melon\\nCornelia      Beans\\nName: food, dtype: object\\n</code>\", \"<code>df[['food', 'score']]\\n</code>\", \"<code>df['Penelope':'Christina'] # slice rows by label\\n</code>\", '<code>df[2:6:2] # slice rows by integer location\\n</code>', '<code>.loc/.iloc</code>', \"<code>df[3:5, 'color']\\nTypeError: unhashable type: 'slice'\\n</code>\"]",
         "title": "pandas iloc vs ix vs loc explanation, how are they different?",
         "_childDocuments_": [
            {
               "up_vote_count": 505,
               "answer_id": 31593712,
               "last_activity_date": 1513451167,
               "path": "3.stack.answer",
               "body_markdown": "*Note: in pandas version 0.20.0 and above, `ix` is [deprecated](http://pandas-docs.github.io/pandas-docs-travis/indexing.html#ix-indexer-is-deprecated) and the use of `loc` and `iloc` is encouraged instead. I have left the parts of this answer that describe `ix` intact as a reference for users of earlier versions of pandas. Examples have been added below showing alternatives to  `ix`*.\r\n\r\n---\r\n\r\nFirst, here&#39;s a recap of the three methods:\r\n\r\n- `loc` gets rows (or columns) with particular *labels* from the index. \r\n- `iloc` gets rows (or columns) at particular *positions* in the index (so it only takes integers).\r\n- `ix` usually tries to behave like `loc` but falls back to behaving like `iloc` if a label is not present in the index.\r\n\r\nIt&#39;s important to note some subtleties that can make `ix` slightly tricky to use:\r\n\r\n- if the index is of integer type, `ix` will only use label-based indexing and not fall back to position-based indexing. If the label is not in the index, an error is raised.\r\n\r\n- if the index does not contain *only* integers, then given an integer, `ix` will immediately use position-based indexing rather than label-based indexing. If however `ix` is given another type (e.g. a string), it can use label-based indexing.\r\n\r\n---\r\n\r\nTo illustrate the differences between the three methods, consider the following Series:\r\n\r\n    &gt;&gt;&gt; s = pd.Series(np.nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])\r\n    &gt;&gt;&gt; s\r\n    49   NaN\r\n    48   NaN\r\n    47   NaN\r\n    46   NaN\r\n    45   NaN\r\n    1    NaN\r\n    2    NaN\r\n    3    NaN\r\n    4    NaN\r\n    5    NaN\r\n\r\nWe&#39;ll look at slicing with the integer value `3`.\r\n\r\nIn this case, `s.iloc[:3]` returns us the first 3 rows (since it treats 3 as a position) and `s.loc[:3]` returns us the first 8 rows (since it treats 3 as a label):\r\n\r\n    &gt;&gt;&gt; s.iloc[:3] # slice the first three rows\r\n    49   NaN\r\n    48   NaN\r\n    47   NaN\r\n\r\n    &gt;&gt;&gt; s.loc[:3] # slice up to and including label 3\r\n    49   NaN\r\n    48   NaN\r\n    47   NaN\r\n    46   NaN\r\n    45   NaN\r\n    1    NaN\r\n    2    NaN\r\n    3    NaN\r\n\r\n    &gt;&gt;&gt; s.ix[:3] # the integer is in the index so s.ix[:3] works like loc\r\n    49   NaN\r\n    48   NaN\r\n    47   NaN\r\n    46   NaN\r\n    45   NaN\r\n    1    NaN\r\n    2    NaN\r\n    3    NaN\r\n\r\nNotice `s.ix[:3]` returns the same Series as `s.loc[:3]` since it looks for the label first rather than working on the position (and the index for `s` is of integer type).\r\n\r\nWhat if we try with an integer label that isn&#39;t in the index (say `6`)?\r\n\r\nHere `s.iloc[:6]` returns the first 6 rows of the Series as expected. However, `s.loc[:6]` raises a KeyError since `6` is not in the index. \r\n\r\n    &gt;&gt;&gt; s.iloc[:6]\r\n    49   NaN\r\n    48   NaN\r\n    47   NaN\r\n    46   NaN\r\n    45   NaN\r\n    1    NaN\r\n\r\n    &gt;&gt;&gt; s.loc[:6]\r\n    KeyError: 6\r\n\r\n    &gt;&gt;&gt; s.ix[:6]\r\n    KeyError: 6\r\n\r\nAs per the subtleties noted above, `s.ix[:6]` now raises a KeyError because it tries to work like `loc` but can&#39;t find a `6` in the index. Because our index is of integer type `ix` doesn&#39;t fall back to behaving like `iloc`.\r\n\r\nIf, however, our index was of mixed type, given an integer `ix` would behave like `iloc` immediately instead of raising a KeyError:\r\n\r\n    &gt;&gt;&gt; s2 = pd.Series(np.nan, index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;, 1, 2, 3, 4, 5])\r\n    &gt;&gt;&gt; s2.index.is_mixed() # index is mix of different types\r\n    True\r\n    &gt;&gt;&gt; s2.ix[:6] # now behaves like iloc given integer\r\n    a   NaN\r\n    b   NaN\r\n    c   NaN\r\n    d   NaN\r\n    e   NaN\r\n    1   NaN\r\n\r\nKeep in mind that `ix` can still accept non-integers and behave like `loc`:\r\n\r\n    &gt;&gt;&gt; s2.ix[:&#39;c&#39;] # behaves like loc given non-integer\r\n    a   NaN\r\n    b   NaN\r\n    c   NaN\r\n\r\nAs general advice, if you&#39;re only indexing using labels, or only indexing using integer positions, stick with `loc` or `iloc` to avoid unexpected results - try not use `ix`.\r\n\r\n---\r\n\r\n### Combining position-based and label-based indexing\r\n\r\nSometimes given a DataFrame, you will want to mix label and positional indexing methods for the rows and columns.\r\n\r\nFor example, consider the following DataFrame. How best to slice the rows up to and including &#39;c&#39; *and* take the first four columns?\r\n\r\n    &gt;&gt;&gt; df = pd.DataFrame(np.nan, \r\n                          index=list(&#39;abcde&#39;),\r\n                          columns=[&#39;x&#39;,&#39;y&#39;,&#39;z&#39;, 8, 9])\r\n    &gt;&gt;&gt; df\r\n        x   y   z   8   9\r\n    a NaN NaN NaN NaN NaN\r\n    b NaN NaN NaN NaN NaN\r\n    c NaN NaN NaN NaN NaN\r\n    d NaN NaN NaN NaN NaN\r\n    e NaN NaN NaN NaN NaN\r\n\r\nIn earlier versions of pandas (before 0.20.0) `ix` lets you do this quite neatly - we can slice the rows by label and the columns by position (note that for the columns, `ix` will default to position-based slicing since  `4` is not a column name):\r\n\r\n    &gt;&gt;&gt; df.ix[:&#39;c&#39;, :4]\r\n        x   y   z   8\r\n    a NaN NaN NaN NaN\r\n    b NaN NaN NaN NaN\r\n    c NaN NaN NaN NaN\r\n\r\nIn later versions of pandas, we can achieve this result using `iloc` and the help of another method:\r\n\r\n    &gt;&gt;&gt; df.iloc[:df.index.get_loc(&#39;c&#39;) + 1, :4]\r\n        x   y   z   8\r\n    a NaN NaN NaN NaN\r\n    b NaN NaN NaN NaN\r\n    c NaN NaN NaN NaN\r\n\r\n[`get_loc()`](http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.Index.get_loc.html) is an index method meaning &quot;get the position of the label in this index&quot;. Note that since slicing with `iloc` is exclusive of its endpoint, we must add 1 to this value if we want row &#39;c&#39; as well.\r\n\r\nThere are further examples in pandas&#39; documentation [here](http://pandas-docs.github.io/pandas-docs-travis/indexing.html#ix-indexer-is-deprecated).\r\n",
               "tags": [],
               "creation_date": 1437670787,
               "last_edit_date": 1513451167,
               "is_accepted": true,
               "id": "31593712",
               "down_vote_count": 0,
               "score": 505
            },
            {
               "up_vote_count": 75,
               "answer_id": 31594055,
               "last_activity_date": 1469899337,
               "path": "3.stack.answer",
               "body_markdown": "`iloc` works based on integer positioning. So no matter what your row labels are, you can always, e.g., get the first row by doing\r\n\r\n    df.iloc[0]\r\n\r\nor the last five rows by doing\r\n\r\n    df.iloc[-5:]\r\n\r\nYou can also use it on the columns. This retrieves the 3rd column:\r\n\r\n    df.iloc[:, 2]    # the : in the first position indicates all rows\r\n\r\nYou can combine them to get intersections of rows and columns:\r\n\r\n    df.iloc[:3, :3] # The upper-left 3 X 3 entries (assuming df has 3+ rows and columns)\r\n\r\nOn the other hand, `.loc` use named indices. Let&#39;s set up a data frame with strings as row and column labels:\r\n\r\n    df = pd.DataFrame(index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], columns=[&#39;time&#39;, &#39;date&#39;, &#39;name&#39;])\r\n\r\nThen we can get the first row by\r\n\r\n    df.loc[&#39;a&#39;]     # equivalent to df.iloc[0]\r\n\r\nand the second two rows of the `&#39;date&#39;` column by \r\n\r\n    df.loc[&#39;b&#39;:, &#39;date&#39;]   # equivalent to df.iloc[1:, 1]\r\n\r\nand so on. Now, it&#39;s probably worth pointing out that the default row and column indices for a `DataFrame` are integers from 0 and in this case `iloc` and `loc` would work in the same way. This is why your three examples are equivalent. **If you had a non-numeric index such as strings or datetimes,** `df.loc[:5]` **would raise an error.** \r\n\r\nAlso, you can do column retrieval just by using the data frame&#39;s `__getitem__`:\r\n\r\n    df[&#39;time&#39;]    # equivalent to df.loc[:, &#39;time&#39;]\r\n\r\nNow suppose you want to mix position and named indexing, that is, indexing using names on rows and positions on columns (to clarify, I mean select from our data frame, rather than creating a data frame with strings in the row index and integers in the column index). This is where `.ix` comes in:\r\n\r\n    df.ix[:2, &#39;time&#39;]    # the first two rows of the &#39;time&#39; column\r\n\r\nEDIT:\r\nI think it&#39;s also worth mentioning that you can pass boolean vectors to the `loc` method as well. For example:\r\n\r\n     b = [True, False, True]\r\n     df.loc[b] \r\n\r\nWill return the 1st and 3rd rows of `df`. This is equivalent to `df[b]` for selection, but it can also be used for assigning via boolean vectors: \r\n\r\n    df.loc[b, &#39;name&#39;] = &#39;Mary&#39;, &#39;John&#39;\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1437671847,
               "last_edit_date": 1469899337,
               "is_accepted": false,
               "id": "31594055",
               "down_vote_count": 0,
               "score": 75
            },
            {
               "up_vote_count": 19,
               "answer_id": 46915810,
               "last_activity_date": 1509768621,
               "path": "3.stack.answer",
               "body_markdown": "In my opinion, the accepted answer is confusing, since it uses a DataFrame with only missing values. I also do not like the term **position-based** for `.iloc` and instead, prefer **integer location** as it is much more descriptive and exactly what `.iloc` stands for. The key word is INTEGER - `.iloc` needs INTEGERS.\r\n___\r\n### .ix is deprecated and ambiguous and should never be used\r\n\r\nBecause `.ix` is deprecated we will only focus on the differences between `.loc` and `.iloc`.\r\n\r\nBefore we talk about the differences, it is important to understand that DataFrames have labels that help identify each column and each index. Let&#39;s take a look at a sample DataFrame:\r\n\r\n    df = pd.DataFrame({&#39;age&#39;:[30, 2, 12, 4, 32, 33, 69],\r\n                       &#39;color&#39;:[&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;white&#39;, &#39;gray&#39;, &#39;black&#39;, &#39;red&#39;],\r\n                       &#39;food&#39;:[&#39;Steak&#39;, &#39;Lamb&#39;, &#39;Mango&#39;, &#39;Apple&#39;, &#39;Cheese&#39;, &#39;Melon&#39;, &#39;Beans&#39;],\r\n                       &#39;height&#39;:[165, 70, 120, 80, 180, 172, 150],\r\n                       &#39;score&#39;:[4.6, 8.3, 9.0, 3.3, 1.8, 9.5, 2.2],\r\n                       &#39;state&#39;:[&#39;NY&#39;, &#39;TX&#39;, &#39;FL&#39;, &#39;AL&#39;, &#39;AK&#39;, &#39;TX&#39;, &#39;TX&#39;]\r\n                       },\r\n                      index=[&#39;Jane&#39;, &#39;Nick&#39;, &#39;Aaron&#39;, &#39;Penelope&#39;, &#39;Dean&#39;, &#39;Christina&#39;, &#39;Cornelia&#39;])\r\n\r\n[![enter image description here][1]][1]\r\n\r\nAll the words in **bold** are the labels. The labels, `age`, `color`, `food`, `height`, `score` and `state` are used for the **columns**. The other labels, `Jane`, `Nick`, `Aaron`, `Penelope`, `Dean`, `Christina`, `Cornelia` are used for the **index**.\r\n____\r\nThe primary ways to select particular rows in a DataFrame are with the `.loc` and `.iloc` indexers. Each of these indexers can also be used to simultaneously select columns but it is easier to just focus on rows for now. Also, each of the indexers use a set of brackets that immediately follow their name to make their selections.\r\n\r\n## .loc selects data only by labels\r\nWe will first talk about the `.loc` indexer which only selects data by the index or column labels. In our sample DataFrame, we have provided meaningful names as values for the index. Many DataFrames will not have any meaningful names and will instead, default to just the integers from 0 to n-1, where n is the length of the DataFrame.\r\n\r\nThere are three different inputs you can use for `.loc`   \r\n\r\n* A string\r\n* A list of strings\r\n* Slice notation using strings as the start and stop values\r\n\r\n**Selecting a single row with .loc with a string**\r\n\r\nTo select a single row of data, place the index label inside of the brackets following `.loc`.\r\n\r\n    df.loc[&#39;Penelope&#39;]\r\n\r\nThis returns the row of data as a Series  \r\n\r\n    age           4\r\n    color     white\r\n    food      Apple\r\n    height       80\r\n    score       3.3\r\n    state        AL\r\n    Name: Penelope, dtype: object\r\n\r\n**Selecting multiple rows with .loc with a list of strings**\r\n\r\n    df.loc[[&#39;Cornelia&#39;, &#39;Jane&#39;, &#39;Dean&#39;]]\r\n\r\nThis returns a DataFrame with the rows in the order specified in the list:\r\n\r\n   \r\n[![enter image description here][2]][2]\r\n\r\n**Selecting multiple rows with .loc with slice notation**\r\n\r\nSlice notation is defined by a start, stop and step values. When slicing by label, pandas includes the stop value in the return. The following slices from Aaron to Dean, inclusive. Its step size is not explicitly defined but defaulted to 1.\r\n\r\n    df.loc[&#39;Aaron&#39;:&#39;Dean&#39;]\r\n\r\n[![enter image description here][3]][3]\r\n\r\nComplex slices can be taken in the same manner as Python lists.\r\n\r\n## .iloc selects data only by integer location\r\nLet&#39;s now turn to `.iloc`. Every row and column of data in a DataFrame has an integer location that defines it. This is in addition to the label that is visually displayed in the output. The integer location is simply the number of rows/columns from the top/left beginning at 0.\r\n\r\nThere are three different inputs you can use for `.iloc`   \r\n\r\n* An integer\r\n* A list of integers\r\n* Slice notation using integers as the start and stop values\r\n\r\n**Selecting a single row with .iloc with an integer**\r\n\r\n    df.iloc[4]\r\n\r\nThis returns the 5th row (integer location 4) as a Series\r\n\r\n    age           32\r\n    color       gray\r\n    food      Cheese\r\n    height       180\r\n    score        1.8\r\n    state         AK\r\n    Name: Dean, dtype: object\r\n\r\n**Selecting multiple rows with .iloc with a list of integers**\r\n\r\n    df.iloc[[2, -2]]\r\n\r\nThis returns a DataFrame of the third and second to last rows:\r\n\r\n[![enter image description here][4]][4]\r\n\r\n\r\n**Selecting multiple rows with .iloc with slice notation**\r\n\r\n    df.iloc[:5:3]\r\n\r\n[![enter image description here][5]][5]\r\n\r\n___\r\n## Simultaneous selection of rows and columns with .loc and .iloc\r\n\r\nOne excellent ability of both `.loc/.iloc` is their ability to select both rows and columns simultaneously. In the examples above, all the columns were returned from each selection. We can choose columns with the same types of inputs as we do for rows. We simply need to separate the row and column selection with a **comma**.\r\n\r\nFor example, we can select rows Jane, and Dean with just the columns height, score and state like this:\r\n\r\n    df.loc[[&#39;Jane&#39;, &#39;Dean&#39;], &#39;height&#39;:]\r\n[![enter image description here][6]][6]\r\n\r\nThis uses a list of labels for the rows and slice notation for the columns\r\n\r\nWe can naturally do similar operations with `.iloc` using only integers.\r\n\r\n    df.iloc[[1,4], 2]\r\n    Nick      Lamb\r\n    Dean    Cheese\r\n    Name: food, dtype: object\r\n\r\n____\r\n### Simultaneous selection with labels and integer location\r\n`.ix` was used to make selections simultaneously with labels and integer location which was useful but confusing and ambiguous at times and thankfully it has been deprecated. In the event that you need to make a selection with a mix of labels and integer locations, you will have to make both your selections labels or integer locations. \r\n\r\nFor instance, if we want to select rows `Nick` and `Cornelia` along with columns 2 and 4, we could use `.loc` by converting the integers to labels with the following:\r\n\r\n  \r\n    col_names = df.columns[[2, 4]]\r\n    df.loc[[&#39;Nick&#39;, &#39;Cornelia&#39;], col_names] \r\n\r\nOr alternatively, convert the index labels to integers with the `get_loc` index method.\r\n\r\n    labels = [&#39;Nick&#39;, &#39;Cornelia&#39;]\r\n    index_ints = [df.index.get_loc(label) for label in labels]\r\n    df.iloc[index_ints, [2, 4]]\r\n\r\n### Boolean Selection\r\nThe .loc indexer can also do boolean selection. For instance, if we are interested in finding all the rows wher age is above 30 and return just the `food` and `score` columns we can do the following:\r\n\r\n    df.loc[df[&#39;age&#39;] &gt; 30, [&#39;food&#39;, &#39;score&#39;]] \r\n\r\nYou can replicate this with `.iloc` but you cannot pass it a boolean series. You must convert the boolean Series into a numpy array like this:\r\n\r\n    df.iloc[(df[&#39;age&#39;] &gt; 30).values, [2, 4]] \r\n\r\n___\r\n### Selecting all rows\r\nIt is possible to use `.loc/.iloc` for just column selection. You can select all the rows by using a colon like this:\r\n\r\n    df.loc[:, &#39;color&#39;:&#39;score&#39;:2]\r\n\r\n[![enter image description here][7]][7]\r\n\r\n____\r\n### The indexing operator, `[]`, can slice can select rows and columns too but not simultaneously.\r\n\r\nMost people are familiar with the primary purpose of the DataFrame indexing operator, which is to select columns. A string selects a single column as a Series and a list of strings selects multiple columns as a DataFrame.\r\n\r\n    df[&#39;food&#39;]\r\n\r\n    Jane          Steak\r\n    Nick           Lamb\r\n    Aaron         Mango\r\n    Penelope      Apple\r\n    Dean         Cheese\r\n    Christina     Melon\r\n    Cornelia      Beans\r\n    Name: food, dtype: object\r\n\r\nUsing a list selects multiple columns\r\n\r\n    df[[&#39;food&#39;, &#39;score&#39;]]\r\n\r\n[![enter image description here][8]][8]\r\n\r\nWhat people are less familiar with, is that, when slice notation is used, then selection happens by row labels or by integer location. This is very confusing and something that I almost never use but it does work.\r\n\r\n    df[&#39;Penelope&#39;:&#39;Christina&#39;] # slice rows by label\r\n\r\n[![enter image description here][9]][9]\r\n\r\n    df[2:6:2] # slice rows by integer location\r\n\r\n[![enter image description here][10]][10]\r\n\r\nThe explicitness of `.loc/.iloc` for selecting rows is highly preferred. The indexing operator alone is unable to select rows and columns simultaneously.\r\n\r\n    df[3:5, &#39;color&#39;]\r\n    TypeError: unhashable type: &#39;slice&#39;\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/4VsqO.png\r\n  [2]: https://i.stack.imgur.com/ST3Q2.png\r\n  [3]: https://i.stack.imgur.com/XXSJ4.png\r\n  [4]: https://i.stack.imgur.com/g7WZx.png\r\n  [5]: https://i.stack.imgur.com/w6vGg.png\r\n  [6]: https://i.stack.imgur.com/RVFe3.png\r\n  [7]: https://i.stack.imgur.com/5f8Y8.png\r\n  [8]: https://i.stack.imgur.com/t1p1V.png\r\n  [9]: https://i.stack.imgur.com/GSWT1.png\r\n  [10]: https://i.stack.imgur.com/lX1Wj.png",
               "tags": [],
               "creation_date": 1508863192,
               "last_edit_date": 1509768621,
               "is_accepted": false,
               "id": "46915810",
               "down_vote_count": 0,
               "score": 19
            }
         ],
         "link": "https://stackoverflow.com/questions/31593201/pandas-iloc-vs-ix-vs-loc-explanation-how-are-they-different",
         "id": "858127-2250"
      },
      {
         "up_vote_count": "2115",
         "path": "2.stack",
         "body_markdown": "[`pip`](https://pip.pypa.io/en/stable/) is a replacement for [`easy_install`](http://setuptools.readthedocs.io/en/latest/easy_install.html). But should I install `pip` using `easy_install` on Windows?  Is there a better way?",
         "view_count": "2139199",
         "answer_count": "34",
         "tags": "['python', 'windows', 'installation', 'pip', 'easy-install']",
         "creation_date": "1295546939",
         "last_edit_date": "1488455895",
         "code_snippet": "['<code>pip</code>', '<code>easy_install</code>', '<code>pip</code>', '<code>easy_install</code>', '<code>get-pip.py</code>', '<code>.py</code>', '<code>.txt</code>', '<code>python get-pip.py\\n</code>', '<code>.msi</code>', '<code>C:\\\\Python27\\\\Scripts\\\\pip.exe</code>', '<code>pip.exe</code>', '<code>C:\\\\Python27\\\\Scripts</code>', '<code>pip</code>', '<code>pip install httpie\\n</code>', '<code>http_proxy</code>', '<code>https_proxy</code>', '<code>http://proxy_url:port\\nhttp://username:password@proxy_url:port\\n</code>', '<code>pip</code>', '<code>python -m pip</code>', '<code>C:\\\\Python2x\\\\</code>', '<code>C:\\\\Python2x</code>', '<code>python setup.py install</code>', '<code>C:\\\\Python2x\\\\Scripts</code>', '<code>pip install package</code>', '<code>pip</code>', '<code>easy_install</code>', '<code>setuptools</code>', '<code>pip uninstall setuptools</code>', '<code>get-pip.py</code>', '<code>python get-pip.py\\n</code>', '<code>pip install -U setuptools</code>', '<code>C:\\\\Python27\\\\Scripts</code>', '<code>Scripts</code>', '<code>cd</code>', '<code>distribute_setup.py</code>', '<code>python distribute_setup.py</code>', '<code>Scripts</code>', '<code>C:\\\\Python27\\\\Scripts</code>', '<code>easy_install pip</code>', '<code>easy_install.exe</code>', '<code>c:\\\\Python2x\\\\Scripts</code>', '<code>x</code>', '<code>Python2x</code>', '<code>easy_install pip</code>', '<code>get-pip.py</code>', '<code>pip.exe</code>', '<code>easy_install.exe</code>', '<code>\"%ProgramFiles%\\\\PythonXX\\\\Scripts\"</code>', '<code>PATH</code>', '<code>pip</code>', '<code>C:\\\\Python34\\\\Scripts\\\\pip3.exe</code>', '<code>pip</code>', '<code>pip3.exe</code>', '<code>pip3 install -U sphinx</code>', '<code>py -m pip install xxx</code>', '<code>pip</code>', '<code>easy_install</code>', '<code>pip</code>', '<code>easy_install</code>', '<code>pypm</code>', '<code>pip</code>', '<code>PyPM</code>', '<code>pip</code>', '<code>PyPM</code>', '<code>PyPM</code>', '<code>PyPM</code>', '<code>pip</code>', '<code>PATH</code>', '<code>cinst python\\ncinst easy.install\\ncinst pip\\n</code>', '<code>cinst pip</code>', '<code>python -c \"exec(\\'try: from urllib2 import urlopen \\\\nexcept: from urllib.request import urlopen\\');f=urlopen(\\'https://bootstrap.pypa.io/get-pip.py\\').read();exec(f)\"\\n</code>', '<code>get-pip.py</code>', '<code>C:\\\\Python27</code>', '<code>C:\\\\Python27\\\\Scripts</code>', '<code>C:\\\\Python33</code>', '<code>C:\\\\Python33\\\\Scripts</code>', '<code>pip</code>', '<code>easy_install</code>', '<code>distribute</code>', '<code>pip</code>', '<code>distribute</code>', '<code>.exe</code>', '<code>distribute</code>', '<code>.exe</code>', '<code>hg clone https://bitbucket.org/tarek/distribute\\ncd distribute\\nhg checkout 0.6.27\\nrem optionally, comment out tag_build and tag_svn_revision in setup.cfg\\nmsvc-build-launcher.cmd\\npython setup.py bdist_win32\\ncd ..\\necho build is in distribute\\\\dist\\n</code>', '<code>pip</code>', '<code>git clone https://github.com/pypa/pip.git\\ncd pip\\ngit checkout 1.1\\npython setup.py bdist_win32\\ncd ..\\necho build is in pip\\\\dist\\n</code>', '<code>launcher.c</code>', '<code>distribute</code>', '<code>win32</code>', '<code>C:\\\\Python27\\\\Scripts\\n</code>', '<code>pip install virtualenv\\n</code>', '<code>error: Unable to find vcvarsall.bat\\n</code>', '<code>virtualenv</code>', '<code>virtualenv</code>', '<code>python virtualenv.py myvirtualenv\\n</code>', '<code>New python executable in myvirtualenv\\\\Scripts\\\\python.exe\\nInstalling setuptools....................................done.\\nInstalling pip.........................done.\\n</code>', '<code>myvirtualenv\\\\Scripts\\\\activate\\n</code>', '<code>(myvirtualenv) PATH\\\\TO\\\\YOUR\\\\PROJECT\\\\FOLDER&gt;pip install package_name\\n</code>', '<code> [HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Wow6432Node\\\\Python\\\\PythonCore\\\\2.6\\\\InstallPath]\\n @=\"C:\\\\\\\\Python26\\\\\\\\\"\\n</code>', '<code> [HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Python\\\\PythonCore\\\\2.6\\\\InstallPath]\\n @=\"C:\\\\\\\\Python26\\\\\\\\\"\\n</code>', '<code>Pip</code>', '<code>Python 2.7.9+ or 3.4+</code>', '<code>python get-pip.py</code>', '<code>sudo python get-pip.py</code>', '<code>Python 2.7.x</code>', '<code>C:\\\\Python27 and C:\\\\Python27\\\\Scripts</code>', '<code>Python 3.3x</code>', '<code>C:\\\\Python33 and C:\\\\Python33\\\\Scripts</code>', '<code>curl http://python-distribute.org/distribute_setup.py | python\\ncurl https://raw.github.com/pypa/pip/master/contrib/get-pip.py | python\\n</code>', '<code>C:\\\\Python33\\\\Scripts</code>', '<code>pip</code>', '<code>C:\\\\Python27</code>', '<code>C:\\\\Python27\\\\Scripts</code>', '<code>pip.exe</code>', '<code>cmd</code>', '<code>pip install package_name</code>', '<code>&gt; conda install &lt;package&gt;               # access distributed binaries\\n\\n&gt; pip install &lt;package&gt;                 # access PyPI packages \\n</code>', '<code>conda</code>', '<code>pandas</code>', '<code>numpy</code>', '<code>git clone https://github.com/chrissimpkins/pip-installer.git\\n</code>', '<code>python pipinstall.py\\n</code>', '<code>pip install virtualenv\\n</code>', '<code>virtualenv venv\\n</code>', '<code>pip install google\\n</code>', '<code>Step 1: wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py\\nStep 2: wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py\\nStep 2: python ez_setup.py\\nStep 3: python get-pip.py\\n</code>', '<code>C:\\\\Python27</code>', '<code>C:\\\\Python27\\\\Scripts</code>', '<code>Downloading/unpacking beautifulsoup4\\n  Getting page https://pypi.python.org/simple/beautifulsoup4/\\n  Could not fetch URL https://pypi.python.org/simple/beautifulsoup4/: **connection error: [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed**\\n  Will skip URL https://pypi.python.org/simple/beautifulsoup4/ when looking for download links for beautifulsoup4\\n</code>', '<code>cd &lt;path to extracted folder&gt;/pip-1.2.1</code>', '<code>python setup.py install</code>', '<code>C:\\\\Python27\\\\Scripts</code>', '<code>C:\\\\Python27\\\\Scripts</code>', '<code>C:\\\\Python27\\\\Lib\\\\site-packages</code>', '<code>requests</code>', '<code>pip install requests\\n</code>', '<code>requests</code>', '<code>python get-pip.py\\n</code>', '<code>virtualenv</code>', '<code>PATH</code>', '<code>pypm install pip</code>']",
         "title": "How do I install pip on Windows?",
         "_childDocuments_": [
            {
               "up_vote_count": 38,
               "answer_id": 4750846,
               "last_activity_date": 1369986832,
               "path": "3.stack.answer",
               "body_markdown": "When I have to use Windows, I use ActivePython, which automatically adds everything to your PATH and includes a package manager called [PyPM][1] which provides *binary* package management making it faster and simpler to install packages.\r\n\r\n`pip` and `easy_install` aren&#39;t exactly the same thing, so there are some things you can get through `pip` but not `easy_install` [and vice versa][2].\r\n\r\nMy recommendation is that you get [ActivePython Community Edition](http://www.activestate.com/activepython/downloads) and don&#39;t worry about the huge hassle of getting everything set up for Python on Windows. Then, you can just use `pypm`.\r\n\r\nIn case you want to use `pip` you have to check the `PyPM` option in the ActiveState installer. After installation you only need to logoff and log on again, and `pip` will be available on the commandline, because it is contained in the ActiveState installer `PyPM` option and the paths have been set by the installer for you already. `PyPM` will also be available, but you do not have to use it.\r\n\r\n\r\n  [1]: http://code.activestate.com/pypm/\r\n  [2]: https://stackoverflow.com/questions/3220404/why-use-pip-over-easy-install/3224103#3224103",
               "tags": [],
               "creation_date": 1295547216,
               "last_edit_date": 1495542891,
               "is_accepted": false,
               "id": "4750846",
               "down_vote_count": 3,
               "score": 35
            },
            {
               "up_vote_count": 293,
               "answer_id": 4921215,
               "last_activity_date": 1391509045,
               "path": "3.stack.answer",
               "body_markdown": "&lt;s&gt;-- **Outdated** -- use distribute, not setuptools as described here. --&lt;/s&gt;  \r\n-- **Outdated #2** -- use setuptools as distribute is deprecated.\r\n\r\nAs you mentioned pip doesn&#39;t include an independent installer, but you can install it with its predecessor easy_install.\r\n\r\nSo:\r\n\r\n 1. Download the last pip version from here: http://pypi.python.org/pypi/pip#downloads\r\n 2. Uncompress it\r\n 3. Download the last easy installer for Windows: (**download the .exe at the bottom of http://pypi.python.org/pypi/setuptools** ). Install it.\r\n 4. copy the uncompressed pip folder **content** into `C:\\Python2x\\` folder (don&#39;t copy the whole folder into it, just the content), because python command doesn&#39;t work outside `C:\\Python2x` folder and then run:  `python setup.py install`\r\n 5. Add your python `C:\\Python2x\\Scripts` to the path\r\n\r\nYou are done. \r\n\r\nNow you can use `pip install package` to easily install packages as in Linux :)\r\n\r\n",
               "tags": [],
               "creation_date": 1297081315,
               "last_edit_date": 1391509045,
               "is_accepted": false,
               "id": "4921215",
               "down_vote_count": 8,
               "score": 285
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 13,
               "answer_id": 7581341,
               "is_accepted": false,
               "last_activity_date": 1317203481,
               "body_markdown": "To install pip *globally* on Python 2.x, easy_install appears to be the best solution as Adri&#225;n states.\r\n\r\nHowever the [installation instructions](http://www.pip-installer.org/en/latest/installing.html) for pip recommend using [virtualenv](http://www.virtualenv.org) since every virtualenv has pip installed in it automatically.  This does not require root access or modify your system Python installation.\r\n\r\nInstalling virtualenv still requires easy_install though.",
               "id": "7581341",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1317203481,
               "score": 12
            },
            {
               "up_vote_count": 206,
               "answer_id": 9038397,
               "last_activity_date": 1454789759,
               "path": "3.stack.answer",
               "body_markdown": "**2014 UPDATE:**\r\n\r\n1) If you have installed Python 3.4 or later, pip is included with Python and should already be working on your system.\r\n\r\n2) If you are running a version below Python 3.4 or if pip was not installed with Python 3.4 for some reason, then you&#39;d probably use pip&#39;s official installation script `get-pip.py`. The pip installer now grabs setuptools for you, and works regardless of architecture (32-bit or 64-bit).\r\n\r\nThe installation [instructions are detailed here][1] and involve:\r\n\r\n&gt; To install or upgrade pip, securely download [get-pip.py][2].\r\n\r\n&gt; Then run the following (which may require administrator access):\r\n\r\n    python get-pip.py\r\n\r\n&gt; To upgrade an existing setuptools (or distribute), run `pip install -U setuptools`\r\n\r\nI&#39;ll leave the two sets of old instructions below for posterity.\r\n\r\n**OLD Answers:**\r\n\r\nFor Windows editions of the **64 bit** variety - 64-bit Windows + Python used to require a separate installation method due to ez_setup, but I&#39;ve tested the new distribute method on 64-bit Windows running 32-bit Python and 64-bit Python, and you can now use the same method for all versions of Windows/Python 2.7X:\r\n\r\n**OLD Method 2** using [distribute][3]:\r\n\r\n 1. Download [distribute][4] - I threw mine in `C:\\Python27\\Scripts` (feel free to create a `Scripts` directory if it doesn&#39;t exist.\r\n 2. Open up a command prompt (on Windows you should check out [conemu2][5] if you don&#39;t use [PowerShell][6]) and change (`cd`) to the directory you&#39;ve downloaded `distribute_setup.py` to.\r\n 3. Run distribute_setup: `python distribute_setup.py` (This will not work if your python installation directory is not added to your path - [go here for help][7])\r\n 4. Change the current directory to the `Scripts` directory for your Python installation (`C:\\Python27\\Scripts`) or add that directory, as well as the Python base installation directory to your %PATH% environment variable.\r\n 5. Install pip using the newly installed setuptools: `easy_install pip`\r\n\r\nThe last step will not work unless you&#39;re either in the directory `easy_install.exe` is located in (C:\\Python27\\Scripts would be the default for Python 2.7), or you have that directory added to your path.\r\n\r\n**OLD Method 1** using ez_setup:\r\n\r\n[from the setuptools page][8] --\r\n\r\n&gt; Download [ez_setup.py][9] and run it; it will download the appropriate .egg file and install it for you. (Currently, the provided .exe installer does not support 64-bit versions of Python for Windows, due to a distutils installer compatibility issue.\r\n\r\nAfter this, you may continue with:\r\n\r\n 2. Add `c:\\Python2x\\Scripts` to the Windows path (replace the `x` in `Python2x` with the actual version number you have installed)\r\n 3. Open a new (!) DOS prompt. From there run `easy_install pip`\r\n\r\n  [1]: http://www.pip-installer.org/en/latest/installing.html\r\n  [2]: https://raw.github.com/pypa/pip/master/contrib/get-pip.py\r\n  [3]: https://pypi.python.org/pypi/distribute\r\n  [4]: http://python-distribute.org/distribute_setup.py\r\n  [5]: http://code.google.com/p/conemu-maximus5/\r\n  [6]: http://en.wikipedia.org/wiki/Windows_PowerShell\r\n  [7]: https://stackoverflow.com/a/6318188/705198\r\n  [8]: http://pypi.python.org/pypi/setuptools#windows\r\n  [9]: http://peak.telecommunity.com/dist/ez_setup.py\r\n",
               "tags": [],
               "creation_date": 1327689794,
               "last_edit_date": 1495541445,
               "is_accepted": false,
               "id": "9038397",
               "down_vote_count": 5,
               "score": 201
            },
            {
               "up_vote_count": 11,
               "answer_id": 9448989,
               "last_activity_date": 1454789837,
               "path": "3.stack.answer",
               "body_markdown": "I just wanted to add one more solution for those having issues installing setuptools from Windows 64-bit. The issue is discussed in this bug on python.org and is still unresolved as of the date of this comment. A simple workaround is mentioned and it works flawlessly. One registry change did the trick for me.\r\n\r\nLink: http://bugs.python.org/issue6792#\r\n\r\nSolution that worked for me...:\r\n\r\nAdd this registry setting for 2.6+ versions of Python:\r\n\r\n     [HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Python\\PythonCore\\2.6\\InstallPath]\r\n     @=&quot;C:\\\\Python26\\\\&quot;\r\n\r\nThis is most likely the registry setting you will already have for Python 2.6+:\r\n\r\n     [HKEY_LOCAL_MACHINE\\SOFTWARE\\Python\\PythonCore\\2.6\\InstallPath]\r\n     @=&quot;C:\\\\Python26\\\\&quot;\r\n\r\nClearly, you will need to replace the 2.6 version with whatever version of Python you are running.\r\n",
               "tags": [],
               "creation_date": 1330211747,
               "last_edit_date": 1454789837,
               "is_accepted": false,
               "id": "9448989",
               "down_vote_count": 1,
               "score": 10
            },
            {
               "up_vote_count": 24,
               "answer_id": 11311788,
               "last_activity_date": 1341326806,
               "path": "3.stack.answer",
               "body_markdown": "## Installers ##\r\n\r\nI&#39;ve built Windows installers for both [distribute][1] and [pip][2] here (the goal being to use `pip` without having to either bootstrap with `easy_install` or save and run Python scripts):\r\n\r\n* [distribute-0.6.27.win32.exe](http://download.sjsoft.com/opensource/distribute-0.6.27.win32.exe)\r\n* [pip-1.1.win32.exe](http://download.sjsoft.com/opensource/pip-1.1.win32.exe)\r\n\r\nOn Windows, simply download and install first `distribute`, then `pip` from the above links. The `distribute` link above does contain stub `.exe` installers, and these are currently 32-bit only. I haven&#39;t tested the effect on 64-bit Windows.\r\n \r\n## Building on Windows ##\r\n\r\nThe process to redo this for new versions is not difficult, and I&#39;ve included it here for reference.\r\n\r\n### Building `distribute` ####\r\n\r\nIn order to get the stub `.exe` files, you need to have a Visual C++ compiler (it is apparently compilable with MinGW as well)\r\n\r\n&lt;!-- language: lang-none --&gt;\r\n\r\n    hg clone https://bitbucket.org/tarek/distribute\r\n    cd distribute\r\n    hg checkout 0.6.27\r\n    rem optionally, comment out tag_build and tag_svn_revision in setup.cfg\r\n    msvc-build-launcher.cmd\r\n    python setup.py bdist_win32\r\n    cd ..\r\n    echo build is in distribute\\dist\r\n\r\n### Building `pip` ####\r\n\r\n&lt;!-- language: lang-none --&gt;\r\n\r\n    git clone https://github.com/pypa/pip.git\r\n    cd pip\r\n    git checkout 1.1\r\n    python setup.py bdist_win32\r\n    cd ..\r\n    echo build is in pip\\dist\r\n\r\n  [1]: http://pypi.python.org/pypi/distribute\r\n  [2]: http://www.pip-installer.org/",
               "tags": [],
               "creation_date": 1341321458,
               "last_edit_date": 1341326806,
               "is_accepted": false,
               "id": "11311788",
               "down_vote_count": 2,
               "score": 22
            },
            {
               "up_vote_count": 1548,
               "answer_id": 12476379,
               "last_activity_date": 1502672440,
               "path": "3.stack.answer",
               "body_markdown": "Python 2.7.9+ and 3.4+\r\n----------------------\r\n\r\nGood news! [Python 3.4][1] (released March 2014) and [Python 2.7.9][2] (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community&#39;s wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins [Ruby][3], [Node.js][4], [Haskell][5], [Perl][6], [Go][7]--almost every other contemporary language with a majority open-source community. Thank you Python.\r\n\r\nOf course, that doesn&#39;t mean Python packaging is problem solved. The experience remains frustrating. I discuss this [in Stack Overflow question *Does Python have a package/module management system?*][8].\r\n\r\nAnd, alas for everyone using Python 2.7.8 or earlier (a sizable portion of the community). There&#39;s no plan to ship Pip to you. Manual instructions follow.\r\n\r\nPython 2 \u2264 2.7.8 and Python 3 \u2264 3.3\r\n-----------------------------------\r\n\r\nFlying in the face of its [&#39;batteries included&#39;][9] motto, Python ships without a package manager. To make matters worse, Pip was--until recently--ironically difficult to install.\r\n\r\n### Official instructions\r\n\r\nPer [https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip][21]:\r\n\r\nDownload [`get-pip.py`][10], being careful to save it as a `.py` file rather than `.txt`. Then, run it from the command prompt:\r\n\r\n    python get-pip.py\r\n\r\nYou possibly need an administrator command prompt to do this. Follow *[Start a Command Prompt as an Administrator][11]* (Microsoft TechNet).\r\n\r\nThis installs the pip package, which (in Windows) contains ...\\Scripts\\pip.exe that path must be in PATH environment variable to use pip from the command line (see second part of &#39;Alternative Instructions&#39; for adding it to your PATH,\r\n\r\n\r\n### Alternative instructions\r\n\r\nThe official documentation tells users to install Pip and each of its dependencies from source. That&#39;s tedious for the experienced, and prohibitively difficult for newbies.\r\n\r\nFor our sake, Christoph Gohlke prepares Windows installers (`.msi`) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:\r\n\r\n1. [Install setuptools][12]\r\n2. [Install pip][13]\r\n\r\nFor me, this installed Pip at `C:\\Python27\\Scripts\\pip.exe`. Find `pip.exe` on your computer, then add its folder (for example, `C:\\Python27\\Scripts`) to your path (Start / Edit environment variables). Now you should be able to run `pip` from the command line. Try installing a package:\r\n\r\n    pip install httpie\r\n\r\nThere you go (hopefully)! Solutions for common problems are given below:\r\n\r\n### Proxy problems\r\n\r\nIf you work in an office, you might be behind a HTTP proxy. If so, set the environment variables [`http_proxy` and `https_proxy`][14]. Most Python applications (and other free software) respect these. Example syntax:\r\n\r\n    http://proxy_url:port\r\n    http://username:password@proxy_url:port\r\n\r\nIf you&#39;re really unlucky, your proxy might be a Microsoft [NTLM][15] proxy. Free software can&#39;t cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy. http://cntlm.sourceforge.net/\r\n\r\n### Unable to find vcvarsall.bat\r\n\r\nPython modules can be part written in C or C++. Pip tries to compile from source. If you don&#39;t have a C/C++ compiler installed and configured, you&#39;ll see this cryptic error message.\r\n\r\n&gt; Error: Unable to find vcvarsall.bat\r\n\r\nYou can fix that by [installing a C++ compiler][16] such as [MinGW][17] or [Visual C++][18]. Microsoft actually ship one specifically for use with Python. Or try *[Microsoft Visual C++ Compiler for Python 2.7][19]*.\r\n\r\nOften though it&#39;s easier to check [Christoph&#39;s site][20] for your package.\r\n\r\n  [1]: https://docs.python.org/3/whatsnew/3.4.html\r\n  [2]: https://docs.python.org/2/whatsnew/2.7.html#pep-477-backport-ensurepip-pep-453-to-python-2-7\r\n  [3]: http://en.wikipedia.org/wiki/Ruby_%28programming_language%29\r\n  [4]: http://en.wikipedia.org/wiki/Node.js\r\n  [5]: http://en.wikipedia.org/wiki/Haskell_%28programming_language%29\r\n  [6]: http://en.wikipedia.org/wiki/Perl\r\n  [7]: http://en.wikipedia.org/wiki/Go_%28programming_language%29\r\n  [8]: https://stackoverflow.com/questions/2436731/does-python-have-a-package-module-management-system/13445719#13445719\r\n  [9]: http://www.python.org/about/\r\n  [10]: https://bootstrap.pypa.io/get-pip.py\r\n  [11]: http://technet.microsoft.com/en-us/library/cc947813(v=ws.10).aspx\r\n  [12]: http://www.lfd.uci.edu/~gohlke/pythonlibs/#setuptools\r\n  [13]: http://www.lfd.uci.edu/~gohlke/pythonlibs/#pip\r\n  [14]: http://docs.python.org/2/library/urllib.html\r\n  [15]: https://en.wikipedia.org/wiki/NT_LAN_Manager\r\n  [16]: https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat\r\n  [17]: http://en.wikipedia.org/wiki/MinGW\r\n  [18]: http://en.wikipedia.org/wiki/Visual_C%2B%2B#32-bit_versions\r\n  [19]: http://aka.ms/vcpython27\r\n  [20]: http://www.lfd.uci.edu/~gohlke/pythonlibs/\r\n  [21]: https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip\r\n",
               "tags": [],
               "creation_date": 1347968733,
               "last_edit_date": 1502672440,
               "is_accepted": true,
               "id": "12476379",
               "down_vote_count": 7,
               "score": 1541
            },
            {
               "up_vote_count": 11,
               "answer_id": 13505059,
               "last_activity_date": 1484543787,
               "path": "3.stack.answer",
               "body_markdown": "**Updated at 2016 :** `Pip` should already be included in `Python 2.7.9+ or 3.4+`, but if for whatever reason it is not there, you can use the following one-liner.\r\n\r\n* Download https://bootstrap.pypa.io/get-pip.py and run it with Administrator permission `python get-pip.py` (If you are on Linux, use `sudo python get-pip.py`)\r\n\r\nPS:\r\n\r\n1. This should already be satisfied in most cases but, if necessary, be sure that your environment variable PATH includes Python&#39;s folders (for example, `Python 2.7.x` on Windows default install: `C:\\Python27 and C:\\Python27\\Scripts`, for `Python 3.3x`: `C:\\Python33 and C:\\Python33\\Scripts`, etc)\r\n\r\n2. I encounter same problem and then found such perhaps easiest way (one liner!) mentioned on official website here: http://www.pip-installer.org/en/latest/installing.html\r\n\r\nCan&#39;t believe there are so many lengthy (perhaps outdated?) answers out there. Feeling thankful to them but, please up-vote this short answer to help more new comers!",
               "tags": [],
               "creation_date": 1353551107,
               "last_edit_date": 1484543787,
               "is_accepted": false,
               "id": "13505059",
               "down_vote_count": 1,
               "score": 10
            },
            {
               "up_vote_count": 178,
               "answer_id": 14407505,
               "last_activity_date": 1485980369,
               "path": "3.stack.answer",
               "body_markdown": "**March 2016 Update:** \r\n\r\nThese answers are outdated or otherwise wordy and difficult.\r\n\r\nIf you&#39;ve got Python 3.4+ or 2.7.9+, it will be [installed by default](https://docs.python.org/3.4/whatsnew/3.4.html#whatsnew-pep-453) on Windows.  Otherwise, in short:\r\n\r\n1. Download the pip installer: \r\n   https://bootstrap.pypa.io/get-pip.py\r\n2. If paranoid, inspect file to confirm it isn&#39;t malicious\r\n   (must b64 decode).\r\n3. Open a console in the download folder as Admin and run\r\n   `get-pip.py`.  Alternatively, right-click its icon in Explorer and choose the &quot;run as Admin...&quot;.\r\n\r\nThe new binaries `pip.exe` (and the deprecated `easy_install.exe`) will be found in the `&quot;%ProgramFiles%\\PythonXX\\Scripts&quot;` folder (or similar), which is likely not in your `PATH` variable.  I recommend adding it.\r\n",
               "tags": [],
               "creation_date": 1358542701,
               "last_edit_date": 1485980369,
               "is_accepted": false,
               "id": "14407505",
               "down_vote_count": 2,
               "score": 176
            },
            {
               "up_vote_count": 25,
               "answer_id": 15294806,
               "last_activity_date": 1454789993,
               "path": "3.stack.answer",
               "body_markdown": "**Update March 2015**\r\n\r\nPython 2.7.9 and later (on the Python 2 series), and Python 3.4 and later include pip by default, so you may have pip already.\r\n\r\nIf you don&#39;t, run this one line command on your prompt (which may require administrator access):\r\n\r\n    python -c &quot;exec(&#39;try: from urllib2 import urlopen \\nexcept: from urllib.request import urlopen&#39;);f=urlopen(&#39;https://bootstrap.pypa.io/get-pip.py&#39;).read();exec(f)&quot;\r\n\r\nIt will install [pip][1]. If [Setuptools][2] is not already installed, `get-pip.py` will install it for you too.\r\n\r\nAs mentioned in comments, the above command will download code from the Pip source code repository at [GitHub][3], and dynamically run it at your environment. So be noticed that this is a shortcut of the steps download, inspect and run, **all with a single command using Python itself**. If you trust Pip, proceed without doubt.\r\n\r\nBe sure that your Windows environment variable PATH includes Python&#39;s folders (for Python 2.7.x default install: `C:\\Python27` and `C:\\Python27\\Scripts`, for Python 3.3x: `C:\\Python33` and `C:\\Python33\\Scripts`, and so on).\r\n\r\n  [1]: http://www.pip-installer.org/\r\n  [2]: https://bitbucket.org/pypa/setuptools/\r\n  [3]: http://en.wikipedia.org/wiki/GitHub\r\n",
               "tags": [],
               "creation_date": 1362748397,
               "last_edit_date": 1454789993,
               "is_accepted": false,
               "id": "15294806",
               "down_vote_count": 1,
               "score": 24
            },
            {
               "up_vote_count": 10,
               "answer_id": 15626900,
               "last_activity_date": 1454790063,
               "path": "3.stack.answer",
               "body_markdown": "The best way I found so far, is just two lines of code:\r\n\r\n    curl http://python-distribute.org/distribute_setup.py | python\r\n    curl https://raw.github.com/pypa/pip/master/contrib/get-pip.py | python\r\n\r\nIt was tested on Windows 8 with [PowerShell][1], Cmd, and [Git][2] Bash ([MinGW][3]).\r\n\r\nAnd you probably want to add the path to your environment. It&#39;s somewhere like `C:\\Python33\\Scripts`.\r\n\r\n  [1]: http://en.wikipedia.org/wiki/Windows_PowerShell\r\n  [2]: http://en.wikipedia.org/wiki/Git_%28software%29\r\n  [3]: http://en.wikipedia.org/wiki/MinGW\r\n",
               "tags": [],
               "creation_date": 1364255181,
               "last_edit_date": 1454790063,
               "is_accepted": false,
               "id": "15626900",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 11,
               "answer_id": 15915700,
               "last_activity_date": 1454790272,
               "path": "3.stack.answer",
               "body_markdown": "To use pip, it is not mandatory that you need to install pip in the system directly. You can use it through [`virtualenv`][1]. What you can do is follow these steps:\r\n\r\n - Download virtualenv tar.gz file from [https://pypi.python.org/pypi/virtualenv][1]\r\n - Unzip it with 7zip or some other tool\r\n\r\nWe normally need to install Python packages for one particular project. So, now create a project folder, let\u2019s say myproject.\r\n\r\n - Copy the **virtualenv.py** file from the decompressed folder of `virtualenv`, and paste inside the **myproject** folder\r\n\r\nNow create a virtual environment, let\u2019s say **myvirtualenv** as follows, inside the **myproject** folder:\r\n\r\n    python virtualenv.py myvirtualenv\r\n\r\nIt will show you:\r\n\r\n    New python executable in myvirtualenv\\Scripts\\python.exe\r\n    Installing setuptools....................................done.\r\n    Installing pip.........................done.\r\n\r\nNow your virtual environment, **myvirtualenv**, is created inside your project folder. You might notice, pip is now installed inside you virtual environment. All you need to do is activate the virtual environment with the following command.\r\n\r\n    myvirtualenv\\Scripts\\activate\r\n\r\nYou will see the following at the command prompt:\r\n\r\n    (myvirtualenv) PATH\\TO\\YOUR\\PROJECT\\FOLDER&gt;pip install package_name\r\n\r\nNow you can start using pip, but make sure you have activated the virtualenv looking at the left of your prompt.\r\n\r\nThis is one of the easiest way to install pip i.e. inside virtual environment, but you need to have virtualenv.py file with you.\r\n\r\nFor more ways to install pip/virtualenv/virtualenvwrapper, you can refer to [thegauraw.tumblr.com][2].\r\n\r\n  [1]: https://pypi.python.org/pypi/virtualenv\r\n  [2]: http://thegauraw.tumblr.com\r\n",
               "tags": [],
               "creation_date": 1365559531,
               "last_edit_date": 1454790272,
               "is_accepted": false,
               "id": "15915700",
               "down_vote_count": 0,
               "score": 11
            },
            {
               "up_vote_count": 32,
               "answer_id": 15966898,
               "last_activity_date": 1389878362,
               "path": "3.stack.answer",
               "body_markdown": "The up-to-date way is to use Windows&#39; package manager [Chocolatey][1].\r\n\r\nOnce this is installed, all you have to do is open a command prompt and run the following the three commands below, which will install Python 2.7, easy_install and pip. It will automatically detect whether you&#39;re on x64 or x86 Windows.\r\n\r\n    cinst python\r\n    cinst easy.install\r\n    cinst pip\r\n\r\nAll of the other Python packages on the Chocolatey Gallery can be found [here][2].\r\n\r\n\r\n  [1]: http://chocolatey.org/\r\n  [2]: http://chocolatey.org/packages?q=python",
               "tags": [],
               "creation_date": 1365756551,
               "last_edit_date": 1389878362,
               "is_accepted": false,
               "id": "15966898",
               "down_vote_count": 2,
               "score": 30
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 17677737,
               "is_accepted": false,
               "last_activity_date": 1373980996,
               "body_markdown": "[PythonXY](https://code.google.com/p/pythonxy/) comes with `pip` included, among [others](https://code.google.com/p/pythonxy/wiki/Welcome?tm=6).",
               "id": "17677737",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1373980996,
               "score": 8
            },
            {
               "up_vote_count": 3,
               "answer_id": 19894333,
               "last_activity_date": 1454790336,
               "path": "3.stack.answer",
               "body_markdown": "It&#39;s very simple:\r\n\r\n    Step 1: wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py\r\n    Step 2: wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py\r\n    Step 2: python ez_setup.py\r\n    Step 3: python get-pip.py\r\n\r\n(Make sure your Python and Python script directory (for example, `C:\\Python27` and `C:\\Python27\\Scripts`) are in the PATH.)\r\n",
               "tags": [],
               "creation_date": 1384113104,
               "last_edit_date": 1454790336,
               "is_accepted": false,
               "id": "19894333",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 5,
               "answer_id": 20205042,
               "last_activity_date": 1385578511,
               "path": "3.stack.answer",
               "body_markdown": "I wrote [this pip install script][1] that wraps both the ez_setup.py and get-pip.py install scripts that were mentioned in Gringo Suave&#39;s answer (and runs a pip install --upgrade setuptools for the latest setuptools version once pip is installed).\r\n\r\nClone the repository with:\r\n\r\n    git clone https://github.com/chrissimpkins/pip-installer.git\r\n\r\nOr download a .zip archive:\r\n\r\n[https://github.com/chrissimpkins/pip-installer/archive/master.zip][2]\r\n\r\nAnd then run the pipinstall.py script in the top level of the repository directory:\r\n\r\n    python pipinstall.py\r\n\r\nThis will give you the latest releases for both applications.  It&#39;s safe to remove the script repository after the install.\r\n\r\n  [1]: https://github.com/chrissimpkins/pip-installer\r\n  [2]: https://github.com/chrissimpkins/pip-installer/archive/master.zip",
               "tags": [],
               "creation_date": 1385419671,
               "last_edit_date": 1385578511,
               "is_accepted": false,
               "id": "20205042",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 20948833,
               "is_accepted": false,
               "last_activity_date": 1389006915,
               "body_markdown": "I had some issues installing in different ways when I followed instructions here. I think it&#39;s very tricky to install in every Windows environment in the same way. In my case I need Python 2.6, 2.7 and 3.3 in the same machine for different purposes so that&#39;s why I think there&#39;re more problems.\r\nBut the following instructions worked perfectly for me, so might be depending on your environment you should try this one:\r\n\r\nhttp://docs.python-guide.org/en/latest/starting/install/win/\r\n\r\nAlso, due to the different environments I found incredible useful to use Virtual Environments, I had websites that use different libraries and it&#39;s much better to encapsulate them into a single folder, check out the instructions, briefly if PIP is installed you just install VirtualEnv:\r\n\r\n    pip install virtualenv\r\n\r\nInto the folder you have all your files run\r\n\r\n    virtualenv venv\r\n\r\nAnd seconds later you have a virtual environment with everything in venv folder, to activate it run venv/Scripts/activate.bat (deactivate the environment is easy, use deactivate.bat). Every library you install will end up in venv\\Lib\\site-packages and it&#39;s easy to move your whole environment somewhere.\r\n\r\nThe only downside I found is some code editors can&#39;t recognize this kind of environments, and you will see warnings in your code because imported libraries are not found. Of course there&#39;re tricky ways to do it but it would be nice editors keep in mind Virtual Environments are very normal nowadays.\r\n\r\nHope it helps.",
               "id": "20948833",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1389006915,
               "score": 5
            },
            {
               "up_vote_count": 40,
               "answer_id": 21182892,
               "last_activity_date": 1395086270,
               "path": "3.stack.answer",
               "body_markdown": "Python 3.4, which  was released in March 2014, comes with `pip` included:  \r\nhttp://docs.python.org/3.4/whatsnew/3.4.html  \r\nSo since the release of Python 3.4, the up-to-date way to install pip on Windows is to just install Python.\r\nWhen sticking to all defaults during installation, pip will be installed to  \r\n`C:\\Python34\\Scripts\\pip3.exe`.",
               "tags": [],
               "creation_date": 1389952714,
               "last_edit_date": 1395086270,
               "is_accepted": false,
               "id": "21182892",
               "down_vote_count": 0,
               "score": 40
            },
            {
               "up_vote_count": 4,
               "answer_id": 21546947,
               "last_activity_date": 1454790754,
               "path": "3.stack.answer",
               "body_markdown": "Working as of Feb 04 2014 :):\r\n\r\nIf you have tried installing pip through the Windows installer file from http://www.lfd.uci.edu/~gohlke/pythonlibs/#pip as suggested by @Colonel Panic, you might have installed the pip package manager successfully, but you might be unable to install any packages with pip. You might also have got the same SSL error as I got when I tried to install [Beautiful Soup&amp;nbsp;4][1] if you look in the pip.log file:\r\n\r\n    Downloading/unpacking beautifulsoup4\r\n      Getting page https://pypi.python.org/simple/beautifulsoup4/\r\n      Could not fetch URL https://pypi.python.org/simple/beautifulsoup4/: **connection error: [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed**\r\n      Will skip URL https://pypi.python.org/simple/beautifulsoup4/ when looking for download links for beautifulsoup4\r\n\r\nThe problem is an issue with an old version of [OpenSSL][2] being incompatible with pip 1.3.1 and above versions. The easy workaround for now, is to install pip 1.2.1, which does not require [SSL][3]:\r\n\r\nInstalling Pip on Windows:\r\n\r\n 1. Download pip 1.2.1 from https://pypi.python.org/packages/source/p/pip/pip-1.2.1.tar.gz\r\n 2. Extract the pip-1.2.1.tar.gz file\r\n 3. Change directory to the extracted folder: `cd &lt;path to extracted folder&gt;/pip-1.2.1`\r\n 4. Run `python setup.py install`\r\n 5. Now make sure `C:\\Python27\\Scripts` is in PATH because pip is installed in the `C:\\Python27\\Scripts` directory unlike `C:\\Python27\\Lib\\site-packages` where Python packages are normally installed\r\n\r\nNow try to install any package using pip.\r\n\r\nFor example, to install the `requests` package using pip, run this from cmd:\r\n\r\n    pip install requests\r\n\r\nWhola! `requests` will be successfully installed and you will get a success message.\r\n\r\n  [1]: https://en.wikipedia.org/wiki/Beautiful_Soup\r\n  [2]: http://en.wikipedia.org/wiki/OpenSSL\r\n  [3]: http://en.wikipedia.org/wiki/Transport_Layer_Security\r\n",
               "tags": [],
               "creation_date": 1391500845,
               "last_edit_date": 1454790754,
               "is_accepted": false,
               "id": "21546947",
               "down_vote_count": 1,
               "score": 3
            },
            {
               "up_vote_count": 15,
               "answer_id": 21647356,
               "last_activity_date": 1454791549,
               "path": "3.stack.answer",
               "body_markdown": "The following works for Python 2.7. Save this script and launch it: &lt;br/&gt; &lt;br/&gt;\r\nhttps://raw.github.com/pypa/pip/master/contrib/get-pip.py &lt;br/&gt; &lt;br/&gt;\r\nPip is installed, then add the path to your environment : &lt;br/&gt;\r\n\r\n    C:\\Python27\\Scripts\r\n\r\nFinally\r\n\r\n    pip install virtualenv\r\n\r\nAlso you need Microsoft [Visual C++ 2008 Express][1] to get the good compiler and avoid these kind of messages when installing packages:\r\n\r\n    error: Unable to find vcvarsall.bat\r\n\r\nIf you have a 64-bit version of Windows 7, you may read *[64-bit Python installation issues on 64-bit Windows 7][2]* to successfully install the Python executable package (issue with registry entries).\r\n\r\n  [1]: http://go.microsoft.com/?linkid=7729279\r\n  [2]: http://tech.valgog.com/2010/01/after-installing-64-bit-windows-7-at.html\r\n",
               "tags": [],
               "creation_date": 1391870481,
               "last_edit_date": 1454791549,
               "is_accepted": false,
               "id": "21647356",
               "down_vote_count": 0,
               "score": 15
            },
            {
               "up_vote_count": 2,
               "answer_id": 22020574,
               "last_activity_date": 1454791053,
               "path": "3.stack.answer",
               "body_markdown": "Alternatively, you can get [pip-Win][1] which is an all-in-one installer for pip and [`virtualenv`][2] on Windows and its GUI.\r\n\r\n - Switch from one Python interpreter (i.e. version) to another (including py and pypy)\r\n - See all installed packages, and whether they are up-to-date\r\n - Install or upgrade a package, or upgrade pip itself\r\n - Create and delete virtual environments, and switch between them\r\n - Run the [IDLE][3] or another Python script, with the selected interpreter\r\n\r\n  [1]: https://sites.google.com/site/pydatalog/python/pip-for-windows\r\n  [2]: https://pypi.python.org/pypi/virtualenv\r\n  [3]: http://en.wikipedia.org/wiki/IDLE_%28Python%29\r\n",
               "tags": [],
               "creation_date": 1393346475,
               "last_edit_date": 1454791053,
               "is_accepted": false,
               "id": "22020574",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 3,
               "answer_id": 22209970,
               "last_activity_date": 1454791114,
               "path": "3.stack.answer",
               "body_markdown": "How to install pip:\r\n\r\n1. Download and install [ActivePython][1]\r\n2. Open a command prompt (CMD)\r\n3. Type `pypm install pip`\r\n\r\n  [1]: http://www.activestate.com/activepython/downloads\r\n",
               "tags": [],
               "creation_date": 1394055756,
               "last_edit_date": 1454791114,
               "is_accepted": false,
               "id": "22209970",
               "down_vote_count": 2,
               "score": 1
            },
            {
               "up_vote_count": 1,
               "answer_id": 24043177,
               "last_activity_date": 1478708245,
               "path": "3.stack.answer",
               "body_markdown": "There is also an issue with `pip` on **64 bit Cygwin**. After installation, the output of `pip` command is always empty, no matters what commands/options do you use (even `pip -V` doesn&#39;t produce any output).\r\n\r\nIf it&#39;s your case, just install the development version of Cygwin&#39;s package *libuuid* called **`libuuid-devel`**. Without that package using of *libuuid* causes a segfault. And `pip` uses that package, so the segfault is the cause of an empty output of `pip` on Cygwin x64. On 32 bit Cygwin it&#39;s working fine even without that package.\r\n\r\nYou can read some details there: https://github.com/kennethreitz/requests/issues/1547",
               "tags": [],
               "creation_date": 1401900623,
               "last_edit_date": 1478708245,
               "is_accepted": false,
               "id": "24043177",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 26009357,
               "is_accepted": false,
               "last_activity_date": 1411537151,
               "body_markdown": "\r\n1. Download script: https://raw.github.com/pypa/pip/master/contrib/get-pip.py\r\n2. Save it on drive somewhere like C:\\pip-script\\get-pip.py\r\n3. Navigate to that path from command prompt and run &quot; python get-pip.py &quot;\r\n\r\nGuide link: http://www.pip-installer.org/en/latest/installing.html#install-pip\r\n\r\nNote: Make sure scripts path like this (C:\\Python27\\Scripts) is added int %PATH% environment variable as well.",
               "id": "26009357",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1411537151,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 30045463,
               "is_accepted": false,
               "last_activity_date": 1430806415,
               "body_markdown": "Just download setuptools-15.2.zip (md5), from here https://pypi.python.org/pypi/setuptools#windows-simplified , and run ez_setup.py.",
               "id": "30045463",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1430806415,
               "score": 2
            },
            {
               "up_vote_count": 7,
               "answer_id": 31150279,
               "last_activity_date": 1484759173,
               "path": "3.stack.answer",
               "body_markdown": "I use the cross-platform [Anaconda][1] package manager from continuum.io on Windows and it is reliable.  It has virtual environment management and a fully featured shell with common utilities (e.g. conda, pip).\r\n\r\n    &gt; conda install &lt;package&gt;               # access distributed binaries\r\n    \r\n    &gt; pip install &lt;package&gt;                 # access PyPI packages \r\n\r\n`conda` also comes with binaries for libraries with non-Python dependencies, e.g. `pandas`, `numpy`, etc.  This proves useful particularly on Windows as it can be  hard to correctly compile C dependencies.\r\n\r\n  [1]: https://www.continuum.io/downloads\r\n",
               "tags": [],
               "creation_date": 1435707349,
               "last_edit_date": 1484759173,
               "is_accepted": false,
               "id": "31150279",
               "down_vote_count": 0,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 34386951,
               "is_accepted": false,
               "last_activity_date": 1450653161,
               "body_markdown": "I think the question makes it seem like the answer is simpler than it really is. Running of pip will sometimes require native compilation of a module (64-bit Numpy is a common example of that). In order for pip&#39;s compilation to succeed, you need Python which was compiled with the same version of MSVC as the one pip is using. Standard Python distributions are compiled with MSVC 2008. You can install an Express version of VC2008, but it is not maintained. Your best bet is to get an express version of a later MSVC and compile Python. Then PIP and Python will be using the same MSVC version.",
               "id": "34386951",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1450653161,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 36277641,
               "is_accepted": false,
               "last_activity_date": 1459235264,
               "body_markdown": "Here how to install pip with easy way.\r\n\r\n 1. copy and paste [these][1] content in a file as **get-pip.py**\r\n 2. copy and paste **get-pip.py** into python folder.`C:\\Python27`\r\n 3. Double click to **get-pip.py** file.it will install pip to your computer.\r\n 4. Now you have to add `C:\\Python27\\Scripts` path to your enviroment variable.Because it includes `pip.exe` file.\r\n 5. Now you are ready to use pip. Open `cmd` and type as &lt;br&gt;`pip install package_name`\r\n\r\n  [1]: https://bootstrap.pypa.io/get-pip.py",
               "id": "36277641",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1459235264,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 38851508,
               "is_accepted": false,
               "last_activity_date": 1470747660,
               "body_markdown": "**pip** is already installed if you&#39;re using Python 2 &gt;=2.7.9 or Python 3 &gt;=3.4 binaries downloaded from [python.org][1], but you&#39;ll need to upgrade pip.\r\n\r\nOn Windows upgrade can be done easily \r\n\r\n   Go to Python command line and run below Python command\r\n   \r\n\r\npython -m pip install -U pip\r\n \r\nInstalling with get-pip.py\r\n\r\n  Download [get-pip.py][2] in the same folder or any other folder of your choice. I am assuming you will download it in the same folder from you have python.exe file and run this command \r\n\r\n    python get-pip.py\r\n\r\n\r\nPip&#39;s [installation guide][3] is pretty clean and simple.\r\n\r\nUsing this you should be able to get started with Pip in under two minutes.\r\n\r\n\r\n  [1]: http://python.org\r\n  [2]: https://bootstrap.pypa.io/get-pip.py\r\n  [3]: https://pip.pypa.io/en/latest/installing/",
               "id": "38851508",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1470747660,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 39200864,
               "is_accepted": false,
               "last_activity_date": 1472456264,
               "body_markdown": "you have to get the get_pip.py file search it on google copy   from there and  save it locally in c drive in pip directory ",
               "id": "39200864",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1472456264,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 44437176,
               "is_accepted": false,
               "last_activity_date": 1496928759,
               "body_markdown": "**For latest Python Download - I have python 3.6 on windows. You don&#39;t have to wonder everything you need is there , take  a breath i will show you how to do it.**\r\n\r\n1. make sure where you install python for me its was in the following directory\r\n[![enter image description here][1]][1]\r\n\r\n\r\n&gt; Now , lets add python and pip into environment variable path settings\r\n&gt; if you are on windows, so that typing pip or python anywhere call\r\n&gt; python aor pip from where they are installed.\r\n\r\nSo, PIP is found under the folder in above screen &quot;**SCRIPTS**&quot;\r\nLets add Python and PIP in environment variable path. \r\n[![enter image description here][2]][2]\r\n\r\n\r\nAlmost Done , Let test with CMD to install goole package using pip.\r\n\r\n    pip install google\r\n[![enter image description here][3]][3]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/ca5z1.png\r\n  [2]: https://i.stack.imgur.com/UETzB.png\r\n  [3]: https://i.stack.imgur.com/Pus98.png\r\n\r\nBYE BYE! ",
               "id": "44437176",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1496928759,
               "score": 4
            },
            {
               "up_vote_count": 1,
               "answer_id": 44552353,
               "last_activity_date": 1497465797,
               "path": "3.stack.answer",
               "body_markdown": "\t\r\nif you even have other problems with pip version you can try this \r\n\r\n    pip install --trusted-host pypi.python.org --upgrade pip",
               "tags": [],
               "creation_date": 1497465739,
               "last_edit_date": 1497465797,
               "is_accepted": false,
               "id": "44552353",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 46232509,
               "is_accepted": false,
               "last_activity_date": 1505454542,
               "body_markdown": "Sometime it easier to understand when you use IDE:\r\n\r\n - Install PyCharm\r\n - And create virtual environment it will automatically install pip\r\n - Then in code you could install any python packages \r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/COGXj.png",
               "id": "46232509",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1505454542,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48494601,
               "is_accepted": false,
               "last_activity_date": 1517201746,
               "body_markdown": "Now, it is bundled with Python. You don&#39;t need to install it.\r\n\r\n    pip -V\r\nThis is how you can check whether pip is installed or not.\r\nIn rare case, if it is not installed, download [get-pip.py][1] file and run it with python as\r\n\r\n    python get-pip.py\r\n\r\n\r\n  [1]: https://bootstrap.pypa.io/get-pip.py",
               "id": "48494601",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1517201746,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/4750806/how-do-i-install-pip-on-windows",
         "id": "858127-2251"
      },
      {
         "up_vote_count": "187",
         "path": "2.stack",
         "body_markdown": "It is possible to install [NumPy][1] with [pip][2] using `pip install numpy`. \r\n\r\nIs there a similar possibility with [SciPy][3]? (Doing `pip install scipy` does not work.)\r\n\r\n---\r\n\r\n**Update**\r\n\r\nThe package SciPy is now available to be installed with `pip`!\r\n\r\n  [1]: http://en.wikipedia.org/wiki/NumPy\r\n  [2]: https://en.wikipedia.org/wiki/Pip_%28package_manager%29\r\n  [3]: http://en.wikipedia.org/wiki/SciPy\r\n",
         "view_count": "290034",
         "answer_count": "14",
         "tags": "['python', 'install', 'scipy', 'pip']",
         "creation_date": "1265469235",
         "last_edit_date": "1460122282",
         "code_snippet": "['<code>pip install numpy</code>', '<code>pip install scipy</code>', '<code>pip</code>', '<code>pip install</code>', '<code>easy_install</code>', '<code>easy_install scipy\\nSearching for scipy\\nReading http://pypi.python.org/simple/scipy/\\nReading http://www.scipy.org\\nReading http://sourceforge.net/project/showfiles.php?group_id=27747&amp;package_id=19531\\nReading http://new.scipy.org/Wiki/Download\\n</code>', '<code>pip</code>', '<code>pip install svn+http://svn.scipy.org/svn/scipy/trunk/#egg=scipy\\n</code>', '<code>pip install git+https://github.com/scipy/scipy.git\\n</code>', '<code>pip install svn+http://svn.scipy.org/svn/scipy/trunk</code>', '<code>pip install http://svn.scipy.org/svn/scipy/!svn/bc/5839/trunk/</code>', '<code>pip install scipy</code>', '<code>brew install gfortran</code>', '<code>pip install numpy</code>', '<code>apt-get</code>', '<code>pip install git+http://github.com/scipy/scipy/</code>', '<code>sudo apt-get install build-essential gfortran libatlas-base-dev python-pip python-dev\\nsudo pip install --upgrade pip\\n</code>', '<code>sudo pip install numpy\\nsudo pip install scipy\\n</code>', '<code>sudo pip install matplotlib   OR  sudo apt-get install python-matplotlib\\nsudo pip install -U scikit-learn\\nsudo pip install pandas\\n</code>', '<code>sudo pip install</code>', '<code>pip install</code>', '<code>libatlas-base-dev gfortran</code>', '<code>libatlas-base-dev</code>', '<code>gfortran</code>', '<code>pip install scipy</code>', '<code>$ sudo apt-get install libamd2.2.0 libblas3gf libc6 libgcc1 libgfortran3 liblapack3gf libumfpack5.4.0 libstdc++6 build-essential gfortran libatlas-sse2-dev python-all-dev\\n</code>', '<code>sudo aptitude install python-scipy</code>', '<code>sudo apt-get build-dep python-scipy</code>', '<code>pip install scipy-0.17.0-cp27-none-win_amd64.whl\\n</code>', '<code>numpy</code>', '<code>pip install -U numpy\\n\\npip install -U scipy\\n</code>', '<code>-U</code>', '<code>pip install</code>', '<code>pip</code>', '<code>pip install scipy\\n</code>', '<code>sudo yum install -y python-pip\\nsudo yum install -y lapack lapack-devel blas blas-devel \\nsudo yum install -y blas-static lapack-static\\nsudo pip install numpy\\nsudo pip install scipy\\n</code>', '<code>public key</code>', '<code>--nogpgcheck</code>', '<code>yum</code>', '<code>yum --nogpgcheck install blas-devel</code>', '<code>dnf</code>', '<code>yum</code>', '<code>pip install --user scipy</code>', '<code>gcc-fortran</code>', '<code>blas</code>', '<code>lapack</code>', '<code>pip install -e git+http://github.com/scipy/scipy/#egg=scipy\\n</code>', '<code>python3</code>', '<code>git clone git://github.com/scipy/scipy.git scipy\\n\\ngit clone git://github.com/numpy/numpy.git numpy\\n</code>', '<code>numpy</code>', '<code>python3 setup.py build --fcompiler=gnu95\\n</code>', '<code>scipy</code>', '<code>python3 setup.py install\\n</code>', '<code> sudo apt-get install libatlas-base-dev gfortran\\n</code>', '<code>py -m pip install --upgrade pip\\npy -m pip install numpy\\npy -m pip install matplotlib\\npy -m pip install scipy\\npy -m pip install scikit-learn\\n</code>', '<code>pip install numpy\\n</code>', '<code>C:\\\\Users\\\\****\\\\Desktop\\\\a&gt; pip install mkl_service-1.1.2-cp35-cp35m-win32.whl\\nProcessing c:\\\\users\\\\****\\\\desktop\\\\a\\\\mkl_service-1.1.2-cp35-cp35m-win32.whl \\nInstalling collected packages: mkl-service    \\nSuccessfully installed mkl-service-1.1.2\\n</code>', '<code>C:\\\\Users\\\\****\\\\Desktop\\\\a&gt;pip install scipy-0.18.1-cp35-cp35m-win32.whl\\nProcessing c:\\\\users\\\\****\\\\desktop\\\\a\\\\scipy-0.18.1-cp35-cp35m-win32.whl\\nInstalling collected packages: scipy\\nSuccessfully installed scipy-0.18.1\\n</code>', '<code>from numpy._distributor_init import NUMPY_MKL  # requires numpy+mkl\\n</code>', '<code>emerge --ask scipy</code>']",
         "title": "Installing SciPy with pip",
         "_childDocuments_": [
            {
               "up_vote_count": 102,
               "answer_id": 2214018,
               "last_activity_date": 1429218744,
               "path": "3.stack.answer",
               "body_markdown": "An attempt to `easy_install` indicates a problem with their [listing][1] in the [Python Package Index][2], which pip searches.\r\n\r\n    easy_install scipy\r\n    Searching for scipy\r\n    Reading http://pypi.python.org/simple/scipy/\r\n    Reading http://www.scipy.org\r\n    Reading http://sourceforge.net/project/showfiles.php?group_id=27747&amp;package_id=19531\r\n    Reading http://new.scipy.org/Wiki/Download\r\n\r\nAll is not lost, however; `pip` can install from [Subversion][3] (SVN), [Git][4], [Mercurial][5], and [Bazaar][6] repositories. SciPy uses SVN:\r\n\r\n    pip install svn+http://svn.scipy.org/svn/scipy/trunk/#egg=scipy\r\n\r\nUpdate (12-2012):\r\n\r\n    pip install git+https://github.com/scipy/scipy.git\r\n\r\nSince NumPy is a dependency, it should be installed as well.\r\n\r\n  [1]: http://pypi.python.org/pypi/scipy/0.7.0\r\n  [2]: http://pypi.python.org/pypi\r\n  [3]: http://en.wikipedia.org/wiki/Apache_Subversion\r\n  [4]: http://en.wikipedia.org/wiki/Git_%28software%29\r\n  [5]: http://en.wikipedia.org/wiki/Mercurial\r\n  [6]: http://en.wikipedia.org/wiki/Bazaar_%28software%29\r\n",
               "tags": [],
               "creation_date": 1265478500,
               "last_edit_date": 1429218744,
               "is_accepted": true,
               "id": "2214018",
               "down_vote_count": 1,
               "score": 101
            },
            {
               "up_vote_count": 13,
               "answer_id": 3625365,
               "last_activity_date": 1423083442,
               "path": "3.stack.answer",
               "body_markdown": "If I first install BLAS, LAPACK and GCC Fortran as system packages (I&#39;m using [Arch Linux][1]), I can get SciPy installed with:\r\n\r\n    pip install scipy\r\n\r\n  [1]: http://en.wikipedia.org/wiki/Arch_Linux\r\n",
               "tags": [],
               "creation_date": 1283416186,
               "last_edit_date": 1423083442,
               "is_accepted": false,
               "id": "3625365",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 33,
               "answer_id": 3865521,
               "is_accepted": false,
               "last_activity_date": 1286294734,
               "body_markdown": "In Ubuntu 10.04 (Lucid), I could successfully `pip install scipy` (within a virtualenv) after installing some of its dependencies, in particular:\r\n\r\n    $ sudo apt-get install libamd2.2.0 libblas3gf libc6 libgcc1 libgfortran3 liblapack3gf libumfpack5.4.0 libstdc++6 build-essential gfortran libatlas-sse2-dev python-all-dev",
               "id": "3865521",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1286294734,
               "score": 33
            },
            {
               "up_vote_count": 3,
               "answer_id": 7668453,
               "last_activity_date": 1423083589,
               "path": "3.stack.answer",
               "body_markdown": "Addon for Ubuntu (Ubuntu&amp;nbsp;10.04 LTS (Lucid Lynx)):\r\n\r\nThe repository moved, but a\r\n\r\n    pip install -e git+http://github.com/scipy/scipy/#egg=scipy\r\n\r\nfailed for me... With the following steps, it finally worked out (as root in a virtual environment, where `python3` is a link to Python 3.2.2):\r\ninstall the Ubuntu dependencies (see elaichi), clone NumPy and SciPy:\r\n\r\n    git clone git://github.com/scipy/scipy.git scipy\r\n\r\n    git clone git://github.com/numpy/numpy.git numpy\r\n\r\nBuild NumPy (within the `numpy` folder):\r\n\r\n    python3 setup.py build --fcompiler=gnu95\r\n\r\nInstall SciPy (within the `scipy` folder):\r\n\r\n    python3 setup.py install\r\n",
               "tags": [],
               "creation_date": 1317853361,
               "last_edit_date": 1423083589,
               "is_accepted": false,
               "id": "7668453",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 203,
               "answer_id": 15355787,
               "last_activity_date": 1444651953,
               "path": "3.stack.answer",
               "body_markdown": "_Prerequisite:_\r\n\r\n    sudo apt-get install build-essential gfortran libatlas-base-dev python-pip python-dev\r\n    sudo pip install --upgrade pip\r\n\r\n_Actual packages:_\r\n\r\n    sudo pip install numpy\r\n    sudo pip install scipy\r\n\r\n_Optional packages:_\r\n\r\n    sudo pip install matplotlib   OR  sudo apt-get install python-matplotlib\r\n    sudo pip install -U scikit-learn\r\n    sudo pip install pandas\r\n\r\n[src][1]\r\n\r\n  [1]: http://pythonadventures.wordpress.com/2011/11/09/install-numpy-and-scipy-on-ubuntu/\r\n\r\n",
               "tags": [],
               "creation_date": 1363074321,
               "last_edit_date": 1444651953,
               "is_accepted": false,
               "id": "15355787",
               "down_vote_count": 2,
               "score": 201
            },
            {
               "up_vote_count": 16,
               "answer_id": 22493784,
               "last_activity_date": 1461250402,
               "path": "3.stack.answer",
               "body_markdown": "I tried all the above and nothing worked for me. This solved all my problems:\r\n\r\n    pip install -U numpy\r\n\r\n    pip install -U scipy\r\n\r\nNote that the `-U` option to `pip install` requests that the package be *upgraded*. Without it, if the package is already installed `pip` will inform you of this and exit without doing anything.\r\n\r\n",
               "tags": [],
               "creation_date": 1395188276,
               "last_edit_date": 1461250402,
               "is_accepted": false,
               "id": "22493784",
               "down_vote_count": 0,
               "score": 16
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 22633734,
               "is_accepted": false,
               "last_activity_date": 1395748257,
               "body_markdown": "For the Arch Linux users:\r\n\r\n`pip install --user scipy` prerequisites the following Arch packages to be installed:\r\n\r\n- `gcc-fortran`\r\n- `blas`\r\n- `lapack`\r\n",
               "id": "22633734",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1395748257,
               "score": 7
            },
            {
               "up_vote_count": 13,
               "answer_id": 28116352,
               "last_activity_date": 1455798435,
               "path": "3.stack.answer",
               "body_markdown": "On Fedora, this works:\r\n\r\n    sudo yum install -y python-pip\r\n    sudo yum install -y lapack lapack-devel blas blas-devel \r\n    sudo yum install -y blas-static lapack-static\r\n    sudo pip install numpy\r\n    sudo pip install scipy\r\n\r\nIf you get any `public key` errors while downloading, add `--nogpgcheck` as parameter to `yum`, for example:\r\n`yum --nogpgcheck install blas-devel`\r\n\r\nOn Fedora **23** onwards, use `dnf` instead of `yum`.",
               "tags": [],
               "creation_date": 1422037582,
               "last_edit_date": 1455798435,
               "is_accepted": false,
               "id": "28116352",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 32395290,
               "is_accepted": false,
               "last_activity_date": 1441360949,
               "body_markdown": "In my case, it wasn&#39;t working until I also installed the following package : libatlas-base-dev, gfortran\r\n\r\n\r\n     sudo apt-get install libatlas-base-dev gfortran\r\n\r\nThen run pip install scipy\r\n",
               "id": "32395290",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1441360949,
               "score": 3
            },
            {
               "up_vote_count": 23,
               "answer_id": 34220168,
               "last_activity_date": 1453921802,
               "path": "3.stack.answer",
               "body_markdown": "To install scipy  on windows  follow these instructions:-\r\n\r\nStep-1 : Press this link http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy to download a scipy .whl file (e.g. scipy-0.17.0-cp34-none-win_amd64.whl).\r\n\r\nStep-2: Go to the directory where that download file is there from the command prompt (cd folder-name ).\r\n\r\nStep-3: Run this command:\r\n\r\n    pip install scipy-0.17.0-cp27-none-win_amd64.whl",
               "tags": [],
               "creation_date": 1449826342,
               "last_edit_date": 1453921802,
               "is_accepted": false,
               "id": "34220168",
               "down_vote_count": 1,
               "score": 22
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 36588996,
               "is_accepted": false,
               "last_activity_date": 1460523750,
               "body_markdown": "For gentoo, it&#39;s in the main repository:\r\n`emerge --ask scipy`",
               "id": "36588996",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1460523750,
               "score": 0
            },
            {
               "up_vote_count": 3,
               "answer_id": 37884499,
               "last_activity_date": 1466176847,
               "path": "3.stack.answer",
               "body_markdown": " 1. install python-3.4.4\r\n 2. scipy-0.15.1-win32-superpack-python3.4\r\n 3. apply the following commend doc\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    py -m pip install --upgrade pip\r\n    py -m pip install numpy\r\n    py -m pip install matplotlib\r\n    py -m pip install scipy\r\n    py -m pip install scikit-learn",
               "tags": [],
               "creation_date": 1466174792,
               "last_edit_date": 1466176847,
               "is_accepted": false,
               "id": "37884499",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 39718011,
               "is_accepted": false,
               "last_activity_date": 1474959468,
               "body_markdown": "Besides all of these answers,\r\nIf you install python of 32bit on your 64bit machine, you have to download scipy of 32-bit irrespective of your machine.\r\nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/\r\nIn the above URL you can download the packages and command is: pip install &lt;package name&gt;",
               "id": "39718011",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1474959468,
               "score": 1
            },
            {
               "up_vote_count": 1,
               "answer_id": 40278616,
               "last_activity_date": 1477577078,
               "path": "3.stack.answer",
               "body_markdown": "**The answer is yes, there is.**\r\n\r\nFirst you can easily install numpy use commands:\r\n\r\n    pip install numpy\r\n\r\nThen you should install mkl, which is required by Scipy, and you can download it [here](http://www.lfd.uci.edu/~gohlke/pythonlibs/ )\r\n\r\nAfter download the file_name.whl you install it \r\n\r\n    C:\\Users\\****\\Desktop\\a&gt; pip install mkl_service-1.1.2-cp35-cp35m-win32.whl\r\n    Processing c:\\users\\****\\desktop\\a\\mkl_service-1.1.2-cp35-cp35m-win32.whl \r\n    Installing collected packages: mkl-service    \r\n    Successfully installed mkl-service-1.1.2\r\nThen at the same website you can download scipy-0.18.1-cp35-cp35m-win32.whl\r\n\r\n*Note:You should download the file_name.whl according to you python version, if you python version is 32bit python3.5 you should download this one, and the &quot;win32&quot; is about your python version, not your operating system version.*\r\n\r\n\r\nThen install file_name.whl like this:\r\n\r\n    C:\\Users\\****\\Desktop\\a&gt;pip install scipy-0.18.1-cp35-cp35m-win32.whl\r\n    Processing c:\\users\\****\\desktop\\a\\scipy-0.18.1-cp35-cp35m-win32.whl\r\n    Installing collected packages: scipy\r\n    Successfully installed scipy-0.18.1\r\n\r\nThen there is only one more thing to do: comment out a specfic line or there will be error messages when you imput command &quot;import scipy&quot;.\r\n\r\nSo comment out this line\r\n\r\n    from numpy._distributor_init import NUMPY_MKL  # requires numpy+mkl\r\nin this file: your_own_path\\lib\\site-packages\\scipy\\__init__.py\r\n\r\nThen you can use SciPy :)\r\n\r\n[Here](https://stackoverflow.com/questions/37267399/importerror-cannot-import-name-numpy-mkl/38020634#38020634) tells you more about the last step.\r\n\r\n[Here](https://stackoverflow.com/questions/1517129/how-do-i-install-scipy-on-64-bit-windows/40275240#40275240) is a similar anwser to a similar question.",
               "tags": [],
               "creation_date": 1477553137,
               "last_edit_date": 1495542877,
               "is_accepted": false,
               "id": "40278616",
               "down_vote_count": 0,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/2213551/installing-scipy-with-pip",
         "id": "858127-2252"
      },
      {
         "up_vote_count": "174",
         "path": "2.stack",
         "body_markdown": "I am getting a bit of headache just because a simple looking, easy statement is throwing some errors in my face.\r\n\r\nI have a json file called strings.json like this:\r\n\r\n    &quot;strings&quot;: [{&quot;-name&quot;: &quot;city&quot;, &quot;#text&quot;: &quot;City&quot;}, {&quot;-name&quot;: &quot;phone&quot;, &quot;#text&quot;: &quot;Phone&quot;}, ...,\r\n                {&quot;-name&quot;: &quot;address&quot;, &quot;#text&quot;: &quot;Address&quot;}]\r\n\r\nI want to read the json file, just that for now. I have these statements which I found out, but it&#39;s not working:\r\n\r\n    import json\r\n    from pprint import pprint\r\n\r\n    with open(&#39;strings.json&#39;) as json_data:\r\n        d = json.loads(json_data)\r\n        json_data.close()\r\n        pprint(d)\r\n\r\nThe error displayed on the console was this:\r\n\r\n&lt;!-- language: lang-none --&gt;\r\n\r\n    Traceback (most recent call last):\r\n    File &quot;/home/.../android/values/manipulate_json.py&quot;, line 5, in &lt;module&gt;\r\n    d = json.loads(json_data)\r\n    File &quot;/usr/lib/python2.7/json/__init__.py&quot;, line 326, in loads\r\n    return _default_decoder.decode(s)\r\n    File &quot;/usr/lib/python2.7/json/decoder.py&quot;, line 366, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n    TypeError: expected string or buffer\r\n    [Finished in 0.1s with exit code 1]\r\n\r\n**Edited**\r\n\r\nChanged from `json.loads` to `json.load`\r\n\r\nand got this:\r\n\r\n&lt;!-- language: lang-none --&gt;\r\n\r\n    Traceback (most recent call last):\r\n    File &quot;/home/.../android/values/manipulate_json.py&quot;, line 5, in &lt;module&gt;\r\n    d = json.load(json_data)\r\n    File &quot;/usr/lib/python2.7/json/__init__.py&quot;, line 278, in load\r\n    **kw)\r\n    File &quot;/usr/lib/python2.7/json/__init__.py&quot;, line 326, in loads\r\n    return _default_decoder.decode(s)\r\n    File &quot;/usr/lib/python2.7/json/decoder.py&quot;, line 369, in decode\r\n    raise ValueError(errmsg(&quot;Extra data&quot;, s, end, len(s)))\r\n    ValueError: Extra data: line 829 column 1 - line 829 column 2 (char 18476 - 18477)\r\n    [Finished in 0.1s with exit code 1]\r\n",
         "view_count": "477858",
         "answer_count": "5",
         "tags": "['python', 'json']",
         "creation_date": "1385399709",
         "last_edit_date": "1486313352",
         "code_snippet": "['<code>\"strings\": [{\"-name\": \"city\", \"#text\": \"City\"}, {\"-name\": \"phone\", \"#text\": \"Phone\"}, ...,\\n            {\"-name\": \"address\", \"#text\": \"Address\"}]\\n</code>', \"<code>import json\\nfrom pprint import pprint\\n\\nwith open('strings.json') as json_data:\\n    d = json.loads(json_data)\\n    json_data.close()\\n    pprint(d)\\n</code>\", '<code>Traceback (most recent call last):\\nFile \"/home/.../android/values/manipulate_json.py\", line 5, in &lt;module&gt;\\nd = json.loads(json_data)\\nFile \"/usr/lib/python2.7/json/__init__.py\", line 326, in loads\\nreturn _default_decoder.decode(s)\\nFile \"/usr/lib/python2.7/json/decoder.py\", line 366, in decode\\nobj, end = self.raw_decode(s, idx=_w(s, 0).end())\\nTypeError: expected string or buffer\\n[Finished in 0.1s with exit code 1]\\n</code>', '<code>json.loads</code>', '<code>json.load</code>', '<code>Traceback (most recent call last):\\nFile \"/home/.../android/values/manipulate_json.py\", line 5, in &lt;module&gt;\\nd = json.load(json_data)\\nFile \"/usr/lib/python2.7/json/__init__.py\", line 278, in load\\n**kw)\\nFile \"/usr/lib/python2.7/json/__init__.py\", line 326, in loads\\nreturn _default_decoder.decode(s)\\nFile \"/usr/lib/python2.7/json/decoder.py\", line 369, in decode\\nraise ValueError(errmsg(\"Extra data\", s, end, len(s)))\\nValueError: Extra data: line 829 column 1 - line 829 column 2 (char 18476 - 18477)\\n[Finished in 0.1s with exit code 1]\\n</code>', '<code>json.load()</code>', \"<code>import json\\n\\nwith open('strings.json') as json_data:\\n    d = json.load(json_data)\\n    print(d)\\n</code>\", '<code>json.loads()</code>', '<code>import json\\n\\nwith open(\"test.json\") as json_file:\\n    json_data = json.load(json_file)\\n    print(json_data)\\n</code>', '<code>{\\n    \"a\": [1,3,\"asdf\",true],\\n    \"b\": {\\n        \"Hello\": \"world\"\\n    }\\n}\\n</code>', \"<code>with open('strings.json') as json_data:\\n    d = json.load(json_data)\\n    pprint(d)\\n</code>\", '<code>json_data.close()</code>', '<code>print(json.dumps(d,sort_keys=True,indent=2))</code>', '<code># Considering \"json_list.json\" is a json file\\n\\nwith open(\\'json_list.json\\') as fd:\\n     json_data = json.load(fd)\\n</code>', \"<code>json_data = json.load(open('json_list.json'))\\n</code>\", '<code>json_data = json.loads(\\'{\"name\" : \"myName\", \"age\":24}\\')\\n</code>']",
         "title": "Reading JSON from a file?",
         "_childDocuments_": [
            {
               "up_vote_count": 302,
               "answer_id": 20199213,
               "last_activity_date": 1470503803,
               "path": "3.stack.answer",
               "body_markdown": "The [`json.load()` method](http://docs.python.org/3/library/json.html#json.load) (without &quot;s&quot; in &quot;load&quot;) can read a file directly:\r\n\r\n    import json\r\n    \r\n    with open(&#39;strings.json&#39;) as json_data:\r\n        d = json.load(json_data)\r\n        print(d)\r\n\r\n\r\nYou were using the [`json.loads()` method](http://docs.python.org/3/library/json.html#json.loads), which is used for *string* arguments only. \r\n\r\nEdit:\r\nThe new message is a totally different problem. In that case, there is some invalid json in that file. For that, I would recommend running the file through a [json validator](http://jsonlint.com/).\r\n\r\nThere are also solutions for fixing json like for example https://stackoverflow.com/questions/18514910/how-do-i-automatically-fix-an-invalid-json-string.",
               "tags": [],
               "creation_date": 1385399972,
               "last_edit_date": 1495539204,
               "is_accepted": true,
               "id": "20199213",
               "down_vote_count": 0,
               "score": 302
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 94,
               "answer_id": 20200782,
               "is_accepted": false,
               "last_activity_date": 1385405177,
               "body_markdown": "Here is a copy of code which works fine for me\r\n\r\n    import json\r\n    \r\n    with open(&quot;test.json&quot;) as json_file:\r\n        json_data = json.load(json_file)\r\n        print(json_data)\r\n\r\nwith the data\r\n\r\n    {\r\n        &quot;a&quot;: [1,3,&quot;asdf&quot;,true],\r\n        &quot;b&quot;: {\r\n            &quot;Hello&quot;: &quot;world&quot;\r\n        }\r\n    }\r\n\r\nyou may want to wrap your json.load line with a try catch because invalid JSON will cause a stacktrace error message.",
               "id": "20200782",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1385405177,
               "score": 94
            },
            {
               "up_vote_count": 33,
               "answer_id": 25984135,
               "last_activity_date": 1433701725,
               "path": "3.stack.answer",
               "body_markdown": "The problem is using **with** statement:\r\n\r\n    with open(&#39;strings.json&#39;) as json_data:\r\n        d = json.load(json_data)\r\n        pprint(d)\r\n\r\nThe file is going to be implicitly closed already. There is no need to call `json_data.close()` again.",
               "tags": [],
               "creation_date": 1411425486,
               "last_edit_date": 1433701725,
               "is_accepted": false,
               "id": "25984135",
               "down_vote_count": 1,
               "score": 32
            },
            {
               "up_vote_count": 7,
               "answer_id": 42087916,
               "last_activity_date": 1497349068,
               "path": "3.stack.answer",
               "body_markdown": "In python 3, we can use below method.\r\n\r\n**Read from file and convert to JSON**\r\n\r\n    # Considering &quot;json_list.json&quot; is a json file\r\n\r\n    with open(&#39;json_list.json&#39;) as fd:\r\n         json_data = json.load(fd)\r\n\r\nor\r\n\r\n    json_data = json.load(open(&#39;json_list.json&#39;))\r\n\r\n**String to JSON**\r\n\r\n    json_data = json.loads(&#39;{&quot;name&quot; : &quot;myName&quot;, &quot;age&quot;:24}&#39;)\r\n\r\n",
               "tags": [],
               "creation_date": 1486464722,
               "last_edit_date": 1497349068,
               "is_accepted": false,
               "id": "42087916",
               "down_vote_count": 0,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48298623,
               "is_accepted": false,
               "last_activity_date": 1516184264,
               "body_markdown": "To add on this, today you are able to use pandas to import json:  \r\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html\r\nYou may want to do a careful use of the orient parameter.",
               "id": "48298623",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1516184264,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/20199126/reading-json-from-a-file",
         "id": "858127-2253"
      },
      {
         "up_vote_count": "237",
         "path": "2.stack",
         "body_markdown": "## Short Question ##\r\n\r\n* What is the proper way to install [`pip`][pip], [`virtualenv`][virtualenv], and [`distribute`][distribute]?\r\n\r\n## Background ##\r\n\r\nIn [my answer][so-answer] to [SO question 4314376][so-question], I recommended using `ez_setup` so that you could then install `pip` and `virtualenv` as follows:\r\n\r\n    curl -O http://peak.telecommunity.com/dist/ez_setup.py\r\n    sudo python ez_setup.py\r\n    sudo easy_install pip\r\n    sudo pip install virtualenv\r\n\r\nI originally pulled these instructions from Jesse Noller&#39;s blog post [So you want to use Python on the Mac?][noller]. I like the idea of keeping a clean global site-packages directory, so the only other packages I install there are  [`virtualenvwrapper`][virtualenvwrapper] and [`distribute`][distribute]. (I recently added [`distribute`][distribute] to my toolbox because of [this Python public service announcement][public]. To install these two packages, I used:\r\n\r\n    sudo pip install virtualenvwrapper\r\n    curl -O http://python-distribute.org/distribute_setup.py\r\n    sudo python distribute_setup.py\r\n\r\n## No more setuptools and easy_install ##\r\n\r\nTo really follow [that Python public service announcement][public], on a fresh Python install, I would do the following:\r\n\r\n    curl -O http://python-distribute.org/distribute_setup.py\r\n    sudo python distribute_setup.py\r\n    sudo easy_install pip\r\n    sudo pip install virtualenv\r\n    sudo pip install virtualenvwrapper\r\n\r\n## Glyph&#39;s Rebuke ##\r\n\r\nIn a comment to [my answer][so-answer] to [SO question 4314376][so-question], SO user [Glyph][] stated:\r\n\r\n&gt; NO. NEVER EVER do `sudo python setup.py install` whatever. Write a ~/.pydistutils.cfg that puts your pip installation into ~/.local or something. Especially files named `ez_setup.py` tend to suck down newer versions of things like setuptools and easy_install, which can potentially break other things on your operating system.\r\n\r\n## Back to the short question ##\r\n\r\nSo [Glyph&#39;s response][so-answer] leads me to my original question:\r\n\r\n* What is the proper way to install [`pip`][pip], [`virtualenv`][virtualenv], and [`distribute`][distribute]?\r\n\r\n\r\n[distribute]: http://packages.python.org/distribute/\r\n[ez_setup]: http://peak.telecommunity.com/DevCenter/EasyInstall\r\n[glyph]: https://stackoverflow.com/users/13564/glyph\r\n[noller]: http://jessenoller.com/2009/03/16/so-you-want-to-use-python-on-the-mac/\r\n[pip]: http://pip.readthedocs.org\r\n[public]: http://s3.pixane.com/pip_distribute.png\r\n[so-answer]:  https://stackoverflow.com/questions/4314376/python-egg-file/4314446#4314446\r\n[so-question]: https://stackoverflow.com/questions/4314376/python-egg-file\r\n[virtualenv]: http://virtualenv.openplans.org/\r\n[virtualenvwrapper]: http://www.doughellmann.com/projects/virtualenvwrapper/\r\n",
         "view_count": "142759",
         "answer_count": "15",
         "tags": "['python', 'virtualenv', 'setuptools', 'distribute']",
         "creation_date": "1291208088",
         "last_edit_date": "1495541448",
         "code_snippet": "['<code>pip</code>', '<code>virtualenv</code>', '<code>distribute</code>', '<code>ez_setup</code>', '<code>pip</code>', '<code>virtualenv</code>', '<code>curl -O http://peak.telecommunity.com/dist/ez_setup.py\\nsudo python ez_setup.py\\nsudo easy_install pip\\nsudo pip install virtualenv\\n</code>', '<code>virtualenvwrapper</code>', '<code>distribute</code>', '<code>distribute</code>', '<code>sudo pip install virtualenvwrapper\\ncurl -O http://python-distribute.org/distribute_setup.py\\nsudo python distribute_setup.py\\n</code>', '<code>curl -O http://python-distribute.org/distribute_setup.py\\nsudo python distribute_setup.py\\nsudo easy_install pip\\nsudo pip install virtualenv\\nsudo pip install virtualenvwrapper\\n</code>', '<code>sudo python setup.py install</code>', '<code>ez_setup.py</code>', '<code>pip</code>', '<code>virtualenv</code>', '<code>distribute</code>', '<code>python distribute_setup.py</code>', '<code>easy_install pip</code>', '<code>virtualenv --distribute venv</code>', '<code>sudo apt-get install python-{pip,virtualenv}</code>', '<code>pip</code>', '<code>virtualenv</code>', '<code>http://python-distribute.org/distribute_setup.py</code>', '<code># Select current version of virtualenv:\\nVERSION=12.0.7\\n# Name your first \"bootstrap\" environment:\\nINITIAL_ENV=bootstrap\\n# Set to whatever python interpreter you want for your first environment:\\nPYTHON=$(which python)\\nURL_BASE=https://pypi.python.org/packages/source/v/virtualenv\\n\\n# --- Real work starts here ---\\ncurl -O $URL_BASE/virtualenv-$VERSION.tar.gz\\ntar xzf virtualenv-$VERSION.tar.gz\\n# Create the first \"bootstrap\" environment.\\n$PYTHON virtualenv-$VERSION/virtualenv.py $INITIAL_ENV\\n# Don\\'t need this anymore.\\nrm -rf virtualenv-$VERSION\\n# Install virtualenv into the environment.\\n$INITIAL_ENV/bin/pip install virtualenv-$VERSION.tar.gz\\n</code>', '<code># Create a second environment from the first:\\n$INITIAL_ENV/bin/virtualenv py-env1\\n# Create more:\\n$INITIAL_ENV/bin/virtualenv py-env2\\n</code>', '<code>--no-site-packges</code>', '<code>--distribute</code>', '<code>python virtualenv.py path-to-bootstrap</code>', '<code>python3 virtualenv.py path-to-bootstrap</code>', '<code>python virtualenv.py TARGET_DIRECTORY</code>', '<code>virtualenv</code>', '<code>--system-site-packages</code>', \"<code>curl -Lo virtualenv-tmp.tar.gz 'https://github.com/pypa/virtualenv/tarball/master'</code>\", '<code>~/.local</code>', '<code>~/.bashrc</code>', '<code>~/.local/bin</code>', '<code>PATH</code>', '<code>~/.local</code>', '<code>PYTHONPATH</code>', '<code>~/.pydistutils.cfg</code>', '<code>[install]\\nprefix=~/.local\\n</code>', '<code>distribute_setup.py</code>', '<code>python distribute_setup.py</code>', '<code>sudo</code>', '<code>site-packages</code>', '<code>which easy_install</code>', '<code>~/.local/bin</code>', '<code>pip install virtualenv</code>', '<code>pip install virtualenvwrapper</code>', '<code>~/.virtualenvs</code>', '<code>~/.bashrc</code>', '<code>export WORKON_HOME\\nsource ~/.local/bin/virtualenvwrapper.sh\\n</code>', '<code>sudo</code>', '<code>~/.local</code>', '<code>virtualenvwrapper</code>', '<code>easy_install pip</code>', '<code>sudo apt-get install python-pip python-dev\\n</code>', '<code>brew install python\\n</code>', '<code>pip</code>', '<code>sudo</code>', '<code>sudo pip install virtualenvwrapper\\n</code>', '<code>sudo apt-get install virtualenvwrapper</code>', '<code>pip3 install virtualenv virtualenvwrapper</code>', '<code>VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3</code>', '<code>.bashrc/.zshrc</code>', '<code>.bashrc</code>', '<code>export WORKON_HOME\\nsource /usr/local/bin/virtualenvwrapper.sh\\n</code>', '<code>. ~/.bashrc\\n</code>', '<code>mkvirtualenv --system-site-packages foo\\n</code>', '<code>kermit@hocus-pocus:~$ sudo apt-get install python-pandas\\nkermit@hocus-pocus:~$ mkvirtualenv --system-site-packages s\\n(s)kermit@hocus-pocus:~$ pip install --upgrade pandas\\n(s)kermit@hocus-pocus:~$ python -c \"import pandas; print(pandas.__version__)\"\\n0.10.1\\n(s)kermit@hocus-pocus:~$ deactivate\\nkermit@hocus-pocus:~$ python -c \"import pandas; print(pandas.__version__)\"\\n0.8.0\\n</code>', '<code>mkvirtualenv --no-site-packages bar\\n</code>', '<code>mkvirtualenv bar\\n</code>', '<code>pip install flask\\n</code>', '<code>export WORKON_HOME=\"$HOME/.virtualenvs\"</code>', '<code>brew install python</code>', '<code>WORKON_HOME</code>', '<code>~/.virtualenvs</code>', '<code>/usr/local/bin/virtualenvwrapper.sh</code>', '<code>workon_home_dir=\"$HOME/.virtualenvs\"</code>', '<code>if [ \"$workon_home_dir\" = \"\" ]</code>', '<code>pip install virtualenv virtualenv-wrapper</code>', '<code>pip install --upgrade distribute</code>', '<code>export PROJECT_HOME=\"$HOME/src\"</code>', '<code>mkproject</code>', '<code>venv</code>', '<code>sudo apt-get install python-virtualenv</code>', '<code>python-pip</code>', '<code>pip</code>', '<code>virtualenv</code>', '<code>sudo</code>', '<code>pip</code>', '<code>get-pip.py</code>', '<code># Initializes the virtualenv \\npythonbrew venv init\\n\\n# Create a virtual/sandboxed environment \\npythonbrew venv create mycoolbundle  \\n\\n# Use it \\npythonbrew venv use mycoolbundle\\n</code>', '<code>pyenv</code>', '<code>cd ~\\ncurl -s https://pypi.python.org/packages/source/p/pip/pip-1.3.1.tar.gz | tar xvz\\ncd pip-1.3.1\\npython setup.py install --user\\ncd ~\\nrm -rf pip-1.3.1\\n\\n$HOME/.local/bin/pip install --user --upgrade pip distribute virtualenvwrapper\\n\\n# Might want these three in your .bashrc\\nexport PATH=$PATH:$HOME/.local/bin\\nexport VIRTUALENVWRAPPER_VIRTUALENV_ARGS=\"--distribute\"\\nsource $HOME/.local/bin/virtualenvwrapper.sh\\n\\nmkvirtualenv mypy\\nworkon mypy\\npip install --upgrade distribute\\npip install pudb # Or whatever other nice package you might want.\\n</code>', '<code>pip freeze</code>', '<code>virtualenv</code>', '<code>curl</code>', '<code>wget</code>', '<code>tar</code>', '<code>pip</code>', '<code>easy_install</code>', '<code>/tmp/initvenv.py</code>', '<code>from __future__ import print_function\\n\\nimport os, sys, shutil, tempfile, subprocess, tarfile, hashlib\\n\\ntry:\\n    from urllib2 import urlopen\\nexcept ImportError:\\n    from urllib.request import urlopen\\n\\ntmp_dir = tempfile.mkdtemp(prefix=\\'initvenv_\\')\\ntry:\\n    # read the latest version from PyPI\\n    f = urlopen(\"https://pypi.python.org/pypi/virtualenv/\")\\n    # retrieve the .tar.gz file\\n    for line in f.read().splitlines():\\n        if isinstance(line, bytes):\\n            line = line.decode(\\'utf-8\\')\\n        if \\'tar.gz&lt;\\' not in line:\\n            continue\\n        for url in line.split(\\'\"\\'):\\n            if url.startswith(\\'http\\'):\\n                url, md5 = url.split(\\'#\\')\\n                assert md5.startswith(\\'md5=\\')\\n                md5 = md5[4:]\\n                break\\n        else:\\n            continue\\n        break\\n    else:\\n        print(\\'tar.gz not found\\')\\n        sys.exit(1)\\n    file_name = url.rsplit(\\'/\\', 1)[1]\\n    print(file_name)\\n    # url = \"https://pypi.python.org/packages/source/v/virtualenv/\" + file_name\\n    os.chdir(tmp_dir)\\n    with open(file_name, \\'wb\\') as fp:\\n        data = urlopen(url).read()\\n        data_md5 = hashlib.md5(data).hexdigest()\\n        if md5 != data_md5:\\n            print(\\'md5 not correct\\')\\n            print(md5)\\n            print(data_md5)\\n            sys.exit(1)\\n        fp.write(data)\\n    tar = tarfile.open(file_name)\\n    tar.extractall()\\n    tar.close()\\n    os.chdir(file_name.replace(\\'.tar.gz\\', \\'\\'))\\n    print(subprocess.check_output([sys.executable, \\'virtualenv.py\\'] +\\n                                  [sys.argv[1]]).decode(\\'utf-8\\'), end=\\'\\')\\n    if len(sys.argv) &gt; 2:\\n        print(subprocess.check_output([\\n            os.path.join(sys.argv[1], \\'bin\\', \\'pip\\'), \\'install\\', \\'virtualenv\\'] +\\n\\n            sys.argv[2:]).decode(\\'utf-8\\'), end=\\'\\')\\nexcept:\\n    raise\\nfinally:\\n    shutil.rmtree(tmp_dir)  # always clean up\\n</code>', '<code>python_binary_to_use_in_venv /tmp/initvenv.py your_venv_name [optional packages]\\n</code>', '<code>distribute</code>', '<code>setuptools</code>', '<code>python /tmp/initvenv.py venv distribute\\n</code>', '<code>InsecurePlatformWarning</code>', '<code>venv</code>', '<code>virtualenv</code>', '<code>venv/bin/virtualenv venv2\\n</code>', '<code>% /opt/python/2.7.10/bin/python /tmp/initvenv.py venv virtualenvwrapper\\n</code>', '<code>% source venv/bin/virtualenvwrapper.sh\\n</code>', \"<code>% mktmpenv \\nNew python executable in tmp-17bdc3054a46b2b/bin/python\\nInstalling setuptools, pip, wheel...done.\\nThis is a temporary environment. It will be deleted when you run 'deactivate'.\\n(tmp-17bdc3054a46b2b)% \\n</code>\", '<code>pip</code>', '<code>request</code>', '<code>pip</code>', '<code>InsecurePlatformWarning</code>', '<code>pyenv</code>', '<code>brew install pyenv</code>', '<code>pyenv global 3.6.1</code>', '<code>pyenv local 2.7.13</code>', '<code>pyenv-virtualenv</code>', '<code>pyenv commands | grep virtualenv</code>', '<code>pyenv</code>', '<code>rbenv</code>', '<code>pyvenv</code>', '<code>dash</code>', '<code>/bin/sh</code>', '<code>bash --login</code>', '<code>pyenv</code>', '<code>pip</code>', '<code>sudo easy_install pip</code>', '<code>sudo pip install virtualenv</code>', '<code>virtualenv my_env</code>', '<code>virtualenv --distribute my_env</code>', '<code>distribute</code>', '<code>pip</code>', '<code>virtualenv</code>', '<code>terminal</code>', '<code># Create a bootstrapenv and activate it:\\n$ cd ~\\n$ python &lt;path to unzipped folder&gt;/virtualenv.py bootstrapenv\\n$ source bootstrapenv/bin/activate\\n\\n# Install virtualenvwrapper:\\n$ pip install virtualenvwrapper\\n$ mkdir -p ~/bootstrapenv/Envs\\n\\n# append it to file `.bashrc`\\n$ vi ~/.bashrc\\n  source ~/bootstrapenv/bin/activate\\n  export WORKON_HOME=~/bootstrapenv/Envs\\n  source ~/bootstrapenv/bin/virtualenvwrapper.sh\\n\\n# run it now.\\n$ source ~/.bashrc\\n</code>', '<code>mkvirtualenv env1</code>', '<code>lsvirtualenv</code>', '<code>virtualenv-1.11.4</code>', '<code>virtualenv-1.11.4.zip</code>', '<code>pyvenv project_dir\\nsource project_dir/bin/activate\\npython --version   \\npython 3.4.*\\n</code>', '<code>deactivate\\n</code>']",
         "title": "What&#39;s the proper way to install pip, virtualenv, and distribute for Python?",
         "_childDocuments_": [
            {
               "up_vote_count": 20,
               "answer_id": 4325047,
               "last_activity_date": 1360079322,
               "path": "3.stack.answer",
               "body_markdown": "I think Glyph means do something like this:\r\n\r\n 1. Create a directory `~/.local`, if it doesn&#39;t already exist.\r\n 2. In your `~/.bashrc`, ensure that `~/.local/bin` is on `PATH` and that `~/.local` is on `PYTHONPATH`.\r\n 3. Create a file `~/.pydistutils.cfg` which contains \r\n\r\n        [install]\r\n        prefix=~/.local\r\nIt&#39;s a standard ConfigParser-format file.\r\n 4. [Download][1] `distribute_setup.py` and run `python distribute_setup.py` (no `sudo`). If it complains about a non-existing `site-packages` directory, create it manually:\r\n\r\n    mkdir -p ~/.local/lib/python2.7/site-packages/\r\n\r\n 5. Run `which easy_install` to verify that it&#39;s coming from `~/.local/bin`\r\n 6. Run `pip install virtualenv`\r\n 7. Run `pip install virtualenvwrapper`\r\n 8. Create a virtual env containing folder, say `~/.virtualenvs`\r\n 9. In `~/.bashrc` add\r\n\r\n        export WORKON_HOME\r\n        source ~/.local/bin/virtualenvwrapper.sh\r\n\r\nThat&#39;s it, no use of `sudo` at all and your Python environment is in `~/.local`, completely separate from the OS&#39;s Python. Disclaimer: Not sure how compatible `virtualenvwrapper` is in this scenario - I couldn&#39;t test it on my system :-)\r\n\r\n\r\n  [1]: http://pypi.python.org/pypi/distribute",
               "tags": [],
               "creation_date": 1291211707,
               "last_edit_date": 1360079322,
               "is_accepted": false,
               "id": "4325047",
               "down_vote_count": 1,
               "score": 19
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 4328091,
               "is_accepted": false,
               "last_activity_date": 1291230639,
               "body_markdown": "There is no problem to do sudo python setup.py install, if you&#39;re sure it&#39;s what you want to do.\r\n\r\nThe difference is that it will use the site-packages directory of your OS as a destination for .py files to be copied.\r\n\r\nso, if you want pip to be accessible os wide, that&#39;s probably the way to go. I do not say that others way are bad, but this is probably fair enough.",
               "id": "4328091",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1291230639,
               "score": 3
            },
            {
               "up_vote_count": 170,
               "answer_id": 5177027,
               "last_activity_date": 1428353537,
               "path": "3.stack.answer",
               "body_markdown": "You can do this without installing **anything** into python itself.\r\n\r\nYou don&#39;t need sudo or any privileges.\r\n\r\nYou don&#39;t need to edit any files.\r\n\r\nInstall virtualenv into a bootstrap virtual environment. Use the that virtual environment to create more. Since virtualenv ships with pip and distribute, you get everything from one install.\r\n\r\n 1. Download virtualenv:\r\n  - http://pypi.python.org/pypi/virtualenv\r\n  - https://pypi.python.org/packages/source/v/virtualenv/virtualenv-12.0.7.tar.gz\r\n    (or whatever is the latest version!)\r\n 2. Unpack the source tarball\r\n 3. Use the unpacked tarball to create a clean virtual environment. This virtual environment will be used to &quot;bootstrap&quot; others. All of your virtual environments will automatically contain pip and distribute.\r\n 4. Using pip, install virtualenv into that bootstrap environment.\r\n 5. Use that bootstrap environment to create more!\r\n\r\nHere is an example in bash:\r\n\r\n    # Select current version of virtualenv:\r\n    VERSION=12.0.7\r\n    # Name your first &quot;bootstrap&quot; environment:\r\n    INITIAL_ENV=bootstrap\r\n    # Set to whatever python interpreter you want for your first environment:\r\n    PYTHON=$(which python)\r\n    URL_BASE=https://pypi.python.org/packages/source/v/virtualenv\r\n\r\n    # --- Real work starts here ---\r\n    curl -O $URL_BASE/virtualenv-$VERSION.tar.gz\r\n    tar xzf virtualenv-$VERSION.tar.gz\r\n    # Create the first &quot;bootstrap&quot; environment.\r\n    $PYTHON virtualenv-$VERSION/virtualenv.py $INITIAL_ENV\r\n    # Don&#39;t need this anymore.\r\n    rm -rf virtualenv-$VERSION\r\n    # Install virtualenv into the environment.\r\n    $INITIAL_ENV/bin/pip install virtualenv-$VERSION.tar.gz\r\n\r\nNow you can use your &quot;bootstrap&quot; environment to create more:\r\n\r\n    # Create a second environment from the first:\r\n    $INITIAL_ENV/bin/virtualenv py-env1\r\n    # Create more:\r\n    $INITIAL_ENV/bin/virtualenv py-env2\r\n\r\nGo nuts!\r\n\r\n**Note**\r\n----------\r\nThis assumes you are not using a really old version of virtualenv.\r\nOld versions required the flags `--no-site-packges` (and depending on the version of Python, `--distribute`). Now you can create your bootstrap environment with just `python virtualenv.py path-to-bootstrap` or `python3 virtualenv.py path-to-bootstrap`.",
               "tags": [],
               "creation_date": 1299130249,
               "last_edit_date": 1428353537,
               "is_accepted": false,
               "id": "5177027",
               "down_vote_count": 1,
               "score": 169
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 5382622,
               "is_accepted": false,
               "last_activity_date": 1300735916,
               "body_markdown": "[Install ActivePython][1]. It includes pip, virtualenv and Distribute.\r\n\r\n\r\n  [1]: http://www.activestate.com/activepython/downloads",
               "id": "5382622",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1300735916,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 7714942,
               "is_accepted": false,
               "last_activity_date": 1318259793,
               "body_markdown": "I came across the same problem recently.  I\u2019m becoming more partial to the \u201calways use a virtualenv\u201d mindset, so my problem was to install virtualenv with pip without installing distribute to my global or user site-packages directory.  To do this, I manually downloaded distribute, pip and virtualenv, and for each one I ran \u201cpython setup.py install --prefix ~/.local/python-private\u201d (with a temporary setting of PYTHONPATH=~/.local/python-private) so that setup scripts were able to find distribute).  I\u2019ve moved the virtualenv script to another directory I have on my PATH and edited it so that the distribute and virtualenv modules can be found on sys.path.  Tada: I did not install anything to /usr, /usr/local or my user site-packages dir, but I can run virtualenv anywhere, and in that virtualenv I get pip.",
               "id": "7714942",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1318259793,
               "score": 2
            },
            {
               "up_vote_count": 16,
               "answer_id": 14718550,
               "last_activity_date": 1398263081,
               "path": "3.stack.answer",
               "body_markdown": "If you follow the steps advised in several tutorials I linked in [this answer][1], you\r\ncan get the desired effect without the somewhat complicated &quot;manual&quot; steps in Walker&#39;s and Vinay&#39;s answers. If you&#39;re on Ubuntu:\r\n\r\n    sudo apt-get install python-pip python-dev\r\n\r\nThe equivalent is achieved in OS X by using homebrew to install python (more details [here](http://www.lowindata.com/2013/installing-scientific-python-on-mac-os-x/)).\r\n\r\n    brew install python\r\n\r\nWith `pip` installed, you can use it to get the remaining packages (you can omit `sudo` in OS X, as you&#39;re using your local python installation).\r\n \r\n    sudo pip install virtualenvwrapper\r\n\r\n(these are the only packages you need installed globally and I doubt that it will clash with anything system-level from the OS. If you want to be super-safe, you can keep the distro&#39;s versions `sudo apt-get install virtualenvwrapper`)\r\n\r\n**Note:** in Ubuntu 14.04 I receive [some errors with pip install](https://bitbucket.org/dhellmann/virtualenvwrapper/issue/233/pip-install-error-in-ubuntu-1404), so I use `pip3 install virtualenv virtualenvwrapper` and add `VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3` to my `.bashrc/.zshrc` file.\r\n\r\nYou then append to your `.bashrc` file\r\n\r\n    export WORKON_HOME\r\n    source /usr/local/bin/virtualenvwrapper.sh\r\n\r\nand source it\r\n\r\n    . ~/.bashrc\r\n\r\nThis is basically it. Now the only decision is whether you want to create a virtualenv to include system-level packages\r\n\r\n    mkvirtualenv --system-site-packages foo\r\n\r\nwhere your existing system packages don&#39;t have to be reinstalled, they are symlinked to the system interpreter&#39;s versions. *Note:* you can still install new packages and upgrade existing included-from-system packages without sudo - I tested it and it works without any disruptions of the system interpreter.\r\n\r\n    kermit@hocus-pocus:~$ sudo apt-get install python-pandas\r\n    kermit@hocus-pocus:~$ mkvirtualenv --system-site-packages s\r\n    (s)kermit@hocus-pocus:~$ pip install --upgrade pandas\r\n    (s)kermit@hocus-pocus:~$ python -c &quot;import pandas; print(pandas.__version__)&quot;\r\n    0.10.1\r\n    (s)kermit@hocus-pocus:~$ deactivate\r\n    kermit@hocus-pocus:~$ python -c &quot;import pandas; print(pandas.__version__)&quot;\r\n    0.8.0\r\n\r\nThe alternative, if you want a completely separated environment, is\r\n\r\n    mkvirtualenv --no-site-packages bar\r\n\r\nor given that this is the default option, simply\r\n\r\n    mkvirtualenv bar\r\n\r\nThe result is that you have a new virtualenv where you can freely and sudolessly install your favourite packages\r\n\r\n    pip install flask\r\n\r\n  [1]: https://stackoverflow.com/a/14717552/544059",
               "tags": [],
               "creation_date": 1360104962,
               "last_edit_date": 1495541448,
               "is_accepted": false,
               "id": "14718550",
               "down_vote_count": 0,
               "score": 16
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 16203156,
               "is_accepted": false,
               "last_activity_date": 1366841718,
               "body_markdown": "I made this procedure for us to use at work. \r\n\r\n&lt;!-- language: bash --&gt;\r\n\r\n    cd ~\r\n    curl -s https://pypi.python.org/packages/source/p/pip/pip-1.3.1.tar.gz | tar xvz\r\n    cd pip-1.3.1\r\n    python setup.py install --user\r\n    cd ~\r\n    rm -rf pip-1.3.1\r\n    \r\n    $HOME/.local/bin/pip install --user --upgrade pip distribute virtualenvwrapper\r\n    \r\n    # Might want these three in your .bashrc\r\n    export PATH=$PATH:$HOME/.local/bin\r\n    export VIRTUALENVWRAPPER_VIRTUALENV_ARGS=&quot;--distribute&quot;\r\n    source $HOME/.local/bin/virtualenvwrapper.sh\r\n    \r\n    mkvirtualenv mypy\r\n    workon mypy\r\n    pip install --upgrade distribute\r\n    pip install pudb # Or whatever other nice package you might want.\r\n\r\nKey points for the security minded:\r\n\r\n1. curl does ssl validation. wget doesn&#39;t.\r\n2. Starting from pip 1.3.1, pip also does ssl validation.\r\n3. Fewer users can upload the pypi tarball than a github tarball.",
               "id": "16203156",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1366841718,
               "score": 4
            },
            {
               "up_vote_count": 0,
               "answer_id": 16206199,
               "last_activity_date": 1366863517,
               "path": "3.stack.answer",
               "body_markdown": "There are good instructions on the Virtualenv official site. https://pypi.python.org/pypi/virtualenv \r\n\r\nBasically what I did, is install `pip` with `sudo easy_install pip`, then used `sudo pip install virtualenv` then created an environment with: `virtualenv my_env` (name it what you want), following that I did: `virtualenv --distribute my_env`; which installed `distribute` and `pip` in my virtualenv. \r\n\r\nAgain, follow the instruction on the `virtualenv` page.\r\n\r\nKind of a hassle, coming from Ruby ;P\r\n\r\n",
               "tags": [],
               "creation_date": 1366862755,
               "last_edit_date": 1366863517,
               "is_accepted": false,
               "id": "16206199",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 10,
               "answer_id": 17339129,
               "last_activity_date": 1462559974,
               "path": "3.stack.answer",
               "body_markdown": "On Ubuntu:\r\n\r\n`sudo apt-get install python-virtualenv`\r\n\r\nThe package `python-pip` is a dependency, so it will be installed as well.",
               "tags": [],
               "creation_date": 1372324263,
               "last_edit_date": 1462559974,
               "is_accepted": false,
               "id": "17339129",
               "down_vote_count": 0,
               "score": 10
            },
            {
               "up_vote_count": 5,
               "answer_id": 17401297,
               "last_activity_date": 1438211856,
               "path": "3.stack.answer",
               "body_markdown": "***Update:** As of July 2013 this project is no longer maintained. The author suggests using [pyenv][1]. (pyenv does not have built-in support for virtualenv, but plays nice with it.)*\r\n\r\n**[Pythonbrew][2]** is a version manager for python and comes with support for virtualenv.\r\n\r\nAfter installing pythonbrew and a python-version using venvs is really easy:\r\n\r\n    # Initializes the virtualenv \r\n    pythonbrew venv init\r\n    \r\n    # Create a virtual/sandboxed environment \r\n    pythonbrew venv create mycoolbundle  \r\n    \r\n    # Use it \r\n    pythonbrew venv use mycoolbundle\r\n\r\n\r\n  [1]: https://github.com/yyuu/pyenv\r\n  [2]: https://github.com/utahta/pythonbrew",
               "tags": [],
               "creation_date": 1372671380,
               "last_edit_date": 1438211856,
               "is_accepted": false,
               "id": "17401297",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 13,
               "answer_id": 21456786,
               "is_accepted": false,
               "last_activity_date": 1391085537,
               "body_markdown": "Python 3.4 onward\r\n--\r\n\r\nPython 3.3 adds the [venv module](http://docs.python.org/3.4/library/venv.html), and Python 3.4 adds the [ensurepip module](http://docs.python.org/3.4/library/ensurepip.html).  This makes bootstrapping pip as easy as:\r\n\r\n&gt; python -m ensurepip\r\n\r\nPerhaps preceded by a call to `venv` to do so inside a virtual environment.\r\n\r\nGuaranteed pip is described in [PEP 453](http://www.python.org/dev/peps/pep-0453/).",
               "id": "21456786",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1391085537,
               "score": 12
            },
            {
               "up_vote_count": 0,
               "answer_id": 22828541,
               "last_activity_date": 1396508368,
               "path": "3.stack.answer",
               "body_markdown": "##Here is a nice way to install virtualenvwrapper(update of [this](https://stackoverflow.com/a/5177027/2351696)).\r\n\r\nDownload [virtualenv-1.11.4](https://pypi.python.org/packages/source/v/virtualenv/virtualenv-1.11.4.tar.gz#md5=9accc2d3f0ec1da479ce2c3d1fdff06e) (you can find latest at [here](https://pypi.python.org/pypi/virtualenv#downloads)), Unzip it,  open `terminal`\r\n\r\n    # Create a bootstrapenv and activate it:\r\n    $ cd ~\r\n    $ python &lt;path to unzipped folder&gt;/virtualenv.py bootstrapenv\r\n    $ source bootstrapenv/bin/activate\r\n\r\n    # Install virtualenvwrapper:\r\n    $ pip install virtualenvwrapper\r\n    $ mkdir -p ~/bootstrapenv/Envs\r\n\r\n    # append it to file `.bashrc`\r\n    $ vi ~/.bashrc\r\n      source ~/bootstrapenv/bin/activate\r\n      export WORKON_HOME=~/bootstrapenv/Envs\r\n      source ~/bootstrapenv/bin/virtualenvwrapper.sh\r\n\r\n    # run it now.\r\n    $ source ~/.bashrc\r\n\r\nThat is it, now you can use `mkvirtualenv env1`, `lsvirtualenv` ..etc\r\n\r\nNote: you can delete `virtualenv-1.11.4` and `virtualenv-1.11.4.zip` from Downloads folders.",
               "tags": [],
               "creation_date": 1396502699,
               "last_edit_date": 1495541908,
               "is_accepted": false,
               "id": "22828541",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 2,
               "answer_id": 30692103,
               "last_activity_date": 1479462677,
               "path": "3.stack.answer",
               "body_markdown": "- You can do this without installing anything into python itself.\r\n\r\n- You don&#39;t need sudo or any privileges.\r\n\r\n- You don&#39;t need to find the latest version of a `virtualenv` tar file\r\n\r\n- You don&#39;t need to edit version info in a bash script to keep things up-to-date.\r\n\r\n- You don&#39;t need `curl`/`wget` or `tar` installed, nor `pip` or `easy_install`\r\n\r\nSave the following to `/tmp/initvenv.py`:\r\n\r\n    from __future__ import print_function\r\n    \r\n    import os, sys, shutil, tempfile, subprocess, tarfile, hashlib\r\n    \r\n    try:\r\n        from urllib2 import urlopen\r\n    except ImportError:\r\n        from urllib.request import urlopen\r\n    \r\n    tmp_dir = tempfile.mkdtemp(prefix=&#39;initvenv_&#39;)\r\n    try:\r\n        # read the latest version from PyPI\r\n        f = urlopen(&quot;https://pypi.python.org/pypi/virtualenv/&quot;)\r\n        # retrieve the .tar.gz file\r\n        for line in f.read().splitlines():\r\n            if isinstance(line, bytes):\r\n                line = line.decode(&#39;utf-8&#39;)\r\n            if &#39;tar.gz&lt;&#39; not in line:\r\n                continue\r\n            for url in line.split(&#39;&quot;&#39;):\r\n                if url.startswith(&#39;http&#39;):\r\n                    url, md5 = url.split(&#39;#&#39;)\r\n                    assert md5.startswith(&#39;md5=&#39;)\r\n                    md5 = md5[4:]\r\n                    break\r\n            else:\r\n                continue\r\n            break\r\n        else:\r\n            print(&#39;tar.gz not found&#39;)\r\n            sys.exit(1)\r\n        file_name = url.rsplit(&#39;/&#39;, 1)[1]\r\n        print(file_name)\r\n        # url = &quot;https://pypi.python.org/packages/source/v/virtualenv/&quot; + file_name\r\n        os.chdir(tmp_dir)\r\n        with open(file_name, &#39;wb&#39;) as fp:\r\n            data = urlopen(url).read()\r\n            data_md5 = hashlib.md5(data).hexdigest()\r\n            if md5 != data_md5:\r\n                print(&#39;md5 not correct&#39;)\r\n                print(md5)\r\n                print(data_md5)\r\n                sys.exit(1)\r\n            fp.write(data)\r\n        tar = tarfile.open(file_name)\r\n        tar.extractall()\r\n        tar.close()\r\n        os.chdir(file_name.replace(&#39;.tar.gz&#39;, &#39;&#39;))\r\n        print(subprocess.check_output([sys.executable, &#39;virtualenv.py&#39;] +\r\n                                      [sys.argv[1]]).decode(&#39;utf-8&#39;), end=&#39;&#39;)\r\n        if len(sys.argv) &gt; 2:\r\n            print(subprocess.check_output([\r\n                os.path.join(sys.argv[1], &#39;bin&#39;, &#39;pip&#39;), &#39;install&#39;, &#39;virtualenv&#39;] +\r\n    \r\n                sys.argv[2:]).decode(&#39;utf-8&#39;), end=&#39;&#39;)\r\n    except:\r\n        raise\r\n    finally:\r\n        shutil.rmtree(tmp_dir)  # always clean up\r\n\r\nand use it as\r\n\r\n    python_binary_to_use_in_venv /tmp/initvenv.py your_venv_name [optional packages]\r\ne.g. (if you really need the `distribute` compatibility layer for `setuptools`)\r\n\r\n    python /tmp/initvenv.py venv distribute\r\n\r\nPlease note that, with older python versions, this might give you `InsecurePlatformWarning`s&#185;. \r\n\r\nOnce you have your virtualenv (name e.g. `venv`) you can setup another virtualenv by using the `virtualenv` just installed:\r\n\r\n    venv/bin/virtualenv venv2\r\n\r\n###virtualenvwrapper\r\n\r\nI recommend taking a look at [virtualenvwrapper][1] as well, after a one time setup:\r\n\r\n    % /opt/python/2.7.10/bin/python /tmp/initvenv.py venv virtualenvwrapper\r\nand activation (can be done from your login script):\r\n\r\n    % source venv/bin/virtualenvwrapper.sh\r\nyou can do things like:\r\n\r\n    % mktmpenv \r\n    New python executable in tmp-17bdc3054a46b2b/bin/python\r\n    Installing setuptools, pip, wheel...done.\r\n    This is a temporary environment. It will be deleted when you run &#39;deactivate&#39;.\r\n    (tmp-17bdc3054a46b2b)% \r\n\r\n\r\n----\r\n\r\n&#185; &lt;sub&gt;I have not found a way to suppress the warning. It could be solved in `pip` and/or `request`, but the developers point to each other as the cause. I got the, often non-realistic, recommendation to upgrade the python version I was using to the latest version. I am sure this would break e.g my Linux Mint 17 install. Fortunately `pip` caches packages, so the Warning is made\r\nonly once per package install.&lt;/sub&gt;\r\n\r\n\r\n  [1]: http://virtualenvwrapper.readthedocs.org/en/latest/",
               "tags": [],
               "creation_date": 1433670282,
               "last_edit_date": 1479462677,
               "is_accepted": false,
               "id": "30692103",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 31064995,
               "is_accepted": false,
               "last_activity_date": 1435293277,
               "body_markdown": "The good news is if you have installed python3.4, pyvenv is already been installed. So, Just\r\n\r\n    pyvenv project_dir\r\n    source project_dir/bin/activate\r\n    python --version   \r\n    python 3.4.*\r\n\r\nNow in this virtual env, you can use pip to install modules for this project.\r\n\r\nLeave this virtual env , just\r\n    \r\n    deactivate\r\n\r\n",
               "id": "31064995",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1435293277,
               "score": 0
            },
            {
               "up_vote_count": 1,
               "answer_id": 43828665,
               "last_activity_date": 1500303819,
               "path": "3.stack.answer",
               "body_markdown": "I&#39;ve had various problems (see below) installing upgraded SSL modules, even inside a virtualenv,  on top of older OS-provided Python versions, so I now use `pyenv`.\r\n\r\n\r\n[pyenv](https://github.com/pyenv/pyenv) makes it very easy to install a new Python versions and supports virtualenvs.  Getting started is **much** easier than the recipes for virtualenv listed in other answers:\r\n\r\n- On Mac, type `brew install pyenv` and on Linux, use [pyenv-installer](https://github.com/pyenv/pyenv-installer)\r\n- this gets you built-in virtualenv support as well as Python version switching (if required)\r\n- works well with Python 2 or 3, can have many versions installed at once\r\n\r\nThis works very well to insulate the &quot;new Python&quot; version and virtualenv from system Python.  Because you can easily use a more recent Python (post 2.7.9), the SSL modules are already upgraded, and of course like any modern virtualenv setup you are insulated from the system Python modules.\r\n\r\nA couple of nice tutorials:\r\n\r\n- [Using pyenv and virtualenv](https://fijiaaron.wordpress.com/2015/06/18/using-pyenv-with-virtualenv-and-pip-cheat-sheet/) - when selecting a Python version, it&#39;s easier to use `pyenv global 3.6.1` (global to current user) or `pyenv local 2.7.13` (local to current directory).  \r\n- [pyenv basics and use with virtualenv](http://akbaribrahim.com/managing-python-virtual-environments-with-pyenv-virtualenv/)\r\n\r\nThe `pyenv-virtualenv` plugin is now built in - type `pyenv commands | grep virtualenv` to check.  I wouldn&#39;t use the pyenv-virtualenvwrapper plugin to start with - see how you get on with pyenv-virtualenv which is more integrated into pyenv, as this covers most of what virtualenvwrapper does.\r\n\r\n`pyenv` is modelled on `rbenv` (a good tool for Ruby version switching) and its only dependency is bash.\r\n\r\n- pyenv is unrelated to the very similarly named `pyvenv` - that is a virtualenv equivalent that&#39;s part of recent Python 3 versions, and doesn&#39;t handle Python version switching\r\n\r\nCaveats\r\n---\r\nTwo warnings about pyenv:\r\n\r\n 1. It only works from a bash or similar shell - or more specifically, the pyenv-virtualenv plugin doesn&#39;t like `dash`, which is `/bin/sh` on Ubuntu or Debian.\r\n 1. It must be run from an interactive login shell (e.g. `bash --login` using a terminal), which is not always easy to achieve with automation tools [such as Ansible](https://github.com/ansible/ansible/issues/4854#issuecomment-39284921).  \r\n\r\nHence pyenv is best for interactive use, and less good for scripting servers.\r\n\r\nSSL module problems\r\n---\r\nOne reason to use `pyenv` is that there are often problems with upgrading Python SSL modules when using older system-provided Python versions:\r\n\r\n- Ubuntu 14.04 [includes Python 2.7.6](https://packages.ubuntu.com/trusty/python2.7) which [requires considerable effort](https://stackoverflow.com/questions/18578439/using-requests-with-tls-doesnt-give-sni-support) to upgrade it to the right SSL modules so it handles SNI (server name indication) as client. (I wrote some Ansible scripts for this which was quite painful and still broke in some cases.)\r\n- upgraded SSL modules will be more important as [Python.org sites move to TLS 1.2 only](https://news.ycombinator.com/item?id=13539034) during 2017 and 2018.\r\n\r\n",
               "tags": [],
               "creation_date": 1494140176,
               "last_edit_date": 1500303819,
               "is_accepted": false,
               "id": "43828665",
               "down_vote_count": 0,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/4324558/whats-the-proper-way-to-install-pip-virtualenv-and-distribute-for-python",
         "id": "858127-2254"
      },
      {
         "up_vote_count": "565",
         "path": "2.stack",
         "body_markdown": "I have a DataFrames from pandas:\r\n\r\n    import pandas as pd\r\n    inp = [{&#39;c1&#39;:10, &#39;c2&#39;:100}, {&#39;c1&#39;:11,&#39;c2&#39;:110}, {&#39;c1&#39;:12,&#39;c2&#39;:120}]\r\n    df = pd.DataFrame(inp)\r\n    print df\r\n\r\nOutput:\r\n\r\n       c1   c2\r\n    0  10  100\r\n    1  11  110\r\n    2  12  120\r\n\r\nNow I want to iterate over the rows of the above frame. For every row I want to be able to access its elements (values in cells) by the name of the columns. So, for example, I would like to have something like that:\r\n\r\n    for row in df.rows:\r\n       print row[&#39;c1&#39;], row[&#39;c2&#39;]\r\n\r\nIs it possible to do that in pandas?\r\n\r\nI found [similar question][1]. But it does not give me the answer I need. For example, it is suggested there to use:\r\n\r\n    for date, row in df.T.iteritems():\r\n\r\nor\r\n\r\n    for row in df.iterrows():\r\n\r\nBut I do not understand what the `row` object is and how I can work with it.\r\n\r\n  [1]: https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas\r\n",
         "view_count": "616820",
         "answer_count": "11",
         "tags": "['python', 'pandas', 'rows', 'dataframe']",
         "creation_date": "1368169489",
         "last_edit_date": "1495541445",
         "code_snippet": "[\"<code>import pandas as pd\\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\\ndf = pd.DataFrame(inp)\\nprint df\\n</code>\", '<code>   c1   c2\\n0  10  100\\n1  11  110\\n2  12  120\\n</code>', \"<code>for row in df.rows:\\n   print row['c1'], row['c2']\\n</code>\", '<code>for date, row in df.T.iteritems():\\n</code>', '<code>for row in df.iterrows():\\n</code>', '<code>row</code>', \"<code>In [18]: for index, row in df.iterrows():\\n   ....:     print row['c1'], row['c2']\\n   ....:     \\n10 100\\n11 110\\n12 120\\n</code>\", '<code>431341610650</code>', '<code>4.31E+11</code>', '<code>itertuples</code>', '<code>iterrows()</code>', '<code>itertuples()</code>', \"<code>df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})\\n\\n%timeit [row.a * 2 for idx, row in df.iterrows()]\\n# =&gt; 10 loops, best of 3: 50.3 ms per loop\\n\\n%timeit [row[1] * 2 for row in df.itertuples()]\\n# =&gt; 1000 loops, best of 3: 541 \u00b5s per loop\\n</code>\", '<code>for a,b,c in izip(df[\"a\"],df[\"b\"],df[\"c\"]:</code>', '<code>iterrows()</code>', '<code>itertuples()</code>', '<code>df</code>', '<code>row[1]</code>', '<code>for index, row in df.iterrows():\\n    print row[\"c1\"], row[\"c2\"]\\n</code>', '<code>for row in df.itertuples(index=True, name=\\'Pandas\\'):\\n    print getattr(row, \"c1\"), getattr(row, \"c2\")\\n</code>', '<code>itertuples()</code>', '<code>iterrows()</code>', '<code>dtype</code>', '<code>new_df = df.apply(lambda x: x * 2)\\n</code>', '<code>df.apply()</code>', \"<code>def valuation_formula(x, y):\\n    return x * y * 0.5\\n\\ndf['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)\\n</code>\", \"<code>for i in range(0, len(df)):\\n    print df.iloc[i]['c1'], df.iloc[i]['c2']\\n</code>\", '<code>0</code>', '<code>range</code>', '<code>itertuples</code>', '<code>iterrows</code>', '<code>for row in df.itertuples():\\n    print \"c1 :\",row.c1,\"c2 :\",row.c2\\n</code>', '<code>dataframe</code>', \"<code>for x in range(len(date_example.index)):\\n    print date_example['Date'].iloc[x]\\n</code>\", '<code>namedtuple</code>', \"<code>from collections import namedtuple\\n\\ndef myiter(d, cols=None):\\n    if cols is None:\\n        v = d.values.tolist()\\n        cols = d.columns.values.tolist()\\n    else:\\n        j = [d.columns.get_loc(c) for c in cols]\\n        v = d.values[:, j].tolist()\\n\\n    n = namedtuple('MyTuple', cols)\\n\\n    for line in iter(v):\\n        yield n(*line)\\n</code>\", '<code>pd.DataFrame.itertuples</code>', '<code>list(myiter(df))\\n\\n[MyTuple(c1=10, c2=100), MyTuple(c1=11, c2=110), MyTuple(c1=12, c2=120)]\\n</code>', '<code>pd.DataFrame.itertuples</code>', '<code>list(df.itertuples(index=False))\\n\\n[Pandas(c1=10, c2=100), Pandas(c1=11, c2=110), Pandas(c1=12, c2=120)]\\n</code>', \"<code>def iterfullA(d):\\n    return list(myiter(d))\\n\\ndef iterfullB(d):\\n    return list(d.itertuples(index=False))\\n\\ndef itersubA(d):\\n    return list(myiter(d, ['col3', 'col4', 'col5', 'col6', 'col7']))\\n\\ndef itersubB(d):\\n    return list(d[['col3', 'col4', 'col5', 'col6', 'col7']].itertuples(index=False))\\n\\nres = pd.DataFrame(\\n    index=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\\n    columns='iterfullA iterfullB itersubA itersubB'.split(),\\n    dtype=float\\n)\\n\\nfor i in res.index:\\n    d = pd.DataFrame(np.random.randint(10, size=(i, 10))).add_prefix('col')\\n    for j in res.columns:\\n        stmt = '{}(d)'.format(j)\\n        setp = 'from __main__ import d, {}'.format(j)\\n        res.at[i, j] = timeit(stmt, setp, number=100)\\n\\nres.groupby(res.columns.str[4:-1], axis=1).plot(loglog=True);\\n</code>\", '<code>intertuples</code>', '<code>interrows</code>', '<code>for i, row in df.iterrows():\\n    for j, column in row.iteritems():\\n        print(column)\\n</code>', \"<code> for ind in df.index:\\n     print df['c1'][ind], df['c2'][ind]\\n</code>\", '<code>numpy</code>', \"<code>subset = row['c1'][0:5]\\nall = row['c1'][:]\\n</code>\", '<code>np.asarray(all)\\nimgs[:] = cv2.resize(imgs[:], (224,224) ) #resize every image in an hdf5 file\\n</code>']",
         "title": "How to iterate over rows in a DataFrame in Pandas?",
         "_childDocuments_": [
            {
               "up_vote_count": 812,
               "answer_id": 16476974,
               "last_activity_date": 1484080631,
               "path": "3.stack.answer",
               "body_markdown": "[iterrows](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html#pandas.DataFrame.iterrows) is a generator which yield both index and row\r\n\r\n    In [18]: for index, row in df.iterrows():\r\n       ....:     print row[&#39;c1&#39;], row[&#39;c2&#39;]\r\n       ....:     \r\n    10 100\r\n    11 110\r\n    12 120",
               "tags": [],
               "creation_date": 1368169678,
               "last_edit_date": 1484080631,
               "is_accepted": true,
               "id": "16476974",
               "down_vote_count": 0,
               "score": 812
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 55,
               "answer_id": 30566899,
               "is_accepted": false,
               "last_activity_date": 1433139884,
               "body_markdown": "You can also use `df.apply()` to iterate over rows and access multiple columns for a function.\r\n\r\n[docs: DataFrame.apply()][1]\r\n\r\n    def valuation_formula(x, y):\r\n        return x * y * 0.5\r\n\r\n    df[&#39;price&#39;] = df.apply(lambda row: valuation_formula(row[&#39;x&#39;], row[&#39;y&#39;]), axis=1)\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html",
               "id": "30566899",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1433139884,
               "score": 54
            },
            {
               "up_vote_count": 101,
               "answer_id": 32680162,
               "last_activity_date": 1464771601,
               "path": "3.stack.answer",
               "body_markdown": "While `iterrows()` is a good option, sometimes `itertuples()` can be much faster:\r\n\r\n    df = pd.DataFrame({&#39;a&#39;: randn(1000), &#39;b&#39;: randn(1000),&#39;N&#39;: randint(100, 1000, (1000)), &#39;x&#39;: &#39;x&#39;})\r\n    \r\n    %timeit [row.a * 2 for idx, row in df.iterrows()]\r\n    # =&gt; 10 loops, best of 3: 50.3 ms per loop\r\n    \r\n    %timeit [row[1] * 2 for row in df.itertuples()]\r\n    # =&gt; 1000 loops, best of 3: 541 &#181;s per loop",
               "tags": [],
               "creation_date": 1442757168,
               "last_edit_date": 1464771601,
               "is_accepted": false,
               "id": "32680162",
               "down_vote_count": 1,
               "score": 100
            },
            {
               "up_vote_count": 31,
               "answer_id": 39370553,
               "last_activity_date": 1478509750,
               "path": "3.stack.answer",
               "body_markdown": "You can use the df.iloc function as follows:\r\n\r\n    for i in range(0, len(df)):\r\n        print df.iloc[i][&#39;c1&#39;], df.iloc[i][&#39;c2&#39;]",
               "tags": [],
               "creation_date": 1473252964,
               "last_edit_date": 1478509750,
               "is_accepted": false,
               "id": "39370553",
               "down_vote_count": 0,
               "score": 31
            },
            {
               "up_vote_count": 87,
               "answer_id": 41022840,
               "last_activity_date": 1513482867,
               "path": "3.stack.answer",
               "body_markdown": "To iterate through DataFrame&#39;s row in pandas one can use:\r\n\r\n* [DataFrame.iterrows()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html)\r\n\r\n        for index, row in df.iterrows():\r\n            print row[&quot;c1&quot;], row[&quot;c2&quot;]\r\n\r\n* [DataFrame.itertuples()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html)\r\n\r\n        for row in df.itertuples(index=True, name=&#39;Pandas&#39;):\r\n            print getattr(row, &quot;c1&quot;), getattr(row, &quot;c2&quot;)\r\n\r\n`itertuples()` is supposed to be faster than `iterrows()`\r\n\r\nBut be aware, according to the docs (pandas 0.21.1 at the moment):\r\n\r\n- iterrows: `dtype` might not match from row to row\r\n &gt; Because iterrows returns a Series for each row, it **does not preserve** dtypes across the rows (dtypes are preserved across columns for DataFrames).\r\n\r\n* iterrows: Do not modify rows\r\n &gt;  You should **never modify** something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.\r\n\r\n Use [DataFrame.apply()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) instead:\r\n\r\n        new_df = df.apply(lambda x: x * 2)\r\n\r\n* itertuples: \r\n &gt; The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (&gt;255), regular tuples are returned.\r\n",
               "tags": [],
               "creation_date": 1481128888,
               "last_edit_date": 1513482867,
               "is_accepted": false,
               "id": "41022840",
               "down_vote_count": 0,
               "score": 87
            },
            {
               "up_vote_count": 9,
               "answer_id": 42741552,
               "last_activity_date": 1491338813,
               "path": "3.stack.answer",
               "body_markdown": "To loop all rows in a `dataframe` you can use:\r\n\r\n    for x in range(len(date_example.index)):\r\n        print date_example[&#39;Date&#39;].iloc[x]\r\n\r\n",
               "tags": [],
               "creation_date": 1489272279,
               "last_edit_date": 1491338813,
               "is_accepted": false,
               "id": "42741552",
               "down_vote_count": 0,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 45356377,
               "is_accepted": false,
               "last_activity_date": 1501173152,
               "body_markdown": "Use *itertuples()*. It is faster than *iterrows()*:\r\n\r\n    for row in df.itertuples():\r\n        print &quot;c1 :&quot;,row.c1,&quot;c2 :&quot;,row.c2\r\n\r\n",
               "id": "45356377",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1501173152,
               "score": 12
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 47073107,
               "is_accepted": false,
               "last_activity_date": 1509618820,
               "body_markdown": "IMHO, the simplest decision\r\n\r\n     for ind in df.index:\r\n         print df[&#39;c1&#39;][ind], df[&#39;c2&#39;][ind]",
               "id": "47073107",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1509618820,
               "score": 4
            },
            {
               "up_vote_count": 9,
               "answer_id": 47149876,
               "last_activity_date": 1510028997,
               "path": "3.stack.answer",
               "body_markdown": "You can write your own iterator that implements `namedtuple`\r\n\r\n    from collections import namedtuple\r\n    \r\n    def myiter(d, cols=None):\r\n        if cols is None:\r\n            v = d.values.tolist()\r\n            cols = d.columns.values.tolist()\r\n        else:\r\n            j = [d.columns.get_loc(c) for c in cols]\r\n            v = d.values[:, j].tolist()\r\n    \r\n        n = namedtuple(&#39;MyTuple&#39;, cols)\r\n    \r\n        for line in iter(v):\r\n            yield n(*line)\r\n\r\nThis is directly comparable to `pd.DataFrame.itertuples`.  I&#39;m aiming at performing the same task with more efficiency.\r\n\r\n___\r\n\r\nFor the given dataframe with my function:\r\n\r\n    list(myiter(df))\r\n\r\n    [MyTuple(c1=10, c2=100), MyTuple(c1=11, c2=110), MyTuple(c1=12, c2=120)]\r\n\r\nOr with `pd.DataFrame.itertuples`:\r\n\r\n    list(df.itertuples(index=False))\r\n\r\n    [Pandas(c1=10, c2=100), Pandas(c1=11, c2=110), Pandas(c1=12, c2=120)]\r\n___\r\n\r\n**A comprehensive test**  \r\nWe test making all columns available and subsetting the columns.  \r\n\r\n    def iterfullA(d):\r\n        return list(myiter(d))\r\n    \r\n    def iterfullB(d):\r\n        return list(d.itertuples(index=False))\r\n    \r\n    def itersubA(d):\r\n        return list(myiter(d, [&#39;col3&#39;, &#39;col4&#39;, &#39;col5&#39;, &#39;col6&#39;, &#39;col7&#39;]))\r\n    \r\n    def itersubB(d):\r\n        return list(d[[&#39;col3&#39;, &#39;col4&#39;, &#39;col5&#39;, &#39;col6&#39;, &#39;col7&#39;]].itertuples(index=False))\r\n    \r\n    res = pd.DataFrame(\r\n        index=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\r\n        columns=&#39;iterfullA iterfullB itersubA itersubB&#39;.split(),\r\n        dtype=float\r\n    )\r\n    \r\n    for i in res.index:\r\n        d = pd.DataFrame(np.random.randint(10, size=(i, 10))).add_prefix(&#39;col&#39;)\r\n        for j in res.columns:\r\n            stmt = &#39;{}(d)&#39;.format(j)\r\n            setp = &#39;from __main__ import d, {}&#39;.format(j)\r\n            res.at[i, j] = timeit(stmt, setp, number=100)\r\n\r\n    res.groupby(res.columns.str[4:-1], axis=1).plot(loglog=True);\r\n\r\n[![enter image description here][1]][1]\r\n\r\n[![enter image description here][2]][2]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/rt88e.png\r\n  [2]: https://i.stack.imgur.com/azbOF.png",
               "tags": [],
               "creation_date": 1510028119,
               "last_edit_date": 1510028997,
               "is_accepted": false,
               "id": "47149876",
               "down_vote_count": 0,
               "score": 9
            },
            {
               "up_vote_count": 1,
               "answer_id": 47598852,
               "last_activity_date": 1512152558,
               "path": "3.stack.answer",
               "body_markdown": "You can also do `numpy` indexing for even greater speed ups. It&#39;s not really iterating but works much better than iteration for certain applications.\r\n\r\n    subset = row[&#39;c1&#39;][0:5]\r\n    all = row[&#39;c1&#39;][:]\r\n\r\nYou may also want to cast it to an array. These indexes/selections are supposed to act like Numpy arrays already but I ran into issues and needed to cast\r\n\r\n    np.asarray(all)\r\n    imgs[:] = cv2.resize(imgs[:], (224,224) ) #resize every image in an hdf5 file\r\n",
               "tags": [],
               "creation_date": 1512150590,
               "last_edit_date": 1512152558,
               "is_accepted": false,
               "id": "47598852",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 48297889,
               "is_accepted": false,
               "last_activity_date": 1516182089,
               "body_markdown": "I was looking for **How to iterate on rows AND columns and ended here so :**\r\n\r\n    for i, row in df.iterrows():\r\n        for j, column in row.iteritems():\r\n            print(column)",
               "id": "48297889",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1516182089,
               "score": 5
            }
         ],
         "link": "https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas",
         "id": "858127-2255"
      },
      {
         "up_vote_count": "353",
         "path": "2.stack",
         "body_markdown": "I have a `DataFrame`:\r\n\r\n    &gt;&gt;&gt; df\r\n                     STK_ID  EPS  cash\r\n    STK_ID RPT_Date                   \r\n    601166 20111231  601166  NaN   NaN\r\n    600036 20111231  600036  NaN    12\r\n    600016 20111231  600016  4.3   NaN\r\n    601009 20111231  601009  NaN   NaN\r\n    601939 20111231  601939  2.5   NaN\r\n    000001 20111231  000001  NaN   NaN\r\n\r\n\r\nThen I just want the records whose `EPS` is not `NaN`, that is, `df.drop(....)` will return the dataframe as below:\r\n\r\n                      STK_ID  EPS  cash\r\n    STK_ID RPT_Date                   \r\n    600016 20111231  600016  4.3   NaN\r\n    601939 20111231  601939  2.5   NaN\r\n\r\nHow do I do that?",
         "view_count": "388225",
         "answer_count": "9",
         "tags": "['python', 'pandas', 'dataframe']",
         "creation_date": "1353057442",
         "last_edit_date": "1483635670",
         "code_snippet": "['<code>DataFrame</code>', '<code>&gt;&gt;&gt; df\\n                 STK_ID  EPS  cash\\nSTK_ID RPT_Date                   \\n601166 20111231  601166  NaN   NaN\\n600036 20111231  600036  NaN    12\\n600016 20111231  600016  4.3   NaN\\n601009 20111231  601009  NaN   NaN\\n601939 20111231  601939  2.5   NaN\\n000001 20111231  000001  NaN   NaN\\n</code>', '<code>EPS</code>', '<code>NaN</code>', '<code>df.drop(....)</code>', '<code>                  STK_ID  EPS  cash\\nSTK_ID RPT_Date                   \\n600016 20111231  600016  4.3   NaN\\n601939 20111231  601939  2.5   NaN\\n</code>', \"<code>df.dropna(subset = ['column1_name', 'column2_name', 'column3_name'])</code>\", '<code>drop</code>', '<code>EPS</code>', \"<code>df = df[np.isfinite(df['EPS'])]\\n</code>\", '<code>pandas.notnull</code>', '<code>np.isfinite</code>', '<code>dropna()</code>', '<code>In [24]: df = pd.DataFrame(np.random.randn(10,3))\\n\\nIn [25]: df.iloc[::2,0] = np.nan; df.iloc[::4,1] = np.nan; df.iloc[::3,2] = np.nan;\\n\\nIn [26]: df\\nOut[26]:\\n          0         1         2\\n0       NaN       NaN       NaN\\n1  2.677677 -1.466923 -0.750366\\n2       NaN  0.798002 -0.906038\\n3  0.672201  0.964789       NaN\\n4       NaN       NaN  0.050742\\n5 -1.250970  0.030561 -2.678622\\n6       NaN  1.036043       NaN\\n7  0.049896 -0.308003  0.823295\\n8       NaN       NaN  0.637482\\n9 -0.310130  0.078891       NaN\\n</code>', '<code>In [27]: df.dropna()     #drop all rows that have any NaN values\\nOut[27]:\\n          0         1         2\\n1  2.677677 -1.466923 -0.750366\\n5 -1.250970  0.030561 -2.678622\\n7  0.049896 -0.308003  0.823295\\n</code>', \"<code>In [28]: df.dropna(how='all')     #drop only if ALL columns are NaN\\nOut[28]:\\n          0         1         2\\n1  2.677677 -1.466923 -0.750366\\n2       NaN  0.798002 -0.906038\\n3  0.672201  0.964789       NaN\\n4       NaN       NaN  0.050742\\n5 -1.250970  0.030561 -2.678622\\n6       NaN  1.036043       NaN\\n7  0.049896 -0.308003  0.823295\\n8       NaN       NaN  0.637482\\n9 -0.310130  0.078891       NaN\\n</code>\", '<code>In [29]: df.dropna(thresh=2)   #Drop row if it does not have at least two values that are **not** NaN\\nOut[29]:\\n          0         1         2\\n1  2.677677 -1.466923 -0.750366\\n2       NaN  0.798002 -0.906038\\n3  0.672201  0.964789       NaN\\n5 -1.250970  0.030561 -2.678622\\n7  0.049896 -0.308003  0.823295\\n9 -0.310130  0.078891       NaN\\n</code>', '<code>In [30]: df.dropna(subset=[1])   #Drop only if NaN in specific column (as asked in the question)\\nOut[30]:\\n          0         1         2\\n1  2.677677 -1.466923 -0.750366\\n2       NaN  0.798002 -0.906038\\n3  0.672201  0.964789       NaN\\n5 -1.250970  0.030561 -2.678622\\n6       NaN  1.036043       NaN\\n7  0.049896 -0.308003  0.823295\\n9 -0.310130  0.078891       NaN\\n</code>', \"<code>df.dropna(subset = ['column_name'])</code>\", \"<code>import pandas as pd\\ndf = df[pd.notnull(df['EPS'])]\\n</code>\", \"<code>df.dropna(subset=['EPS'])</code>\", '<code>notnull</code>', '<code>In [332]: df[df.EPS.notnull()]\\nOut[332]:\\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\\n2  600016  20111231    600016  4.3   NaN\\n4  601939  20111231    601939  2.5   NaN\\n\\n\\nIn [334]: df[~df.EPS.isnull()]\\nOut[334]:\\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\\n2  600016  20111231    600016  4.3   NaN\\n4  601939  20111231    601939  2.5   NaN\\n\\n\\nIn [347]: df[~np.isnan(df.EPS)]\\nOut[347]:\\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\\n2  600016  20111231    600016  4.3   NaN\\n4  601939  20111231    601939  2.5   NaN\\n</code>', \"<code>df.dropna(subset=['EPS'], how='all', inplace = True)\\n</code>\", \"<code>how='all'</code>\", \"<code>'all'</code>\", \"<code>'any'</code>\", \"<code>filtered_df = df[df['EPS'].notnull()]\\n</code>\", '<code>np.nan != np.nan</code>', '<code>In [149]: df.query(\"EPS == EPS\")\\nOut[149]:\\n                 STK_ID  EPS  cash\\nSTK_ID RPT_Date\\n600016 20111231  600016  4.3   NaN\\n601939 20111231  601939  2.5   NaN\\n</code>', '<code>df = df[(df.EPS &gt; 2.0) &amp; (df.EPS &lt;4.0)]\\n</code>', '<code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</code>', '<code>df = df[(df.EPS &gt; 2.0) &amp; (df.EPS &lt;4.0)]</code>', '<code>df = df[df.EPS &gt;= 0]\\n</code>', '<code>df = df[df.EPS &lt;= 0]\\n</code>']",
         "title": "How to drop rows of Pandas DataFrame whose value in certain columns is NaN",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 307,
               "answer_id": 13413845,
               "is_accepted": true,
               "last_activity_date": 1353058478,
               "body_markdown": "Don&#39;t `drop`. Just take rows where `EPS` is **finite**:\r\n\r\n    df = df[np.isfinite(df[&#39;EPS&#39;])]",
               "id": "13413845",
               "tags": [],
               "down_vote_count": 9,
               "creation_date": 1353058478,
               "score": 298
            },
            {
               "up_vote_count": 519,
               "answer_id": 13434501,
               "last_activity_date": 1502669096,
               "path": "3.stack.answer",
               "body_markdown": "This question is already resolved, but... \r\n\r\n...also consider the solution suggested by Wouter in [his original comment][1]. The ability to handle missing data, including `dropna()`, is built into pandas explicitly. Aside from potentially improved performance over doing it manually, these functions also come with a variety of options which may be useful. \r\n\r\n    In [24]: df = pd.DataFrame(np.random.randn(10,3))\r\n    \r\n    In [25]: df.iloc[::2,0] = np.nan; df.iloc[::4,1] = np.nan; df.iloc[::3,2] = np.nan;\r\n    \r\n    In [26]: df\r\n    Out[26]:\r\n              0         1         2\r\n    0       NaN       NaN       NaN\r\n    1  2.677677 -1.466923 -0.750366\r\n    2       NaN  0.798002 -0.906038\r\n    3  0.672201  0.964789       NaN\r\n    4       NaN       NaN  0.050742\r\n    5 -1.250970  0.030561 -2.678622\r\n    6       NaN  1.036043       NaN\r\n    7  0.049896 -0.308003  0.823295\r\n    8       NaN       NaN  0.637482\r\n    9 -0.310130  0.078891       NaN\r\n\r\n---\r\n    \r\n    In [27]: df.dropna()     #drop all rows that have any NaN values\r\n    Out[27]:\r\n              0         1         2\r\n    1  2.677677 -1.466923 -0.750366\r\n    5 -1.250970  0.030561 -2.678622\r\n    7  0.049896 -0.308003  0.823295\r\n\r\n---\r\n    \r\n    In [28]: df.dropna(how=&#39;all&#39;)     #drop only if ALL columns are NaN\r\n    Out[28]:\r\n              0         1         2\r\n    1  2.677677 -1.466923 -0.750366\r\n    2       NaN  0.798002 -0.906038\r\n    3  0.672201  0.964789       NaN\r\n    4       NaN       NaN  0.050742\r\n    5 -1.250970  0.030561 -2.678622\r\n    6       NaN  1.036043       NaN\r\n    7  0.049896 -0.308003  0.823295\r\n    8       NaN       NaN  0.637482\r\n    9 -0.310130  0.078891       NaN\r\n    \r\n---\r\n\r\n    In [29]: df.dropna(thresh=2)   #Drop row if it does not have at least two values that are **not** NaN\r\n    Out[29]:\r\n              0         1         2\r\n    1  2.677677 -1.466923 -0.750366\r\n    2       NaN  0.798002 -0.906038\r\n    3  0.672201  0.964789       NaN\r\n    5 -1.250970  0.030561 -2.678622\r\n    7  0.049896 -0.308003  0.823295\r\n    9 -0.310130  0.078891       NaN\r\n\r\n---\r\n\r\n    In [30]: df.dropna(subset=[1])   #Drop only if NaN in specific column (as asked in the question)\r\n    Out[30]:\r\n              0         1         2\r\n    1  2.677677 -1.466923 -0.750366\r\n    2       NaN  0.798002 -0.906038\r\n    3  0.672201  0.964789       NaN\r\n    5 -1.250970  0.030561 -2.678622\r\n    6       NaN  1.036043       NaN\r\n    7  0.049896 -0.308003  0.823295\r\n    9 -0.310130  0.078891       NaN\r\n\r\nThere are also other options (See docs at http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html), including dropping columns instead of rows. \r\n\r\nPretty handy! \r\n\r\n  [1]: https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-of-certain-column-is-nan/13434501#comment18328797_13413590\r\n",
               "tags": [],
               "creation_date": 1353184053,
               "last_edit_date": 1502669096,
               "is_accepted": false,
               "id": "13434501",
               "down_vote_count": 0,
               "score": 519
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 67,
               "answer_id": 23235618,
               "is_accepted": false,
               "last_activity_date": 1398231465,
               "body_markdown": "I know this has already been answered, but just for the sake of a purely pandas solution to this specific question as opposed to the general description from Aman (which was wonderful) and in case anyone else happens upon this:\r\n\r\n    import pandas as pd\r\n    df = df[pd.notnull(df[&#39;EPS&#39;])]\r\n\r\n",
               "id": "23235618",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1398231465,
               "score": 67
            },
            {
               "up_vote_count": 0,
               "answer_id": 33044502,
               "last_activity_date": 1444415127,
               "path": "3.stack.answer",
               "body_markdown": "For some reason none of the previously submitted answers worked for me. This basic solution did:\r\n\r\n    df = df[df.EPS &gt;= 0]\r\n\r\nThough of course that will drop rows with negative numbers, too. So if you want those it&#39;s probably smart to add this after, too.\r\n\r\n    df = df[df.EPS &lt;= 0]",
               "tags": [],
               "creation_date": 1444413602,
               "last_edit_date": 1444415127,
               "is_accepted": false,
               "id": "33044502",
               "down_vote_count": 1,
               "score": -1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 18,
               "answer_id": 34082664,
               "is_accepted": false,
               "last_activity_date": 1449212516,
               "body_markdown": "You could use dataframe method [notnull](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.notnull.html) or inverse of [isnull](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isnull.html), or [numpy.isnan](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.isnan.html):\r\n\r\n    In [332]: df[df.EPS.notnull()]\r\n    Out[332]:\r\n       STK_ID  RPT_Date  STK_ID.1  EPS  cash\r\n    2  600016  20111231    600016  4.3   NaN\r\n    4  601939  20111231    601939  2.5   NaN\r\n\r\n\r\n    In [334]: df[~df.EPS.isnull()]\r\n    Out[334]:\r\n       STK_ID  RPT_Date  STK_ID.1  EPS  cash\r\n    2  600016  20111231    600016  4.3   NaN\r\n    4  601939  20111231    601939  2.5   NaN\r\n\r\n\r\n    In [347]: df[~np.isnan(df.EPS)]\r\n    Out[347]:\r\n       STK_ID  RPT_Date  STK_ID.1  EPS  cash\r\n    2  600016  20111231    600016  4.3   NaN\r\n    4  601939  20111231    601939  2.5   NaN",
               "id": "34082664",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1449212516,
               "score": 18
            },
            {
               "up_vote_count": 0,
               "answer_id": 36015676,
               "last_activity_date": 1485472328,
               "path": "3.stack.answer",
               "body_markdown": "It  may be added at that &#39;&amp;&#39; can be used to add additional conditions e.g.\r\n\r\n\r\n    df = df[(df.EPS &gt; 2.0) &amp; (df.EPS &lt;4.0)]\r\n\r\nWhen evaluating the statements pandas needs parenthesis.",
               "tags": [],
               "creation_date": 1458056011,
               "last_edit_date": 1485472328,
               "is_accepted": false,
               "id": "36015676",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 43530064,
               "is_accepted": false,
               "last_activity_date": 1492722956,
               "body_markdown": "yet another solution which uses the fact that `np.nan != np.nan`:\r\n\r\n    In [149]: df.query(&quot;EPS == EPS&quot;)\r\n    Out[149]:\r\n                     STK_ID  EPS  cash\r\n    STK_ID RPT_Date\r\n    600016 20111231  600016  4.3   NaN\r\n    601939 20111231  601939  2.5   NaN",
               "id": "43530064",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1492722956,
               "score": 6
            },
            {
               "up_vote_count": 15,
               "answer_id": 45466263,
               "last_activity_date": 1503308975,
               "path": "3.stack.answer",
               "body_markdown": "You can use this:\r\n\r\n    df.dropna(subset=[&#39;EPS&#39;], how=&#39;all&#39;, inplace = True)",
               "tags": [],
               "creation_date": 1501691321,
               "last_edit_date": 1503308975,
               "is_accepted": false,
               "id": "45466263",
               "down_vote_count": 0,
               "score": 15
            },
            {
               "up_vote_count": 8,
               "answer_id": 47455251,
               "last_activity_date": 1512376496,
               "path": "3.stack.answer",
               "body_markdown": "**Simplest of all solutions:**\r\n\r\n    filtered_df = df[df[&#39;EPS&#39;].notnull()]\r\n\r\n&gt; The above solution is way better than using np.isfinite()",
               "tags": [],
               "creation_date": 1511438891,
               "last_edit_date": 1512376496,
               "is_accepted": false,
               "id": "47455251",
               "down_vote_count": 0,
               "score": 8
            }
         ],
         "link": "https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-in-certain-columns-is-nan",
         "id": "858127-2256"
      },
      {
         "up_vote_count": "332",
         "path": "2.stack",
         "body_markdown": "I&#39;m trying to get the number of rows of dataframe df with Pandas, and here is my code.\r\n\r\n###Method 1:\r\n\r\n    total_rows = df.count\r\n    print total_rows +1\r\n\r\n###Method 2:\r\n\r\n    total_rows = df[&#39;First_columnn_label&#39;].count\r\n    print total_rows +1\r\n\r\n\r\nBoth the code snippets give me this error:\r\n\r\n&gt; TypeError: unsupported operand type(s) for +: &#39;instancemethod&#39; and &#39;int&#39;\r\n\r\nWhat am I doing wrong?\r\n\r\nAccording to [the answer][1] given by [@root][2] the best (the fastest) way to check df length is to call:\r\n\r\n    len(df.index)\r\n\r\n  [1]: https://stackoverflow.com/a/15943975/4230591\r\n  [2]: https://stackoverflow.com/users/1199589/root\r\n\r\n",
         "view_count": "551535",
         "answer_count": "13",
         "tags": "['python', 'pandas', 'dataframe']",
         "creation_date": "1365668048",
         "last_edit_date": "1495540051",
         "code_snippet": "['<code>total_rows = df.count\\nprint total_rows +1\\n</code>', \"<code>total_rows = df['First_columnn_label'].count\\nprint total_rows +1\\n</code>\", '<code>len(df.index)\\n</code>', '<code>df.count()</code>', '<code>df.shape[0]</code>', '<code>.shape</code>', '<code>len(DataFrame.index)</code>', '<code>.shape</code>', '<code>In [1]: import numpy as np\\n\\nIn [2]: import pandas as pd\\n\\nIn [3]: df = pd.DataFrame(np.arange(9).reshape(3,3))\\n\\nIn [4]: df\\nOut[4]: \\n   0  1  2\\n0  0  1  2\\n1  3  4  5\\n2  6  7  8\\n\\nIn [5]: df.shape\\nOut[5]: (3, 3)\\n\\nIn [6]: timeit df.shape\\n1000000 loops, best of 3: 1.17 us per loop\\n\\nIn [7]: timeit df[0].count()\\n10000 loops, best of 3: 56 us per loop\\n\\nIn [8]: len(df.index)\\nOut[8]: 3\\n\\nIn [9]: timeit len(df.index)\\n1000000 loops, best of 3: 381 ns per loop\\n</code>', '<code>len(df.index)</code>', '<code>df[0].count()</code>', '<code>count</code>', '<code>NaN</code>', '<code>len(df.index)</code>', '<code>df[0].count()</code>', '<code>count</code>', '<code>shape</code>', '<code>df.empty</code>', '<code>len(df)</code>', '<code>__len__()</code>', '<code>Returns length of index</code>', '<code>In [7]: timeit len(df.index)\\n1000000 loops, best of 3: 248 ns per loop\\n\\nIn [8]: timeit len(df)\\n1000000 loops, best of 3: 573 ns per loop\\n</code>', '<code>len(df.index)</code>', '<code>Count_Row=df.shape[0] #gives number of row count\\nCount_Col=df.shape[1] #gives number of col count\\n</code>', '<code>df.axes</code>', '<code>len()</code>', '<code>total_rows=len(df.axes[0])\\ntotal_cols=len(df.axes[1])\\n</code>', '<code>df.index\\n</code>', '<code>df.columns\\n</code>', '<code>len(func)</code>', '<code>len(df.index)</code>', '<code>shape[0] and shape[1]</code>', '<code>R</code>', '<code>len(df.columns)  \\n## Here:\\n#df is your data.frame\\n#df.columns return a string, it contains column\\'s titles of the df. \\n#Then, \"len()\" gets the length of it.\\n</code>', \"<code>len(df.index) #It's similar.\\n</code>\", '<code>total_rows = len(df)\\n</code>', '<code>df.shape</code>', '<code>df.shape[0]</code>', '<code>df.shape[1]</code>', '<code>df.shape[0]\\nlen(df)\\n</code>', '<code>len(df)</code>', '<code>len(df.index)</code>', '<code>df.shape[0]</code>', '<code>@property</code>', '<code>len</code>', '<code>df.shape??\\nType:        property\\nString form: &lt;property object at 0x1127b33c0&gt;\\nSource:     \\n# df.shape.fget\\n@property\\ndef shape(self):\\n    \"\"\"\\n    Return a tuple representing the dimensionality of the DataFrame.\\n    \"\"\"\\n    return len(self.index), len(self.columns)\\n</code>', '<code>df.__len__??\\nSignature: df.__len__()\\nSource:   \\n    def __len__(self):\\n        \"\"\"Returns length of info axis, but here we use the index \"\"\"\\n        return len(self.index)\\nFile:      ~/miniconda2/lib/python2.7/site-packages/pandas/core/frame.py\\nType:      instancemethod\\n</code>', '<code>len(df.index)</code>', '<code>len(df)</code>', '<code>df.shape[0]</code>', '<code>def nrow(df):\\n    print(\"{:,}\".format(df.shape[0]))\\n</code>', '<code>nrow(my_df)\\n12,456,789\\n</code>', '<code>df.pipe(len)\\n</code>', '<code>row_count = (\\n      pd.DataFrame(np.random.rand(3,4))\\n      .reset_index()\\n      .pipe(len)\\n)\\n</code>', '<code>len(df)\\n</code>']",
         "title": "How do I get the row count of a Pandas dataframe?",
         "_childDocuments_": [
            {
               "up_vote_count": 435,
               "answer_id": 15943975,
               "last_activity_date": 1493281909,
               "path": "3.stack.answer",
               "body_markdown": "You can use the `.shape` property or just `len(DataFrame.index)`. However, there are notable performance differences ( the `.shape` property is faster):\r\n\r\n    In [1]: import numpy as np\r\n    \r\n    In [2]: import pandas as pd\r\n    \r\n    In [3]: df = pd.DataFrame(np.arange(9).reshape(3,3))\r\n    \r\n    In [4]: df\r\n    Out[4]: \r\n       0  1  2\r\n    0  0  1  2\r\n    1  3  4  5\r\n    2  6  7  8\r\n    \r\n    In [5]: df.shape\r\n    Out[5]: (3, 3)\r\n\r\n    In [6]: timeit df.shape\r\n    1000000 loops, best of 3: 1.17 us per loop\r\n    \r\n    In [7]: timeit df[0].count()\r\n    10000 loops, best of 3: 56 us per loop\r\n\r\n    In [8]: len(df.index)\r\n    Out[8]: 3\r\n    \r\n    In [9]: timeit len(df.index)\r\n    1000000 loops, best of 3: 381 ns per loop\r\n\r\n---\r\n\r\nEDIT: As @Dan Allen noted in the comments `len(df.index)` and `df[0].count()` are not interchangeable as `count` excludes `NaN`s,",
               "tags": [],
               "creation_date": 1365668669,
               "last_edit_date": 1493281909,
               "is_accepted": true,
               "id": "15943975",
               "down_vote_count": 0,
               "score": 435
            },
            {
               "up_vote_count": 80,
               "answer_id": 18317067,
               "last_activity_date": 1376925016,
               "path": "3.stack.answer",
               "body_markdown": "Use `len(df)`. This works as of pandas 0.11 or maybe even earlier.\r\n\r\n`__len__()` is currently (0.12) documented with `Returns length of index`. Timing info, set up the same way as in root&#39;s answer:\r\n\r\n    In [7]: timeit len(df.index)\r\n    1000000 loops, best of 3: 248 ns per loop\r\n    \r\n    In [8]: timeit len(df)\r\n    1000000 loops, best of 3: 573 ns per loop\r\n\r\nDue to one additional function call it is a bit slower than calling `len(df.index)` directly, but this should not play any role in most use cases.\r\n\r\n",
               "tags": [],
               "creation_date": 1376924565,
               "last_edit_date": 1376925016,
               "is_accepted": false,
               "id": "18317067",
               "down_vote_count": 1,
               "score": 79
            },
            {
               "up_vote_count": 13,
               "answer_id": 32103678,
               "last_activity_date": 1440012501,
               "path": "3.stack.answer",
               "body_markdown": "Apart from above answers use can use `df.axes` to get the tuple with row and column indexes and then use `len()` function:\r\n\r\n    total_rows=len(df.axes[0])\r\n    total_cols=len(df.axes[1])",
               "tags": [],
               "creation_date": 1440011237,
               "last_edit_date": 1440012501,
               "is_accepted": false,
               "id": "32103678",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 74,
               "answer_id": 35523946,
               "last_activity_date": 1455975341,
               "path": "3.stack.answer",
               "body_markdown": "suppose df is your dataframe then:\r\n\r\n    Count_Row=df.shape[0] #gives number of row count\r\n    Count_Col=df.shape[1] #gives number of col count\r\n   ",
               "tags": [],
               "creation_date": 1455975005,
               "last_edit_date": 1455975341,
               "is_accepted": false,
               "id": "35523946",
               "down_vote_count": 1,
               "score": 73
            },
            {
               "up_vote_count": 10,
               "answer_id": 38025280,
               "last_activity_date": 1481741742,
               "path": "3.stack.answer",
               "body_markdown": "For getting rows use\r\n\r\n    df.index\r\n\r\nand for columns use\r\n\r\n    df.columns\r\n\r\nYou can always use `len(func)` for getting the count of list, hence you can use\r\n`len(df.index)` for getting the number of rows.\r\n\r\nBut keep in mind, as stated by @root, using `shape[0] and shape[1]` for getting the number of rows and columns, respectively, is a faster option.\r\n",
               "tags": [],
               "creation_date": 1466832218,
               "last_edit_date": 1481741742,
               "is_accepted": false,
               "id": "38025280",
               "down_vote_count": 0,
               "score": 10
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 39764266,
               "is_accepted": false,
               "last_activity_date": 1475134901,
               "body_markdown": "I come to pandas from `R` background, and I see that pandas is more complicated when it comes to selecting row or column.\r\nI had to wrestle with it for a while, then I found some ways to deal with:\r\n\r\ngetting the number of columns:\r\n\r\n    len(df.columns)  \r\n    ## Here:\r\n    #df is your data.frame\r\n    #df.columns return a string, it contains column&#39;s titles of the df. \r\n    #Then, &quot;len()&quot; gets the length of it.\r\n\r\n\r\ngetting the number of rows:\r\n\r\n    len(df.index) #It&#39;s similar.\r\n    ",
               "id": "39764266",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1475134901,
               "score": 5
            },
            {
               "up_vote_count": 2,
               "answer_id": 40449682,
               "last_activity_date": 1478438721,
               "path": "3.stack.answer",
               "body_markdown": "`df.shape` returns the shape of the data frame in the form of a tuple (no. of rows, no. of cols).\r\n\r\nYou can simply access no. of rows or no. of cols with `df.shape[0]` or `df.shape[1]`, respectively, which is same as accessing the values of the tuple.\r\n",
               "tags": [],
               "creation_date": 1478437643,
               "last_edit_date": 1478438721,
               "is_accepted": false,
               "id": "40449682",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 46334240,
               "is_accepted": false,
               "last_activity_date": 1505959154,
               "body_markdown": "For dataframe df, a printed comma formatted row count used while exploring data:    \r\n\r\n    def nrow(df):\r\n        print(&quot;{:,}&quot;.format(df.shape[0]))\r\n\r\nExample:\r\n\r\n    nrow(my_df)\r\n    12,456,789",
               "id": "46334240",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1505959154,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 46382893,
               "is_accepted": false,
               "last_activity_date": 1506190865,
               "body_markdown": "you can try:\r\n \r\n    total_rows = len(df)",
               "id": "46382893",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1506190865,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47388179,
               "is_accepted": false,
               "last_activity_date": 1511168189,
               "body_markdown": "Row count (use any of):\r\n\r\n    df.shape[0]\r\n    len(df)\r\n\r\n",
               "id": "47388179",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1511168189,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47705589,
               "is_accepted": false,
               "last_activity_date": 1512689831,
               "body_markdown": "...building on Jan-Philip Gehrcke&#39;s answer. \r\n\r\nThe reason why `len(df)` or `len(df.index)` is faster than `df.shape[0]`. Look at the code. df.shape is a `@property` that runs a DataFrame method calling `len` twice.\r\n\r\n    df.shape??\r\n    Type:        property\r\n    String form: &lt;property object at 0x1127b33c0&gt;\r\n    Source:     \r\n    # df.shape.fget\r\n    @property\r\n    def shape(self):\r\n        &quot;&quot;&quot;\r\n        Return a tuple representing the dimensionality of the DataFrame.\r\n        &quot;&quot;&quot;\r\n        return len(self.index), len(self.columns)\r\n\r\nAnd beneath the hood of len(df)\r\n\r\n    df.__len__??\r\n    Signature: df.__len__()\r\n    Source:   \r\n        def __len__(self):\r\n            &quot;&quot;&quot;Returns length of info axis, but here we use the index &quot;&quot;&quot;\r\n            return len(self.index)\r\n    File:      ~/miniconda2/lib/python2.7/site-packages/pandas/core/frame.py\r\n    Type:      instancemethod\r\n\r\n`len(df.index)` will be slightly faster than `len(df)` since it has one less function call, but this is always faster than `df.shape[0]`",
               "id": "47705589",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512689831,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48918940,
               "is_accepted": false,
               "last_activity_date": 1519268304,
               "body_markdown": "In case you want to get the row count in the middle of a chained operation, you can use:\r\n\r\n    df.pipe(len)\r\n\r\nExample:\r\n\r\n    row_count = (\r\n          pd.DataFrame(np.random.rand(3,4))\r\n          .reset_index()\r\n          .pipe(len)\r\n    )\r\n\r\nThis can be useful if you don&#39;t want to put a long statement inside a len() function.\r\n\r\nYou could use \\__len__() instead but \\__len__() looks a bit weird. ",
               "id": "48918940",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519268304,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48959857,
               "is_accepted": false,
               "last_activity_date": 1519451814,
               "body_markdown": "try this\r\n\r\n    len(df)\r\nthis will return the row count of the dataframe.",
               "id": "48959857",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519451814,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe",
         "id": "858127-2257"
      },
      {
         "up_vote_count": "216",
         "path": "2.stack",
         "body_markdown": "Assume I have this:\r\n\r\n    [\r\n    {&quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 10},\r\n    {&quot;name&quot;: &quot;Mark&quot;, &quot;age&quot;: 5},\r\n    {&quot;name&quot;: &quot;Pam&quot;, &quot;age&quot;: 7}\r\n    ]\r\n\r\n\r\nand by searching &quot;Pam&quot; as name, I want to retrieve the related dictionary: {name: &quot;Pam&quot;, age: 7}\r\n\r\nHow to achieve this ?",
         "view_count": "220675",
         "answer_count": "15",
         "tags": "['python', 'search', 'dictionary']",
         "creation_date": "1325060754",
         "last_edit_date": "1357652582",
         "code_snippet": "['<code>[\\n{\"name\": \"Tom\", \"age\": 10},\\n{\"name\": \"Mark\", \"age\": 5},\\n{\"name\": \"Pam\", \"age\": 7}\\n]\\n</code>', '<code>&gt;&gt;&gt; dicts = [\\n...     { \"name\": \"Tom\", \"age\": 10 },\\n...     { \"name\": \"Mark\", \"age\": 5 },\\n...     { \"name\": \"Pam\", \"age\": 7 },\\n...     { \"name\": \"Dick\", \"age\": 12 }\\n... ]\\n\\n&gt;&gt;&gt; (item for item in dicts if item[\"name\"] == \"Pam\").next()\\n{\\'age\\': 7, \\'name\\': \\'Pam\\'}\\n</code>', '<code>[item for item in dicts if item[\"name\"] == \"Pam\"][0]</code>', '<code>upper()</code>', '<code>next((item for item in dicts if item[\"name\"] == \"Pam\"))</code>', '<code>people = [\\n{\\'name\\': \"Tom\", \\'age\\': 10},\\n{\\'name\\': \"Mark\", \\'age\\': 5},\\n{\\'name\\': \"Pam\", \\'age\\': 7}\\n]\\n\\nfilter(lambda person: person[\\'name\\'] == \\'Pam\\', people)\\n</code>', \"<code>[{'age': 7, 'name': 'Pam'}]\\n</code>\", '<code>len()</code>', '<code>list()</code>', '<code>r</code>', '<code>list</code>', \"<code>def search(name, people):\\n    return [element for element in people if element['name'] == name]\\n</code>\", '<code>.next()</code>', '<code>&gt;&gt;&gt; dicts = [\\n     { \"name\": \"Tom\", \"age\": 10 },\\n     { \"name\": \"Mark\", \"age\": 5 },\\n     { \"name\": \"Pam\", \"age\": 7 },\\n     { \"name\": \"Dick\", \"age\": 12 }\\n ]\\n&gt;&gt;&gt; next(item for item in dicts if item[\"name\"] == \"Pam\")\\n{\\'age\\': 7, \\'name\\': \\'Pam\\'}\\n</code>', '<code>&gt;&gt;&gt; next((item for item in dicts if item[\"name\"] == \"Pam\"), False)\\n{\\'name\\': \\'Pam\\', \\'age\\': 7}\\n&gt;&gt;&gt; next((item for item in dicts if item[\"name\"] == \"Sam\"), False)\\nFalse\\n&gt;&gt;&gt;\\n</code>', '<code>people = [\\n{\\'name\\': \"Tom\", \\'age\\': 10},\\n{\\'name\\': \"Mark\", \\'age\\': 5},\\n{\\'name\\': \"Pam\", \\'age\\': 7}\\n]\\n\\ndef search(name):\\n    for p in people:\\n        if p[\\'name\\'] == name:\\n            return p\\n\\nsearch(\"Pam\")\\n</code>', '<code>def search(list, key, value):     for item in list:         if item[key] == value:             return item</code>', '<code>def search_dictionaries(key, value, list_of_dictionaries):\\n    return [element for element in list_of_dictionaries if element[key] == value]\\n</code>', '<code>next((item for item in dicts if item.get(\"name\") and item[\"name\"] == \"Pam\"), None)\\n</code>', \"<code>names = [{'name':'Tom', 'age': 10}, {'name': 'Mark', 'age': 5}, {'name': 'Pam', 'age': 7}]\\nresultlist = [d    for d in names     if d.get('name', '') == 'Pam']\\nfirst_result = resultlist[0]\\n</code>\", \"<code>def get_records(key, store=dict()):\\n    '''Return a list of all records containing name==key from our store\\n    '''\\n    assert key is not None\\n    return [d for d in store if d['name']==key]\\n</code>\", '<code>import pandas as pd\\n\\nlistOfDicts = [\\n{\"name\": \"Tom\", \"age\": 10},\\n{\"name\": \"Mark\", \"age\": 5},\\n{\"name\": \"Pam\", \"age\": 7}\\n]\\n\\n# Create a data frame, keys are used as column headers.\\n# Dict items with the same key are entered into the same respective column.\\ndf = pd.DataFrame(listOfDicts)\\n\\n# The pandas dataframe allows you to pick out specific values like so:\\n\\ndf2 = df[ (df[\\'name\\'] == \\'Pam\\') &amp; (df[\\'age\\'] == 7) ]\\n\\n# Alternate syntax, same thing\\n\\ndf2 = df[ (df.name == \\'Pam\\') &amp; (df.age == 7) ]\\n</code>', '<code>setup_large = \\'dicts = [];\\\\\\n[dicts.extend(({ \"name\": \"Tom\", \"age\": 10 },{ \"name\": \"Mark\", \"age\": 5 },\\\\\\n{ \"name\": \"Pam\", \"age\": 7 },{ \"name\": \"Dick\", \"age\": 12 })) for _ in range(25000)];\\\\\\nfrom operator import itemgetter;import pandas as pd;\\\\\\ndf = pd.DataFrame(dicts);\\'\\n\\nsetup_small = \\'dicts = [];\\\\\\ndicts.extend(({ \"name\": \"Tom\", \"age\": 10 },{ \"name\": \"Mark\", \"age\": 5 },\\\\\\n{ \"name\": \"Pam\", \"age\": 7 },{ \"name\": \"Dick\", \"age\": 12 }));\\\\\\nfrom operator import itemgetter;import pandas as pd;\\\\\\ndf = pd.DataFrame(dicts);\\'\\n\\nmethod1 = \\'[item for item in dicts if item[\"name\"] == \"Pam\"]\\'\\nmethod2 = \\'df[df[\"name\"] == \"Pam\"]\\'\\n\\nimport timeit\\nt = timeit.Timer(method1, setup_small)\\nprint(\\'Small Method LC: \\' + str(t.timeit(100)))\\nt = timeit.Timer(method2, setup_small)\\nprint(\\'Small Method Pandas: \\' + str(t.timeit(100)))\\n\\nt = timeit.Timer(method1, setup_large)\\nprint(\\'Large Method LC: \\' + str(t.timeit(100)))\\nt = timeit.Timer(method2, setup_large)\\nprint(\\'Large Method Pandas: \\' + str(t.timeit(100)))\\n\\n#Small Method LC: 0.000191926956177\\n#Small Method Pandas: 0.044392824173\\n#Large Method LC: 1.98827004433\\n#Large Method Pandas: 0.324505090714\\n</code>', '<code>dicts=[\\n{\"name\": \"Tom\", \"age\": 10},\\n{\"name\": \"Mark\", \"age\": 5},\\n{\"name\": \"Pam\", \"age\": 7}\\n]\\n\\nfrom collections import defaultdict\\ndicts_by_name=defaultdict(list)\\nfor d in dicts:\\n    dicts_by_name[d[\\'name\\']]=d\\n\\nprint dicts_by_name[\\'Tom\\']\\n\\n#output\\n#&gt;&gt;&gt;\\n#{\\'age\\': 10, \\'name\\': \\'Tom\\'}\\n</code>', \"<code>import time\\n\\n# Build list of dicts\\nlist_of_dicts = list()\\nfor i in range(100000):\\n    list_of_dicts.append({'id': i, 'name': 'Tom'})\\n\\n# Build dict of dicts\\ndict_of_dicts = dict()\\nfor i in range(100000):\\n    dict_of_dicts[i] = {'name': 'Tom'}\\n\\n\\n# Find the one with ID of 99\\n\\n# 1. iterate through the list\\nlod_ts = time.time()\\nfor elem in list_of_dicts:\\n    if elem['id'] == 99999:\\n        break\\nlod_tf = time.time()\\nlod_td = lod_tf - lod_ts\\n\\n# 2. Use filter\\nf_ts = time.time()\\nx = filter(lambda k: k['id'] == 99999, list_of_dicts)\\nf_tf = time.time()\\nf_td = f_tf- f_ts\\n\\n# 3. find it in dict of dicts\\ndod_ts = time.time()\\nx = dict_of_dicts[99999]\\ndod_tf = time.time()\\ndod_td = dod_tf - dod_ts\\n\\n\\nprint 'List of Dictionries took: %s' % lod_td\\nprint 'Using filter took: %s' % f_td\\nprint 'Dict of Dicts took: %s' % dod_td\\n</code>\", '<code>List of Dictionries took: 0.0099310874939\\nUsing filter took: 0.0121960639954\\nDict of Dicts took: 4.05311584473e-06\\n</code>', '<code>def find_dict_in_list(dicts, default=None, **kwargs):\\n    \"\"\"Find first matching :obj:`dict` in :obj:`list`.\\n\\n    :param list dicts: List of dictionaries.\\n    :param dict default: Optional. Default dictionary to return.\\n        Defaults to `None`.\\n    :param **kwargs: `key=value` pairs to match in :obj:`dict`.\\n\\n    :returns: First matching :obj:`dict` from `dicts`.\\n    :rtype: dict\\n\\n    \"\"\"\\n\\n    rval = default\\n    for d in dicts:\\n        is_found = False\\n\\n        # Search for keys in dict.\\n        for k, v in kwargs.items():\\n            if d.get(k, None) == v:\\n                is_found = True\\n\\n            else:\\n                is_found = False\\n                break\\n\\n        if is_found:\\n            rval = d\\n            break\\n\\n    return rval\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Tests\\n    dicts = []\\n    keys = \\'spam eggs shrubbery knight\\'.split()\\n\\n    start = 0\\n    for _ in range(4):\\n        dct = {k: v for k, v in zip(keys, range(start, start+4))}\\n        dicts.append(dct)\\n        start += 4\\n\\n    # Find each dict based on \\'spam\\' key only.  \\n    for x in range(len(dicts)):\\n        spam = x*4\\n        assert find_dict_in_list(dicts, spam=spam) == dicts[x]\\n\\n    # Find each dict based on \\'spam\\' and \\'shrubbery\\' keys.\\n    for x in range(len(dicts)):\\n        spam = x*4\\n        assert find_dict_in_list(dicts, spam=spam, shrubbery=spam+2) == dicts[x]\\n\\n    # Search for one correct key, one incorrect key:\\n    for x in range(len(dicts)):\\n        spam = x*4\\n        assert find_dict_in_list(dicts, spam=spam, shrubbery=spam+1) is None\\n\\n    # Search for non-existent dict.\\n    for x in range(len(dicts)):\\n        spam = x+100\\n        assert find_dict_in_list(dicts, spam=spam) is None\\n</code>', '<code>from random import randint\\nfrom timeit import timeit\\n\\n\\nlist_dicts = []\\nfor _ in range(1000):     # number of dicts in the list\\n    dict_tmp = {}\\n    for i in range(10):   # number of keys for each dict\\n        dict_tmp[f\"key{i}\"] = randint(0,50)\\n    list_dicts.append( dict_tmp )\\n\\n\\n\\ndef a():\\n    # normal iteration over all elements\\n    for dict_ in list_dicts:\\n        if dict_[\"key3\"] == 20:\\n            pass\\n\\ndef b():\\n    # use \\'generator\\'\\n    for dict_ in (x for x in list_dicts if x[\"key3\"] == 20):\\n        pass\\n\\ndef c():\\n    # use \\'list\\'\\n    for dict_ in [x for x in list_dicts if x[\"key3\"] == 20]:\\n        pass\\n\\ndef d():\\n    # use \\'filter\\'\\n    for dict_ in filter(lambda x: x[\\'key3\\'] == 20, list_dicts):\\n        pass\\n</code>', '<code>1.7303 # normal list iteration \\n1.3849 # generator expression \\n1.3158 # list comprehension \\n7.7848 # filter\\n</code>']",
         "title": "Python list of dictionaries search",
         "_childDocuments_": [
            {
               "up_vote_count": 18,
               "answer_id": 8653558,
               "last_activity_date": 1371723052,
               "path": "3.stack.answer",
               "body_markdown": "    people = [\r\n    {&#39;name&#39;: &quot;Tom&quot;, &#39;age&#39;: 10},\r\n    {&#39;name&#39;: &quot;Mark&quot;, &#39;age&#39;: 5},\r\n    {&#39;name&#39;: &quot;Pam&quot;, &#39;age&#39;: 7}\r\n    ]\r\n    \r\n    def search(name):\r\n        for p in people:\r\n            if p[&#39;name&#39;] == name:\r\n                return p\r\n   \r\n    search(&quot;Pam&quot;)",
               "tags": [],
               "creation_date": 1325061016,
               "last_edit_date": 1371723052,
               "is_accepted": false,
               "id": "8653558",
               "down_vote_count": 1,
               "score": 17
            },
            {
               "up_vote_count": 278,
               "answer_id": 8653568,
               "last_activity_date": 1376085855,
               "path": "3.stack.answer",
               "body_markdown": "You can use a [generator expression][1]:\r\n\r\n    &gt;&gt;&gt; dicts = [\r\n    ...     { &quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 10 },\r\n    ...     { &quot;name&quot;: &quot;Mark&quot;, &quot;age&quot;: 5 },\r\n    ...     { &quot;name&quot;: &quot;Pam&quot;, &quot;age&quot;: 7 },\r\n    ...     { &quot;name&quot;: &quot;Dick&quot;, &quot;age&quot;: 12 }\r\n    ... ]\r\n    \r\n    &gt;&gt;&gt; (item for item in dicts if item[&quot;name&quot;] == &quot;Pam&quot;).next()\r\n    {&#39;age&#39;: 7, &#39;name&#39;: &#39;Pam&#39;}\r\n\r\n\r\n  [1]: http://www.python.org/dev/peps/pep-0289/",
               "tags": [],
               "creation_date": 1325061108,
               "last_edit_date": 1376085855,
               "is_accepted": false,
               "id": "8653568",
               "down_vote_count": 0,
               "score": 278
            },
            {
               "up_vote_count": 27,
               "answer_id": 8653572,
               "last_activity_date": 1325097014,
               "path": "3.stack.answer",
               "body_markdown": "You can use a [list comprehension][1]:\r\n\r\n    def search(name, people):\r\n        return [element for element in people if element[&#39;name&#39;] == name]\r\n\r\n\r\n  [1]: http://docs.python.org/tutorial/datastructures.html#list-comprehensions",
               "tags": [],
               "creation_date": 1325061129,
               "last_edit_date": 1325097014,
               "is_accepted": false,
               "id": "8653572",
               "down_vote_count": 1,
               "score": 26
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 8653574,
               "is_accepted": false,
               "last_activity_date": 1325061143,
               "body_markdown": "My first thought would be that you might want to consider creating a dictionary of these dictionaries ... if, for example, you were going to be searching it more a than small number of times.\r\n\r\nHowever that might be a premature optimization.  What would be wrong with:\r\n\r\n    def get_records(key, store=dict()):\r\n        &#39;&#39;&#39;Return a list of all records containing name==key from our store\r\n        &#39;&#39;&#39;\r\n        assert key is not None\r\n        return [d for d in store if d[&#39;name&#39;]==key]\r\n\r\n\r\n",
               "id": "8653574",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1325061143,
               "score": 4
            },
            {
               "up_vote_count": 5,
               "answer_id": 8653597,
               "last_activity_date": 1480495985,
               "path": "3.stack.answer",
               "body_markdown": "    names = [{&#39;name&#39;:&#39;Tom&#39;, &#39;age&#39;: 10}, {&#39;name&#39;: &#39;Mark&#39;, &#39;age&#39;: 5}, {&#39;name&#39;: &#39;Pam&#39;, &#39;age&#39;: 7}]\r\n    resultlist = [d    for d in names     if d.get(&#39;name&#39;, &#39;&#39;) == &#39;Pam&#39;]\r\n    first_result = resultlist[0]\r\n\r\nThis is one way...",
               "tags": [],
               "creation_date": 1325061255,
               "last_edit_date": 1480495985,
               "is_accepted": false,
               "id": "8653597",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 8653685,
               "is_accepted": false,
               "last_activity_date": 1325061894,
               "body_markdown": "You have to go through all elements of the list. There is not a shortcut!\r\n\r\nUnless somewhere else you keep a dictionary of the names pointing to the items of the list, but then you have to take care of the consequences of popping an element from your list. ",
               "id": "8653685",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1325061894,
               "score": -1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 8653933,
               "is_accepted": false,
               "last_activity_date": 1325063742,
               "body_markdown": "    dicts=[\r\n    {&quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 10},\r\n    {&quot;name&quot;: &quot;Mark&quot;, &quot;age&quot;: 5},\r\n    {&quot;name&quot;: &quot;Pam&quot;, &quot;age&quot;: 7}\r\n    ]\r\n    \r\n    from collections import defaultdict\r\n    dicts_by_name=defaultdict(list)\r\n    for d in dicts:\r\n        dicts_by_name[d[&#39;name&#39;]]=d\r\n    \r\n    print dicts_by_name[&#39;Tom&#39;]\r\n    \r\n    #output\r\n    #&gt;&gt;&gt;\r\n    #{&#39;age&#39;: 10, &#39;name&#39;: &#39;Tom&#39;}",
               "id": "8653933",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1325063742,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 24845196,
               "is_accepted": false,
               "last_activity_date": 1405805762,
               "body_markdown": "This is a general way of searching a value in a list of dictionaries:\r\n\r\n    def search_dictionaries(key, value, list_of_dictionaries):\r\n        return [element for element in list_of_dictionaries if element[key] == value]",
               "id": "24845196",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1405805762,
               "score": 5
            },
            {
               "up_vote_count": 98,
               "answer_id": 25373204,
               "last_activity_date": 1514319684,
               "path": "3.stack.answer",
               "body_markdown": "This looks to me the most pythonic way:\r\n\r\n    people = [\r\n    {&#39;name&#39;: &quot;Tom&quot;, &#39;age&#39;: 10},\r\n    {&#39;name&#39;: &quot;Mark&quot;, &#39;age&#39;: 5},\r\n    {&#39;name&#39;: &quot;Pam&quot;, &#39;age&#39;: 7}\r\n    ]\r\n    \r\n    filter(lambda person: person[&#39;name&#39;] == &#39;Pam&#39;, people)\r\n\r\nresult (returned as a list in Python 2):\r\n\r\n    [{&#39;age&#39;: 7, &#39;name&#39;: &#39;Pam&#39;}]\r\n\r\nNote: In Python 3, a filter object is returned.",
               "tags": [],
               "creation_date": 1408401979,
               "last_edit_date": 1514319684,
               "is_accepted": false,
               "id": "25373204",
               "down_vote_count": 1,
               "score": 97
            },
            {
               "up_vote_count": 23,
               "answer_id": 31988734,
               "last_activity_date": 1479736627,
               "path": "3.stack.answer",
               "body_markdown": "@Fr&#233;d&#233;ric Hamidi&#39;s answer is great.  In Python 3.x the syntax for `.next()` changed slightly.  Thus a slight modification:\r\n\r\n    &gt;&gt;&gt; dicts = [\r\n         { &quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 10 },\r\n         { &quot;name&quot;: &quot;Mark&quot;, &quot;age&quot;: 5 },\r\n         { &quot;name&quot;: &quot;Pam&quot;, &quot;age&quot;: 7 },\r\n         { &quot;name&quot;: &quot;Dick&quot;, &quot;age&quot;: 12 }\r\n     ]\r\n    &gt;&gt;&gt; next(item for item in dicts if item[&quot;name&quot;] == &quot;Pam&quot;)\r\n    {&#39;age&#39;: 7, &#39;name&#39;: &#39;Pam&#39;}\r\n\r\nAs mentioned in the comments by @Matt, you can add a default value as such:\r\n\r\n    &gt;&gt;&gt; next((item for item in dicts if item[&quot;name&quot;] == &quot;Pam&quot;), False)\r\n    {&#39;name&#39;: &#39;Pam&#39;, &#39;age&#39;: 7}\r\n    &gt;&gt;&gt; next((item for item in dicts if item[&quot;name&quot;] == &quot;Sam&quot;), False)\r\n    False\r\n    &gt;&gt;&gt;\r\n\r\n",
               "tags": [],
               "creation_date": 1439470124,
               "last_edit_date": 1479736627,
               "is_accepted": false,
               "id": "31988734",
               "down_vote_count": 0,
               "score": 23
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 34190875,
               "is_accepted": false,
               "last_activity_date": 1449703090,
               "body_markdown": "To add just a tiny bit to @Fr&#233;d&#233;ricHamidi.\r\n\r\nIn case you are not sure a key is in the the list of dicts, something like this would help:\r\n\r\n    next((item for item in dicts if item.get(&quot;name&quot;) and item[&quot;name&quot;] == &quot;Pam&quot;), None)\r\n",
               "id": "34190875",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1449703090,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 34827254,
               "is_accepted": false,
               "last_activity_date": 1452949296,
               "body_markdown": "Here is a comparison using iterating throuhg list, using filter+lambda or refactoring(if needed or valid to your case) your code to dict of dicts rather than list of dicts\r\n\r\n    import time\r\n    \r\n    # Build list of dicts\r\n    list_of_dicts = list()\r\n    for i in range(100000):\r\n        list_of_dicts.append({&#39;id&#39;: i, &#39;name&#39;: &#39;Tom&#39;})\r\n    \r\n    # Build dict of dicts\r\n    dict_of_dicts = dict()\r\n    for i in range(100000):\r\n        dict_of_dicts[i] = {&#39;name&#39;: &#39;Tom&#39;}\r\n    \r\n    \r\n    # Find the one with ID of 99\r\n    \r\n    # 1. iterate through the list\r\n    lod_ts = time.time()\r\n    for elem in list_of_dicts:\r\n        if elem[&#39;id&#39;] == 99999:\r\n            break\r\n    lod_tf = time.time()\r\n    lod_td = lod_tf - lod_ts\r\n    \r\n    # 2. Use filter\r\n    f_ts = time.time()\r\n    x = filter(lambda k: k[&#39;id&#39;] == 99999, list_of_dicts)\r\n    f_tf = time.time()\r\n    f_td = f_tf- f_ts\r\n    \r\n    # 3. find it in dict of dicts\r\n    dod_ts = time.time()\r\n    x = dict_of_dicts[99999]\r\n    dod_tf = time.time()\r\n    dod_td = dod_tf - dod_ts\r\n    \r\n    \r\n    print &#39;List of Dictionries took: %s&#39; % lod_td\r\n    print &#39;Using filter took: %s&#39; % f_td\r\n    print &#39;Dict of Dicts took: %s&#39; % dod_td\r\n\r\n\r\nAnd the output is this:\r\n\r\n    List of Dictionries took: 0.0099310874939\r\n    Using filter took: 0.0121960639954\r\n    Dict of Dicts took: 4.05311584473e-06\r\n\r\n\r\n**Conclusion:**\r\nClearly having a dictionary of dicts is the most efficient way to be able to search in those cases, where you know say you will be searching by id&#39;s only.\r\ninterestingly using filter is the slowest solution.",
               "id": "34827254",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1452949296,
               "score": 0
            },
            {
               "up_vote_count": 4,
               "answer_id": 39280934,
               "last_activity_date": 1472933565,
               "path": "3.stack.answer",
               "body_markdown": "Have you ever tried out the pandas package? It&#39;s perfect for this kind of search task and optimized too.\r\n\r\n    import pandas as pd\r\n\r\n    listOfDicts = [\r\n    {&quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 10},\r\n    {&quot;name&quot;: &quot;Mark&quot;, &quot;age&quot;: 5},\r\n    {&quot;name&quot;: &quot;Pam&quot;, &quot;age&quot;: 7}\r\n    ]\r\n\r\n    # Create a data frame, keys are used as column headers.\r\n    # Dict items with the same key are entered into the same respective column.\r\n    df = pd.DataFrame(listOfDicts)\r\n\r\n    # The pandas dataframe allows you to pick out specific values like so:\r\n    \r\n    df2 = df[ (df[&#39;name&#39;] == &#39;Pam&#39;) &amp; (df[&#39;age&#39;] == 7) ]\r\n\r\n    # Alternate syntax, same thing\r\n\r\n    df2 = df[ (df.name == &#39;Pam&#39;) &amp; (df.age == 7) ]\r\n\r\n\r\n\r\nI&#39;ve added a little bit of benchmarking below to illustrate pandas&#39; faster runtimes on a larger scale i.e. 100k+ entries:\r\n\r\n    setup_large = &#39;dicts = [];\\\r\n    [dicts.extend(({ &quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 10 },{ &quot;name&quot;: &quot;Mark&quot;, &quot;age&quot;: 5 },\\\r\n    { &quot;name&quot;: &quot;Pam&quot;, &quot;age&quot;: 7 },{ &quot;name&quot;: &quot;Dick&quot;, &quot;age&quot;: 12 })) for _ in range(25000)];\\\r\n    from operator import itemgetter;import pandas as pd;\\\r\n    df = pd.DataFrame(dicts);&#39;\r\n    \r\n    setup_small = &#39;dicts = [];\\\r\n    dicts.extend(({ &quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 10 },{ &quot;name&quot;: &quot;Mark&quot;, &quot;age&quot;: 5 },\\\r\n    { &quot;name&quot;: &quot;Pam&quot;, &quot;age&quot;: 7 },{ &quot;name&quot;: &quot;Dick&quot;, &quot;age&quot;: 12 }));\\\r\n    from operator import itemgetter;import pandas as pd;\\\r\n    df = pd.DataFrame(dicts);&#39;\r\n    \r\n    method1 = &#39;[item for item in dicts if item[&quot;name&quot;] == &quot;Pam&quot;]&#39;\r\n    method2 = &#39;df[df[&quot;name&quot;] == &quot;Pam&quot;]&#39;\r\n    \r\n    import timeit\r\n    t = timeit.Timer(method1, setup_small)\r\n    print(&#39;Small Method LC: &#39; + str(t.timeit(100)))\r\n    t = timeit.Timer(method2, setup_small)\r\n    print(&#39;Small Method Pandas: &#39; + str(t.timeit(100)))\r\n    \r\n    t = timeit.Timer(method1, setup_large)\r\n    print(&#39;Large Method LC: &#39; + str(t.timeit(100)))\r\n    t = timeit.Timer(method2, setup_large)\r\n    print(&#39;Large Method Pandas: &#39; + str(t.timeit(100)))\r\n\r\n    #Small Method LC: 0.000191926956177\r\n    #Small Method Pandas: 0.044392824173\r\n    #Large Method LC: 1.98827004433\r\n    #Large Method Pandas: 0.324505090714",
               "tags": [],
               "creation_date": 1472764373,
               "last_edit_date": 1472933565,
               "is_accepted": false,
               "id": "39280934",
               "down_vote_count": 1,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48324488,
               "is_accepted": false,
               "last_activity_date": 1516288365,
               "body_markdown": "I found this thread when I was searching for an answer to the same\r\nquestion. While I realize that it&#39;s a late answer, I thought I&#39;d\r\ncontribute it in case it&#39;s useful to anyone else:\r\n\r\n    def find_dict_in_list(dicts, default=None, **kwargs):\r\n        &quot;&quot;&quot;Find first matching :obj:`dict` in :obj:`list`.\r\n        \r\n        :param list dicts: List of dictionaries.\r\n        :param dict default: Optional. Default dictionary to return.\r\n            Defaults to `None`.\r\n        :param **kwargs: `key=value` pairs to match in :obj:`dict`.\r\n        \r\n        :returns: First matching :obj:`dict` from `dicts`.\r\n        :rtype: dict\r\n        \r\n        &quot;&quot;&quot;\r\n        \r\n        rval = default\r\n        for d in dicts:\r\n            is_found = False\r\n            \r\n            # Search for keys in dict.\r\n            for k, v in kwargs.items():\r\n                if d.get(k, None) == v:\r\n                    is_found = True\r\n                \r\n                else:\r\n                    is_found = False\r\n                    break\r\n                \r\n            if is_found:\r\n                rval = d\r\n                break\r\n            \r\n        return rval\r\n\r\n\r\n    if __name__ == &#39;__main__&#39;:\r\n        # Tests\r\n        dicts = []\r\n        keys = &#39;spam eggs shrubbery knight&#39;.split()\r\n        \r\n        start = 0\r\n        for _ in range(4):\r\n            dct = {k: v for k, v in zip(keys, range(start, start+4))}\r\n            dicts.append(dct)\r\n            start += 4\r\n        \r\n        # Find each dict based on &#39;spam&#39; key only.  \r\n        for x in range(len(dicts)):\r\n            spam = x*4\r\n            assert find_dict_in_list(dicts, spam=spam) == dicts[x]\r\n            \r\n        # Find each dict based on &#39;spam&#39; and &#39;shrubbery&#39; keys.\r\n        for x in range(len(dicts)):\r\n            spam = x*4\r\n            assert find_dict_in_list(dicts, spam=spam, shrubbery=spam+2) == dicts[x]\r\n            \r\n        # Search for one correct key, one incorrect key:\r\n        for x in range(len(dicts)):\r\n            spam = x*4\r\n            assert find_dict_in_list(dicts, spam=spam, shrubbery=spam+1) is None\r\n            \r\n        # Search for non-existent dict.\r\n        for x in range(len(dicts)):\r\n            spam = x+100\r\n            assert find_dict_in_list(dicts, spam=spam) is None\r\n",
               "id": "48324488",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1516288365,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48958217,
               "is_accepted": false,
               "last_activity_date": 1519433469,
               "body_markdown": "I tested various methods to go through a list of dictionaries and return the dictionaries where key x has a certain value.  \r\n\r\n**Results:**   \r\n\r\n - Speed: list comprehension &gt; generator expression &gt;&gt; normal list iteration &gt;&gt;&gt; filter.    \r\n - All scale linear with the number of dicts in the list (10x list size -&gt; 10x time).  \r\n - The keys per dictionary does not affect speed significantly for large amounts (thousands) of keys. Please see this graph I calculated: https://imgur.com/a/quQzv (method names see below).  \r\n\r\nAll tests done with **Python 3.6**.4, W7x64.\r\n  \r\n\r\n\r\n\r\n    from random import randint\r\n    from timeit import timeit\r\n    \r\n    \r\n    list_dicts = []\r\n    for _ in range(1000):     # number of dicts in the list\r\n        dict_tmp = {}\r\n        for i in range(10):   # number of keys for each dict\r\n            dict_tmp[f&quot;key{i}&quot;] = randint(0,50)\r\n        list_dicts.append( dict_tmp )\r\n    \r\n    \r\n    \r\n    def a():\r\n        # normal iteration over all elements\r\n        for dict_ in list_dicts:\r\n            if dict_[&quot;key3&quot;] == 20:\r\n                pass\r\n    \r\n    def b():\r\n        # use &#39;generator&#39;\r\n        for dict_ in (x for x in list_dicts if x[&quot;key3&quot;] == 20):\r\n            pass\r\n             \r\n    def c():\r\n        # use &#39;list&#39;\r\n        for dict_ in [x for x in list_dicts if x[&quot;key3&quot;] == 20]:\r\n            pass\r\n            \r\n    def d():\r\n        # use &#39;filter&#39;\r\n        for dict_ in filter(lambda x: x[&#39;key3&#39;] == 20, list_dicts):\r\n            pass\r\n\r\nResults:\r\n\r\n    1.7303 # normal list iteration \r\n    1.3849 # generator expression \r\n    1.3158 # list comprehension \r\n    7.7848 # filter",
               "id": "48958217",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519433469,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/8653516/python-list-of-dictionaries-search",
         "id": "858127-2258"
      },
      {
         "up_vote_count": "174",
         "body_markdown": "Most operations in `pandas` can be accomplished with operator chaining (`groupby`, `aggregate`, `apply`, etc), but the only way I&#39;ve found to filter rows is via normal bracket indexing\r\n\r\n    df_filtered = df[df[&#39;column&#39;] == value]\r\n\r\nThis is unappealing as it requires I assign `df` to a variable before being able to filter on its values.  Is there something more like the following?\r\n\r\n    df_filtered = df.mask(lambda x: x[&#39;column&#39;] == value)",
         "view_count": "241062",
         "answer_count": "12",
         "tags": "['pandas']",
         "creation_date": "1344446737",
         "path": "2.stack",
         "code_snippet": "['<code>pandas</code>', '<code>groupby</code>', '<code>aggregate</code>', '<code>apply</code>', \"<code>df_filtered = df[df['column'] == value]\\n</code>\", '<code>df</code>', \"<code>df_filtered = df.mask(lambda x: x['column'] == value)\\n</code>\", '<code>In [96]: df\\nOut[96]:\\n   A  B  C  D\\na  1  4  9  1\\nb  4  5  0  2\\nc  5  5  1  0\\nd  1  3  9  6\\n\\nIn [99]: df[(df.A == 1) &amp; (df.D == 6)]\\nOut[99]:\\n   A  B  C  D\\nd  1  3  9  6\\n</code>', \"<code>In [90]: def mask(df, key, value):\\n   ....:     return df[df[key] == value]\\n   ....:\\n\\nIn [92]: pandas.DataFrame.mask = mask\\n\\nIn [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))\\n\\nIn [95]: df.ix['d','A'] = df.ix['a', 'A']\\n\\nIn [96]: df\\nOut[96]:\\n   A  B  C  D\\na  1  4  9  1\\nb  4  5  0  2\\nc  5  5  1  0\\nd  1  3  9  6\\n\\nIn [97]: df.mask('A', 1)\\nOut[97]:\\n   A  B  C  D\\na  1  4  9  1\\nd  1  3  9  6\\n\\nIn [98]: df.mask('A', 1).mask('D', 6)\\nOut[98]:\\n   A  B  C  D\\nd  1  3  9  6\\n</code>\", '<code>(df.A == 1) &amp; (df.D == 6)</code>', '<code>pandas.</code>', '<code>import pandas as pd</code>', '<code>import pandas as pd</code>', '<code>def mask(df, f):\\n  return df[f(df)]\\n</code>', '<code>df.mask(lambda x: x[0] &lt; 0).mask(lambda x: x[1] &gt; 0)\\n</code>', '<code>DataFrame</code>', \"<code>df = pd.DataFrame( np.random.randn(30,3), columns = ['a','b','c'])\\ndf_filtered = df.query('a&gt;0').query('0&lt;b&lt;2')\\n</code>\", \"<code>df_filtered = df.query('a&gt;0 and 0&lt;b&lt;2')\\n</code>\", \"<code>df.query('a in list([1,2])')</code>\", \"<code>s = set([1,2]); df.query('a in @s')</code>\", '<code>pandas.DataFrame.query</code>', '<code>query</code>', '<code>df</code>', \"<code>import pandas as pd\\nimport numpy as np\\n\\nnp.random.seed([3,1415])\\ndf = pd.DataFrame(\\n    np.random.randint(10, size=(10, 5)),\\n    columns=list('ABCDE')\\n)\\n\\ndf\\n\\n   A  B  C  D  E\\n0  0  2  7  3  8\\n1  7  0  6  8  6\\n2  0  2  0  4  9\\n3  7  3  2  4  3\\n4  3  6  7  7  4\\n5  5  3  7  5  9\\n6  8  7  6  4  7\\n7  6  2  6  6  5\\n8  2  8  7  5  8\\n9  4  7  6  1  5\\n</code>\", '<code>query</code>', '<code>D &gt; B</code>', \"<code>df.query('D &gt; B')\\n\\n   A  B  C  D  E\\n0  0  2  7  3  8\\n1  7  0  6  8  6\\n2  0  2  0  4  9\\n3  7  3  2  4  3\\n4  3  6  7  7  4\\n5  5  3  7  5  9\\n7  6  2  6  6  5\\n</code>\", \"<code>df.query('D &gt; B').query('C &gt; B')\\n# equivalent to\\n# df.query('D &gt; B and C &gt; B')\\n# but defeats the purpose of demonstrating chaining\\n\\n   A  B  C  D  E\\n0  0  2  7  3  8\\n1  7  0  6  8  6\\n4  3  6  7  7  4\\n5  5  3  7  5  9\\n7  6  2  6  6  5\\n</code>\", '<code>In [96]: df\\nOut[96]:\\n   A  B  C  D\\na  1  4  9  1\\nb  4  5  0  2\\nc  5  5  1  0\\nd  1  3  9  6\\n\\nIn [99]: df[(df.A == 1) &amp; (df.D == 6)]\\nOut[99]:\\n   A  B  C  D\\nd  1  3  9  6\\n</code>', '<code>(... == True)</code>', '<code>df[((df.A==1) == True) | ((df.D==6) == True)]\\n</code>', '<code>df[(df.A==1) | (df.D==6)]</code>', \"<code>df.pipe(lambda d: d[d['column'] == value])\\n</code>\", \"<code>a.join(b).pipe(lambda df: df[df.column_to_filter == 'VALUE'])</code>\", '<code>.loc</code>', \"<code>import numpy as np\\nimport pandas as pd\\n\\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\\ndf.loc[lambda df: df.A == 80]  # equivalent to df[df.A == 80] but chainable\\n\\ndf.sort_values('A').loc[lambda df: df.A &gt; 80].loc[lambda df: df.B &gt; df.A]\\n</code>\", '<code>.loc</code>', '<code>pd.DataFrame = apply_masks()\\n</code>', '<code>A = pd.DataFrame(np.random.randn(4, 4), columns=[\"A\", \"B\", \"C\", \"D\"])\\nA.le_mask(\"A\", 0.7).ge_mask(\"B\", 0.2)... (May be repeated as necessary\\n</code>', \"<code>import pandas as pd\\n\\ndef eq_mask(df, key, value):\\n    return df[df[key] == value]\\n\\ndef ge_mask(df, key, value):\\n    return df[df[key] &gt;= value]\\n\\ndef gt_mask(df, key, value):\\n    return df[df[key] &gt; value]\\n\\ndef le_mask(df, key, value):\\n    return df[df[key] &lt;= value]\\n\\ndef lt_mask(df, key, value):\\n    return df[df[key] &lt; value]\\n\\ndef ne_mask(df, key, value):\\n    return df[df[key] != value]\\n\\ndef gen_mask(df, f):\\n    return df[f(df)]\\n\\ndef apply_masks():\\n\\n    pd.DataFrame.eq_mask = eq_mask\\n    pd.DataFrame.ge_mask = ge_mask\\n    pd.DataFrame.gt_mask = gt_mask\\n    pd.DataFrame.le_mask = le_mask\\n    pd.DataFrame.lt_mask = lt_mask\\n    pd.DataFrame.ne_mask = ne_mask\\n    pd.DataFrame.gen_mask = gen_mask\\n\\n    return pd.DataFrame\\n\\nif __name__ == '__main__':\\n    pass\\n</code>\", '<code>from condition import Condition as C\\n</code>', \"<code>df = pd.DataFrame([[1, 2, True],\\n                   [3, 4, False], \\n                   [5, 7, True]],\\n                  index=range(3), columns=['a', 'b', 'c'])\\n# On specific column:\\nprint(df.loc[C('a') &gt; 2])\\nprint(df.loc[-C('a') == C('b')])\\nprint(df.loc[~C('c')])\\n# On entire DataFrame:\\nprint(df.loc[C().sum(axis=1) &gt; 3])\\nprint(df.loc[C(['a', 'b']).diff(axis=1)['b'] &gt; 1])\\n</code>\", \"<code>data = pd.read_csv('ugly_db.csv').loc[~(C() == '$null$').any(axis=1)]\\n</code>\", \"<code>df.loc[C('cond1')].loc[C('cond2')]\\n</code>\", \"<code>df.loc[C('cond1') &amp; C('cond2')]\\n</code>\", '<code>cond2</code>', '<code>cond1</code>', '<code>True</code>', '<code>loc</code>', \"<code>df_filtered = df.loc[df['column'] == value]\\n</code>\", \"<code>df_filtered = df.loc[df['column'] == value, ['year', 'column']]\\n</code>\", \"<code>res =  df\\\\\\n    .loc[df['station']=='USA', ['TEMP', 'RF']]\\\\\\n    .groupby('year')\\\\\\n    .agg(np.nanmean)\\n</code>\", '<code>DataFrame.xs()</code>', '<code>query</code>', \"<code>import pandas as pd\\nimport numpy as np\\n\\nnp.random.seed([3,1415])\\ndf = pd.DataFrame(\\n    np.random.randint(3, size=(10, 5)),\\n    columns=list('ABCDE')\\n)\\n\\ndf\\n# Out[55]: \\n#    A  B  C  D  E\\n# 0  0  2  2  2  2\\n# 1  1  1  2  0  2\\n# 2  0  2  0  0  2\\n# 3  0  2  2  0  1\\n# 4  0  1  1  2  0\\n# 5  0  0  0  1  2\\n# 6  1  0  1  1  1\\n# 7  0  0  2  0  2\\n# 8  2  2  2  2  2\\n# 9  1  2  0  2  1\\n\\ndf.set_index(['A', 'D']).xs([0, 2]).reset_index()\\n# Out[57]: \\n#    A  D  B  C  E\\n# 0  0  2  2  2  2\\n# 1  0  2  1  1  0\\n</code>\", \"<code>df[np.logical_and(df['A'] == 1 ,df['B'] == 6)]\\n</code>\"]",
         "title": "pandas: filter rows of DataFrame with operator chaining",
         "_childDocuments_": [
            {
               "up_vote_count": 233,
               "answer_id": 11872393,
               "last_activity_date": 1482247406,
               "path": "3.stack.answer",
               "body_markdown": "I&#39;m not entirely sure what you want, and your last line of code does not help either, but anyway:\r\n\r\n\r\n&quot;Chained&quot; filtering is done by &quot;chaining&quot; the criteria in the boolean index.\r\n\r\n    In [96]: df\r\n    Out[96]:\r\n       A  B  C  D\r\n    a  1  4  9  1\r\n    b  4  5  0  2\r\n    c  5  5  1  0\r\n    d  1  3  9  6\r\n\r\n    In [99]: df[(df.A == 1) &amp; (df.D == 6)]\r\n    Out[99]:\r\n       A  B  C  D\r\n    d  1  3  9  6\r\n\r\n\r\nIf you want to chain methods, you can add your own mask method and use that one.\r\n\r\n\r\n    In [90]: def mask(df, key, value):\r\n       ....:     return df[df[key] == value]\r\n       ....:\r\n\r\n    In [92]: pandas.DataFrame.mask = mask\r\n\r\n    In [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list(&#39;abcd&#39;), columns=list(&#39;ABCD&#39;))\r\n\r\n    In [95]: df.ix[&#39;d&#39;,&#39;A&#39;] = df.ix[&#39;a&#39;, &#39;A&#39;]\r\n\r\n    In [96]: df\r\n    Out[96]:\r\n       A  B  C  D\r\n    a  1  4  9  1\r\n    b  4  5  0  2\r\n    c  5  5  1  0\r\n    d  1  3  9  6\r\n\r\n    In [97]: df.mask(&#39;A&#39;, 1)\r\n    Out[97]:\r\n       A  B  C  D\r\n    a  1  4  9  1\r\n    d  1  3  9  6\r\n\r\n    In [98]: df.mask(&#39;A&#39;, 1).mask(&#39;D&#39;, 6)\r\n    Out[98]:\r\n       A  B  C  D\r\n    d  1  3  9  6",
               "tags": [],
               "creation_date": 1344456643,
               "last_edit_date": 1482247406,
               "is_accepted": true,
               "id": "11872393",
               "down_vote_count": 0,
               "score": 233
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 53,
               "answer_id": 11893375,
               "is_accepted": false,
               "last_activity_date": 1344554459,
               "body_markdown": "The answer from @lodagro is great. I would extend it by generalizing the mask function as:\r\n\r\n    def mask(df, f):\r\n      return df[f(df)]\r\n\r\nThen you can do stuff like:\r\n\r\n    df.mask(lambda x: x[0] &lt; 0).mask(lambda x: x[1] &gt; 0)",
               "id": "11893375",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1344554459,
               "score": 53
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 16074650,
               "is_accepted": false,
               "last_activity_date": 1366260268,
               "body_markdown": "If you would like to apply all of the common boolean masks as well as a general purpose mask you can chuck the following in a file and then simply assign them all as follows:\r\n   \r\n    pd.DataFrame = apply_masks()\r\nUsage:\r\n\r\n    A = pd.DataFrame(np.random.randn(4, 4), columns=[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;])\r\n    A.le_mask(&quot;A&quot;, 0.7).ge_mask(&quot;B&quot;, 0.2)... (May be repeated as necessary\r\n\r\nIt&#39;s a little bit hacky but it can make things a little bit cleaner if you&#39;re continuously chopping and changing datasets according to filters.\r\nThere&#39;s also a general purpose filter adapted from Daniel Velkov above in the gen_mask function which you can use with lambda functions or otherwise if desired.\r\n\r\nFile to be saved (I use masks.py):\r\n\r\n    import pandas as pd\r\n\r\n    def eq_mask(df, key, value):\r\n        return df[df[key] == value]\r\n        \r\n    def ge_mask(df, key, value):\r\n        return df[df[key] &gt;= value]\r\n        \r\n    def gt_mask(df, key, value):\r\n        return df[df[key] &gt; value]\r\n        \r\n    def le_mask(df, key, value):\r\n        return df[df[key] &lt;= value]\r\n        \r\n    def lt_mask(df, key, value):\r\n        return df[df[key] &lt; value]\r\n        \r\n    def ne_mask(df, key, value):\r\n        return df[df[key] != value]\r\n        \r\n    def gen_mask(df, f):\r\n        return df[f(df)]\r\n        \r\n    def apply_masks():\r\n        \r\n        pd.DataFrame.eq_mask = eq_mask\r\n        pd.DataFrame.ge_mask = ge_mask\r\n        pd.DataFrame.gt_mask = gt_mask\r\n        pd.DataFrame.le_mask = le_mask\r\n        pd.DataFrame.lt_mask = lt_mask\r\n        pd.DataFrame.ne_mask = ne_mask\r\n        pd.DataFrame.gen_mask = gen_mask\r\n        \r\n        return pd.DataFrame\r\n        \r\n    if __name__ == &#39;__main__&#39;:\r\n        pass",
               "id": "16074650",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1366260268,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 50,
               "answer_id": 28159296,
               "is_accepted": false,
               "last_activity_date": 1422308659,
               "body_markdown": "Filters can be chained using a Pandas [query](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html):\r\n\r\n\r\n    df = pd.DataFrame( np.random.randn(30,3), columns = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;])\r\n    df_filtered = df.query(&#39;a&gt;0&#39;).query(&#39;0&lt;b&lt;2&#39;)\r\n\r\n\r\nFilters can also be combined in a single query:\r\n\r\n    df_filtered = df.query(&#39;a&gt;0 and 0&lt;b&lt;2&#39;)",
               "id": "28159296",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1422308659,
               "score": 50
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 10,
               "answer_id": 29247205,
               "is_accepted": false,
               "last_activity_date": 1427256024,
               "body_markdown": "I had the same question except that I wanted to combine the criteria into an OR condition.  The format given by Wouter Overmeire combines the criteria into an AND condition such that both must be satisfied:\r\n\r\n    In [96]: df\r\n    Out[96]:\r\n       A  B  C  D\r\n    a  1  4  9  1\r\n    b  4  5  0  2\r\n    c  5  5  1  0\r\n    d  1  3  9  6\r\n    \r\n    In [99]: df[(df.A == 1) &amp; (df.D == 6)]\r\n    Out[99]:\r\n       A  B  C  D\r\n    d  1  3  9  6\r\n\r\nBut I found that, if you wrap each condition in `(... == True)` and join the criteria with a pipe, the criteria are combined in an OR condition, satisfied whenever either of them is true: \r\n\r\n    df[((df.A==1) == True) | ((df.D==6) == True)]\r\n",
               "id": "29247205",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1427256024,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 37490608,
               "is_accepted": false,
               "last_activity_date": 1464375659,
               "body_markdown": "My answer is similar to the others. If you do not want to create a new function you can use what pandas has defined for you already. Use the pipe method.\r\n\r\n    df.pipe(lambda d: d[d[&#39;column&#39;] == value])",
               "id": "37490608",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1464375659,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 11,
               "answer_id": 44350821,
               "is_accepted": false,
               "last_activity_date": 1496551230,
               "body_markdown": "[**`pandas.DataFrame.query`**][1]  \r\n`query` was made for exactly this purpose.  Consider the dataframe `df`\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n\r\n    np.random.seed([3,1415])\r\n    df = pd.DataFrame(\r\n        np.random.randint(10, size=(10, 5)),\r\n        columns=list(&#39;ABCDE&#39;)\r\n    )\r\n\r\n    df\r\n\r\n       A  B  C  D  E\r\n    0  0  2  7  3  8\r\n    1  7  0  6  8  6\r\n    2  0  2  0  4  9\r\n    3  7  3  2  4  3\r\n    4  3  6  7  7  4\r\n    5  5  3  7  5  9\r\n    6  8  7  6  4  7\r\n    7  6  2  6  6  5\r\n    8  2  8  7  5  8\r\n    9  4  7  6  1  5\r\n\r\nLet&#39;s use `query` to filter all rows where `D &gt; B`\r\n\r\n    df.query(&#39;D &gt; B&#39;)\r\n\r\n       A  B  C  D  E\r\n    0  0  2  7  3  8\r\n    1  7  0  6  8  6\r\n    2  0  2  0  4  9\r\n    3  7  3  2  4  3\r\n    4  3  6  7  7  4\r\n    5  5  3  7  5  9\r\n    7  6  2  6  6  5\r\n\r\nWhich we chain\r\n\r\n    df.query(&#39;D &gt; B&#39;).query(&#39;C &gt; B&#39;)\r\n    # equivalent to\r\n    # df.query(&#39;D &gt; B and C &gt; B&#39;)\r\n    # but defeats the purpose of demonstrating chaining\r\n\r\n       A  B  C  D  E\r\n    0  0  2  7  3  8\r\n    1  7  0  6  8  6\r\n    4  3  6  7  7  4\r\n    5  5  3  7  5  9\r\n    7  6  2  6  6  5\r\n\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html",
               "id": "44350821",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1496551230,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 45316991,
               "is_accepted": false,
               "last_activity_date": 1501040133,
               "body_markdown": "If you set your columns to search as indexes, then you can use `DataFrame.xs()` to take a cross section. This is not as versatile as the `query` answers, but it might be useful in some situations.\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    \r\n    np.random.seed([3,1415])\r\n    df = pd.DataFrame(\r\n        np.random.randint(3, size=(10, 5)),\r\n        columns=list(&#39;ABCDE&#39;)\r\n    )\r\n    \r\n    df\r\n    # Out[55]: \r\n    #    A  B  C  D  E\r\n    # 0  0  2  2  2  2\r\n    # 1  1  1  2  0  2\r\n    # 2  0  2  0  0  2\r\n    # 3  0  2  2  0  1\r\n    # 4  0  1  1  2  0\r\n    # 5  0  0  0  1  2\r\n    # 6  1  0  1  1  1\r\n    # 7  0  0  2  0  2\r\n    # 8  2  2  2  2  2\r\n    # 9  1  2  0  2  1\r\n    \r\n    df.set_index([&#39;A&#39;, &#39;D&#39;]).xs([0, 2]).reset_index()\r\n    # Out[57]: \r\n    #    A  D  B  C  E\r\n    # 0  0  2  2  2  2\r\n    # 1  0  2  1  1  0\r\n\r\n",
               "id": "45316991",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1501040133,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 46052461,
               "is_accepted": false,
               "last_activity_date": 1504606465,
               "body_markdown": "Since [version 0.18.1](https://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-callable) the `.loc` method accepts a callable for selection. Together with lambda functions you can create very flexible chainable filters:\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n\r\n    df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list(&#39;ABCD&#39;))\r\n    df.loc[lambda df: df.A == 80]  # equivalent to df[df.A == 80] but chainable\r\n\r\n    df.sort_values(&#39;A&#39;).loc[lambda df: df.A &gt; 80].loc[lambda df: df.B &gt; df.A]\r\n    \r\nIf all you&#39;re doing is filtering, you can also omit the `.loc`.\r\n",
               "id": "46052461",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1504606465,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47517280,
               "is_accepted": false,
               "last_activity_date": 1511806329,
               "body_markdown": "This solution is more hackish in terms of implementation, but I find it much cleaner in terms of usage, and it is certainly more general than the others proposed.\r\n\r\nhttps://github.com/toobaz/generic_utils/blob/master/generic_utils/pandas/condition.py\r\n\r\nYou don&#39;t need to download the entire repo: saving the file and doing\r\n\r\n    from condition import Condition as C\r\n\r\nshould suffice. Then you use it like this:\r\n\r\n    df = pd.DataFrame([[1, 2, True],\r\n                       [3, 4, False], \r\n                       [5, 7, True]],\r\n                      index=range(3), columns=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\r\n    # On specific column:\r\n    print(df.loc[C(&#39;a&#39;) &gt; 2])\r\n    print(df.loc[-C(&#39;a&#39;) == C(&#39;b&#39;)])\r\n    print(df.loc[~C(&#39;c&#39;)])\r\n    # On entire DataFrame:\r\n    print(df.loc[C().sum(axis=1) &gt; 3])\r\n    print(df.loc[C([&#39;a&#39;, &#39;b&#39;]).diff(axis=1)[&#39;b&#39;] &gt; 1])\r\n\r\nA slightly less stupid usage example:\r\n\r\n    data = pd.read_csv(&#39;ugly_db.csv&#39;).loc[~(C() == &#39;$null$&#39;).any(axis=1)]\r\n\r\n\r\nBy the way: even in the case in which you are just using boolean cols,\r\n\r\n    df.loc[C(&#39;cond1&#39;)].loc[C(&#39;cond2&#39;)]\r\n\r\ncan be much more efficient than\r\n\r\n    df.loc[C(&#39;cond1&#39;) &amp; C(&#39;cond2&#39;)]\r\n\r\nbecause it evaluates ``cond2`` only where ``cond1`` is ``True``.\r\n\r\nDISCLAIMER: I first gave this answer [elsewhere][1] because I hadn&#39;t seen this.\r\n\r\n\r\n  [1]: https://stackoverflow.com/a/47516891/2858145",
               "id": "47517280",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1511806329,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48437582,
               "is_accepted": false,
               "last_activity_date": 1516864943,
               "body_markdown": "You can also leverage the *numpy* library for logical operations. Its pretty fast. \r\n\r\n    df[np.logical_and(df[&#39;A&#39;] == 1 ,df[&#39;B&#39;] == 6)]\r\n",
               "id": "48437582",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1516864943,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 48495408,
               "is_accepted": false,
               "last_activity_date": 1517206716,
               "body_markdown": "Just want to add a demonstration using `loc` to filter not only by rows but also by columns and some merits to the chained operation.\r\n\r\nThe code below can filter the rows by value.\r\n\r\n    df_filtered = df.loc[df[&#39;column&#39;] == value]\r\n\r\nBy modifying it a bit you can filter the columns as well.\r\n\r\n    df_filtered = df.loc[df[&#39;column&#39;] == value, [&#39;year&#39;, &#39;column&#39;]]\r\n\r\nSo why do we want a chained method? The answer is that it is simple to read if you have many operations. For example,\r\n\r\n    res =  df\\\r\n        .loc[df[&#39;station&#39;]==&#39;USA&#39;, [&#39;TEMP&#39;, &#39;RF&#39;]]\\\r\n        .groupby(&#39;year&#39;)\\\r\n        .agg(np.nanmean)",
               "id": "48495408",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1517206716,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/11869910/pandas-filter-rows-of-dataframe-with-operator-chaining",
         "id": "858127-2259"
      },
      {
         "up_vote_count": "226",
         "path": "2.stack",
         "body_markdown": "I have the following code to do this, but how can I do it better? Right now I think it&#39;s better than nested loops, but it starts to get Perl-one-linerish when you have a generator in a list comprehension. \r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    day_count = (end_date - start_date).days + 1\r\n    for single_date in [d for d in (start_date + timedelta(n) for n in range(day_count)) if d &lt;= end_date]:\r\n        print strftime(&quot;%Y-%m-%d&quot;, single_date.timetuple())\r\n\r\n\r\n## Notes\r\n\r\n* I&#39;m not actually using this to print. That&#39;s just for demo purposes. \r\n* The `start_date` and `end_date` variables are `datetime.date` objects because I don&#39;t need the timestamps. (They&#39;re going to be used to generate a report).\r\n\r\n## Sample Output \r\nFor a start date of `2009-05-30` and an end date of `2009-06-09`:\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    2009-05-30\r\n    2009-05-31\r\n    2009-06-01\r\n    2009-06-02\r\n    2009-06-03\r\n    2009-06-04\r\n    2009-06-05\r\n    2009-06-06\r\n    2009-06-07\r\n    2009-06-08\r\n    2009-06-09",
         "view_count": "151241",
         "answer_count": "15",
         "tags": "['python', 'datetime', 'iteration']",
         "creation_date": "1246306562",
         "last_edit_date": "1358208619",
         "code_snippet": "['<code>day_count = (end_date - start_date).days + 1\\nfor single_date in [d for d in (start_date + timedelta(n) for n in range(day_count)) if d &lt;= end_date]:\\n    print strftime(\"%Y-%m-%d\", single_date.timetuple())\\n</code>', '<code>start_date</code>', '<code>end_date</code>', '<code>datetime.date</code>', '<code>2009-05-30</code>', '<code>2009-06-09</code>', '<code>2009-05-30\\n2009-05-31\\n2009-06-01\\n2009-06-02\\n2009-06-03\\n2009-06-04\\n2009-06-05\\n2009-06-06\\n2009-06-07\\n2009-06-08\\n2009-06-09\\n</code>', '<code>for single_date in (start_date + timedelta(n) for n in range(day_count)):\\n    print ...\\n</code>', '<code>from datetime import timedelta, date\\n\\ndef daterange(start_date, end_date):\\n    for n in range(int ((end_date - start_date).days)):\\n        yield start_date + timedelta(n)\\n\\nstart_date = date(2013, 1, 1)\\nend_date = date(2015, 6, 2)\\nfor single_date in daterange(start_date, end_date):\\n    print single_date.strftime(\"%Y-%m-%d\")\\n</code>', '<code>range()</code>', '<code>end_date</code>', '<code>range()</code>', '<code>(start_date + datetime.timedelta(n) for n in range((end_date - start_date).days))</code>', '<code>d = start_date\\ndelta = datetime.timedelta(days=1)\\nwhile d &lt;= end_date:\\n    print d.strftime(\"%Y-%m-%d\")\\n    d += delta\\n</code>', '<code>dateutil</code>', '<code>from datetime import date\\nfrom dateutil.rrule import rrule, DAILY\\n\\na = date(2009, 5, 30)\\nb = date(2009, 6, 9)\\n\\nfor dt in rrule(DAILY, dtstart=a, until=b):\\n    print dt.strftime(\"%Y-%m-%d\")\\n</code>', '<code>relative delta</code>', '<code>until</code>', '<code>daterange</code>', '<code>end_date</code>', '<code>import pandas as pd\\ndaterange = pd.date_range(start_date, end_date)\\n</code>', '<code>for single_date in daterange:\\n    print (single_date.strftime(\"%Y-%m-%d\"))\\n</code>', '<code>print(daterange)\\n</code>', '<code>import datetime\\n\\ndef daterange(start, stop, step=datetime.timedelta(days=1), inclusive=False):\\n  # inclusive=False to behave like range by default\\n  if step.days &gt; 0:\\n    while start &lt; stop:\\n      yield start\\n      start = start + step\\n      # not +=! don\\'t modify object passed in if it\\'s mutable\\n      # since this function is not restricted to\\n      # only types from datetime module\\n  elif step.days &lt; 0:\\n    while start &gt; stop:\\n      yield start\\n      start = start + step\\n  if inclusive and start == stop:\\n    yield start\\n\\n# ...\\n\\nfor date in daterange(start_date, end_date, inclusive=True):\\n  print strftime(\"%Y-%m-%d\", date.timetuple())\\n</code>', '<code>day_count</code>', '<code>delta</code>', '<code>import datetime as dt\\n\\nstart_date = dt.datetime(2012, 12,1)\\nend_date = dt.datetime(2012, 12,5)\\n\\ntotal_days = (end_date - start_date).days + 1 #inclusive 5 days\\n\\nfor day_number in range(total_days):\\n    current_date = (start_date + dt.timedelta(days = day_number)).date()\\n    print current_date\\n</code>', '<code>import datetime\\n\\ndef daterange(start, stop, step_days=1):\\n    current = start\\n    step = datetime.timedelta(step_days)\\n    if step_days &gt; 0:\\n        while current &lt; stop:\\n            yield current\\n            current += step\\n    elif step_days &lt; 0:\\n        while current &gt; stop:\\n            yield current\\n            current += step\\n    else:\\n        raise ValueError(\"daterange() step_days argument must not be zero\")\\n\\nif __name__ == \"__main__\":\\n    from pprint import pprint as pp\\n    lo = datetime.date(2008, 12, 27)\\n    hi = datetime.date(2009, 1, 5)\\n    pp(list(daterange(lo, hi)))\\n    pp(list(daterange(hi, lo, -1)))\\n    pp(list(daterange(lo, hi, 7)))\\n    pp(list(daterange(hi, lo, -7))) \\n    assert not list(daterange(lo, hi, -1))\\n    assert not list(daterange(hi, lo))\\n    assert not list(daterange(lo, hi, -7))\\n    assert not list(daterange(hi, lo, 7)) \\n</code>', '<code>arange</code>', '<code>import numpy as np\\nfrom datetime import datetime, timedelta\\nd0 = datetime(2009, 1,1)\\nd1 = datetime(2010, 1,1)\\ndt = timedelta(days = 1)\\ndates = np.arange(d0, d1, dt).astype(datetime)\\n</code>', '<code>astype</code>', '<code>numpy.datetime64</code>', '<code>datetime.datetime</code>', '<code>dates = np.arange(d0, d1, dt).astype(datetime.datetime)</code>', '<code>import datetime\\n\\ndef daterange(start, end, step=datetime.timedelta(1)):\\n    curr = start\\n    while curr &lt; end:\\n        yield curr\\n        curr += step\\n</code>', '<code>for i in range(16):\\n    print datetime.date.today() + datetime.timedelta(days=i)\\n</code>', '<code>import calendar\\nfrom datetime import datetime, timedelta\\n\\ndef days_in_month(dt):\\n    return calendar.monthrange(dt.year, dt.month)[1]\\n\\ndef monthly_range(dt_start, dt_end):\\n    forward = dt_end &gt;= dt_start\\n    finish = False\\n    dt = dt_start\\n\\n    while not finish:\\n        yield dt.date()\\n        if forward:\\n            days = days_in_month(dt)\\n            dt = dt + timedelta(days=days)            \\n            finish = dt &gt; dt_end\\n        else:\\n            _tmp_dt = dt.replace(day=1) - timedelta(days=1)\\n            dt = (_tmp_dt.replace(day=dt.day))\\n            finish = dt &lt; dt_end\\n</code>', '<code>date_start = datetime(2016, 6, 1)\\ndate_end = datetime(2017, 1, 1)\\n\\nfor p in monthly_range(date_start, date_end):\\n    print(p)\\n</code>', '<code>2016-06-01\\n2016-07-01\\n2016-08-01\\n2016-09-01\\n2016-10-01\\n2016-11-01\\n2016-12-01\\n2017-01-01\\n</code>', '<code>date_start = datetime(2017, 1, 1)\\ndate_end = datetime(2016, 6, 1)\\n\\nfor p in monthly_range(date_start, date_end):\\n    print(p)\\n</code>', '<code>2017-01-01\\n2016-12-01\\n2016-11-01\\n2016-10-01\\n2016-09-01\\n2016-08-01\\n2016-07-01\\n2016-06-01\\n</code>', '<code>import datetime\\nfor i in range(0, 100):\\n    print((datetime.date.today() + datetime.timedelta(i)).isoformat())\\n</code>', '<code>2016-06-29\\n2016-06-30\\n2016-07-01\\n2016-07-02\\n2016-07-03\\n2016-07-04\\n</code>', '<code>print((datetime.date.today() + datetime.timedelta(i)).isoformat())</code>', '<code>for d in map( lambda x: startDate+datetime.timedelta(days=x), xrange( (stopDate-startDate).days ) ):\\n  # Do stuff here\\n</code>', '<code>for d in map( lambda x: startTime+x*stepTime, xrange( (stopTime-startTime).total_seconds() / stepTime.total_seconds() ) ):\\n  # Do stuff here\\n</code>', '<code>def total_seconds( td ):\\n  return float(td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\\n</code>', \"<code>import datetime\\nfrom datetime import timedelta\\n\\n\\nDATE_FORMAT = '%Y/%m/%d'\\n\\ndef daterange(start, end):\\n      def convert(date):\\n            try:\\n                  date = datetime.datetime.strptime(date, DATE_FORMAT)\\n                  return date.date()\\n            except TypeError:\\n                  return date\\n\\n      def get_date(n):\\n            return datetime.datetime.strftime(convert(start) + timedelta(days=n), DATE_FORMAT)\\n\\n      days = (convert(end) - convert(start)).days\\n      if days &lt;= 0:\\n            raise ValueError('The start date must be before the end date.')\\n      for n in range(0, days):\\n            yield get_date(n)\\n\\n\\nstart = '2014/12/1'\\nend = '2014/12/31'\\nprint list(daterange(start, end))\\n\\nstart_ = datetime.date.today()\\nend = '2015/12/1'\\nprint list(daterange(start, end))\\n</code>\", '<code>def count_timedelta(delta, step, seconds_in_interval):\\n    \"\"\"Helper function for iterate.  Finds the number of intervals in the timedelta.\"\"\"\\n    return int(delta.total_seconds() / (seconds_in_interval * step))\\n\\n\\ndef range_dt(start, end, step=1, interval=\\'day\\'):\\n    \"\"\"Iterate over datetimes or dates, similar to builtin range.\"\"\"\\n    intervals = functools.partial(count_timedelta, (end - start), step)\\n\\n    if interval == \\'week\\':\\n        for i in range(intervals(3600 * 24 * 7)):\\n            yield start + datetime.timedelta(weeks=i) * step\\n\\n    elif interval == \\'day\\':\\n        for i in range(intervals(3600 * 24)):\\n            yield start + datetime.timedelta(days=i) * step\\n\\n    elif interval == \\'hour\\':\\n        for i in range(intervals(3600)):\\n            yield start + datetime.timedelta(hours=i) * step\\n\\n    elif interval == \\'minute\\':\\n        for i in range(intervals(60)):\\n            yield start + datetime.timedelta(minutes=i) * step\\n\\n    elif interval == \\'second\\':\\n        for i in range(intervals(1)):\\n            yield start + datetime.timedelta(seconds=i) * step\\n\\n    elif interval == \\'millisecond\\':\\n        for i in range(intervals(1 / 1000)):\\n            yield start + datetime.timedelta(milliseconds=i) * step\\n\\n    elif interval == \\'microsecond\\':\\n        for i in range(intervals(1e-6)):\\n            yield start + datetime.timedelta(microseconds=i) * step\\n\\n    else:\\n        raise AttributeError(\"Interval must be \\'week\\', \\'day\\', \\'hour\\' \\'second\\', \\\\\\n            \\'microsecond\\' or \\'millisecond\\'.\")\\n</code>']",
         "title": "Iterating through a range of dates in Python",
         "_childDocuments_": [
            {
               "up_vote_count": 336,
               "answer_id": 1060330,
               "last_activity_date": 1433327718,
               "path": "3.stack.answer",
               "body_markdown": "Why are there two nested iterations? For me it produces the same list of data with only one iteration:\r\n\r\n    for single_date in (start_date + timedelta(n) for n in range(day_count)):\r\n        print ...\r\n\r\nAnd no list gets stored, only one generator is iterated over. Also the &quot;if&quot; in the generator seems to be unnecessary.\r\n\r\nAfter all, a linear sequence should only require one iterator, not two.\r\n\r\n## Update after discussion with John Machin: ##\r\n\r\nMaybe the most elegant solution is using a generator function to completely hide/abstract the iteration over the range of dates:\r\n\r\n    from datetime import timedelta, date\r\n    \r\n    def daterange(start_date, end_date):\r\n        for n in range(int ((end_date - start_date).days)):\r\n            yield start_date + timedelta(n)\r\n    \r\n    start_date = date(2013, 1, 1)\r\n    end_date = date(2015, 6, 2)\r\n    for single_date in daterange(start_date, end_date):\r\n        print single_date.strftime(&quot;%Y-%m-%d&quot;)\r\n\r\nNB: For consistency with the built-in `range()` function this iteration stops **before** reaching the `end_date`. So for inclusive iteration use the next day, as you would with `range()`.",
               "tags": [],
               "creation_date": 1246307234,
               "last_edit_date": 1433327718,
               "is_accepted": true,
               "id": "1060330",
               "down_vote_count": 6,
               "score": 330
            },
            {
               "up_vote_count": 139,
               "answer_id": 1060352,
               "last_activity_date": 1298495570,
               "path": "3.stack.answer",
               "body_markdown": "This might be more clear:\r\n\r\n  \r\n    \r\n    d = start_date\r\n    delta = datetime.timedelta(days=1)\r\n    while d &lt;= end_date:\r\n        print d.strftime(&quot;%Y-%m-%d&quot;)\r\n        d += delta",
               "tags": [],
               "creation_date": 1246307503,
               "last_edit_date": 1298495570,
               "is_accepted": false,
               "id": "1060352",
               "down_vote_count": 0,
               "score": 139
            },
            {
               "up_vote_count": 12,
               "answer_id": 1060376,
               "last_activity_date": 1246311132,
               "path": "3.stack.answer",
               "body_markdown": "    import datetime\r\n\r\n    def daterange(start, stop, step=datetime.timedelta(days=1), inclusive=False):\r\n      # inclusive=False to behave like range by default\r\n      if step.days &gt; 0:\r\n        while start &lt; stop:\r\n          yield start\r\n          start = start + step\r\n          # not +=! don&#39;t modify object passed in if it&#39;s mutable\r\n          # since this function is not restricted to\r\n          # only types from datetime module\r\n      elif step.days &lt; 0:\r\n        while start &gt; stop:\r\n          yield start\r\n          start = start + step\r\n      if inclusive and start == stop:\r\n        yield start\r\n    \r\n    # ...\r\n\r\n    for date in daterange(start_date, end_date, inclusive=True):\r\n      print strftime(&quot;%Y-%m-%d&quot;, date.timetuple())\r\n\r\nThis function does more than you strictly require, by supporting negative step, etc. As long as you factor out your range logic, then you don&#39;t need the separate `day_count` and most importantly the code becomes easier to read as you call the function from multiple places.",
               "tags": [],
               "creation_date": 1246307713,
               "last_edit_date": 1246311132,
               "is_accepted": false,
               "id": "1060376",
               "down_vote_count": 0,
               "score": 12
            },
            {
               "up_vote_count": 122,
               "answer_id": 1061779,
               "last_activity_date": 1347300936,
               "path": "3.stack.answer",
               "body_markdown": "Use the [`dateutil`][1] library:\r\n\r\n    from datetime import date\r\n    from dateutil.rrule import rrule, DAILY\r\n\r\n    a = date(2009, 5, 30)\r\n    b = date(2009, 6, 9)\r\n\r\n    for dt in rrule(DAILY, dtstart=a, until=b):\r\n        print dt.strftime(&quot;%Y-%m-%d&quot;)\r\n\r\nThis python library has many more advanced features, some very useful, like `relative delta`s\u2014and is implemented as a single file (module) that&#39;s easily included into a project.\r\n\r\n  [1]: http://labix.org/python-dateutil",
               "tags": [],
               "creation_date": 1246337377,
               "last_edit_date": 1347300936,
               "is_accepted": false,
               "id": "1061779",
               "down_vote_count": 1,
               "score": 121
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 1063240,
               "is_accepted": false,
               "last_activity_date": 1246362551,
               "body_markdown": "    import datetime\r\n    \r\n    def daterange(start, stop, step_days=1):\r\n        current = start\r\n        step = datetime.timedelta(step_days)\r\n        if step_days &gt; 0:\r\n            while current &lt; stop:\r\n                yield current\r\n                current += step\r\n        elif step_days &lt; 0:\r\n            while current &gt; stop:\r\n                yield current\r\n                current += step\r\n        else:\r\n            raise ValueError(&quot;daterange() step_days argument must not be zero&quot;)\r\n            \r\n    if __name__ == &quot;__main__&quot;:\r\n        from pprint import pprint as pp\r\n        lo = datetime.date(2008, 12, 27)\r\n        hi = datetime.date(2009, 1, 5)\r\n        pp(list(daterange(lo, hi)))\r\n        pp(list(daterange(hi, lo, -1)))\r\n        pp(list(daterange(lo, hi, 7)))\r\n        pp(list(daterange(hi, lo, -7))) \r\n        assert not list(daterange(lo, hi, -1))\r\n        assert not list(daterange(hi, lo))\r\n        assert not list(daterange(lo, hi, -7))\r\n        assert not list(daterange(hi, lo, 7)) \r\n\r\n",
               "id": "1063240",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1246362551,
               "score": 4
            },
            {
               "up_vote_count": 2,
               "answer_id": 3059362,
               "last_activity_date": 1347300250,
               "path": "3.stack.answer",
               "body_markdown": "&lt;!-- language: lang-py --&gt;\r\n\r\n    for i in range(16):\r\n        print datetime.date.today() + datetime.timedelta(days=i)",
               "tags": [],
               "creation_date": 1276756009,
               "last_edit_date": 1347300250,
               "is_accepted": false,
               "id": "3059362",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 0,
               "answer_id": 6673470,
               "last_activity_date": 1310528540,
               "path": "3.stack.answer",
               "body_markdown": "What about the following for doing a range incremented by days:\r\n\r\n\r\n    for d in map( lambda x: startDate+datetime.timedelta(days=x), xrange( (stopDate-startDate).days ) ):\r\n      # Do stuff here\r\n\r\n - startDate and stopDate are datetime.date objects\r\n\r\nFor a generic version:\r\n\r\n    for d in map( lambda x: startTime+x*stepTime, xrange( (stopTime-startTime).total_seconds() / stepTime.total_seconds() ) ):\r\n      # Do stuff here\r\n\r\n - startTime and stopTime are datetime.date or datetime.datetime object\r\n   (both should be the same type)\r\n - stepTime is a timedelta object\r\n\r\n\r\nNote that .total_seconds() is only supported after python 2.7 If you are stuck with an earlier version you can write your own function:\r\n\r\n    def total_seconds( td ):\r\n      return float(td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\r\n\r\n",
               "tags": [],
               "creation_date": 1310524514,
               "last_edit_date": 1310528540,
               "is_accepted": false,
               "id": "6673470",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 9,
               "answer_id": 15969361,
               "last_activity_date": 1400852124,
               "path": "3.stack.answer",
               "body_markdown": "Why not try:\r\n\r\n    import datetime as dt\r\n    \r\n    start_date = dt.datetime(2012, 12,1)\r\n    end_date = dt.datetime(2012, 12,5)\r\n    \r\n    total_days = (end_date - start_date).days + 1 #inclusive 5 days\r\n    \r\n    for day_number in range(total_days):\r\n        current_date = (start_date + dt.timedelta(days = day_number)).date()\r\n        print current_date",
               "tags": [],
               "creation_date": 1365763631,
               "last_edit_date": 1400852124,
               "is_accepted": false,
               "id": "15969361",
               "down_vote_count": 0,
               "score": 9
            },
            {
               "up_vote_count": 31,
               "answer_id": 23853523,
               "last_activity_date": 1406099840,
               "path": "3.stack.answer",
               "body_markdown": "Pandas is great for time series in general, and has direct support for date ranges.\r\n\r\n    import pandas as pd\r\n    daterange = pd.date_range(start_date, end_date)\r\n\r\nYou can then loop over the daterange to print the date:\r\n\r\n    for single_date in daterange:\r\n        print (single_date.strftime(&quot;%Y-%m-%d&quot;))\r\n\r\nIt also has lots of options to make life easier. For example if you only wanted weekdays, you would just swap in bdate_range. See http://pandas.pydata.org/pandas-docs/stable/timeseries.html#generating-ranges-of-timestamps\r\n\r\nThe power of Pandas is really its dataframes, which support vectorized operations (much like numpy) that make operations across large quantities of data very fast and easy.\r\n\r\nEDIT:\r\nYou could also completely skip the for loop and just print it directly, which is easier and more efficient:\r\n\r\n    print(daterange)\r\n",
               "tags": [],
               "creation_date": 1401007486,
               "last_edit_date": 1406099840,
               "is_accepted": false,
               "id": "23853523",
               "down_vote_count": 0,
               "score": 31
            },
            {
               "up_vote_count": 0,
               "answer_id": 24977763,
               "last_activity_date": 1406433712,
               "path": "3.stack.answer",
               "body_markdown": "This function has some extra features:\r\n\r\n - can pass a string matching the DATE_FORMAT for start or end and it is converted to a date object\r\n - can pass a date object for start or end\r\n - error checking in case the end is older than the start\r\n\r\n        \r\n        import datetime\r\n        from datetime import timedelta\r\n        \r\n        \r\n        DATE_FORMAT = &#39;%Y/%m/%d&#39;\r\n        \r\n        def daterange(start, end):\r\n        \t  def convert(date):\r\n        \t\t    try:\r\n        \t\t\t      date = datetime.datetime.strptime(date, DATE_FORMAT)\r\n        \t\t\t      return date.date()\r\n        \t\t    except TypeError:\r\n        \t\t\t      return date\r\n        \t\t\t\r\n        \t  def get_date(n):\r\n        \t\t    return datetime.datetime.strftime(convert(start) + timedelta(days=n), DATE_FORMAT)\r\n        \t\r\n        \t  days = (convert(end) - convert(start)).days\r\n        \t  if days &lt;= 0:\r\n        \t\t    raise ValueError(&#39;The start date must be before the end date.&#39;)\r\n        \t  for n in range(0, days):\r\n        \t\t    yield get_date(n)\r\n        \t\r\n        \t\r\n        start = &#39;2014/12/1&#39;\r\n        end = &#39;2014/12/31&#39;\r\n        print list(daterange(start, end))\r\n        \r\n        start_ = datetime.date.today()\r\n        end = &#39;2015/12/1&#39;\r\n        print list(daterange(start, end))\r\n",
               "tags": [],
               "creation_date": 1406433335,
               "last_edit_date": 1406433712,
               "is_accepted": false,
               "id": "24977763",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 1,
               "answer_id": 36831624,
               "last_activity_date": 1516577227,
               "path": "3.stack.answer",
               "body_markdown": "**Show the last n days from today:**\r\n\r\n    import datetime\r\n    for i in range(0, 100):\r\n\t    print((datetime.date.today() + datetime.timedelta(i)).isoformat())\r\n\r\n\r\n**Output:** \r\n\r\n\t2016-06-29\r\n\t2016-06-30\r\n\t2016-07-01\r\n\t2016-07-02\r\n\t2016-07-03\r\n\t2016-07-04\r\n\t\r\n",
               "tags": [],
               "creation_date": 1461555261,
               "last_edit_date": 1516577227,
               "is_accepted": false,
               "id": "36831624",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 3,
               "answer_id": 37508911,
               "last_activity_date": 1464519207,
               "path": "3.stack.answer",
               "body_markdown": "Numpy&#39;s `arange` function can be applied to dates:\r\n\r\n    import numpy as np\r\n    from datetime import datetime, timedelta\r\n    d0 = datetime(2009, 1,1)\r\n    d1 = datetime(2010, 1,1)\r\n    dt = timedelta(days = 1)\r\n    dates = np.arange(d0, d1, dt).astype(datetime)\r\n\r\nThe use of `astype` is to convert from `numpy.datetime64` to an array of `datetime.datetime` objects.\r\n",
               "tags": [],
               "creation_date": 1464518500,
               "last_edit_date": 1464519207,
               "is_accepted": false,
               "id": "37508911",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 38402483,
               "is_accepted": false,
               "last_activity_date": 1468605567,
               "body_markdown": "Here&#39;s code for a general date range function, similar to Ber&#39;s answer, but more flexible:\r\n\r\n    def count_timedelta(delta, step, seconds_in_interval):\r\n        &quot;&quot;&quot;Helper function for iterate.  Finds the number of intervals in the timedelta.&quot;&quot;&quot;\r\n        return int(delta.total_seconds() / (seconds_in_interval * step))\r\n    \r\n    \r\n    def range_dt(start, end, step=1, interval=&#39;day&#39;):\r\n        &quot;&quot;&quot;Iterate over datetimes or dates, similar to builtin range.&quot;&quot;&quot;\r\n        intervals = functools.partial(count_timedelta, (end - start), step)\r\n    \r\n        if interval == &#39;week&#39;:\r\n            for i in range(intervals(3600 * 24 * 7)):\r\n                yield start + datetime.timedelta(weeks=i) * step\r\n    \r\n        elif interval == &#39;day&#39;:\r\n            for i in range(intervals(3600 * 24)):\r\n                yield start + datetime.timedelta(days=i) * step\r\n    \r\n        elif interval == &#39;hour&#39;:\r\n            for i in range(intervals(3600)):\r\n                yield start + datetime.timedelta(hours=i) * step\r\n    \r\n        elif interval == &#39;minute&#39;:\r\n            for i in range(intervals(60)):\r\n                yield start + datetime.timedelta(minutes=i) * step\r\n    \r\n        elif interval == &#39;second&#39;:\r\n            for i in range(intervals(1)):\r\n                yield start + datetime.timedelta(seconds=i) * step\r\n    \r\n        elif interval == &#39;millisecond&#39;:\r\n            for i in range(intervals(1 / 1000)):\r\n                yield start + datetime.timedelta(milliseconds=i) * step\r\n    \r\n        elif interval == &#39;microsecond&#39;:\r\n            for i in range(intervals(1e-6)):\r\n                yield start + datetime.timedelta(microseconds=i) * step\r\n    \r\n        else:\r\n            raise AttributeError(&quot;Interval must be &#39;week&#39;, &#39;day&#39;, &#39;hour&#39; &#39;second&#39;, \\\r\n                &#39;microsecond&#39; or &#39;millisecond&#39;.&quot;)\r\n\r\n",
               "id": "38402483",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1468605567,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 40023824,
               "is_accepted": false,
               "last_activity_date": 1476368888,
               "body_markdown": "This is the most human-readable solution I can think of.\r\n\r\n    import datetime\r\n\r\n    def daterange(start, end, step=datetime.timedelta(1)):\r\n        curr = start\r\n        while curr &lt; end:\r\n            yield curr\r\n            curr += step",
               "id": "40023824",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1476368888,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 41822494,
               "is_accepted": false,
               "last_activity_date": 1485243257,
               "body_markdown": "I have a similar problem, but I need to iterate monthly instead of daily.\r\n\r\nThis is my solution\r\n\r\n    import calendar\r\n    from datetime import datetime, timedelta\r\n\r\n    def days_in_month(dt):\r\n        return calendar.monthrange(dt.year, dt.month)[1]\r\n\r\n    def monthly_range(dt_start, dt_end):\r\n        forward = dt_end &gt;= dt_start\r\n        finish = False\r\n        dt = dt_start\r\n    \r\n        while not finish:\r\n            yield dt.date()\r\n            if forward:\r\n                days = days_in_month(dt)\r\n                dt = dt + timedelta(days=days)            \r\n                finish = dt &gt; dt_end\r\n            else:\r\n                _tmp_dt = dt.replace(day=1) - timedelta(days=1)\r\n                dt = (_tmp_dt.replace(day=dt.day))\r\n                finish = dt &lt; dt_end\r\n\r\n\r\n**Example #1**\r\n\r\n    date_start = datetime(2016, 6, 1)\r\n    date_end = datetime(2017, 1, 1)\r\n\r\n    for p in monthly_range(date_start, date_end):\r\n        print(p)\r\n\r\n**Output**\r\n\r\n    2016-06-01\r\n    2016-07-01\r\n    2016-08-01\r\n    2016-09-01\r\n    2016-10-01\r\n    2016-11-01\r\n    2016-12-01\r\n    2017-01-01\r\n\r\n**Example #2**\r\n\r\n    date_start = datetime(2017, 1, 1)\r\n    date_end = datetime(2016, 6, 1)\r\n\r\n    for p in monthly_range(date_start, date_end):\r\n        print(p)\r\n\r\n**Output**\r\n\r\n    2017-01-01\r\n    2016-12-01\r\n    2016-11-01\r\n    2016-10-01\r\n    2016-09-01\r\n    2016-08-01\r\n    2016-07-01\r\n    2016-06-01\r\n",
               "id": "41822494",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1485243257,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python",
         "id": "858127-2260"
      },
      {
         "up_vote_count": "653",
         "path": "2.stack",
         "body_markdown": "I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it&#39;s out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.\r\n\r\nOne day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  I&#39;m not talking about &quot;big data&quot; that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.\r\n\r\nMy first thought is to use `HDFStore` to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:\r\n\r\nWhat are some best-practice workflows for accomplishing the following:\r\n\r\n 1. Loading flat files into a permanent, on-disk database structure\r\n 2. Querying that database to retrieve data to feed into a pandas data structure\r\n 3. Updating the database after manipulating pieces in pandas\r\n\r\nReal-world examples would be much appreciated, especially from anyone who uses pandas on &quot;large data&quot;.\r\n\r\nEdit -- an example of how I would like this to work:\r\n\r\n 1. Iteratively import a large flat-file and store it in a permanent, on-disk database structure.  These files are typically too large to fit in memory.\r\n 2. In order to use Pandas, I would like to read subsets of this data (usually just a few columns at a time) that can fit in memory.\r\n 3. I would create new columns by performing various operations on the selected columns.\r\n 4. I would then have to append these new columns into the database structure.\r\n\r\nI am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.\r\n\r\nEdit -- Responding to Jeff&#39;s questions specifically:\r\n\r\n 1. I am building consumer credit risk models. The kinds of data include phone, SSN and address characteristics; property values; derogatory information like criminal records, bankruptcies, etc... The datasets I use every day have nearly 1,000 to 2,000 fields on average of mixed data types: continuous, nominal and ordinal variables of both numeric and character data.  I rarely append rows, but I do perform many operations that create new columns.\r\n 2. Typical operations involve combining several columns using conditional logic into a new, compound column. For example, `if var1 &gt; 2 then newvar = &#39;A&#39; elif var2 = 4 then newvar = &#39;B&#39;`.  The result of these operations is a new column for every record in my dataset.\r\n 3. Finally, I would like to append these new columns into the on-disk data structure.  I would repeat step 2, exploring the data with crosstabs and descriptive statistics trying to find interesting, intuitive relationships to model.\r\n 4. A typical project file is usually about 1GB.  Files are organized into such a manner where a row consists of a record of consumer data.  Each row has the same number of columns for every record.  This will always be the case.\r\n 5. It&#39;s pretty rare that I would subset by rows when creating a new column.  However, it&#39;s pretty common for me to subset on rows when creating reports or generating descriptive statistics.  For example, I might want to create a simple frequency for a specific line of business, say Retail credit cards.  To do this, I would select only those records where the line of business = retail in addition to whichever columns I want to report on.  When creating new columns, however, I would pull all rows of data and only the columns I need for the operations.\r\n 6. The modeling process requires that I analyze every column, look for interesting relationships with some outcome variable, and create new compound columns that describe those relationships.  The columns that I explore are usually done in small sets.  For example, I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan.  Once those are explored and new columns are created, I then move on to another group of columns, say college education, and repeat the process.  What I&#39;m doing is creating candidate variables that explain the relationship between my data and some outcome.  At the very end of this process, I apply some learning techniques that create an equation out of those compound columns.\r\n\r\nIt is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).\r\n",
         "view_count": "193259",
         "answer_count": "12",
         "tags": "['python', 'mongodb', 'pandas', 'hdf5', 'large-data']",
         "creation_date": "1357834832",
         "last_edit_date": "1506102294",
         "code_snippet": "['<code>HDFStore</code>', \"<code>if var1 &gt; 2 then newvar = 'A' elif var2 = 4 then newvar = 'B'</code>\", '<code>0.10.1</code>', \"<code>import numpy as np\\nimport pandas as pd\\n\\n# create a store\\nstore = pd.HDFStore('mystore.h5')\\n\\n# this is the key to your storage:\\n#    this maps your fields to a specific group, and defines \\n#    what you want to have as data_columns.\\n#    you might want to create a nice class wrapping this\\n#    (as you will want to have this map and its inversion)  \\ngroup_map = dict(\\n    A = dict(fields = ['field_1','field_2',.....], dc = ['field_1',....,'field_5']),\\n    B = dict(fields = ['field_10',......        ], dc = ['field_10']),\\n    .....\\n    REPORTING_ONLY = dict(fields = ['field_1000','field_1001',...], dc = []),\\n\\n)\\n\\ngroup_map_inverted = dict()\\nfor g, v in group_map.items():\\n    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))\\n</code>\", '<code>append_to_multiple</code>', \"<code>for f in files:\\n   # read in the file, additional options hmay be necessary here\\n   # the chunksize is not strictly necessary, you may be able to slurp each \\n   # file into memory in which case just eliminate this part of the loop \\n   # (you can also change chunksize if necessary)\\n   for chunk in pd.read_table(f, chunksize=50000):\\n       # we are going to append to each table by group\\n       # we are not going to create indexes at this time\\n       # but we *ARE* going to create (some) data_columns\\n\\n       # figure out the field groupings\\n       for g, v in group_map.items():\\n             # create the frame for this group\\n             frame = chunk.reindex(columns = v['fields'], copy = False)    \\n\\n             # append it\\n             store.append(g, frame, index=False, data_columns = v['dc'])\\n</code>\", \"<code>frame = store.select(group_that_I_want)\\n# you can optionally specify:\\n# columns = a list of the columns IN THAT GROUP (if you wanted to\\n#     select only say 3 out of the 20 columns in this sub-table)\\n# and a where clause if you want a subset of the rows\\n\\n# do calculations on this frame\\nnew_frame = cool_function_on_frame(frame)\\n\\n# to 'add columns', create a new group (you probably want to\\n# limit the columns in this new_group to be only NEW ones\\n# (e.g. so you don't overlap from the other tables)\\n# add this info to the group_map\\nstore.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)\\n</code>\", \"<code># This may be a bit tricky; and depends what you are actually doing.\\n# I may need to modify this function to be a bit more general:\\nreport_data = store.select_as_multiple([groups_1,groups_2,.....], where =['field_1&gt;0', 'field_1000=foo'], selector = group_1)\\n</code>\", \"<code>store.select(group, where = ['field_1000=foo', 'field_1001&gt;0'])\\n</code>\", '<code>dict</code>', '<code>chunksize</code>', '<code>read_csv</code>', '<code>aCollection.insert((a[1].to_dict() for a in df.iterrows()))\\n</code>', \"<code>pd.DataFrame(list(mongoCollection.find({'anAttribute':{'$gt':2887000, '$lt':2889000}})))\\n</code>\", '<code>.find()</code>', '<code>ichunked</code>', \"<code>aJoinDF = pandas.DataFrame(list(mongoCollection.find({'anAttribute':{'$in':Att_Keys}})))\\n</code>\", '<code>aJoinDF</code>', \"<code>df = pandas.merge(df, aJoinDF, on=aKey, how='left')\\n</code>\", '<code>collection.update({primarykey:foo},{key:change})\\n</code>', '<code>dict</code>', '<code>In [96]: test.insert((a[1].to_dict() for a in df.iterrows())) --------------- InvalidDocument: Cannot encode object: 0</code>', '<code>def transpose_table(h_in, table_path, h_out, group_name=\"data\", group_path=\"/\"):\\n    # Get a reference to the input data.\\n    tb = h_in.getNode(table_path)\\n    # Create the output group to hold the columns.\\n    grp = h_out.createGroup(group_path, group_name, filters=tables.Filters(complevel=1))\\n    for col_name in tb.colnames:\\n        logger.debug(\"Processing %s\", col_name)\\n        # Get the data.\\n        col_data = tb.col(col_name)\\n        # Create the output array.\\n        arr = h_out.createCArray(grp,\\n                                 col_name,\\n                                 tables.Atom.from_dtype(col_data.dtype),\\n                                 col_data.shape)\\n        # Store the data.\\n        arr[:] = col_data\\n    h_out.flush()\\n</code>', '<code>def read_hdf5(hdf5_path, group_path=\"/data\", columns=None):\\n    \"\"\"Read a transposed data set from a HDF5 file.\"\"\"\\n    if isinstance(hdf5_path, tables.file.File):\\n        hf = hdf5_path\\n    else:\\n        hf = tables.openFile(hdf5_path)\\n\\n    grp = hf.getNode(group_path)\\n    if columns is None:\\n        data = [(child.name, child[:]) for child in grp]\\n    else:\\n        data = [(child.name, child[:]) for child in grp if child.name in columns]\\n\\n    # Convert any float32 columns to float64 for processing.\\n    for i in range(len(data)):\\n        name, vec = data[i]\\n        if vec.dtype == np.float32:\\n            data[i] = (name, vec.astype(np.float64))\\n\\n    if not isinstance(hdf5_path, tables.file.File):\\n        hf.close()\\n    return pd.DataFrame.from_items(data)\\n</code>', \"<code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(int(1e8), 5))\\n&gt;&gt;&gt; df.info()\\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nRangeIndex: 100000000 entries, 0 to 99999999\\nData columns (total 5 columns):\\n...\\ndtypes: float64(5)\\nmemory usage: 3.7 GB\\n\\n&gt;&gt;&gt; df.astype(np.float32).info()\\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nRangeIndex: 100000000 entries, 0 to 99999999\\nData columns (total 5 columns):\\n...\\ndtypes: float32(5)\\nmemory usage: 1.9 GB\\n</code>\", \"<code>import dask.dataframe as dd\\ndf = dd.read_csv('2015-*-*.csv')\\ndf.groupby(df.user_id).value.mean().compute()\\n</code>\", \"<code>import pandas as pd\\ndf = pd.read_csv('2015-01-01.csv')\\ndf.groupby(df.user_id).value.mean()\\n</code>\", \"<code>from dask.distributed import Client\\nclient = Client('scheduler:port')\\n\\nfutures = []\\nfor fn in filenames:\\n    future = client.submit(load, fn)\\n    futures.append(future)\\n\\nsummary = client.submit(summarize, futures)\\nsummary.result()\\n</code>\", '<code>def addDateColumn():\\n\"\"\"Adds time to the daily rainfall data. Reads the csv as chunks of 100k \\n   rows at a time and outputs them, appending as needed, to a single csv. \\n   Uses the column of the raster names to get the date.\\n\"\"\"\\n    df = pd.read_csv(pathlist[1]+\"CHIRPS_tanz.csv\", iterator=True, \\n                     chunksize=100000) #read csv file as 100k chunks\\n\\n    \\'\\'\\'Do some stuff\\'\\'\\'\\n\\n    count = 1 #for indexing item in time list \\n    for chunk in df: #for each 100k rows\\n        newtime = [] #empty list to append repeating times for different rows\\n        toiterate = chunk[chunk.columns[2]] #ID of raster nums to base time\\n        while count &lt;= toiterate.max():\\n            for i in toiterate: \\n                if i ==count:\\n                    newtime.append(newyears[count])\\n            count+=1\\n        print \"Finished\", str(chunknum), \"chunks\"\\n        chunk[\"time\"] = newtime #create new column in dataframe based on time\\n        outname = \"CHIRPS_tanz_time2.csv\"\\n        #append each output to same csv, using no header\\n        chunk.to_csv(pathlist[2]+outname, mode=\\'a\\', header=None, index=None)\\n</code>']",
         "title": "&quot;Large data&quot; work flows using pandas",
         "_childDocuments_": [
            {
               "up_vote_count": 418,
               "answer_id": 14268804,
               "last_activity_date": 1471485106,
               "path": "3.stack.answer",
               "body_markdown": "I routinely use tens of gigabytes of data in just this fashion\r\ne.g. I have tables on disk that I read via queries, create data and append back.\r\n\r\nIt&#39;s worth reading [the docs](http://pandas-docs.github.io/pandas-docs-travis/io.html#hdf5-pytables) and [late in this thread](https://groups.google.com/forum/m/?fromgroups#!topic/pydata/cmw1F3OFJSc) for several suggestions for how to store your data.\r\n\r\nDetails which will affect how you store your data, like:  \r\n*Give as much detail as you can; and I can help you develop a structure.*\r\n\r\n1. Size of data, # of rows, columns, types of columns; are you appending\r\n   rows, or just columns? \r\n2. What will typical operations look like. E.g. do a query on columns to select a bunch of rows and specific columns, then do an operation (in-memory), create new columns, save these.  \r\n(Giving a toy example could enable us to offer more specific recommendations.)\r\n3. After that processing, then what do you do? Is step 2 ad hoc, or repeatable?\r\n4. Input flat files: how many, rough total size in Gb. How are these organized e.g. by records? Does each one contains different fields, or do they have some records per file with all of the fields in each file?\r\n5. Do you ever select subsets of rows (records) based on criteria (e.g. select the rows with field A &gt; 5)? and then do something, or do you just select fields A, B, C with all of the records (and then do something)?\r\n6. Do you &#39;work on&#39; all of your columns (in groups), or are there a good proportion that you may only use for reports (e.g. you want to keep the data around, but don&#39;t need to pull in that column explicity until final results time)?\r\n\r\nSolution\r\n--------\r\n\r\n*Ensure you have [pandas at least `0.10.1`](http://pandas.pydata.org/getpandas.html) installed.*\r\n\r\nRead [iterating files chunk-by-chunk](http://pandas-docs.github.io/pandas-docs-travis/io.html#iterating-through-files-chunk-by-chunk) and [multiple table queries](http://pandas-docs.github.io/pandas-docs-travis/io.html#multiple-table-queries).\r\n\r\nSince pytables is optimized to operate on row-wise (which is what you query on), we will create a table for each group of fields. This way it&#39;s easy to select a small group of fields (which will work with a big table, but it&#39;s more efficient to do it this way... I think I may be able to fix this limitation in the future... this is more intuitive anyhow):  \r\n(The following is pseudocode.)\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n\r\n    # create a store\r\n    store = pd.HDFStore(&#39;mystore.h5&#39;)\r\n\r\n    # this is the key to your storage:\r\n    #    this maps your fields to a specific group, and defines \r\n    #    what you want to have as data_columns.\r\n    #    you might want to create a nice class wrapping this\r\n    #    (as you will want to have this map and its inversion)  \r\n    group_map = dict(\r\n        A = dict(fields = [&#39;field_1&#39;,&#39;field_2&#39;,.....], dc = [&#39;field_1&#39;,....,&#39;field_5&#39;]),\r\n        B = dict(fields = [&#39;field_10&#39;,......        ], dc = [&#39;field_10&#39;]),\r\n        .....\r\n        REPORTING_ONLY = dict(fields = [&#39;field_1000&#39;,&#39;field_1001&#39;,...], dc = []),\r\n\r\n    )\r\n\r\n    group_map_inverted = dict()\r\n    for g, v in group_map.items():\r\n        group_map_inverted.update(dict([ (f,g) for f in v[&#39;fields&#39;] ]))\r\n    \r\n\r\nReading in the files and creating the storage (essentially doing what `append_to_multiple` does):\r\n\r\n    for f in files:\r\n       # read in the file, additional options hmay be necessary here\r\n       # the chunksize is not strictly necessary, you may be able to slurp each \r\n       # file into memory in which case just eliminate this part of the loop \r\n       # (you can also change chunksize if necessary)\r\n       for chunk in pd.read_table(f, chunksize=50000):\r\n           # we are going to append to each table by group\r\n           # we are not going to create indexes at this time\r\n           # but we *ARE* going to create (some) data_columns\r\n\r\n           # figure out the field groupings\r\n           for g, v in group_map.items():\r\n                 # create the frame for this group\r\n                 frame = chunk.reindex(columns = v[&#39;fields&#39;], copy = False)    \r\n                 \r\n                 # append it\r\n                 store.append(g, frame, index=False, data_columns = v[&#39;dc&#39;])\r\n\r\nNow you have all of the tables in the file (actually you could store them in separate files if you wish, you would prob have to add the filename to the group_map, but probably this isn&#39;t necessary).\r\n\r\nThis is how you get columns and create new ones:\r\n\r\n    frame = store.select(group_that_I_want)\r\n    # you can optionally specify:\r\n    # columns = a list of the columns IN THAT GROUP (if you wanted to\r\n    #     select only say 3 out of the 20 columns in this sub-table)\r\n    # and a where clause if you want a subset of the rows\r\n\r\n    # do calculations on this frame\r\n    new_frame = cool_function_on_frame(frame)\r\n\r\n    # to &#39;add columns&#39;, create a new group (you probably want to\r\n    # limit the columns in this new_group to be only NEW ones\r\n    # (e.g. so you don&#39;t overlap from the other tables)\r\n    # add this info to the group_map\r\n    store.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)\r\n\r\nWhen you are ready for post_processing:\r\n\r\n    # This may be a bit tricky; and depends what you are actually doing.\r\n    # I may need to modify this function to be a bit more general:\r\n    report_data = store.select_as_multiple([groups_1,groups_2,.....], where =[&#39;field_1&gt;0&#39;, &#39;field_1000=foo&#39;], selector = group_1)\r\n\r\nAbout data_columns, you don&#39;t actually need to define **ANY** data_columns; they allow you to sub-select rows based on the column. E.g. something like:\r\n\r\n    store.select(group, where = [&#39;field_1000=foo&#39;, &#39;field_1001&gt;0&#39;])\r\n\r\nThey may be most interesting to you in the final report generation stage (essentially a data column is segregated from other columns, which might impact efficiency somewhat if you define a lot).\r\n\r\nYou also might want to:\r\n\r\n- create a function which takes a list of fields, looks up the groups in the groups_map, then selects these and concatenates the results so you get the resulting frame (this is essentially what select_as_multiple does). *This way the structure would be pretty transparent to you.*\r\n- indexes on certain data columns (makes row-subsetting much faster).\r\n- enable compression.\r\n\r\nLet me know when you have questions!\r\n",
               "tags": [],
               "creation_date": 1357858642,
               "last_edit_date": 1471485106,
               "is_accepted": true,
               "id": "14268804",
               "down_vote_count": 0,
               "score": 418
            },
            {
               "up_vote_count": 44,
               "answer_id": 14287518,
               "last_activity_date": 1408024241,
               "path": "3.stack.answer",
               "body_markdown": "This is the case for pymongo.  I have also prototyped using sql server, sqlite, HDF, ORM (SQLAlchemy) in python.  First and foremost pymongo is a document based DB, so each person would be a document (`dict` of attributes).  Many people form a collection and you can have many collections (people, stock market, income).\r\n\r\npd.dateframe -&gt; pymongo Note: I use the `chunksize` in `read_csv` to keep it to 5 to 10k records(pymongo drops the socket if larger)\r\n\r\n    aCollection.insert((a[1].to_dict() for a in df.iterrows()))\r\n\r\nquerying: gt = greater than...\r\n\r\n    pd.DataFrame(list(mongoCollection.find({&#39;anAttribute&#39;:{&#39;$gt&#39;:2887000, &#39;$lt&#39;:2889000}})))\r\n\r\n`.find()` returns an iterator so I commonly use `ichunked` to chop into smaller iterators.  \r\n\r\nHow about a join since I normally get 10 data sources to paste together:\r\n\r\n    aJoinDF = pandas.DataFrame(list(mongoCollection.find({&#39;anAttribute&#39;:{&#39;$in&#39;:Att_Keys}})))\r\n\r\nthen (in my case sometimes I have to agg on `aJoinDF` first before its &quot;mergeable&quot;.)\r\n\r\n    df = pandas.merge(df, aJoinDF, on=aKey, how=&#39;left&#39;)\r\n\r\nAnd you can then write the new info to your main collection via the update method below. (logical collection vs physical datasources).\r\n\r\n    collection.update({primarykey:foo},{key:change})\r\n\r\nOn smaller lookups, just denormalize.  For example, you have code in the document and you just add the field code text and do a `dict` lookup as you create documents.\r\n\r\nNow you have a nice dataset based around a person, you can unleash your logic on each case and make more attributes. Finally you can read into pandas your 3 to memory max key indicators and do pivots/agg/data exploration.  This works for me for 3 million records with numbers/big text/categories/codes/floats/...\r\n\r\nYou can also use the two methods built into MongoDB (MapReduce and aggregate framework). [See here for more info about the aggregate framework][1], as it seems to be easier than MapReduce and looks handy for quick aggregate work.  Notice I didn&#39;t need to define my fields or relations, and I can add items to a document.  At the current state of the rapidly changing numpy, pandas, python toolset, MongoDB helps me just get to work :)\r\n\r\n\r\n  [1]: http://docs.mongodb.org/manual/tutorial/aggregation-examples/",
               "tags": [],
               "creation_date": 1357942312,
               "last_edit_date": 1408024241,
               "is_accepted": false,
               "id": "14287518",
               "down_vote_count": 2,
               "score": 42
            },
            {
               "up_vote_count": 35,
               "answer_id": 15558350,
               "last_activity_date": 1363966687,
               "path": "3.stack.answer",
               "body_markdown": "I spotted this a little late, but I work with a similar problem (mortgage prepayment models). My solution has been to skip the pandas HDFStore layer and use straight pytables. I save each column as an individual HDF5 array in my final file.\r\n\r\nMy basic workflow is to first get a CSV file from the database. I gzip it, so it&#39;s not as huge. Then I convert that to a row-oriented HDF5 file, by iterating over it in python, converting each row to a real data type, and writing it to a HDF5 file. That takes some tens of minutes, but it doesn&#39;t use any memory, since it&#39;s only operating row-by-row. Then I &quot;transpose&quot; the row-oriented HDF5 file into a column-oriented HDF5 file.\r\n\r\nThe table transpose looks like:\r\n\r\n    def transpose_table(h_in, table_path, h_out, group_name=&quot;data&quot;, group_path=&quot;/&quot;):\r\n        # Get a reference to the input data.\r\n        tb = h_in.getNode(table_path)\r\n        # Create the output group to hold the columns.\r\n        grp = h_out.createGroup(group_path, group_name, filters=tables.Filters(complevel=1))\r\n        for col_name in tb.colnames:\r\n            logger.debug(&quot;Processing %s&quot;, col_name)\r\n            # Get the data.\r\n            col_data = tb.col(col_name)\r\n            # Create the output array.\r\n            arr = h_out.createCArray(grp,\r\n                                     col_name,\r\n                                     tables.Atom.from_dtype(col_data.dtype),\r\n                                     col_data.shape)\r\n            # Store the data.\r\n            arr[:] = col_data\r\n        h_out.flush()\r\n\r\nReading it back in then looks like:\r\n\r\n    def read_hdf5(hdf5_path, group_path=&quot;/data&quot;, columns=None):\r\n        &quot;&quot;&quot;Read a transposed data set from a HDF5 file.&quot;&quot;&quot;\r\n        if isinstance(hdf5_path, tables.file.File):\r\n            hf = hdf5_path\r\n        else:\r\n            hf = tables.openFile(hdf5_path)\r\n\r\n        grp = hf.getNode(group_path)\r\n        if columns is None:\r\n            data = [(child.name, child[:]) for child in grp]\r\n        else:\r\n            data = [(child.name, child[:]) for child in grp if child.name in columns]\r\n\r\n        # Convert any float32 columns to float64 for processing.\r\n        for i in range(len(data)):\r\n            name, vec = data[i]\r\n            if vec.dtype == np.float32:\r\n                data[i] = (name, vec.astype(np.float64))\r\n\r\n        if not isinstance(hdf5_path, tables.file.File):\r\n            hf.close()\r\n        return pd.DataFrame.from_items(data)\r\n\r\nNow, I generally run this on a machine with a ton of memory, so I may not be careful enough with my memory usage. For example, by default the load operation reads the whole data set.\r\n\r\nThis generally works for me, but it&#39;s a bit clunky, and I can&#39;t use the fancy pytables magic.\r\n\r\nEdit: The real advantage of this approach, over the array-of-records pytables default, is that I can then load the data into R using h5r, which can&#39;t handle tables. Or, at least, I&#39;ve been unable to get it to load heterogeneous tables.\r\n",
               "tags": [],
               "creation_date": 1363900770,
               "last_edit_date": 1363966687,
               "is_accepted": false,
               "id": "15558350",
               "down_vote_count": 0,
               "score": 35
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 53,
               "answer_id": 19739768,
               "is_accepted": false,
               "last_activity_date": 1383376447,
               "body_markdown": "If your datasets are between 1 and 20GB, you should get a workstation with 48GB of RAM. Then Pandas can hold the entire dataset in RAM. I know its not the answer you&#39;re looking for here, but doing scientific computing on a notebook with 4GB of RAM isn&#39;t reasonable.",
               "id": "19739768",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1383376447,
               "score": 50
            },
            {
               "up_vote_count": 89,
               "answer_id": 20690383,
               "last_activity_date": 1387812087,
               "path": "3.stack.answer",
               "body_markdown": "I think the answers above are missing a simple approach that I&#39;ve found very useful. \r\n\r\nWhen I have a file that is too large to load in memory, I break up the file into multiple smaller files (either by row or cols)\r\n\r\nExample: In case of 30 days worth of trading data of ~30GB size, I break it into a file per day of ~1GB size. I subsequently process each file separately and aggregate results at the end\r\n\r\nOne of the biggest advantages is that it allows parallel processing of the files (either multiple threads or processes)\r\n\r\nThe other advantage is that file manipulation (like adding/removing dates in the example) can be accomplished by regular shell commands, which is not be possible in more advanced/complicated file formats\r\n\r\nThis approach doesn&#39;t cover all scenarios, but is very useful in a lot of them",
               "tags": [],
               "creation_date": 1387482408,
               "last_edit_date": 1387812087,
               "is_accepted": false,
               "id": "20690383",
               "down_vote_count": 0,
               "score": 89
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 26286140,
               "is_accepted": false,
               "last_activity_date": 1412881636,
               "body_markdown": "Consider [Ruffus][1] if you go the simple path of creating a data pipeline which is broken down into multiple smaller files. \r\n\r\n  [1]: http://www.ruffus.org.uk/",
               "id": "26286140",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1412881636,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 46,
               "answer_id": 27282644,
               "is_accepted": false,
               "last_activity_date": 1417644580,
               "body_markdown": "I know this is an old thread but I think the [Blaze][0] library is worth checking out.  It&#39;s built for these types of situations.\r\n\r\n**From the docs:**\r\n    \r\nBlaze extends the usability of NumPy and Pandas to distributed and out-of-core computing. Blaze provides an interface similar to that of the NumPy ND-Array or Pandas DataFrame but maps these familiar interfaces onto a variety of other computational engines like Postgres or Spark.\r\n\r\n\r\n**Edit:** By the way, it&#39;s supported by ContinuumIO and Travis Oliphant, author of NumPy.\r\n\r\n\r\n[0]:https://github.com/ContinuumIO/blaze",
               "id": "27282644",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1417644580,
               "score": 46
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 11,
               "answer_id": 29910919,
               "is_accepted": false,
               "last_activity_date": 1430198541,
               "body_markdown": "One more variation\r\n\r\nMany of the operations done in pandas can also be done as a db query (sql, mongo)\r\n\r\nUsing a RDBMS or mongodb allows you to perform some of the aggregations in the DB Query (which is optimized for large data, and uses cache and indexes efficiently)\r\n\r\nLater, you can perform post processing using pandas.\r\n\r\nThe advantage of this method is that you gain the DB optimizations for working with large data, while still defining the logic in a high level declarative syntax - and not having to deal with the details of deciding what to do in memory and what to do out of core.\r\n\r\nAnd although the query language and pandas are different, it&#39;s usually not complicated to translate part of the logic from one to another.",
               "id": "29910919",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1430198541,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 40,
               "answer_id": 36188131,
               "is_accepted": false,
               "last_activity_date": 1458765053,
               "body_markdown": "There is now, two years after the question, an &#39;out-of-core&#39; pandas equivalent: [dask][1]. It is excellent! Though it does not support all of pandas functionality, you can get really far with it.\r\n\r\n\r\n  [1]: http://dask.pydata.org/en/latest/",
               "id": "36188131",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1458765053,
               "score": 40
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 39856364,
               "is_accepted": false,
               "last_activity_date": 1475595176,
               "body_markdown": "I recently came across a similar issue. I found simply reading the data in chunks and appending it as I write it in chunks to the same csv works well. My problem was adding a date column based on information in another table, using the value of certain columns as follows. This may help those confused by dask and hdf5 but more familiar with pandas like myself. \r\n\r\n    def addDateColumn():\r\n    &quot;&quot;&quot;Adds time to the daily rainfall data. Reads the csv as chunks of 100k \r\n       rows at a time and outputs them, appending as needed, to a single csv. \r\n       Uses the column of the raster names to get the date.\r\n    &quot;&quot;&quot;\r\n        df = pd.read_csv(pathlist[1]+&quot;CHIRPS_tanz.csv&quot;, iterator=True, \r\n                         chunksize=100000) #read csv file as 100k chunks\r\n\r\n        &#39;&#39;&#39;Do some stuff&#39;&#39;&#39;\r\n\r\n        count = 1 #for indexing item in time list \r\n        for chunk in df: #for each 100k rows\r\n            newtime = [] #empty list to append repeating times for different rows\r\n            toiterate = chunk[chunk.columns[2]] #ID of raster nums to base time\r\n            while count &lt;= toiterate.max():\r\n                for i in toiterate: \r\n                    if i ==count:\r\n                        newtime.append(newyears[count])\r\n                count+=1\r\n            print &quot;Finished&quot;, str(chunknum), &quot;chunks&quot;\r\n            chunk[&quot;time&quot;] = newtime #create new column in dataframe based on time\r\n            outname = &quot;CHIRPS_tanz_time2.csv&quot;\r\n            #append each output to same csv, using no header\r\n            chunk.to_csv(pathlist[2]+outname, mode=&#39;a&#39;, header=None, index=None)",
               "id": "39856364",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1475595176,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 43025589,
               "is_accepted": false,
               "last_activity_date": 1490507985,
               "body_markdown": "One trick I found helpful for &quot;large data&quot; use cases is to reduce the volume of the data by reducing float precision to 32-bit. It&#39;s not applicable in all cases, but in many applications 64-bit precision is overkill and the 2x memory savings are worth it. To make an obvious point even more obvious:\r\n\r\n    &gt;&gt;&gt; df = pd.DataFrame(np.random.randn(int(1e8), 5))\r\n    &gt;&gt;&gt; df.info()\r\n    &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\r\n    RangeIndex: 100000000 entries, 0 to 99999999\r\n    Data columns (total 5 columns):\r\n    ...\r\n    dtypes: float64(5)\r\n    memory usage: 3.7 GB\r\n    \r\n    &gt;&gt;&gt; df.astype(np.float32).info()\r\n    &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\r\n    RangeIndex: 100000000 entries, 0 to 99999999\r\n    Data columns (total 5 columns):\r\n    ...\r\n    dtypes: float32(5)\r\n    memory usage: 1.9 GB\r\n",
               "id": "43025589",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1490507985,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 47426521,
               "is_accepted": false,
               "last_activity_date": 1511322940,
               "body_markdown": "As noted by others, after some years an &#39;out-of-core&#39; pandas equivalent has emerged: [dask][1]. Though dask is not a drop-in replacement of pandas and all of its functionality it stands out for several reasons:\r\n\r\nDask is a flexible parallel computing library for analytic computing that is optimized for dynamic task scheduling for interactive computational workloads of\r\n\u201cBig Data\u201d collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments and scales from laptops to clusters.\r\n\r\n&gt; Dask emphasizes the following virtues:  \r\n&gt; \r\n&gt; - Familiar: Provides parallelized NumPy array and Pandas DataFrame    objects\r\n&gt; - Flexible: Provides a task scheduling interface for more    custom workloads and integration with other projects.\r\n&gt; - Native: Enables distributed computing in Pure Python with access to the    PyData stack.\r\n&gt; - Fast: Operates with low overhead, low latency, and    minimal serialization necessary for fast numerical algorithms  \r\n&gt; - Scales up: Runs resiliently on clusters with 1000s of cores   Scales    down: Trivial to set up and run on a laptop in a single process  \r\n&gt; - Responsive: Designed with interactive computing in mind it provides    rapid feedback and diagnostics to aid humans\r\n\r\nand to add a simple code sample:\r\n\r\n    import dask.dataframe as dd\r\n    df = dd.read_csv(&#39;2015-*-*.csv&#39;)\r\n    df.groupby(df.user_id).value.mean().compute()\r\n\r\nreplaces some pandas code like this:\r\n\r\n    import pandas as pd\r\n    df = pd.read_csv(&#39;2015-01-01.csv&#39;)\r\n    df.groupby(df.user_id).value.mean()\r\n\r\nand, especially noteworthy, provides through the concurrent.futures interface a general for the submission of custom tasks:\r\n\r\n    from dask.distributed import Client\r\n    client = Client(&#39;scheduler:port&#39;)\r\n    \r\n    futures = []\r\n    for fn in filenames:\r\n        future = client.submit(load, fn)\r\n        futures.append(future)\r\n    \r\n    summary = client.submit(summarize, futures)\r\n    summary.result()\r\n\r\n\r\n  [1]: http://dask.pydata.org/en/latest/\r\n\r\n\r\n\r\n",
               "id": "47426521",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1511322940,
               "score": 4
            }
         ],
         "link": "https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas",
         "id": "858127-2261"
      },
      {
         "up_vote_count": "251",
         "path": "2.stack",
         "body_markdown": "I wonder if there is a direct way to import the contents of a csv file into a record array, much in the way that R&#39;s `read.table()`, `read.delim()`, and `read.csv()` family imports data to R&#39;s data frame? \r\n\r\nOr is the best way to use [csv.reader()][1] and then apply something like `numpy.core.records.fromrecords()`?\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python",
         "view_count": "348720",
         "answer_count": "8",
         "tags": "['python', 'numpy', 'scipy', 'genfromtxt']",
         "creation_date": "1282192913",
         "last_edit_date": "1495541448",
         "code_snippet": "['<code>read.table()</code>', '<code>read.delim()</code>', '<code>read.csv()</code>', '<code>numpy.core.records.fromrecords()</code>', '<code>genfromtxt()</code>', '<code>delimiter</code>', \"<code>from numpy import genfromtxt\\nmy_data = genfromtxt('my_file.csv', delimiter=',')\\n</code>\", '<code>nan</code>', \"<code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 155: ordinal not in range(128)</code>\", '<code>read_csv</code>', '<code>pandas</code>', \"<code>import pandas as pd\\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\\ndf.values\\narray([[ 1. ,  2. ,  3. ],\\n       [ 4. ,  5.5,  6. ]])\\n</code>\", '<code>genfromtxt</code>', '<code>dtype=None</code>', '<code>genfromtxt</code>', '<code>myfile.csv</code>', \"<code>1.0, 2, 3\\n4, 5.5, 6\\n\\nimport numpy as np\\nnp.genfromtxt('myfile.csv',delimiter=',')\\n</code>\", '<code>array([[ 1. ,  2. ,  3. ],\\n       [ 4. ,  5.5,  6. ]])\\n</code>', \"<code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\\n</code>\", \"<code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \\n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\\n</code>\", '<code>recfromcsv()</code>', \"<code>numpy.recfromcsv(fname, delimiter=',', filling_values=numpy.nan, case_sensitive=True, deletechars='', replace_space=' ')</code>\", '<code>from numpy import genfromtxt\\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\\n</code>', '<code>import csv\\nimport numpy as np\\nwith open(dest_file,\\'r\\') as dest_f:\\n    data_iter = csv.reader(dest_f, \\n                           delimiter = delimiter, \\n                           quotechar = \\'\"\\')\\n    data = [data for data in data_iter]\\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)    \\n</code>', '<code>import numpy as np\\ncsv = np.genfromtxt(\\'test.csv\\',delimiter=\",\")\\nprint(csv)\\n</code>', '<code>import pandas as p\\nimport numpy as n\\n\\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\\nprint(closingValue)\\n</code>', '<code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\\n\\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\\n</code>', \"<code>from numpy import genfromtxt\\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\\n</code>\", \"<code>from pandas import read_csv\\ndf = read_csv('/home/hvn/me/notebook/train.csv')\\n</code>\", '<code>du -h ~/me/notebook/train.csv  \\n 59M    /home/hvn/me/notebook/train.csv\\n</code>', \"<code>$ pip freeze | egrep -i 'pandas|numpy'\\nnumpy==1.13.3\\npandas==0.20.2\\n</code>\", \"<code>import numpy as np \\ndata = np.loadtxt('c:\\\\\\\\1.csv',delimiter=',',skiprows=0)  \\n</code>\"]",
         "title": "How to read csv into record array in numpy?",
         "_childDocuments_": [
            {
               "up_vote_count": 395,
               "answer_id": 3519314,
               "last_activity_date": 1330700749,
               "path": "3.stack.answer",
               "body_markdown": "You can use Numpy&#39;s `genfromtxt()` method to do so, by setting the `delimiter` kwarg to a comma.\r\n\r\n    from numpy import genfromtxt\r\n    my_data = genfromtxt(&#39;my_file.csv&#39;, delimiter=&#39;,&#39;)\r\n\r\nMore information on the function can be found at its respective [documentation][1].\r\n\r\n\r\n  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html",
               "tags": [],
               "creation_date": 1282199694,
               "last_edit_date": 1330700749,
               "is_accepted": true,
               "id": "3519314",
               "down_vote_count": 1,
               "score": 394
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 63,
               "answer_id": 4724179,
               "is_accepted": false,
               "last_activity_date": 1295354675,
               "body_markdown": "You can also try `recfromcsv()` which can guess data types and return a properly formatted record array.",
               "id": "4724179",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1295354675,
               "score": 63
            },
            {
               "up_vote_count": 102,
               "answer_id": 26296194,
               "last_activity_date": 1412935153,
               "path": "3.stack.answer",
               "body_markdown": "I would recommend the [`read_csv`][1] function from the `pandas` library:\r\n\r\n    import pandas as pd\r\n    df=pd.read_csv(&#39;myfile.csv&#39;, sep=&#39;,&#39;,header=None)\r\n    df.values\r\n    array([[ 1. ,  2. ,  3. ],\r\n           [ 4. ,  5.5,  6. ]])\r\n\r\nThis gives a pandas [DataFrame][2] - allowing [many useful data manipulation functions which are not directly available with numpy record arrays][3].\r\n\r\n&gt; DataFrame is a 2-dimensional labeled data structure with columns of\r\n&gt; potentially different types. You can think of it like a spreadsheet or\r\n&gt; SQL table...\r\n\r\n*************************************************************************************\r\n\r\nI would also recommend `genfromtxt`. However, since the question asks for a [record array][4], as opposed to a normal array, the `dtype=None` parameter needs to be added to the `genfromtxt` call:\r\n\r\nGiven an input file, `myfile.csv`:\r\n \r\n\r\n    1.0, 2, 3\r\n    4, 5.5, 6\r\n\r\n    import numpy as np\r\n    np.genfromtxt(&#39;myfile.csv&#39;,delimiter=&#39;,&#39;)\r\n\r\ngives an array:\r\n\r\n    array([[ 1. ,  2. ,  3. ],\r\n           [ 4. ,  5.5,  6. ]])\r\n\r\nand \r\n\r\n    np.genfromtxt(&#39;myfile.csv&#39;,delimiter=&#39;,&#39;,dtype=None)\r\n\r\ngives a record array:\r\n\r\n    array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \r\n          dtype=[(&#39;f0&#39;, &#39;&lt;f8&#39;), (&#39;f1&#39;, &#39;&lt;f8&#39;), (&#39;f2&#39;, &#39;&lt;i4&#39;)])\r\n\r\nThis has the advantage that file with [multiple data types (including strings) can be easily imported][5].\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\r\n  [2]: http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\r\n  [3]: https://stackoverflow.com/a/11077215/1461850\r\n  [4]: http://docs.scipy.org/doc/numpy/user/basics.rec.html\r\n  [5]: https://stackoverflow.com/a/15481761",
               "tags": [],
               "creation_date": 1412933425,
               "last_edit_date": 1495541448,
               "is_accepted": false,
               "id": "26296194",
               "down_vote_count": 1,
               "score": 101
            },
            {
               "up_vote_count": 56,
               "answer_id": 28554340,
               "last_activity_date": 1447928124,
               "path": "3.stack.answer",
               "body_markdown": "I timed the \r\n\r\n    import numpy as np\r\n    np.genfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\r\n\r\nversus \r\n\r\n    import csv\r\n    import numpy as np\r\n    with open(dest_file,&#39;r&#39;) as dest_f:\r\n        data_iter = csv.reader(dest_f, \r\n                               delimiter = delimiter, \r\n                               quotechar = &#39;&quot;&#39;)\r\n        data = [data for data in data_iter]\r\n    data_array = np.asarray(data, dtype = &lt;whatever options&gt;)    \r\n\r\non 4.6 million rows with about 70 columns and found that the numpy path took 2 min 16s and the csv-list comprehension method took 13s.\r\n\r\nI would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as numpy. i suspect the pandas method would have similar interpreter overhead.\r\n\r\n",
               "tags": [],
               "creation_date": 1424145157,
               "last_edit_date": 1447928124,
               "is_accepted": false,
               "id": "28554340",
               "down_vote_count": 2,
               "score": 54
            },
            {
               "up_vote_count": 4,
               "answer_id": 44669986,
               "last_activity_date": 1500986859,
               "path": "3.stack.answer",
               "body_markdown": "You can use this code to send csv file data in to a array\r\n\r\n    import numpy as np\r\n    csv = np.genfromtxt(&#39;test.csv&#39;,delimiter=&quot;,&quot;)\r\n    print(csv)",
               "tags": [],
               "creation_date": 1498031568,
               "last_edit_date": 1500986859,
               "is_accepted": false,
               "id": "44669986",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 2,
               "answer_id": 45478277,
               "last_activity_date": 1502567107,
               "path": "3.stack.answer",
               "body_markdown": "I tried this:\r\n\r\n    import pandas as p\r\n    import numpy as n\r\n    \r\n    closingValue = p.read_csv(&quot;&lt;FILENAME&gt;&quot;, usecols=[4], dtype=float)\r\n    print(closingValue)\r\n\r\n",
               "tags": [],
               "creation_date": 1501747355,
               "last_edit_date": 1502567107,
               "is_accepted": false,
               "id": "45478277",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 2,
               "answer_id": 46727805,
               "last_activity_date": 1507893291,
               "path": "3.stack.answer",
               "body_markdown": "As I tried both ways using Numpy and Pandas, using pandas has a lot of advantages:\r\n- faster\r\n- less CPU usage\r\n- 1/3 RAM usage compare to Numpy genfromtxt\r\n\r\nThis is my test code:\r\n\r\n    $ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\r\n    2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\r\n    0inputs+24outputs (0major+107147minor)pagefaults 0swaps\r\n    \r\n    23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\r\n    0inputs+0outputs (0major+416145minor)pagefaults 0swaps\r\n\r\n \r\ntest_numpy_csv.py\r\n\r\n    from numpy import genfromtxt\r\n    train = genfromtxt(&#39;/home/hvn/me/notebook/train.csv&#39;, delimiter=&#39;,&#39;)\r\n\r\ntest_pandas.py\r\n\r\n    from pandas import read_csv\r\n    df = read_csv(&#39;/home/hvn/me/notebook/train.csv&#39;)\r\n  \r\n\r\nDatafile: \r\n\r\n    du -h ~/me/notebook/train.csv  \r\n     59M\t/home/hvn/me/notebook/train.csv\r\n\r\nWith numpy and pandas at versions:\r\n\r\n    $ pip freeze | egrep -i &#39;pandas|numpy&#39;\r\n    numpy==1.13.3\r\n    pandas==0.20.2\r\n\r\n",
               "tags": [],
               "creation_date": 1507890504,
               "last_edit_date": 1507893291,
               "is_accepted": false,
               "id": "46727805",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48520438,
               "is_accepted": false,
               "last_activity_date": 1517312080,
               "body_markdown": "Using numpy.loadtxt  \r\n\r\nA quite simple method. But it requires all the elements being float (int and so on)\r\n\r\n    import numpy as np \r\n    data = np.loadtxt(&#39;c:\\\\1.csv&#39;,delimiter=&#39;,&#39;,skiprows=0)  ",
               "id": "48520438",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1517312080,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/3518778/how-to-read-csv-into-record-array-in-numpy",
         "id": "858127-2262"
      },
      {
         "up_vote_count": "321",
         "path": "2.stack",
         "body_markdown": "I want to convert a table, represented as a list of lists, into a Pandas DataFrame. As an extremely simplified example:\r\n\r\n    a = [[&#39;a&#39;, &#39;1.2&#39;, &#39;4.2&#39;], [&#39;b&#39;, &#39;70&#39;, &#39;0.03&#39;], [&#39;x&#39;, &#39;5&#39;, &#39;0&#39;]]\r\n    df = pd.DataFrame(a)\r\n\r\nWhat is the best way to convert the columns to the appropriate types, in this case columns 2 and 3 into floats? Is there a way to specify the types while converting to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the type for each column? Ideally I would like to do this in a dynamic way because there can be hundreds of columns and I don&#39;t want to specify exactly which columns are of which type. All I can guarantee is that each columns contains values of the same type.",
         "view_count": "550984",
         "answer_count": "5",
         "tags": "['python', 'pandas', 'dataframe', 'types', 'casting']",
         "creation_date": "1365465210",
         "last_edit_date": "1508360938",
         "code_snippet": "[\"<code>a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\\ndf = pd.DataFrame(a)\\n</code>\", \"<code>df.convert_objects(convert_dates='coerce', convert_numeric=True)</code>\", '<code>read_csv</code>', '<code>pd.io.sql.read_from</code>', '<code>sql</code>', \"<code>df = pd.DataFrame(a, dtype='float')</code>\", '<code>dtype</code>', '<code>pd.to_numeric</code>', '<code>apply</code>', '<code>errors</code>', '<code>NaN</code>', '<code>s</code>', \"<code>&gt;&gt;&gt; s = pd.Series(['1', '2', '4.7', 'pandas', '10'])\\n&gt;&gt;&gt; s\\n0         1\\n1         2\\n2       4.7\\n3    pandas\\n4        10\\ndtype: object\\n</code>\", \"<code>&gt;&gt;&gt; pd.to_numeric(s) # or pd.to_numeric(s, errors='raise')\\nValueError: Unable to parse string\\n</code>\", '<code>NaN</code>', \"<code>&gt;&gt;&gt; pd.to_numeric(s, errors='coerce')\\n0     1.0\\n1     2.0\\n2     4.7\\n3     NaN\\n4    10.0\\ndtype: float64\\n</code>\", \"<code>&gt;&gt;&gt; pd.to_numeric(s, errors='ignore')\\n# the original Series is returned untouched\\n</code>\", '<code>DataFrame.apply</code>', \"<code>&gt;&gt;&gt; a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\\n&gt;&gt;&gt; df = pd.DataFrame(a, columns=['col1','col2','col3'])\\n&gt;&gt;&gt; df\\n  col1 col2  col3\\n0    a  1.2   4.2\\n1    b   70  0.03\\n2    x    5     0\\n</code>\", \"<code>df[['col2','col3']] = df[['col2','col3']].apply(pd.to_numeric)\\n</code>\", '<code>float64</code>', \"<code>df.apply(pd.to_numeric, errors='ignore')\\n</code>\", '<code>pd.to_datetime</code>', '<code>pd.to_timedelta</code>', '<code>infer_objects()</code>', \"<code>&gt;&gt;&gt; df = pd.DataFrame({'a': [7, 1, 5], 'b': ['3','2','1']}, dtype='object')\\n&gt;&gt;&gt; df.dtypes\\na    object\\nb    object\\ndtype: object\\n</code>\", '<code>infer_objects()</code>', '<code>&gt;&gt;&gt; df = df.infer_objects()\\n&gt;&gt;&gt; df.dtypes\\na     int64\\nb    object\\ndtype: object\\n</code>', '<code>df.astype(int)</code>', '<code>.convert_objects</code>', '<code>0.17</code>', '<code>df.to_numeric</code>', '<code>pd.to_numeric</code>', '<code>convert_objects</code>', '<code>astype</code>', '<code>.astype(numpy.int32)</code>', \"<code>a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\\ndf\\nOut[16]: \\n  one  two three\\n0   a  1.2   4.2\\n1   b   70  0.03\\n2   x    5     0\\n\\ndf.dtypes\\nOut[17]: \\none      object\\ntwo      object\\nthree    object\\n\\ndf[['two', 'three']] = df[['two', 'three']].astype(float)\\n\\ndf.dtypes\\nOut[19]: \\none       object\\ntwo      float64\\nthree    float64\\n</code>\", '<code>pd.DataFrame</code>', '<code>dtype</code>', '<code>SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_index,col_indexer] = value instead</code>', '<code>ValueError: Cannot convert NA to integer</code>', \"<code>df = pd.DataFrame(a, columns=['one', 'two', 'three'], dtype={'one': str, 'two': int, 'three': float})</code>\", '<code>dict(enumerate(my_list))</code>', \"<code>df[['col.name1', 'col.name2'...]] = df[['col.name1', 'col.name2'..]].astype('data_type')\\n</code>\", '<code>bool</code>', '<code>True</code>', '<code># df is the DataFrame, and column_list is a list of columns as strings (e.g [\"col1\",\"col2\",\"col3\"])\\n# dependencies: pandas\\n\\ndef coerce_df_columns_to_numeric(df, column_list):\\n    df[column_list] = df[column_list].apply(pd.to_numeric, errors=\\'coerce\\')\\n</code>', \"<code>import pandas as pd\\n\\ndef coerce_df_columns_to_numeric(df, column_list):\\n    df[column_list] = df[column_list].apply(pd.to_numeric, errors='coerce')\\n\\na = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\\ndf = pd.DataFrame(a, columns=['col1','col2','col3'])\\n\\ncoerce_df_columns_to_numeric(df, ['col2','col3'])\\n</code>\", \"<code>d1 = pd.DataFrame(columns=[ 'float_column' ], dtype=float)\\nd1 = d1.append(pd.DataFrame(columns=[ 'string_column' ], dtype=str))\\n</code>\", '<code>In[8}:  d1.dtypes\\nOut[8]: \\nfloat_column     float64\\nstring_column     object\\ndtype: object\\n</code>']",
         "title": "Change data type of columns in Pandas",
         "_childDocuments_": [
            {
               "up_vote_count": 346,
               "answer_id": 16134561,
               "last_activity_date": 1371649199,
               "path": "3.stack.answer",
               "body_markdown": "How about this? \r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    a = [[&#39;a&#39;, &#39;1.2&#39;, &#39;4.2&#39;], [&#39;b&#39;, &#39;70&#39;, &#39;0.03&#39;], [&#39;x&#39;, &#39;5&#39;, &#39;0&#39;]]\r\n    df = pd.DataFrame(a, columns=[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])\r\n    df\r\n    Out[16]: \r\n      one  two three\r\n    0   a  1.2   4.2\r\n    1   b   70  0.03\r\n    2   x    5     0\r\n\r\n    df.dtypes\r\n    Out[17]: \r\n    one      object\r\n    two      object\r\n    three    object\r\n\r\n    df[[&#39;two&#39;, &#39;three&#39;]] = df[[&#39;two&#39;, &#39;three&#39;]].astype(float)\r\n    \r\n    df.dtypes\r\n    Out[19]: \r\n    one       object\r\n    two      float64\r\n    three    float64\r\n\r\n",
               "tags": [],
               "creation_date": 1366568127,
               "last_edit_date": 1371649199,
               "is_accepted": false,
               "id": "16134561",
               "down_vote_count": 2,
               "score": 344
            },
            {
               "up_vote_count": 370,
               "answer_id": 28648923,
               "last_activity_date": 1501324853,
               "path": "3.stack.answer",
               "body_markdown": "You can use [`pd.to_numeric`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html) (introduced in version 0.17) to convert a column or a Series to a numeric type. The function can also be applied over multiple columns of a DataFrame using `apply`.\r\n\r\nImportantly, the function also takes an `errors` key word argument that lets you force not-numeric values to be `NaN`, or simply ignore columns containing these values.\r\n\r\nExample uses are shown below.\r\n\r\n### Individual column / Series\r\n\r\nHere&#39;s an example using a Series of strings `s` which has the object dtype:\r\n\r\n    &gt;&gt;&gt; s = pd.Series([&#39;1&#39;, &#39;2&#39;, &#39;4.7&#39;, &#39;pandas&#39;, &#39;10&#39;])\r\n    &gt;&gt;&gt; s\r\n    0         1\r\n    1         2\r\n    2       4.7\r\n    3    pandas\r\n    4        10\r\n    dtype: object\r\n\r\nThe function&#39;s default behaviour is to raise if it can&#39;t convert a value. In this case, it can&#39;t cope with the string &#39;pandas&#39;:\r\n\r\n    &gt;&gt;&gt; pd.to_numeric(s) # or pd.to_numeric(s, errors=&#39;raise&#39;)\r\n    ValueError: Unable to parse string\r\n\r\nRather than fail, we might want &#39;pandas&#39; to be considered a missing/bad value. We can coerce invalid values to `NaN` as follows:\r\n\r\n    &gt;&gt;&gt; pd.to_numeric(s, errors=&#39;coerce&#39;)\r\n    0     1.0\r\n    1     2.0\r\n    2     4.7\r\n    3     NaN\r\n    4    10.0\r\n    dtype: float64\r\n\r\nThe third option is just to ignore the operation if an invalid value is encountered:\r\n\r\n    &gt;&gt;&gt; pd.to_numeric(s, errors=&#39;ignore&#39;)\r\n    # the original Series is returned untouched\r\n\r\n### Multiple columns / entire DataFrames\r\n\r\nWe might want to apply this operation to multiple columns. Processing each column in turn is tedious, so we can use `DataFrame.apply` to have the function act on each column.\r\n\r\nBorrowing the DataFrame from the question:\r\n\r\n    &gt;&gt;&gt; a = [[&#39;a&#39;, &#39;1.2&#39;, &#39;4.2&#39;], [&#39;b&#39;, &#39;70&#39;, &#39;0.03&#39;], [&#39;x&#39;, &#39;5&#39;, &#39;0&#39;]]\r\n    &gt;&gt;&gt; df = pd.DataFrame(a, columns=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;])\r\n    &gt;&gt;&gt; df\r\n      col1 col2  col3\r\n    0    a  1.2   4.2\r\n    1    b   70  0.03\r\n    2    x    5     0\r\n\r\nThen we can write:\r\n\r\n    df[[&#39;col2&#39;,&#39;col3&#39;]] = df[[&#39;col2&#39;,&#39;col3&#39;]].apply(pd.to_numeric)\r\n\r\nand now &#39;col2&#39; and &#39;col3&#39; have dtype `float64` as desired.\r\n\r\nHowever, we might not know which of our columns can be converted reliably to a numeric type. In that case we can just write:\r\n\r\n    df.apply(pd.to_numeric, errors=&#39;ignore&#39;)\r\n\r\nThen the function will be applied to the *whole* DataFrame. Columns that can be converted to a numeric type will be converted, while columns that cannot (e.g. they contain non-digit strings or dates) will be left alone.\r\n\r\nThere is also `pd.to_datetime` and `pd.to_timedelta` for conversion to dates and timestamps.\r\n\r\n### Soft conversions\r\n\r\nVersion 0.21.0 introduces the method `infer_objects()` for converting columns of a DataFrame that have an object datatype to a more specific type.\r\n\r\nFor example, let&#39;s create a DataFrame with two columns of object type, with one holding integers and the other holding strings of integers:\r\n\r\n    &gt;&gt;&gt; df = pd.DataFrame({&#39;a&#39;: [7, 1, 5], &#39;b&#39;: [&#39;3&#39;,&#39;2&#39;,&#39;1&#39;]}, dtype=&#39;object&#39;)\r\n    &gt;&gt;&gt; df.dtypes\r\n    a    object\r\n    b    object\r\n    dtype: object\r\n\r\nThen using `infer_objects()`, we can change the type of column &#39;a&#39; to int64:\r\n\r\n    &gt;&gt;&gt; df = df.infer_objects()\r\n    &gt;&gt;&gt; df.dtypes\r\n    a     int64\r\n    b    object\r\n    dtype: object\r\n\r\nColumn &#39;b&#39; has been left alone since its values were strings, not integers. If we wanted to try and force the conversion of both columns to an integer type, we could use `df.astype(int)` instead.\r\n\r\n",
               "tags": [],
               "creation_date": 1424540222,
               "last_edit_date": 1501324853,
               "is_accepted": true,
               "id": "28648923",
               "down_vote_count": 2,
               "score": 368
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 44536326,
               "is_accepted": false,
               "last_activity_date": 1497418941,
               "body_markdown": "Here is a function that takes as its arguments a DataFrame and a list of columns and coerces all data in the columns to numbers.\r\n\r\n    # df is the DataFrame, and column_list is a list of columns as strings (e.g [&quot;col1&quot;,&quot;col2&quot;,&quot;col3&quot;])\r\n    # dependencies: pandas\r\n\r\n    def coerce_df_columns_to_numeric(df, column_list):\r\n        df[column_list] = df[column_list].apply(pd.to_numeric, errors=&#39;coerce&#39;)\r\n\r\nSo, for your example:\r\n\r\n\timport pandas as pd\r\n\r\n\tdef coerce_df_columns_to_numeric(df, column_list):\r\n\t    df[column_list] = df[column_list].apply(pd.to_numeric, errors=&#39;coerce&#39;)\r\n\r\n\ta = [[&#39;a&#39;, &#39;1.2&#39;, &#39;4.2&#39;], [&#39;b&#39;, &#39;70&#39;, &#39;0.03&#39;], [&#39;x&#39;, &#39;5&#39;, &#39;0&#39;]]\r\n\tdf = pd.DataFrame(a, columns=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;])\r\n\r\n\tcoerce_df_columns_to_numeric(df, [&#39;col2&#39;,&#39;col3&#39;])",
               "id": "44536326",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1497418941,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 45026465,
               "is_accepted": false,
               "last_activity_date": 1499752608,
               "body_markdown": "How about creating two dataframes, each with different data types for their columns, and then appending them together?\r\n\r\n    d1 = pd.DataFrame(columns=[ &#39;float_column&#39; ], dtype=float)\r\n    d1 = d1.append(pd.DataFrame(columns=[ &#39;string_column&#39; ], dtype=str))\r\n\r\n**Results**\r\n\r\n    In[8}:  d1.dtypes\r\n    Out[8]: \r\n    float_column     float64\r\n    string_column     object\r\n    dtype: object\r\n\r\nAfter the dataframe is created, you can populate it with floating point variables in the 1st column, and strings (or any data type you desire) in the 2nd column. \r\n",
               "id": "45026465",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1499752608,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 47303880,
               "is_accepted": false,
               "last_activity_date": 1510738681,
               "body_markdown": "this below code will change datatype of column.\r\n\r\n    df[[&#39;col.name1&#39;, &#39;col.name2&#39;...]] = df[[&#39;col.name1&#39;, &#39;col.name2&#39;..]].astype(&#39;data_type&#39;)\r\nin place of data type you can give your datatype .what do you want like str,float,int etc.",
               "id": "47303880",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1510738681,
               "score": 6
            }
         ],
         "link": "https://stackoverflow.com/questions/15891038/change-data-type-of-columns-in-pandas",
         "id": "858127-2263"
      },
      {
         "up_vote_count": "313",
         "path": "2.stack",
         "body_markdown": "I am trying to understand, what is monkey patching or a monkey patch? \r\n\r\nIs that something like methods/operators overloading or delegating? \r\n\r\nDoes it have anything common with these things?",
         "view_count": "103186",
         "answer_count": "7",
         "tags": "['python', 'terminology', 'monkeypatching']",
         "creation_date": "1302548741",
         "last_edit_date": "1505991418",
         "code_snippet": "['<code>get_data</code>', '<code>get_data</code>', '<code>get_data</code>', '<code>get_data</code>', '<code>get_data</code>', '<code>get_data</code>', '<code>pointing to the original get_data function</code>', '<code>get_data</code>', '<code>get_data</code>', '<code>get_data</code>', '<code>from SomeOtherProduct.SomeModule import SomeClass\\n\\ndef speak(self):\\n    return \"ook ook eee eee eee!\"\\n\\nSomeClass.speak = speak\\n</code>', '<code>import pandas as pd\\ndef just_foo_cols(self):\\n    \"\"\"Get a list of column names containing the string \\'foo\\'\\n\\n    \"\"\"\\n    return [x for x in self.columns if \\'foo\\' in x]\\n\\npd.DataFrame.just_foo_cols = just_foo_cols # monkey-patch the DataFrame class\\ndf = pd.DataFrame([list(range(4))], columns=[\"A\",\"foo\",\"foozball\",\"bar\"])\\ndf.just_foo_cols()\\ndel pd.DataFrame.just_foo_cols # you can also remove the new method\\n</code>', '<code>import pandas as pd\\n</code>', '<code>def just_foo_cols(self):\\n    \"\"\"Get a list of column names containing the string \\'foo\\'\\n\\n    \"\"\"\\n    return [x for x in self.columns if \\'foo\\' in x]\\n</code>', '<code>pd.DataFrame.just_foo_cols = just_foo_cols # monkey-patch the DataFrame class\\n</code>', '<code>df = pd.DataFrame([list(range(4))], columns=[\"A\",\"foo\",\"foozball\",\"bar\"])\\ndf.just_foo_cols()\\ndel pd.DataFrame.just_foo_cols # you can also remove the new method\\n</code>', \"<code>import datasource\\n\\ndef get_data(self):\\n    '''monkey patch datasource.Structure with this to simulate error'''\\n    raise datasource.DataRetrievalError\\n\\ndatasource.Structure.get_data = get_data\\n</code>\", '<code>Structure</code>', '<code>def setUp(self):\\n    # retain a pointer to the actual real method:\\n    self.real_get_data = datasource.Structure.get_data\\n    # monkey patch it:\\n    datasource.Structure.get_data = get_data\\n\\ndef tearDown(self):\\n    # give the real method back to the Structure object:\\n    datasource.Structure.get_data = self.real_get_data\\n</code>', '<code>mock</code>', '<code>mock</code>', '<code>patch</code>', '<code>mock</code>']",
         "title": "What is monkey patching?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 5626225,
               "is_accepted": false,
               "last_activity_date": 1302548932,
               "body_markdown": "According to [Wikipedia][1]:\r\n\r\n&gt; In Python, the term monkey patch only\r\n&gt; refers to dynamic modifications of a\r\n&gt; class or module at runtime, motivated\r\n&gt; by the intent to patch existing\r\n&gt; third-party code as a workaround to a\r\n&gt; bug or feature which does not act as\r\n&gt; you desire.\r\n\r\n\r\n  [1]: http://en.wikipedia.org/wiki/Monkey_patch",
               "id": "5626225",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1302548932,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 326,
               "answer_id": 5626250,
               "is_accepted": true,
               "last_activity_date": 1302549057,
               "body_markdown": "No, it&#39;s not like any of those things. It&#39;s simply the dynamic replacement of attributes at runtime.\r\n\r\nFor instance, consider a class that has a method `get_data`. This method does an external lookup (on a database or web API, for example), and various other methods in the class call it. However, in a unit test, you don&#39;t want to depend on the external data source - so you dynamically replace the `get_data` method with a stub that returns some fixed data.\r\n\r\nBecause Python classes are mutable, and methods are just attributes of the class, you can do this as much as you like - and, in fact, you can even replace classes and functions in a module in exactly the same way.",
               "id": "5626250",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1302549057,
               "score": 326
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 17,
               "answer_id": 5626255,
               "is_accepted": false,
               "last_activity_date": 1302549073,
               "body_markdown": "First: monkey patching is an evil hack (in my opinion).\r\n\r\nIt is often used to replace a method on the module or class level with a custom implementation.\r\n\r\nThe most common usecase is adding a workaround for a bug in a module or class when you can&#39;t replace the original code. In this case you replace the &quot;wrong&quot; code through monkey patching with an implementation inside your own module/package.",
               "id": "5626255",
               "tags": [],
               "down_vote_count": 4,
               "creation_date": 1302549073,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 5626835,
               "is_accepted": false,
               "last_activity_date": 1302552074,
               "body_markdown": "Monkey patching can only be done in dynamic languages, of which python is a good example.  Changing a method at runtime instead of updating the object definition is one example;similarly, adding attributes (whether methods or variables) at runtime is considered monkey patching.  These are often done when working with modules you don&#39;t have the source for, such that the object definitions can&#39;t be easily changed.\r\n\r\nThis is considered bad because it means that an object&#39;s definition does not completely or accurately describe how it actually behaves.",
               "id": "5626835",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1302552074,
               "score": 8
            },
            {
               "up_vote_count": 285,
               "answer_id": 6647776,
               "last_activity_date": 1388653852,
               "path": "3.stack.answer",
               "body_markdown": "&gt; A MonkeyPatch is a piece of Python code which extends or modifies\r\n&gt; other code at runtime (typically at startup).\r\n\r\nA simple example looks like this:\r\n\r\n    from SomeOtherProduct.SomeModule import SomeClass\r\n    \r\n    def speak(self):\r\n        return &quot;ook ook eee eee eee!&quot;\r\n\r\n    SomeClass.speak = speak\r\n\r\n\r\n**Source:** [MonkeyPatch][1] page on Zope wiki.\r\n\r\n\r\n  [1]: https://web.archive.org/web/20120730014107/http://wiki.zope.org/zope2/MonkeyPatch",
               "tags": [],
               "creation_date": 1310374356,
               "last_edit_date": 1388653852,
               "is_accepted": false,
               "id": "6647776",
               "down_vote_count": 0,
               "score": 285
            },
            {
               "up_vote_count": 79,
               "answer_id": 27466499,
               "last_activity_date": 1484933946,
               "path": "3.stack.answer",
               "body_markdown": "&gt; # What is a monkey patch?\r\n\r\nSimply put, monkey patching is making changes to a module or class while the program is running. \r\n\r\n# Example in usage\r\n\r\nThere&#39;s an example of monkey-patching in the Pandas documentation:\r\n\r\n    import pandas as pd\r\n    def just_foo_cols(self):\r\n        &quot;&quot;&quot;Get a list of column names containing the string &#39;foo&#39;\r\n    \r\n        &quot;&quot;&quot;\r\n        return [x for x in self.columns if &#39;foo&#39; in x]\r\n    \r\n    pd.DataFrame.just_foo_cols = just_foo_cols # monkey-patch the DataFrame class\r\n    df = pd.DataFrame([list(range(4))], columns=[&quot;A&quot;,&quot;foo&quot;,&quot;foozball&quot;,&quot;bar&quot;])\r\n    df.just_foo_cols()\r\n    del pd.DataFrame.just_foo_cols # you can also remove the new method\r\n\r\n\r\nTo break this down, first we import our module:\r\n\r\n\r\n    import pandas as pd\r\n\r\nNext we create a method definition, which exists unbound and free outside the scope of any class definitions (since the distinction is fairly meaningless between a function and an unbound method, Python 3 does away with the unbound method):\r\n\r\n    def just_foo_cols(self):\r\n        &quot;&quot;&quot;Get a list of column names containing the string &#39;foo&#39;\r\n    \r\n        &quot;&quot;&quot;\r\n        return [x for x in self.columns if &#39;foo&#39; in x]\r\n   \r\nNext we simply attach that method to the class we want to use it on:\r\n\r\n    pd.DataFrame.just_foo_cols = just_foo_cols # monkey-patch the DataFrame class\r\n\r\nAnd then we can use the method on an instance of the class, and delete the method when we&#39;re done:\r\n\r\n    df = pd.DataFrame([list(range(4))], columns=[&quot;A&quot;,&quot;foo&quot;,&quot;foozball&quot;,&quot;bar&quot;])\r\n    df.just_foo_cols()\r\n    del pd.DataFrame.just_foo_cols # you can also remove the new method\r\n\r\n## Caveat for name-mangling\r\n\r\nIf you&#39;re using name-mangling (prefixing attributes with a double-underscore, which alters the name, and which I don&#39;t recommend) you&#39;ll have to name-mangle manually if you do this. Since I don&#39;t recommend name-mangling, I will not demonstrate it here.\r\n\r\n# Testing Example\r\n\r\nHow can we use this knowledge, for example, in testing?\r\n\r\nSay we need to simulate a data retrieval call to an outside data source that results in an error, because we want to ensure correct behavior in such a case.  We can monkey patch the data structure to ensure this behavior. (So using a similar method name as suggested by Daniel Roseman:)\r\n\r\n    import datasource\r\n\r\n    def get_data(self):\r\n        &#39;&#39;&#39;monkey patch datasource.Structure with this to simulate error&#39;&#39;&#39;\r\n        raise datasource.DataRetrievalError\r\n\r\n    datasource.Structure.get_data = get_data\r\n\r\nAnd when we test it for behavior that relies on this method raising an error, if correctly implemented, we&#39;ll get that behavior in the test results.\r\n\r\nJust doing the above will alter the `Structure` object for the life of the process, so you&#39;ll want to use setups and teardowns in your unittests to avoid doing that, e.g.:\r\n\r\n    def setUp(self):\r\n        # retain a pointer to the actual real method:\r\n        self.real_get_data = datasource.Structure.get_data\r\n        # monkey patch it:\r\n        datasource.Structure.get_data = get_data\r\n\r\n    def tearDown(self):\r\n        # give the real method back to the Structure object:\r\n        datasource.Structure.get_data = self.real_get_data\r\n\r\n(While the above is fine, it would probably be a better idea to use the `mock` library to patch the code. `mock`&#39;s `patch` decorator would be less error prone than doing the above, which would require more lines of code and thus more opportunities to introduce errors. I have yet to review the code in `mock` but I imagine it uses monkey-patching in a similar way.)\r\n\r\n",
               "tags": [],
               "creation_date": 1418533068,
               "last_edit_date": 1484933946,
               "is_accepted": false,
               "id": "27466499",
               "down_vote_count": 0,
               "score": 79
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 34795134,
               "is_accepted": false,
               "last_activity_date": 1452790108,
               "body_markdown": "Monkey patching is reopening the existing classes or methods in class at runtime and changing the behavior, which should be used cautiously, or you should use it only when you really need to.\r\n\r\nAs Python is a dynamic programming language, Classes are mutable so you can reopen them and modify or even replace them.",
               "id": "34795134",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1452790108,
               "score": 3
            }
         ],
         "link": "https://stackoverflow.com/questions/5626193/what-is-monkey-patching",
         "id": "858127-2264"
      },
      {
         "up_vote_count": "743",
         "path": "2.stack",
         "body_markdown": "When deleting a column in a DataFrame I use:\r\n\r\n    del df[&#39;column_name&#39;]\r\n\r\nand this works great. Why can&#39;t I use:\r\n\r\n    del df.column_name\r\n\r\n*As you can access the column/Series as `df.column_name`, I expect this to work.*",
         "view_count": "885324",
         "answer_count": "12",
         "tags": "['python', 'pandas', 'design', 'dataframe', 'magic-methods']",
         "creation_date": "1353047200",
         "last_edit_date": "1504076220",
         "code_snippet": "[\"<code>del df['column_name']\\n</code>\", '<code>del df.column_name\\n</code>', '<code>df.column_name</code>', '<code>del df.column_name</code>', '<code>del df[name]</code>', '<code>df.__delitem__(name)</code>', '<code>class A(object): def __init__(self): self.var = 1</code>', '<code>a = A(); del a.var</code>', '<code>del df[name]</code>', '<code>df.__delitem__(name)</code>', '<code>del df.name</code>', '<code>del a.var</code>', '<code>drop</code>', \"<code>df = df.drop('column_name', 1)\\n</code>\", '<code>1</code>', '<code>0</code>', '<code>1</code>', '<code>df</code>', \"<code>df.drop('column_name', axis=1, inplace=True)\\n</code>\", '<code>df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index \\n</code>', '<code>del</code>', '<code>drop</code>', '<code>del</code>', '<code>drop</code>', '<code>Panel</code>', \"<code>columns = ['Col1', 'Col2', ...]\\ndf.drop(columns, inplace=True, axis=1)\\n</code>\", '<code>inplace=True</code>', '<code>df = df.drop(columns, axis=1)\\n</code>', '<code>inplace</code>', '<code>df.drop(list,inplace=True,axis=1)</code>', '<code>del</code>', '<code>df.drop(df.columns[[0,1,3]], axis=1, inplace=True)\\n</code>', '<code>df.drop(df.columns[[0]], axis=1, inplace=True)\\n</code>', '<code>inplace</code>', '<code>column-name</code>', \"<code>df.pop('column-name')\\n</code>\", \"<code>df = DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6]), ('C', [7,8, 9])], orient='index', columns=['one', 'two', 'three'])\\n</code>\", '<code>print df</code>', '<code>   one  two  three\\nA    1    2      3\\nB    4    5      6\\nC    7    8      9\\n</code>', '<code>df.drop(df.columns[[0]], axis=1, inplace=True)</code>', '<code>print df</code>', '<code>   two  three\\nA    2      3\\nB    5      6\\nC    8      9\\n</code>', \"<code>three = df.pop('three')</code>\", '<code>print df</code>', '<code>   two\\nA    2\\nB    5\\nC    8\\n</code>', \"<code>df.T.pop('A')</code>\", '<code>del df.column_name</code>', \"<code>del df['column']</code>\", \"<code>df.__delitem__('column')</code>\", \"<code>del df['column_name']</code>\", '<code>del df.column_name</code>', '<code>del df.column_name</code>', '<code>__delattr__</code>', \"<code>del df['column_name']</code>\", '<code>del df.dtypes</code>', '<code>__delattr__</code>', '<code>.ix</code>', '<code>.loc</code>', '<code>.iloc</code>', '<code>del df.column_name</code>', '<code>del df.dtypes</code>', '<code>__del__</code>', '<code>__delattr__</code>', '<code>del</code>', '<code>.__del__</code>', '<code>del</code>', '<code>__delattr__</code>', '<code>__delitem__</code>', '<code>__</code>', '<code>__</code>', \"<code>df.drop(['column_name'], axis = 1, inplace = True, errors = 'ignore')\\n</code>\", \"<code>errors= 'ignore'</code>\", \"<code>df.drop(['column_1','column_2'], axis=1 , inplace=True,errors= 'ignore')</code>\", \"<code>df.drop(['col_name_1','col_name_2',...,'col_name_N'],inplace=True,axis=1,errors='ignore')\\n</code>\", '<code>[]</code>', '<code>df.column_name</code>', '<code>In [1]: df = DataFrame([[1, 2, 3], [4, 5, 6]])\\n\\nIn [2]: df[1]\\nOut[2]: \\n0    2\\n1    5\\nName: 1\\n\\nIn [3]: df.1\\n  File \"&lt;ipython-input-3-e4803c0d1066&gt;\", line 1\\n    df.1\\n       ^\\nSyntaxError: invalid syntax\\n</code>', \"<code>df.drop([col for col in ['col_name_1','col_name_2',...,'col_name_N'] if col in df], \\n        axis=1, inplace=True)\\n</code>\", \"<code>df.drop(dlst, 1, errors='ignore')</code>\", '<code>df.reindex_axis(np.setdiff1d(df.columns.values, dlst), 1)\\n</code>', '<code>pd.DataFrame</code>', '<code>df</code>', '<code>dlst</code>', \"<code>df = pd.DataFrame(dict(zip('ABCDEFGHIJ', range(1, 11))), range(3))\\ndlst = list('HIJKLM')\\n</code>\", '<code>df\\n\\n   A  B  C  D  E  F  G  H  I   J\\n0  1  2  3  4  5  6  7  8  9  10\\n1  1  2  3  4  5  6  7  8  9  10\\n2  1  2  3  4  5  6  7  8  9  10\\n</code>', \"<code>dlst\\n\\n['H', 'I', 'J', 'K', 'L', 'M']\\n</code>\", \"<code>df.drop(dlst, 1, errors='ignore')\\n\\n   A  B  C  D  E  F  G\\n0  1  2  3  4  5  6  7\\n1  1  2  3  4  5  6  7\\n2  1  2  3  4  5  6  7\\n</code>\", '<code>df.columns.difference(dlst)</code>', \"<code>Index(['A', 'B', 'C', 'D', 'E', 'F', 'G'], dtype='object')\\n</code>\", '<code>np.setdiff1d(df.columns.values, dlst)</code>', \"<code>array(['A', 'B', 'C', 'D', 'E', 'F', 'G'], dtype=object)\\n</code>\", \"<code>df.columns.drop(dlst, errors='ignore')</code>\", \"<code>Index(['A', 'B', 'C', 'D', 'E', 'F', 'G'], dtype='object')\\n</code>\", '<code>list(set(df.columns.values.tolist()).difference(dlst))</code>', \"<code># does not preserve order\\n['E', 'D', 'B', 'F', 'G', 'A', 'C']\\n</code>\", '<code>[x for x in df.columns.values.tolist() if x not in dlst]</code>', \"<code>['A', 'B', 'C', 'D', 'E', 'F', 'G']\\n</code>\", '<code> cols = [x for x in df.columns.values.tolist() if x not in dlst]\\n</code>', '<code>df.loc[:, cols]</code>', '<code>df[cols]</code>', '<code>df.reindex(columns=cols)</code>', '<code>df.reindex_axis(cols, 1)</code>', '<code>   A  B  C  D  E  F  G\\n0  1  2  3  4  5  6  7\\n1  1  2  3  4  5  6  7\\n2  1  2  3  4  5  6  7\\n</code>', '<code>~df.columns.isin(dlst)</code>', '<code>~np.in1d(df.columns.values, dlst)</code>', '<code>[x not in dlst for x in df.columns.values.tolist()]</code>', '<code>(df.columns.values[:, None] != dlst).all(1)</code>', '<code>bools = [x not in dlst for x in df.columns.values.tolist()]\\n</code>', '<code>df.loc[: bools]</code>', '<code>   A  B  C  D  E  F  G\\n0  1  2  3  4  5  6  7\\n1  1  2  3  4  5  6  7\\n2  1  2  3  4  5  6  7\\n</code>', \"<code>setdiff1d = lambda df, dlst: np.setdiff1d(df.columns.values, dlst)\\ndifference = lambda df, dlst: df.columns.difference(dlst)\\ncolumndrop = lambda df, dlst: df.columns.drop(dlst, errors='ignore')\\nsetdifflst = lambda df, dlst: list(set(df.columns.values.tolist()).difference(dlst))\\ncomprehension = lambda df, dlst: [x for x in df.columns.values.tolist() if x not in dlst]\\n\\nloc = lambda df, cols: df.loc[:, cols]\\nslc = lambda df, cols: df[cols]\\nridx = lambda df, cols: df.reindex(columns=cols)\\nridxa = lambda df, cols: df.reindex_axis(cols, 1)\\n\\nisin = lambda df, dlst: ~df.columns.isin(dlst)\\nin1d = lambda df, dlst: ~np.in1d(df.columns.values, dlst)\\ncomp = lambda df, dlst: [x not in dlst for x in df.columns.values.tolist()]\\nbrod = lambda df, dlst: (df.columns.values[:, None] != dlst).all(1)\\n</code>\", '<code>res1 = pd.DataFrame(\\n    index=pd.MultiIndex.from_product([\\n        \\'loc slc ridx ridxa\\'.split(),\\n        \\'setdiff1d difference columndrop setdifflst comprehension\\'.split(),\\n    ], names=[\\'Select\\', \\'Label\\']),\\n    columns=[10, 30, 100, 300, 1000],\\n    dtype=float\\n)\\n\\nres2 = pd.DataFrame(\\n    index=pd.MultiIndex.from_product([\\n        \\'loc\\'.split(),\\n        \\'isin in1d comp brod\\'.split(),\\n    ], names=[\\'Select\\', \\'Label\\']),\\n    columns=[10, 30, 100, 300, 1000],\\n    dtype=float\\n)\\n\\nres = res1.append(res2).sort_index()\\n\\ndres = pd.Series(index=res.columns, name=\\'drop\\')\\n\\nfor j in res.columns:\\n    dlst = list(range(j))\\n    cols = list(range(j // 2, j + j // 2))\\n    d = pd.DataFrame(1, range(10), cols)\\n    dres.at[j] = timeit(\\'d.drop(dlst, 1, errors=\"ignore\")\\', \\'from __main__ import d, dlst\\', number=100)\\n    for s, l in res.index:\\n        stmt = \\'{}(d, {}(d, dlst))\\'.format(s, l)\\n        setp = \\'from __main__ import d, dlst, {}, {}\\'.format(s, l)\\n        res.at[(s, l), j] = timeit(stmt, setp, number=100)\\n\\nrs = res / dres\\n</code>', '<code>rs\\n\\n                          10        30        100       300        1000\\nSelect Label                                                           \\nloc    brod           0.747373  0.861979  0.891144  1.284235   3.872157\\n       columndrop     1.193983  1.292843  1.396841  1.484429   1.335733\\n       comp           0.802036  0.732326  1.149397  3.473283  25.565922\\n       comprehension  1.463503  1.568395  1.866441  4.421639  26.552276\\n       difference     1.413010  1.460863  1.587594  1.568571   1.569735\\n       in1d           0.818502  0.844374  0.994093  1.042360   1.076255\\n       isin           1.008874  0.879706  1.021712  1.001119   0.964327\\n       setdiff1d      1.352828  1.274061  1.483380  1.459986   1.466575\\n       setdifflst     1.233332  1.444521  1.714199  1.797241   1.876425\\nridx   columndrop     0.903013  0.832814  0.949234  0.976366   0.982888\\n       comprehension  0.777445  0.827151  1.108028  3.473164  25.528879\\n       difference     1.086859  1.081396  1.293132  1.173044   1.237613\\n       setdiff1d      0.946009  0.873169  0.900185  0.908194   1.036124\\n       setdifflst     0.732964  0.823218  0.819748  0.990315   1.050910\\nridxa  columndrop     0.835254  0.774701  0.907105  0.908006   0.932754\\n       comprehension  0.697749  0.762556  1.215225  3.510226  25.041832\\n       difference     1.055099  1.010208  1.122005  1.119575   1.383065\\n       setdiff1d      0.760716  0.725386  0.849949  0.879425   0.946460\\n       setdifflst     0.710008  0.668108  0.778060  0.871766   0.939537\\nslc    columndrop     1.268191  1.521264  2.646687  1.919423   1.981091\\n       comprehension  0.856893  0.870365  1.290730  3.564219  26.208937\\n       difference     1.470095  1.747211  2.886581  2.254690   2.050536\\n       setdiff1d      1.098427  1.133476  1.466029  2.045965   3.123452\\n       setdifflst     0.833700  0.846652  1.013061  1.110352   1.287831\\n</code>', \"<code>fig, axes = plt.subplots(2, 2, figsize=(8, 6), sharey=True)\\nfor i, (n, g) in enumerate([(n, g.xs(n)) for n, g in rs.groupby('Select')]):\\n    ax = axes[i // 2, i % 2]\\n    g.plot.bar(ax=ax, title=n)\\n    ax.legend_.remove()\\nfig.tight_layout()\\n</code>\", \"<code>df.drop(dlst, 1, errors='ignore')</code>\", '<code>reindex</code>', '<code>reindex_axis</code>', '<code>list(set(df.columns.values.tolist()).difference(dlst))</code>', '<code>drop</code>', '<code>np.setdiff1d</code>', '<code>rs.idxmin().pipe(\\n    lambda x: pd.DataFrame(\\n        dict(idx=x.values, val=rs.lookup(x.values, x.index)),\\n        x.index\\n    )\\n)\\n\\n                      idx       val\\n10     (ridx, setdifflst)  0.653431\\n30    (ridxa, setdifflst)  0.746143\\n100   (ridxa, setdifflst)  0.816207\\n300    (ridx, setdifflst)  0.780157\\n1000  (ridxa, setdifflst)  0.861622\\n</code>', '<code>index</code>', '<code>columns</code>', '<code>rename</code>', '<code>reindex</code>', \"<code>df.drop(columns=['column_a', 'column_c'])\\n</code>\", '<code>axis</code>']",
         "title": "Delete column from pandas DataFrame using python del",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 24,
               "answer_id": 13415772,
               "is_accepted": false,
               "last_activity_date": 1353065627,
               "body_markdown": "It&#39;s good practice to always use the `[]` notation, one reason is that attribute notation (`df.column_name`) does not work for numbered indices:\r\n\r\n    In [1]: df = DataFrame([[1, 2, 3], [4, 5, 6]])\r\n    \r\n    In [2]: df[1]\r\n    Out[2]: \r\n    0    2\r\n    1    5\r\n    Name: 1\r\n    \r\n    In [3]: df.1\r\n      File &quot;&lt;ipython-input-3-e4803c0d1066&gt;&quot;, line 1\r\n        df.1\r\n           ^\r\n    SyntaxError: invalid syntax",
               "id": "13415772",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1353065627,
               "score": 23
            },
            {
               "up_vote_count": 436,
               "answer_id": 13485766,
               "last_activity_date": 1511756594,
               "path": "3.stack.answer",
               "body_markdown": "It&#39;s difficult to make `del df.column_name` work simply as the result of syntactic limitations in Python. `del df[name]` gets translated to `df.__delitem__(name)` under the covers by Python.",
               "tags": [],
               "creation_date": 1353467551,
               "last_edit_date": 1511756594,
               "is_accepted": true,
               "id": "13485766",
               "down_vote_count": 8,
               "score": 428
            },
            {
               "up_vote_count": 1416,
               "answer_id": 18145399,
               "last_activity_date": 1510955403,
               "path": "3.stack.answer",
               "body_markdown": "The best way to do this in pandas is to use [``pd.drop()``][1]:\r\n\r\n    df = df.drop(&#39;column_name&#39;, 1)\r\n\r\nwhere `1` is the *axis* number (`0` for rows and `1` for columns.)\r\n\r\nTo delete the column without having to reassign `df` you can do:\r\n\r\n    df.drop(&#39;column_name&#39;, axis=1, inplace=True)\r\n\r\nFinally, to drop by column *number* instead of by column *label*, try this to delete, e.g. the 1st, 2nd and 4th columns:\r\n\r\n    df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index \r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html",
               "tags": [],
               "creation_date": 1376046729,
               "last_edit_date": 1510955403,
               "is_accepted": false,
               "id": "18145399",
               "down_vote_count": 2,
               "score": 1414
            },
            {
               "up_vote_count": 162,
               "answer_id": 22596982,
               "last_activity_date": 1504650105,
               "path": "3.stack.answer",
               "body_markdown": "    columns = [&#39;Col1&#39;, &#39;Col2&#39;, ...]\r\n    df.drop(columns, inplace=True, axis=1)\r\n\r\nThis will delete one or more columns in-place. Note that `inplace=True` was added in pandas v0.13 and won&#39;t work on older versions, do you&#39;d have to do assign the result back in that case:\r\n\r\n    df = df.drop(columns, axis=1)",
               "tags": [],
               "creation_date": 1395608277,
               "last_edit_date": 1504650105,
               "is_accepted": false,
               "id": "22596982",
               "down_vote_count": 2,
               "score": 160
            },
            {
               "up_vote_count": 70,
               "answer_id": 31431997,
               "last_activity_date": 1441580654,
               "path": "3.stack.answer",
               "body_markdown": "Drop by index\r\n-------------\r\ndelete first, second and fourth columns:\r\n\r\n    df.drop(df.columns[[0,1,3]], axis=1, inplace=True)\r\n\r\ndelete first column:\r\n\r\n    df.drop(df.columns[[0]], axis=1, inplace=True)\r\n\r\n\r\nThere is an optional parameter `inplace` so that the original\r\ndata can be modified without creating a copy.\r\n\r\n\r\nPopped\r\n------\r\n[Column selection, addition, deletion](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion)\r\n\r\ndelete column `column-name`:\r\n\r\n    df.pop(&#39;column-name&#39;)\r\n\r\nExamples:\r\n---------\r\n\r\n\tdf = DataFrame.from_items([(&#39;A&#39;, [1, 2, 3]), (&#39;B&#39;, [4, 5, 6]), (&#39;C&#39;, [7,8, 9])], orient=&#39;index&#39;, columns=[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])\r\n\t\r\n\r\n`print df`:\r\n\r\n\t   one  two  three\r\n\tA    1    2      3\r\n\tB    4    5      6\r\n\tC    7    8      9\r\n\r\n`df.drop(df.columns[[0]], axis=1, inplace=True)`  \r\n`print df`:\r\n\r\n\t   two  three\r\n\tA    2      3\r\n\tB    5      6\r\n\tC    8      9\r\n\r\n`three = df.pop(&#39;three&#39;)`  \r\n`print df`:\r\n\r\n\t   two\r\n\tA    2\r\n\tB    5\r\n\tC    8\r\n",
               "tags": [],
               "creation_date": 1436967443,
               "last_edit_date": 1441580654,
               "is_accepted": false,
               "id": "31431997",
               "down_vote_count": 1,
               "score": 69
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 31,
               "answer_id": 34576537,
               "is_accepted": false,
               "last_activity_date": 1451824189,
               "body_markdown": "A nice addition is the ability to **drop columns only if they exist**, this way you can cover more use cases, and it will only drop the existing columns from the labels passed to it: \r\n\r\nsimply add **errors=&#39;ignore&#39;** ,e.g:\r\n\r\n    df.drop([&#39;col_name_1&#39;,&#39;col_name_2&#39;,...,&#39;col_name_N&#39;],inplace=True,axis=1,errors=&#39;ignore&#39;)\r\n\r\n* this is new from pandas 0.16.1, docs are [here][1]\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.drop.html\r\n",
               "id": "34576537",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1451824189,
               "score": 30
            },
            {
               "up_vote_count": 20,
               "answer_id": 35385805,
               "last_activity_date": 1479827742,
               "path": "3.stack.answer",
               "body_markdown": "In pandas 0.16.1+ you can drop columns only if they exist per the solution posted by @eiTanLaVi.  Prior to that version, you can achieve the same result via a conditional list comprehension:\r\n\r\n    df.drop([col for col in [&#39;col_name_1&#39;,&#39;col_name_2&#39;,...,&#39;col_name_N&#39;] if col in df], \r\n            axis=1, inplace=True)",
               "tags": [],
               "creation_date": 1455400713,
               "last_edit_date": 1479827742,
               "is_accepted": false,
               "id": "35385805",
               "down_vote_count": 1,
               "score": 19
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 36749161,
               "is_accepted": false,
               "last_activity_date": 1461167738,
               "body_markdown": "the dot syntax works in **JS** but not in **python**.\r\n\r\n**Python**: del df[&#39;column_name&#39;]\r\n\r\n**JS**:     del df[&#39;column_name&#39;] OR del df.column_name",
               "id": "36749161",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1461167738,
               "score": 2
            },
            {
               "up_vote_count": 35,
               "answer_id": 36958937,
               "last_activity_date": 1477084809,
               "path": "3.stack.answer",
               "body_markdown": "from version 0.16.1 you can do \r\n\r\n    df.drop([&#39;column_name&#39;], axis = 1, inplace = True, errors = &#39;ignore&#39;)",
               "tags": [],
               "creation_date": 1462042668,
               "last_edit_date": 1477084809,
               "is_accepted": false,
               "id": "36958937",
               "down_vote_count": 1,
               "score": 34
            },
            {
               "up_vote_count": 52,
               "answer_id": 37000877,
               "last_activity_date": 1462269674,
               "path": "3.stack.answer",
               "body_markdown": "The actual question posed, missed by most answers here is:\r\n###Why can&#39;t I use `del df.column_name`?\r\n\r\nAt first we need to understand the problem, which requires us to dive into [*python magic methods*][1].\r\n\r\nAs Wes points out in his answer `del df[&#39;column&#39;]` maps to the python *magic method* `df.__delitem__(&#39;column&#39;)` which is [implemented in pandas to drop the column][2]\r\n\r\nHowever, as pointed out in the link above about [*python magic methods*][1]:\r\n\r\n&gt; In fact, __del__ should almost never be used because of the precarious circumstances under which it is called; use it with caution!\r\n\r\nYou could argue that `del df[&#39;column_name&#39;]` should not be used or encouraged, and thereby `del df.column_name` should not even be considered.\r\n\r\nHowever, in theory, `del df.column_name` could be implemeted to work in pandas using [the *magic method `__delattr__`*][3]. This does however introduce certain problems, problems which the `del df[&#39;column_name&#39;]` implementation already has, but in lesser degree.\r\n\r\n##Example Problem\r\nWhat if I define a column in a dataframe called &quot;dtypes&quot; or &quot;columns&quot;.\r\n\r\nThen assume I want to delete these columns.\r\n\r\n`del df.dtypes` would make the `__delattr__` method confused as if it should delete the &quot;dtypes&quot; attribute or the &quot;dtypes&quot; column.\r\n\r\n##Architectural questions behind this problem\r\n\r\n 1. Is a dataframe a\r\n    collection of *columns*?\r\n 2. Is a dataframe a collection of *rows*?\r\n 3. Is a column an *attribute* of a dataframe?\r\n\r\n###Pandas answers:\r\n 1. Yes, in all ways\r\n 2. No, but if you want it to be, you can use the `.ix`, `.loc` or `.iloc` methods.\r\n 3. Maybe, do you want to *read* data? Then **yes**, *unless* the name of the attribute is already taken by another attribute belonging to the dataframe. Do you want to *modify* data? Then **no**.\r\n\r\n#TLDR;\r\nYou cannot do `del df.column_name` because pandas has a quite wildly grown architecture that needs to be reconsidered in order for this kind of *cognitive dissonance* not to occur to its users.\r\n\r\n###Protip:\r\nDon&#39;t use df.column_name, It may be pretty, but it causes *cognitive dissonance*\r\n\r\n###Zen of Python quotes that fits in here:\r\nThere are multiple ways of deleting a column.\r\n&gt; There should be one-- and preferably only one --obvious way to do it.\r\n\r\nColumns are sometimes attributes but sometimes not.\r\n&gt; Special cases aren&#39;t special enough to break the rules.\r\n\r\nDoes `del df.dtypes` delete the dtypes attribute or the dtypes column?\r\n&gt; In the face of ambiguity, refuse the temptation to guess.\r\n\r\n\r\n  [1]: http://www.rafekettler.com/magicmethods.html\r\n  [2]: https://github.com/pydata/pandas/blob/c6110e25b3eceb2f25022c2aa9ccea03c0b8b359/pandas/core/generic.py#L1580\r\n  [3]: http://www.rafekettler.com/magicmethods.html#access",
               "tags": [],
               "creation_date": 1462268931,
               "last_edit_date": 1462269674,
               "is_accepted": false,
               "id": "37000877",
               "down_vote_count": 3,
               "score": 49
            },
            {
               "up_vote_count": 7,
               "answer_id": 46314092,
               "last_activity_date": 1505917735,
               "path": "3.stack.answer",
               "body_markdown": "#TL;DR\r\nA lot of effort to find a marginally more efficient solution.  Difficult to justify the added complexity while sacrificing the simplicity of `df.drop(dlst, 1, errors=&#39;ignore&#39;)`\r\n\r\n    df.reindex_axis(np.setdiff1d(df.columns.values, dlst), 1)\r\n\r\n**Preamble**  \r\nDeleting a column is semantically the same as selecting the other columns.  I&#39;ll show a few additional methods to consider.  \r\n\r\nI&#39;ll also focus on the general solution of deleting multiple columns at once and allowing for the attempt to delete columns not present.  \r\n\r\nUsing these solutions are general and will work for the simple case as well.\r\n___\r\n**Setup**  \r\nConsider the `pd.DataFrame` `df` and list to delete `dlst`\r\n\r\n    df = pd.DataFrame(dict(zip(&#39;ABCDEFGHIJ&#39;, range(1, 11))), range(3))\r\n    dlst = list(&#39;HIJKLM&#39;)\r\n\r\n___\r\n\r\n    df\r\n    \r\n       A  B  C  D  E  F  G  H  I   J\r\n    0  1  2  3  4  5  6  7  8  9  10\r\n    1  1  2  3  4  5  6  7  8  9  10\r\n    2  1  2  3  4  5  6  7  8  9  10\r\n\r\n___\r\n\r\n    dlst\r\n    \r\n    [&#39;H&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;M&#39;]\r\n\r\nThe result should look like:\r\n\r\n    df.drop(dlst, 1, errors=&#39;ignore&#39;)\r\n\r\n       A  B  C  D  E  F  G\r\n    0  1  2  3  4  5  6  7\r\n    1  1  2  3  4  5  6  7\r\n    2  1  2  3  4  5  6  7\r\n\r\n___\r\n\r\nSince I&#39;m equating deleting a column to selecting the other columns, I&#39;ll break it into two types:\r\n\r\n1. Label selection\r\n2. Boolean selection\r\n\r\n___\r\n\r\n#Label Selection  \r\nWe start by manufacturing the list/array of labels that represent the columns we want to keep and without the columns we want to delete.\r\n\r\n1. `df.columns.difference(dlst)`\r\n\r\n        Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;], dtype=&#39;object&#39;)\r\n\r\n2. `np.setdiff1d(df.columns.values, dlst)`\r\n\r\n        array([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;], dtype=object)\r\n\r\n3. `df.columns.drop(dlst, errors=&#39;ignore&#39;)`\r\n\r\n        Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;], dtype=&#39;object&#39;)\r\n\r\n4. `list(set(df.columns.values.tolist()).difference(dlst))`\r\n\r\n        # does not preserve order\r\n        [&#39;E&#39;, &#39;D&#39;, &#39;B&#39;, &#39;F&#39;, &#39;G&#39;, &#39;A&#39;, &#39;C&#39;]\r\n\r\n5. `[x for x in df.columns.values.tolist() if x not in dlst]`\r\n\r\n        [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;]\r\n___\r\n\r\n**Columns from Labels**  \r\nFor the sake of comparing the selection process, assume:\r\n\r\n     cols = [x for x in df.columns.values.tolist() if x not in dlst]\r\n\r\nThen we can evaluate  \r\n\r\n1. `df.loc[:, cols]`\r\n2. `df[cols]`\r\n3. `df.reindex(columns=cols)`\r\n4. `df.reindex_axis(cols, 1)`\r\n\r\nWhich all evaluate to:\r\n\r\n       A  B  C  D  E  F  G\r\n    0  1  2  3  4  5  6  7\r\n    1  1  2  3  4  5  6  7\r\n    2  1  2  3  4  5  6  7\r\n\r\n___\r\n\r\n# Boolean Slice\r\nWe can construct an array/list of booleans for slicing\r\n\r\n1. `~df.columns.isin(dlst)`\r\n2. `~np.in1d(df.columns.values, dlst)`\r\n3. `[x not in dlst for x in df.columns.values.tolist()]`\r\n4. `(df.columns.values[:, None] != dlst).all(1)`\r\n\r\n**Columns from Boolean**  \r\nFor the sake of comparison  \r\n\r\n    bools = [x not in dlst for x in df.columns.values.tolist()]\r\n\r\n1. `df.loc[: bools]`\r\n\r\nWhich all evaluate to:\r\n\r\n       A  B  C  D  E  F  G\r\n    0  1  2  3  4  5  6  7\r\n    1  1  2  3  4  5  6  7\r\n    2  1  2  3  4  5  6  7\r\n\r\n___\r\n\r\n**Robust Timing**  \r\n\r\n*Functions*  \r\n\r\n    setdiff1d = lambda df, dlst: np.setdiff1d(df.columns.values, dlst)\r\n    difference = lambda df, dlst: df.columns.difference(dlst)\r\n    columndrop = lambda df, dlst: df.columns.drop(dlst, errors=&#39;ignore&#39;)\r\n    setdifflst = lambda df, dlst: list(set(df.columns.values.tolist()).difference(dlst))\r\n    comprehension = lambda df, dlst: [x for x in df.columns.values.tolist() if x not in dlst]\r\n    \r\n    loc = lambda df, cols: df.loc[:, cols]\r\n    slc = lambda df, cols: df[cols]\r\n    ridx = lambda df, cols: df.reindex(columns=cols)\r\n    ridxa = lambda df, cols: df.reindex_axis(cols, 1)\r\n    \r\n    isin = lambda df, dlst: ~df.columns.isin(dlst)\r\n    in1d = lambda df, dlst: ~np.in1d(df.columns.values, dlst)\r\n    comp = lambda df, dlst: [x not in dlst for x in df.columns.values.tolist()]\r\n    brod = lambda df, dlst: (df.columns.values[:, None] != dlst).all(1)\r\n\r\n**Testing**  \r\n\r\n    res1 = pd.DataFrame(\r\n        index=pd.MultiIndex.from_product([\r\n            &#39;loc slc ridx ridxa&#39;.split(),\r\n            &#39;setdiff1d difference columndrop setdifflst comprehension&#39;.split(),\r\n        ], names=[&#39;Select&#39;, &#39;Label&#39;]),\r\n        columns=[10, 30, 100, 300, 1000],\r\n        dtype=float\r\n    )\r\n    \r\n    res2 = pd.DataFrame(\r\n        index=pd.MultiIndex.from_product([\r\n            &#39;loc&#39;.split(),\r\n            &#39;isin in1d comp brod&#39;.split(),\r\n        ], names=[&#39;Select&#39;, &#39;Label&#39;]),\r\n        columns=[10, 30, 100, 300, 1000],\r\n        dtype=float\r\n    )\r\n    \r\n    res = res1.append(res2).sort_index()\r\n    \r\n    dres = pd.Series(index=res.columns, name=&#39;drop&#39;)\r\n    \r\n    for j in res.columns:\r\n        dlst = list(range(j))\r\n        cols = list(range(j // 2, j + j // 2))\r\n        d = pd.DataFrame(1, range(10), cols)\r\n        dres.at[j] = timeit(&#39;d.drop(dlst, 1, errors=&quot;ignore&quot;)&#39;, &#39;from __main__ import d, dlst&#39;, number=100)\r\n        for s, l in res.index:\r\n            stmt = &#39;{}(d, {}(d, dlst))&#39;.format(s, l)\r\n            setp = &#39;from __main__ import d, dlst, {}, {}&#39;.format(s, l)\r\n            res.at[(s, l), j] = timeit(stmt, setp, number=100)\r\n\r\n    rs = res / dres\r\n\r\n___\r\n\r\n    rs\r\n\r\n                              10        30        100       300        1000\r\n    Select Label                                                           \r\n    loc    brod           0.747373  0.861979  0.891144  1.284235   3.872157\r\n           columndrop     1.193983  1.292843  1.396841  1.484429   1.335733\r\n           comp           0.802036  0.732326  1.149397  3.473283  25.565922\r\n           comprehension  1.463503  1.568395  1.866441  4.421639  26.552276\r\n           difference     1.413010  1.460863  1.587594  1.568571   1.569735\r\n           in1d           0.818502  0.844374  0.994093  1.042360   1.076255\r\n           isin           1.008874  0.879706  1.021712  1.001119   0.964327\r\n           setdiff1d      1.352828  1.274061  1.483380  1.459986   1.466575\r\n           setdifflst     1.233332  1.444521  1.714199  1.797241   1.876425\r\n    ridx   columndrop     0.903013  0.832814  0.949234  0.976366   0.982888\r\n           comprehension  0.777445  0.827151  1.108028  3.473164  25.528879\r\n           difference     1.086859  1.081396  1.293132  1.173044   1.237613\r\n           setdiff1d      0.946009  0.873169  0.900185  0.908194   1.036124\r\n           setdifflst     0.732964  0.823218  0.819748  0.990315   1.050910\r\n    ridxa  columndrop     0.835254  0.774701  0.907105  0.908006   0.932754\r\n           comprehension  0.697749  0.762556  1.215225  3.510226  25.041832\r\n           difference     1.055099  1.010208  1.122005  1.119575   1.383065\r\n           setdiff1d      0.760716  0.725386  0.849949  0.879425   0.946460\r\n           setdifflst     0.710008  0.668108  0.778060  0.871766   0.939537\r\n    slc    columndrop     1.268191  1.521264  2.646687  1.919423   1.981091\r\n           comprehension  0.856893  0.870365  1.290730  3.564219  26.208937\r\n           difference     1.470095  1.747211  2.886581  2.254690   2.050536\r\n           setdiff1d      1.098427  1.133476  1.466029  2.045965   3.123452\r\n           setdifflst     0.833700  0.846652  1.013061  1.110352   1.287831\r\n\r\n___\r\n\r\n    fig, axes = plt.subplots(2, 2, figsize=(8, 6), sharey=True)\r\n    for i, (n, g) in enumerate([(n, g.xs(n)) for n, g in rs.groupby(&#39;Select&#39;)]):\r\n        ax = axes[i // 2, i % 2]\r\n        g.plot.bar(ax=ax, title=n)\r\n        ax.legend_.remove()\r\n    fig.tight_layout()\r\n\r\nThis is relative to the time it takes to run `df.drop(dlst, 1, errors=&#39;ignore&#39;)`.  It seems like after all that effort, we only improve performance modestly.\r\n\r\n[![enter image description here][1]][1]\r\n\r\nIf fact the best solutions use `reindex` or `reindex_axis` on the hack `list(set(df.columns.values.tolist()).difference(dlst))`.  A close second and still very marginally better than `drop` is `np.setdiff1d`.\r\n\r\n    rs.idxmin().pipe(\r\n        lambda x: pd.DataFrame(\r\n            dict(idx=x.values, val=rs.lookup(x.values, x.index)),\r\n            x.index\r\n        )\r\n    )\r\n    \r\n                          idx       val\r\n    10     (ridx, setdifflst)  0.653431\r\n    30    (ridxa, setdifflst)  0.746143\r\n    100   (ridxa, setdifflst)  0.816207\r\n    300    (ridx, setdifflst)  0.780157\r\n    1000  (ridxa, setdifflst)  0.861622\r\n\r\n  [1]: https://i.stack.imgur.com/208z6.png",
               "tags": [],
               "creation_date": 1505886199,
               "last_edit_date": 1505917735,
               "is_accepted": false,
               "id": "46314092",
               "down_vote_count": 0,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 46913256,
               "is_accepted": false,
               "last_activity_date": 1508855463,
               "body_markdown": "# Pandas 0.21+ Answer\r\nPandas version 0.21 has slightly changed the drop method to include both the `index` and `columns` parameters to match the signature of the `rename` and `reindex` methods. \r\n\r\n    df.drop(columns=[&#39;column_a&#39;, &#39;column_c&#39;])\r\n\r\nPersonally, I prefer using the `axis` parameter to denote columns or index because it is the predominant keyword parameter used in nearly all pandas methods. But, now you have some added choices in version 0.21.",
               "id": "46913256",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1508855463,
               "score": 7
            }
         ],
         "link": "https://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe-using-python-del",
         "id": "858127-2265"
      },
      {
         "up_vote_count": "215",
         "path": "2.stack",
         "body_markdown": "I want to perform my own complex operations on financial data in dataframes in a sequential manner.\r\n\r\nFor example I am using the following MSFT CSV file taken from [Yahoo Finance](http://finance.yahoo.com/q/hp?s=MSFT):\r\n\r\n    Date,Open,High,Low,Close,Volume,Adj Close\r\n    2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\r\n    2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\r\n    2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\r\n    2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\r\n    \r\n    ....\r\n\r\nI then do the following:\r\n\r\n    #!/usr/bin/env python\r\n    from pandas import *\r\n    \r\n    df = read_csv(&#39;table.csv&#39;)\r\n    \r\n    for i, row in enumerate(df.values):\r\n        date = df.index[i]\r\n        open, high, low, close, adjclose = row\r\n        #now perform analysis on open/close based on date, etc..\r\n\r\nIs that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? `df.iteritems` unfortunately only iterates column by column.\r\n",
         "view_count": "243116",
         "answer_count": "9",
         "tags": "['python', 'performance', 'for-loop', 'pandas']",
         "creation_date": "1319121974",
         "last_edit_date": "1483561486",
         "code_snippet": "['<code>Date,Open,High,Low,Close,Volume,Adj Close\\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\\n\\n....\\n</code>', \"<code>#!/usr/bin/env python\\nfrom pandas import *\\n\\ndf = read_csv('table.csv')\\n\\nfor i, row in enumerate(df.values):\\n    date = df.index[i]\\n    open, high, low, close, adjclose = row\\n    #now perform analysis on open/close based on date, etc..\\n</code>\", '<code>df.iteritems</code>', '<code>df.apply()</code>', '<code>unutbu</code>', '<code>The key to speed with NumPy arrays is to perform your operations on the whole array at once</code>', '<code>for index, row in df.iterrows():\\n\\n    # do some logic here\\n</code>', '<code>itertuples()</code>', '<code>iterrows</code>', '<code>itertuples</code>', '<code>itertuples</code>', '<code>iterrows</code>', '<code>close</code>', '<code>pct_change = close[1:]/close[:-1]\\n</code>', '<code>pct_change = []\\nfor row in close:\\n    pct_change.append(...)\\n</code>', '<code>for i, row in enumerate(...)</code>', '<code>for date, row in df.T.iteritems():\\n   # do some logic here\\n</code>', '<code>def my_algo(ndarray[object] dates, ndarray[float64_t] open,\\n            ndarray[float64_t] low, ndarray[float64_t] high,\\n            ndarray[float64_t] close, ndarray[float64_t] volume):\\n    cdef:\\n        Py_ssize_t i, n\\n        float64_t foo\\n    n = len(dates)\\n\\n    for i from 0 &lt;= i &lt; n:\\n        foo = close[i] - open[i] # will be extremely fast\\n</code>', '<code>df.iterrows()</code>', '<code>df.T.iteritems()</code>', '<code>df.iterrows()</code>', \"<code>t = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})\\nB = []\\nC = []\\nA = time.time()\\nfor i,r in t.iterrows():\\n    C.append((r['a'], r['b']))\\nB.append(time.time()-A)\\n\\nC = []\\nA = time.time()\\nfor ir in t.itertuples():\\n    C.append((ir[1], ir[2]))    \\nB.append(time.time()-A)\\n\\nC = []\\nA = time.time()\\nfor r in zip(t['a'], t['b']):\\n    C.append((r[0], r[1]))\\nB.append(time.time()-A)\\n\\nprint B\\n</code>\", '<code>[0.5639059543609619, 0.017839908599853516, 0.005645036697387695]\\n</code>', '<code>zip()</code>', '<code>list(zip())</code>', '<code>t.index</code>', '<code>iterrows</code>', '<code>itertuples</code>', '<code>iterkv</code>', '<code>iterkv</code>', '<code>iterrows(): Iterate over the rows of a DataFrame as (index, Series) pairs.... itertuples(): Iterate over the rows of a DataFrame as tuples of the values. This is a lot faster as iterrows(), and is in most cases preferable to use to iterate over the values of a DataFrame.</code>', '<code>df[b] = df[a].apply(lambda col: do stuff with col here)\\n</code>', '<code>apply</code>', \"<code>df['c'] = df[['a','b']].apply(lambda x: do stuff with x[0] and x[1] here, axis=1)</code>\", '<code>x</code>', '<code>col</code>', '<code>iterrows</code>', '<code>itertuples</code>', '<code>itertuples</code>', '<code>iterrows</code>', '<code>iterrows</code>', '<code>itertuples</code>', '<code>itertuples</code>', \"<code>&gt;&gt;&gt; df = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]},\\n                      index=['a', 'b'])\\n&gt;&gt;&gt; df\\n   col1  col2\\na     1   0.1\\nb     2   0.2\\n&gt;&gt;&gt; for row in df.itertuples():\\n...     print(row.col1, row.col2)\\n...\\n1, 0.1\\n2, 0.2\\n</code>\", '<code>&gt;&gt;&gt; for index in df.index:\\n...     print (\"df[\" + str(index) + \"][\\'B\\']=\" + str(df[\\'B\\'][index]))\\n</code>', '<code>&gt;&gt;&gt; for index, row in df.iterrows():\\n...     print (\"df[\" + str(index) + \"][\\'B\\']=\" + str(row[\\'B\\']))\\n</code>', '<code>&gt;&gt;&gt; for row in df.itertuples():\\n...     print (\"df[\" + str(row.Index) + \"][\\'B\\']=\" + str(row.B))\\n</code>', \"<code>df[0]['B']=125\\ndf[1]['B']=415\\ndf[2]['B']=23\\ndf[3]['B']=456\\ndf[4]['B']=189\\ndf[5]['B']=456\\ndf[6]['B']=12\\n</code>\"]",
         "title": "What is the most efficient way to loop through dataframes with pandas?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 123,
               "answer_id": 7837947,
               "is_accepted": false,
               "last_activity_date": 1319122936,
               "body_markdown": "Pandas is based on NumPy arrays.\r\nThe key to speed with NumPy arrays is to perform your operations on the whole array at once, never row-by-row or item-by-item.\r\n\r\nFor example, if `close` is a 1-d array, and you want the day-over-day percent change,\r\n\r\n    pct_change = close[1:]/close[:-1]\r\n\r\nThis computes the entire array of percent changes as one statement, instead of \r\n\r\n    pct_change = []\r\n    for row in close:\r\n        pct_change.append(...)\r\n\r\nSo try to avoid the Python loop `for i, row in enumerate(...)` entirely, and\r\nthink about how to perform your calculations with operations on the entire array (or dataframe) as a whole, rather than row-by-row.",
               "id": "7837947",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1319122936,
               "score": 121
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 67,
               "answer_id": 7849789,
               "is_accepted": false,
               "last_activity_date": 1319202293,
               "body_markdown": "You can loop through the rows by transposing and then calling iteritems:\r\n\r\n    for date, row in df.T.iteritems():\r\n       # do some logic here\r\n\r\nI am not certain about efficiency in that case. To get the best possible performance in an iterative algorithm, you might want to explore writing it in [Cython][1], so you could do something like:\r\n\r\n    def my_algo(ndarray[object] dates, ndarray[float64_t] open,\r\n                ndarray[float64_t] low, ndarray[float64_t] high,\r\n                ndarray[float64_t] close, ndarray[float64_t] volume):\r\n        cdef:\r\n            Py_ssize_t i, n\r\n            float64_t foo\r\n        n = len(dates)\r\n    \r\n        for i from 0 &lt;= i &lt; n:\r\n            foo = close[i] - open[i] # will be extremely fast\r\n\r\nI would recommend writing the algorithm in pure Python first, make sure it works and see how fast it is-- if it&#39;s not fast enough, convert things to Cython like this with minimal work to get something that&#39;s about as fast as hand-coded C/C++.\r\n\r\n\r\n  [1]: http://cython.org",
               "id": "7849789",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1319202293,
               "score": 66
            },
            {
               "up_vote_count": 253,
               "answer_id": 11617194,
               "last_activity_date": 1438262866,
               "path": "3.stack.answer",
               "body_markdown": "The newest versions of pandas now include a built-in function for iterating over rows. \r\n\r\n    for index, row in df.iterrows():\r\n\r\n        # do some logic here\r\n\r\nOr, if you want it faster use `itertuples()`\r\n\r\nBut, unutbu&#39;s suggestion to use numpy functions to avoid iterating over rows will produce the fastest code. ",
               "tags": [],
               "creation_date": 1343063379,
               "last_edit_date": 1438262866,
               "is_accepted": true,
               "id": "11617194",
               "down_vote_count": 1,
               "score": 252
            },
            {
               "up_vote_count": 20,
               "answer_id": 11706782,
               "last_activity_date": 1343537606,
               "path": "3.stack.answer",
               "body_markdown": "I checked out `iterrows` after noticing [Nick Crawford&#39;s](https://stackoverflow.com/users/475872/nick-crawford) answer, but found that it yields (index, Series) tuples. Not sure which would work best for you, but I ended up using the `itertuples` method for my problem, which yields (index, row_value1...) tuples.\r\n\r\nThere&#39;s also `iterkv`, which iterates through (column, series) tuples.",
               "tags": [],
               "creation_date": 1343537606,
               "last_edit_date": 1495540967,
               "is_accepted": false,
               "id": "11706782",
               "down_vote_count": 0,
               "score": 20
            },
            {
               "up_vote_count": 14,
               "answer_id": 19415186,
               "last_activity_date": 1423109774,
               "path": "3.stack.answer",
               "body_markdown": "Just as a small addition, you can also do an apply if you have a complex function that you apply to a single column:\r\n\r\nhttp://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html\r\n\r\n    df[b] = df[a].apply(lambda col: do stuff with col here)\r\n",
               "tags": [],
               "creation_date": 1381963091,
               "last_edit_date": 1423109774,
               "is_accepted": false,
               "id": "19415186",
               "down_vote_count": 0,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 26930092,
               "is_accepted": false,
               "last_activity_date": 1415968251,
               "body_markdown": "Another suggestion would be to combine groupby with vectorized calculations if subsets of the rows shared characteristics which allowed you to do so. ",
               "id": "26930092",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1415968251,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 57,
               "answer_id": 34311080,
               "is_accepted": false,
               "last_activity_date": 1450265984,
               "body_markdown": "Like what has been mentioned before, pandas object is most efficient when process the whole array at once. However for those who really need to loop through a pandas DataFrame to perform something, like me, I found at least three ways to do it. I have done a short test to see which one of the three is the least time consuming.\r\n\r\n    t = pd.DataFrame({&#39;a&#39;: range(0, 10000), &#39;b&#39;: range(10000, 20000)})\r\n    B = []\r\n    C = []\r\n    A = time.time()\r\n    for i,r in t.iterrows():\r\n        C.append((r[&#39;a&#39;], r[&#39;b&#39;]))\r\n    B.append(time.time()-A)\r\n\r\n    C = []\r\n    A = time.time()\r\n    for ir in t.itertuples():\r\n        C.append((ir[1], ir[2]))    \r\n    B.append(time.time()-A)\r\n\r\n    C = []\r\n    A = time.time()\r\n    for r in zip(t[&#39;a&#39;], t[&#39;b&#39;]):\r\n        C.append((r[0], r[1]))\r\n    B.append(time.time()-A)\r\n    \r\n    print B\r\n\r\nResult:\r\n\r\n    [0.5639059543609619, 0.017839908599853516, 0.005645036697387695]\r\n\r\nThis is probably not the best way to measure the time consumption but it&#39;s quick for me.\r\n\r\nHere are some pros and cons IMHO:\r\n\r\n - .iterrows(): return index and row items in separate variables, but significantly slower\r\n - .itertuples(): faster than .iterrows(), but return index together with row items, ir[0] is the index\r\n - zip: quickest, but no access to index of the row",
               "id": "34311080",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1450265984,
               "score": 57
            },
            {
               "up_vote_count": 1,
               "answer_id": 47153661,
               "last_activity_date": 1514131553,
               "path": "3.stack.answer",
               "body_markdown": "As [@joris][1] pointed out, `iterrows` is much slower than `itertuples` and `itertuples` is approximately 100 times fater than `iterrows`, and I tested speed of both methods in a DataFrame with 5027505 records the result is for `iterrows`, it is 1200it/s, and  `itertuples` is 120000it/s.\r\n\r\nIf you use `itertuples`, note that every element in the for loop is a namedtuple, so to get the value in each column, you can refer to the following example code\r\n\r\n    &gt;&gt;&gt; df = pd.DataFrame({&#39;col1&#39;: [1, 2], &#39;col2&#39;: [0.1, 0.2]},\r\n                          index=[&#39;a&#39;, &#39;b&#39;])\r\n    &gt;&gt;&gt; df\r\n       col1  col2\r\n    a     1   0.1\r\n    b     2   0.2\r\n    &gt;&gt;&gt; for row in df.itertuples():\r\n    ...     print(row.col1, row.col2)\r\n    ...\r\n    1, 0.1\r\n    2, 0.2\r\n\r\n  [1]: https://stackoverflow.com/users/653364/joris\r\n",
               "tags": [],
               "creation_date": 1510045117,
               "last_edit_date": 1514131553,
               "is_accepted": false,
               "id": "47153661",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48606632,
               "is_accepted": false,
               "last_activity_date": 1517736496,
               "body_markdown": "You have three options:\r\n\r\nBy [index][1] (simplest):\r\n\r\n    &gt;&gt;&gt; for index in df.index:\r\n    ...     print (&quot;df[&quot; + str(index) + &quot;][&#39;B&#39;]=&quot; + str(df[&#39;B&#39;][index]))\r\n\r\nWith [iterrows][2] (most used):\r\n\r\n    &gt;&gt;&gt; for index, row in df.iterrows():\r\n    ...     print (&quot;df[&quot; + str(index) + &quot;][&#39;B&#39;]=&quot; + str(row[&#39;B&#39;]))\r\n\r\nWith [itertuples][3] (fastest):\r\n\r\n    &gt;&gt;&gt; for row in df.itertuples():\r\n    ...     print (&quot;df[&quot; + str(row.Index) + &quot;][&#39;B&#39;]=&quot; + str(row.B))\r\n\r\nThree options display something like:\r\n\r\n    df[0][&#39;B&#39;]=125\r\n    df[1][&#39;B&#39;]=415\r\n    df[2][&#39;B&#39;]=23\r\n    df[3][&#39;B&#39;]=456\r\n    df[4][&#39;B&#39;]=189\r\n    df[5][&#39;B&#39;]=456\r\n    df[6][&#39;B&#39;]=12\r\n\r\nSource: [neural-networks.io][4]\r\n\r\n\r\n  [1]: https://www.neural-networks.io/en/python/dataframes.php#iterate-over-rows-by-index\r\n  [2]: https://www.neural-networks.io/en/python/dataframes.php#iterate-over-rows-with-iterrows\r\n  [3]: https://www.neural-networks.io/en/python/dataframes.php#iterate-over-rows-with-itertuples\r\n  [4]: https://www.neural-networks.io/en/python/dataframes.php\r\n",
               "id": "48606632",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1517736496,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas",
         "id": "858127-2266"
      },
      {
         "up_vote_count": "198",
         "path": "2.stack",
         "body_markdown": "I work with Series and DataFrames on the terminal a lot. The default `__repr__` for a Series returns a reducted sample, with some head and tail values, but the rest missing.\r\n\r\nIs there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.",
         "view_count": "147923",
         "answer_count": "5",
         "tags": "['python', 'pandas', 'dataframe']",
         "creation_date": "1380656767",
         "last_edit_date": "1380657227",
         "code_snippet": "['<code>__repr__</code>', \"<code>pd.set_option('display.max_rows', 1000)</code>\", '<code>display.max_rows</code>', '<code>__repr__</code>', '<code>set_option</code>', \"<code>with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\\n    print(df)\\n</code>\", '<code>display</code>', '<code>print</code>', '<code>None</code>', '<code>with pd.option_context()</code>', '<code>precision</code>', '<code>max_colwidth</code>', '<code>expand_frame_repr</code>', '<code>colheader_justify</code>', '<code>date_yearfirst</code>', '<code>encoding</code>', '<code>display(df)</code>', '<code>print(df)</code>', \"<code>def print_full(x):\\n    pd.set_option('display.max_rows', len(x))\\n    print(x)\\n    pd.reset_option('display.max_rows')\\n</code>\", '<code>.table-striped</code>', '<code>print(df.to_string())\\n</code>', '<code>with pd.option_context()</code>', '<code>precision</code>', '<code>max_colwidth</code>', '<code>expand_frame_repr</code>', '<code>colheader_justify</code>', '<code>date_yearfirst</code>', '<code>encoding</code>', \"<code>pd.set_option('display.max_columns', None)\\npd.set_option('display.max_rows', None)\\n</code>\", \"<code>pd.describe_option('display')\\n</code>\", '<code>-1</code>', '<code>None</code>', \"<code>pd.set_option('display.height',1000)\\npd.set_option('display.max_rows',500)\\npd.set_option('display.max_columns',500)\\npd.set_option('display.width',1000)\\n</code>\"]",
         "title": "Is there a way to (pretty) print the entire Pandas Series / DataFrame?",
         "_childDocuments_": [
            {
               "up_vote_count": 139,
               "answer_id": 19126566,
               "last_activity_date": 1488104743,
               "path": "3.stack.answer",
               "body_markdown": "Sure, if this comes up a lot, make a function like this one. You can even configure it to load every time you start IPython: https://ipython.org/ipython-doc/1/config/overview.html       ,\r\n\r\n    def print_full(x):\r\n        pd.set_option(&#39;display.max_rows&#39;, len(x))\r\n        print(x)\r\n        pd.reset_option(&#39;display.max_rows&#39;)\r\n\r\nAs for coloring, getting too elaborate with colors sounds counterproductive to me, but I agree something like [bootstrap&#39;s ``.table-striped``](http://getbootstrap.com/2.3.2/base-css.html#tables) would be nice. You could always [create an issue](https://github.com/pydata/pandas/issues) to suggest this feature.",
               "tags": [],
               "creation_date": 1380664110,
               "last_edit_date": 1488104743,
               "is_accepted": false,
               "id": "19126566",
               "down_vote_count": 2,
               "score": 137
            },
            {
               "up_vote_count": 303,
               "answer_id": 30691921,
               "last_activity_date": 1507155188,
               "path": "3.stack.answer",
               "body_markdown": "You can also use the [option_context][1], with one or more options:\r\n\r\n    with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, 3):\r\n        print(df)\r\n\r\nThis will automatically return the options to their previous values.\r\n\r\n**Addition**:\r\nIf you are working on jupyter-notebook, using `display` instead of `print` will use jupyter rich display logic.\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.option_context.html",
               "tags": [],
               "creation_date": 1433668935,
               "last_edit_date": 1507155188,
               "is_accepted": true,
               "id": "30691921",
               "down_vote_count": 1,
               "score": 302
            },
            {
               "up_vote_count": 27,
               "answer_id": 37347783,
               "last_activity_date": 1490370681,
               "path": "3.stack.answer",
               "body_markdown": "Simply use this for printing entire frames:\r\n\r\n    pd.set_option(&#39;display.max_columns&#39;, None)\r\n    pd.set_option(&#39;display.max_rows&#39;, None)\r\n\r\nAlso you can create a pretty-print function with context manager, like in examples above.\r\n    \r\n\r\nFor full list of useful options, see:\r\n\r\n    pd.describe_option(&#39;display&#39;)\r\n\r\nFew examples of use:\r\nhttp://pandas.pydata.org/pandas-docs/stable/options.html",
               "tags": [],
               "creation_date": 1463750334,
               "last_edit_date": 1490370681,
               "is_accepted": false,
               "id": "37347783",
               "down_vote_count": 1,
               "score": 26
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 131,
               "answer_id": 39923958,
               "is_accepted": false,
               "last_activity_date": 1475865936,
               "body_markdown": "No need to hack settings. There is a simple way:\r\n\r\n    print(df.to_string())\r\n\r\n",
               "id": "39923958",
               "tags": [],
               "down_vote_count": 5,
               "creation_date": 1475865936,
               "score": 126
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47686565,
               "is_accepted": false,
               "last_activity_date": 1512613312,
               "body_markdown": "Try this\r\n\r\n    pd.set_option(&#39;display.height&#39;,1000)\r\n    pd.set_option(&#39;display.max_rows&#39;,500)\r\n    pd.set_option(&#39;display.max_columns&#39;,500)\r\n    pd.set_option(&#39;display.width&#39;,1000)",
               "id": "47686565",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1512613312,
               "score": -1
            }
         ],
         "link": "https://stackoverflow.com/questions/19124601/is-there-a-way-to-pretty-print-the-entire-pandas-series-dataframe",
         "id": "858127-2267"
      },
      {
         "up_vote_count": "530",
         "path": "2.stack",
         "body_markdown": "I am trying to use IPython notebook on MacOS X with Python 2.7.2 and IPython 1.1.0.\r\n\r\nI cannot get matplotlib graphics to show up inline.\r\n\r\n    import matplotlib\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    %matplotlib inline  \r\n\r\nI have also tried `%pylab inline` and the ipython command line arguments `--pylab=inline` but this makes no difference.\r\n\r\n    x = np.linspace(0, 3*np.pi, 500)\r\n    plt.plot(x, np.sin(x**2))\r\n    plt.title(&#39;A simple chirp&#39;)\r\n    plt.show()\r\n\r\n\r\nInstead of inline graphics, I get this:\r\n\r\n    &lt;matplotlib.figure.Figure at 0x110b9c450&gt;\r\n\r\nAnd `matplotlib.get_backend()` shows that I have the `&#39;module://IPython.kernel.zmq.pylab.backend_inline&#39;` backend.",
         "view_count": "611428",
         "answer_count": "10",
         "tags": "['python', 'matplotlib', 'ipython', 'ipython-notebook']",
         "creation_date": "1381944882",
         "last_edit_date": "1439528771",
         "code_snippet": "['<code>import matplotlib\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n%matplotlib inline  \\n</code>', '<code>%pylab inline</code>', '<code>--pylab=inline</code>', \"<code>x = np.linspace(0, 3*np.pi, 500)\\nplt.plot(x, np.sin(x**2))\\nplt.title('A simple chirp')\\nplt.show()\\n</code>\", '<code>&lt;matplotlib.figure.Figure at 0x110b9c450&gt;\\n</code>', '<code>matplotlib.get_backend()</code>', \"<code>'module://IPython.kernel.zmq.pylab.backend_inline'</code>\", '<code>&lt;matplotlib.figure.Figure at 0x110b9c450&gt;</code>', '<code>&lt;matplotlib.text.Text at 0x94f9320&gt;</code>', '<code>%matplotlib inline</code>', '<code>MacOSX</code>', '<code>%matplotlib inline</code>', '<code>%matplotlib inline\\n\\nimport matplotlib\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n</code>', \"<code>c.IPKernelApp.matplotlib=&lt;CaselessStrEnum&gt;\\n  Default: None\\n  Choices: ['auto', 'gtk', 'gtk3', 'inline', 'nbagg', 'notebook', 'osx', 'qt', 'qt4', 'qt5', 'tk', 'wx']\\n  Configure matplotlib for interactive use with the default matplotlib backend.\\n</code>\", '<code>--pylab inline</code>', \"<code>import matplotlib' do versus </code>\", '<code>%matplotlib notebook\\n\\nimport matplotlib.pyplot as plt\\n</code>', '<code>%matplotlib nbagg\\n\\nimport matplotlib.pyplot as plt\\n</code>', \"<code>%config InlineBackend.figure_format='retina'</code>\", '<code>%matplotlib notebook</code>', '<code>%matplotlib inline</code>', '<code>%matplotlib notebook</code>', '<code>%matplotlib inline</code>', '<code>%matplotlib inline</code>', '<code>ipython notebook --pylab inline</code>', '<code>%pylab inline</code>', '<code>%matplotlib inline</code>', '<code>~/.ipython/profile_default/ipython_config.py</code>', \"<code>c.InteractiveShellApp.matplotlib = 'inline'</code>\", '<code>ipython_notebook_config.py</code>', '<code>--pylab</code>', '<code>%matplotlib inline</code>', '<code>import matplotlib\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n%matplotlib inline  \\n</code>', '<code>%matplotlib inline</code>', '<code>plt.show()</code>', '<code>plt.show()</code>', '<code>%matplotlib inline</code>', '<code>%matplotlib inline</code>', '<code>df_randNumbers1 = pd.DataFrame(np.random.randint(0,100,size=(100, 6)), columns=list(\\'ABCDEF\\'))\\n\\ndf_randNumbers1.ix[:,[\"A\",\"B\"]].plot.kde()\\n</code>', '<code>()</code>', '<code>df_randNumbers1.ix[:,[\"A\",\"B\"]].plot.kde\\n</code>', '<code>&lt;bound method FramePlotMethods.kde of &lt;pandas.tools.plotting.FramePlotMethods object at 0x000001DDAF029588&gt;&gt;\\n</code>', '<code>()</code>', '<code>kde</code>', '<code>kde</code>', '<code>%matplotlib inline</code>']",
         "title": "How to make IPython notebook matplotlib plot inline",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 19416267,
               "is_accepted": false,
               "last_activity_date": 1381969727,
               "body_markdown": "I found a workaround that is quite satisfactory.  I installed [Anaconda Python][1] and this now works out of the box for me.\r\n\r\n\r\n  [1]: http://docs.continuum.io/anaconda/index.html",
               "id": "19416267",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1381969727,
               "score": 4
            },
            {
               "up_vote_count": 21,
               "answer_id": 24444083,
               "last_activity_date": 1432843410,
               "path": "3.stack.answer",
               "body_markdown": "I&#39;m not sure why joaquin posted his answer as a comment, but it is the correct answer:\r\n\r\nstart ipython with `ipython notebook --pylab inline`\r\n\r\nEdit: Ok, this is now deprecated as per comment below. Use the %pylab magic.",
               "tags": [],
               "creation_date": 1403844055,
               "last_edit_date": 1432843410,
               "is_accepted": false,
               "id": "24444083",
               "down_vote_count": 2,
               "score": 19
            },
            {
               "up_vote_count": 752,
               "answer_id": 24884342,
               "last_activity_date": 1435883852,
               "path": "3.stack.answer",
               "body_markdown": "I used `%matplotlib inline` in the first cell of the notebook and it works. I think you should try:\r\n\r\n    %matplotlib inline\r\n    \r\n    import matplotlib\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n\r\nYou can also always start all your IPython kernels in inline mode by default by setting the following config options in your config files:\r\n\r\n    c.IPKernelApp.matplotlib=&lt;CaselessStrEnum&gt;\r\n      Default: None\r\n      Choices: [&#39;auto&#39;, &#39;gtk&#39;, &#39;gtk3&#39;, &#39;inline&#39;, &#39;nbagg&#39;, &#39;notebook&#39;, &#39;osx&#39;, &#39;qt&#39;, &#39;qt4&#39;, &#39;qt5&#39;, &#39;tk&#39;, &#39;wx&#39;]\r\n      Configure matplotlib for interactive use with the default matplotlib backend.",
               "tags": [],
               "creation_date": 1406023284,
               "last_edit_date": 1435883852,
               "is_accepted": true,
               "id": "24884342",
               "down_vote_count": 2,
               "score": 750
            },
            {
               "up_vote_count": 8,
               "answer_id": 25351828,
               "last_activity_date": 1453758869,
               "path": "3.stack.answer",
               "body_markdown": "I have to agree with foobarbecue (I don&#39;t have enough recs to be able to simply insert a comment under his post):\r\n\r\nIt&#39;s now recommended that python notebook isn&#39;t started wit the argument `--pylab`, and according to Fernando Perez (creator of ipythonnb) `%matplotlib inline` should be the initial notebook command.\r\n\r\nSee here: http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Part%203%20-%20Plotting%20with%20Matplotlib.ipynb",
               "tags": [],
               "creation_date": 1408296963,
               "last_edit_date": 1453758869,
               "is_accepted": false,
               "id": "25351828",
               "down_vote_count": 0,
               "score": 8
            },
            {
               "up_vote_count": 3,
               "answer_id": 27436973,
               "last_activity_date": 1418360398,
               "path": "3.stack.answer",
               "body_markdown": "I did the anaconda install but matplotlib is not plotting\r\n\r\nIt starts plotting when i did this\r\n\r\n    import matplotlib\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    %matplotlib inline  ",
               "tags": [],
               "creation_date": 1418359112,
               "last_edit_date": 1418360398,
               "is_accepted": false,
               "id": "27436973",
               "down_vote_count": 1,
               "score": 2
            },
            {
               "up_vote_count": 56,
               "answer_id": 29573210,
               "last_activity_date": 1505817816,
               "path": "3.stack.answer",
               "body_markdown": "&lt;kbd&gt;Ctrl&lt;/kbd&gt; + &lt;kbd&gt;Enter&lt;/kbd&gt;\r\n\r\n&gt; `%matplotlib inline`\r\n\r\nMagic Line :D\r\n\r\nSee: [Plotting with Matplotlib][1].\r\n\r\n\r\n  [1]: http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Part%203%20-%20Plotting%20with%20Matplotlib.ipynb",
               "tags": [],
               "creation_date": 1428717666,
               "last_edit_date": 1505817816,
               "is_accepted": false,
               "id": "29573210",
               "down_vote_count": 0,
               "score": 56
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 32380490,
               "is_accepted": false,
               "last_activity_date": 1441296187,
               "body_markdown": "To make matplotlib inline by default in Jupyter (IPython 3):\r\n\r\n1. Edit file `~/.ipython/profile_default/ipython_config.py`\r\n\r\n2. Add line `c.InteractiveShellApp.matplotlib = &#39;inline&#39;`\r\n\r\nPlease note that adding this line to `ipython_notebook_config.py` would not work.\r\nOtherwise it works well with Jupyter and IPython 3.1.0",
               "id": "32380490",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1441296187,
               "score": 9
            },
            {
               "up_vote_count": 141,
               "answer_id": 34222212,
               "last_activity_date": 1461310049,
               "path": "3.stack.answer",
               "body_markdown": "If your matplotlib version is above 1.4, it is also possible to use\r\n\r\nIPython 3.x and above\r\n\r\n    %matplotlib notebook\r\n\r\n    import matplotlib.pyplot as plt\r\n\r\nolder versions\r\n\r\n    %matplotlib nbagg\r\n\r\n    import matplotlib.pyplot as plt\r\n\r\nBoth will activate the [nbagg backend](http://matplotlib.org/users/whats_new.html#the-nbagg-backend), which enables interactivity.\r\n\r\n[![Example plot with the nbagg backend][1]][1]\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/u9YvH.png",
               "tags": [],
               "creation_date": 1449832397,
               "last_edit_date": 1461310049,
               "is_accepted": false,
               "id": "34222212",
               "down_vote_count": 0,
               "score": 141
            },
            {
               "up_vote_count": 0,
               "answer_id": 43165068,
               "last_activity_date": 1491113716,
               "path": "3.stack.answer",
               "body_markdown": "You can simulate this problem with a syntax mistake, however, `%matplotlib inline` won&#39;t resolve the issue. \r\n\r\nFirst an example of the right way to create a plot. Everything works as expected with the imports and magic that [eNord9][1] supplied. \r\n\r\n    df_randNumbers1 = pd.DataFrame(np.random.randint(0,100,size=(100, 6)), columns=list(&#39;ABCDEF&#39;))\r\n\r\n    df_randNumbers1.ix[:,[&quot;A&quot;,&quot;B&quot;]].plot.kde()\r\n\r\nHowever, by leaving the `()` off the end of the plot type you receive a somewhat ambiguous non-error.\r\n\r\nErronious code:\r\n\r\n    df_randNumbers1.ix[:,[&quot;A&quot;,&quot;B&quot;]].plot.kde\r\n\r\nExample error:\r\n\r\n    &lt;bound method FramePlotMethods.kde of &lt;pandas.tools.plotting.FramePlotMethods object at 0x000001DDAF029588&gt;&gt;\r\n\r\nOther than this one line message, there is no stack trace or other obvious reason to think you made a syntax error. The plot doesn&#39;t print. \r\n\r\n\r\n  [1]: https://stackoverflow.com/users/3864073/enord9",
               "tags": [],
               "creation_date": 1491112182,
               "last_edit_date": 1495542398,
               "is_accepted": false,
               "id": "43165068",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 1,
               "answer_id": 44840398,
               "last_activity_date": 1505817861,
               "path": "3.stack.answer",
               "body_markdown": "On Ubuntu you can remove `%matplotlib inline` as long as you have  `plt.show()` after creation of graph.",
               "tags": [],
               "creation_date": 1498805791,
               "last_edit_date": 1505817861,
               "is_accepted": false,
               "id": "44840398",
               "down_vote_count": 0,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/19410042/how-to-make-ipython-notebook-matplotlib-plot-inline",
         "id": "858127-2268"
      },
      {
         "up_vote_count": "181",
         "body_markdown": "I want to create a list of dates, starting with today, and going back an arbitrary number of days, say, in my example 100 days. Is there a better way to do it than this?\r\n\r\n    import datetime\r\n    \r\n    a = datetime.datetime.today()\r\n    numdays = 100\r\n    dateList = []\r\n    for x in range (0, numdays):\r\n        dateList.append(a - datetime.timedelta(days = x))\r\n    print dateList\r\n\r\n\r\n",
         "view_count": "177775",
         "answer_count": "15",
         "tags": "['python', 'datetime', 'date']",
         "creation_date": "1245002639",
         "path": "2.stack",
         "code_snippet": "['<code>import datetime\\n\\na = datetime.datetime.today()\\nnumdays = 100\\ndateList = []\\nfor x in range (0, numdays):\\n    dateList.append(a - datetime.timedelta(days = x))\\nprint dateList\\n</code>', '<code>base = datetime.datetime.today()\\ndate_list = [base - datetime.timedelta(days=x) for x in range(0, numdays)]\\n</code>', '<code>range(0, numdays)</code>', '<code>datetimes = [start + timedelta(seconds=x*60+randint(-30, 30)) for x in range (0, range_end*5, 5)]</code>', '<code>range_end = 10</code>', '<code>Pandas</code>', '<code>pd.date_range()</code>', '<code>import pandas as pd\\ndatelist = pd.date_range(pd.datetime.today(), periods=100).tolist()\\n</code>', '<code>bdate_range</code>', '<code>list(map(pd.Timestamp.to_pydatetime, datelist))</code>', '<code>import datetime\\n\\ndef date_generator():\\n  from_date = datetime.datetime.today()\\n  while True:\\n    yield from_date\\n    from_date = from_date - datetime.timedelta(days=1)\\n</code>', '<code>&gt;&gt;&gt; import itertools\\n&gt;&gt;&gt; dates = itertools.islice(date_generator(), 3)\\n&gt;&gt;&gt; list(dates)\\n[datetime.datetime(2009, 6, 14, 19, 12, 21, 703890), datetime.datetime(2009, 6, 13, 19, 12, 21, 703890), datetime.datetime(2009, 6, 12, 19, 12, 21, 703890)]\\n</code>', '<code>date_generator = (datetime.datetime.today() - datetime.timedelta(days=i) for i in itertools.count())\\n</code>', '<code>&gt;&gt;&gt; dates = itertools.islice(date_generator, 3)\\n&gt;&gt;&gt; list(dates)\\n[datetime.datetime(2009, 6, 15, 1, 32, 37, 286765), datetime.datetime(2009, 6, 14, 1, 32, 37, 286836), datetime.datetime(2009, 6, 13, 1, 32, 37, 286859)]\\n</code>', '<code>import datetime\\n\\nstart = datetime.datetime.strptime(\"21-06-2014\", \"%d-%m-%Y\")\\nend = datetime.datetime.strptime(\"07-07-2014\", \"%d-%m-%Y\")\\ndate_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\\n\\nfor date in date_generated:\\n    print date.strftime(\"%d-%m-%Y\")\\n</code>', '<code>def date_range(start_date, end_date):\\n    for ordinal in range(start_date.toordinal(), end_date.toordinal()):\\n        yield datetime.date.fromordinal(ordinal)\\n</code>', '<code>date_range = [\\n    datetime.date.fromordinal(ordinal) \\n    for ordinal in range(\\n        start_date.toordinal(),\\n        end_date.toordinal(),\\n    )\\n]\\n</code>', '<code>from dateutil import rrule\\nfrom datetime import datetime\\n\\nlist(rrule.rrule(rrule.DAILY,count=100,dtstart=datetime.now()))\\n</code>', '<code>dateutil.rrule</code>', '<code>dateutil.rrule</code>', '<code>rrule</code>', '<code>range()</code>', '<code>import datetime\\nstart_date = datetime.date(2011, 01, 01)\\nend_date   = datetime.date(2014, 01, 01)\\n\\ndates_2011_2013 = [ start_date + datetime.timedelta(n) for n in range(int ((end_date - start_date).days))]\\n</code>', '<code>from __builtin__ import range as _range\\nfrom datetime import datetime, timedelta\\n\\ndef range(*args):\\n    if len(args) != 3:\\n        return _range(*args)\\n    start, stop, step = args\\n    if start &lt; stop:\\n        cmp = lambda a, b: a &lt; b\\n        inc = lambda a: a + step\\n    else:\\n        cmp = lambda a, b: a &gt; b\\n        inc = lambda a: a - step\\n    output = [start]\\n    while cmp(start, stop):\\n        start = inc(start)\\n        output.append(start)\\n\\n    return output\\n\\nprint range(datetime(2011, 5, 1), datetime(2011, 10, 1), timedelta(days=30))\\n</code>', '<code>output = []</code>', '<code>while cmp(...)</code>', '<code>range(0,10,1)</code>', '<code>_range(0,10,1)</code>', '<code>date_range</code>', '<code>import datetime\\nfrom time import mktime\\n\\ndef convert_date_to_datetime(date_object):\\n    date_tuple = date_object.timetuple()\\n    date_timestamp = mktime(date_tuple)\\n    return datetime.datetime.fromtimestamp(date_timestamp)\\n\\ndef date_range(how_many=7):\\n    for x in range(0, how_many):\\n        some_date = datetime.datetime.today() - datetime.timedelta(days=x)\\n        some_datetime = convert_date_to_datetime(some_date.date())\\n        yield some_datetime\\n\\ndef pick_two_dates(how_many=7):\\n    a = b = convert_date_to_datetime(datetime.datetime.now().date())\\n    for each_date in date_range(how_many):\\n        b = a\\n        a = each_date\\n        if a == b:\\n            continue\\n        yield b, a\\n</code>', '<code>python -c \"import sys,datetime; print(\\'\\\\n\\'.join([(datetime.datetime.today() - datetime.timedelta(days=x)).strftime(\\\\\"%Y/%m/%d\\\\\") for x in range(0,int(sys.argv[1])) if (datetime.datetime.today() - datetime.timedelta(days=x)).isoweekday()&lt;6]))\" 10\\n</code>', '<code>python -c \"import sys,datetime; print(\\'\\\\n\\'.join([(datetime.datetime.strptime(sys.argv[1],\\\\\"%Y/%m/%d\\\\\") - datetime.timedelta(days=x)).strftime(\\\\\"%Y/%m/%d \\\\\") for x in range(0,int(sys.argv[2])) if (datetime.datetime.today() - datetime.timedelta(days=x)).isoweekday()&lt;6]))\" 2015/12/30 10\\n</code>', '<code>python -c \"import sys,datetime; print(\\'\\\\n\\'.join([(datetime.datetime.strptime(sys.argv[1],\\\\\"%Y/%m/%d\\\\\") + datetime.timedelta(days=x)).strftime(\\\\\"%Y/%m/%d\\\\\") for x in range(0,int((datetime.datetime.strptime(sys.argv[2], \\\\\"%Y/%m/%d\\\\\") - datetime.datetime.strptime(sys.argv[1], \\\\\"%Y/%m/%d\\\\\")).days)) if (datetime.datetime.strptime(sys.argv[1], \\\\\"%Y/%m/%d\\\\\") + datetime.timedelta(days=x)).isoweekday()&lt;6]))\" 2015/12/15 2015/12/30\\n</code>', '<code>from matplotlib.dates import drange\\nimport datetime\\n\\nbase = datetime.date.today()\\nend  = base + datetime.timedelta(days=100)\\ndelta = datetime.timedelta(days=1)\\nl = drange(base, end, delta)\\n</code>', '<code>l</code>', '<code>start</code>', '<code>end</code>', '<code>start = datetime.datetime(2017,1,1)\\nend = datetime.datetime.today()\\ndaterange = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\\n</code>', \"<code>import datetime;\\nprint [(datetime.date.today() - datetime.timedelta(days=x)).strftime('%Y-%m-%d') for x in range(-5, 0)]\\n</code>\", \"<code>['2017-12-11', '2017-12-10', '2017-12-09', '2017-12-08', '2017-12-07']\\n</code>\", '<code>from datetime import datetime, timedelta\\nfrom dateutil import parser\\ndef getDateRange(begin, end):\\n    \"\"\"  \"\"\"\\n    beginDate = parser.parse(begin)\\n    endDate =  parser.parse(end)\\n    delta = endDate-beginDate\\n    numdays = delta.days + 1\\n    dayList = [datetime.strftime(beginDate + timedelta(days=x), \\'%Y%m%d\\') for x in range(0, numdays)]\\n    return dayList\\n</code>', '<code>import datetime    \\ndef date_generator():\\n    cur = base = datetime.date.today()\\n    end  = base + datetime.timedelta(days=100)\\n    delta = datetime.timedelta(days=1)\\n    while(end&gt;base):\\n        base = base+delta\\n        print base\\n\\ndate_generator()\\n</code>', '<code>end</code>', '<code>base - datetime.timedelta</code>']",
         "title": "Creating a range of dates in Python",
         "_childDocuments_": [
            {
               "up_vote_count": 280,
               "answer_id": 993367,
               "last_activity_date": 1397593563,
               "path": "3.stack.answer",
               "body_markdown": "Marginally better...\r\n\r\n    base = datetime.datetime.today()\r\n    date_list = [base - datetime.timedelta(days=x) for x in range(0, numdays)]",
               "tags": [],
               "creation_date": 1245002792,
               "last_edit_date": 1397593563,
               "is_accepted": true,
               "id": "993367",
               "down_vote_count": 1,
               "score": 279
            },
            {
               "up_vote_count": 35,
               "answer_id": 993369,
               "last_activity_date": 1245026061,
               "path": "3.stack.answer",
               "body_markdown": "You can write a generator function that returns date objects starting from today:\r\n\r\n    import datetime\r\n\r\n    def date_generator():\r\n      from_date = datetime.datetime.today()\r\n      while True:\r\n        yield from_date\r\n        from_date = from_date - datetime.timedelta(days=1)\r\n\r\nThis generator returns dates starting from today and going backwards one day at a time. Here is how to take the first 3 dates:\r\n\r\n    &gt;&gt;&gt; import itertools\r\n    &gt;&gt;&gt; dates = itertools.islice(date_generator(), 3)\r\n    &gt;&gt;&gt; list(dates)\r\n    [datetime.datetime(2009, 6, 14, 19, 12, 21, 703890), datetime.datetime(2009, 6, 13, 19, 12, 21, 703890), datetime.datetime(2009, 6, 12, 19, 12, 21, 703890)]\r\n\r\nThe advantage of this approach over a loop or list comprehension is that you can go back as many times as you want.\r\n\r\n**Edit**\r\n\r\nA more compact version using a generator expression instead of a function:\r\n\r\n    date_generator = (datetime.datetime.today() - datetime.timedelta(days=i) for i in itertools.count())\r\n\r\nUsage:\r\n\r\n    &gt;&gt;&gt; dates = itertools.islice(date_generator, 3)\r\n    &gt;&gt;&gt; list(dates)\r\n    [datetime.datetime(2009, 6, 15, 1, 32, 37, 286765), datetime.datetime(2009, 6, 14, 1, 32, 37, 286836), datetime.datetime(2009, 6, 13, 1, 32, 37, 286859)]",
               "tags": [],
               "creation_date": 1245002817,
               "last_edit_date": 1245026061,
               "is_accepted": false,
               "id": "993369",
               "down_vote_count": 0,
               "score": 35
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 6046264,
               "is_accepted": false,
               "last_activity_date": 1305728475,
               "body_markdown": "A bit of a late answer I know, but I just had the same problem and decided that Python&#39;s internal range function was a bit lacking in this respect so I&#39;ve overridden it in a util module of mine.\r\n\r\n    from __builtin__ import range as _range\r\n    from datetime import datetime, timedelta\r\n\r\n    def range(*args):\r\n        if len(args) != 3:\r\n            return _range(*args)\r\n        start, stop, step = args\r\n        if start &lt; stop:\r\n            cmp = lambda a, b: a &lt; b\r\n            inc = lambda a: a + step\r\n        else:\r\n            cmp = lambda a, b: a &gt; b\r\n            inc = lambda a: a - step\r\n        output = [start]\r\n        while cmp(start, stop):\r\n            start = inc(start)\r\n            output.append(start)\r\n\r\n        return output\r\n\r\n    print range(datetime(2011, 5, 1), datetime(2011, 10, 1), timedelta(days=30))\r\n\r\n",
               "id": "6046264",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1305728475,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 9984132,
               "is_accepted": false,
               "last_activity_date": 1333402269,
               "body_markdown": "Here is gist I created, from my own code, this might help. (I know the question is too old, but others can use it)\r\n\r\nhttps://gist.github.com/2287345\r\n\r\n(same thing below)\r\n\r\n    import datetime\r\n    from time import mktime\r\n    \r\n    def convert_date_to_datetime(date_object):\r\n        date_tuple = date_object.timetuple()\r\n        date_timestamp = mktime(date_tuple)\r\n        return datetime.datetime.fromtimestamp(date_timestamp)\r\n    \r\n    def date_range(how_many=7):\r\n        for x in range(0, how_many):\r\n            some_date = datetime.datetime.today() - datetime.timedelta(days=x)\r\n            some_datetime = convert_date_to_datetime(some_date.date())\r\n            yield some_datetime\r\n    \r\n    def pick_two_dates(how_many=7):\r\n        a = b = convert_date_to_datetime(datetime.datetime.now().date())\r\n        for each_date in date_range(how_many):\r\n            b = a\r\n            a = each_date\r\n            if a == b:\r\n                continue\r\n            yield b, a",
               "id": "9984132",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1333402269,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 27,
               "answer_id": 11324695,
               "is_accepted": false,
               "last_activity_date": 1341388328,
               "body_markdown": "yeah, reinvent the wheel....\r\njust search the forum and you&#39;ll get something like this: \r\n\r\n    from dateutil import rrule\r\n    from datetime import datetime\r\n    \r\n    list(rrule.rrule(rrule.DAILY,count=100,dtstart=datetime.now()))",
               "id": "11324695",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1341388328,
               "score": 24
            },
            {
               "up_vote_count": 124,
               "answer_id": 23190286,
               "last_activity_date": 1515435389,
               "path": "3.stack.answer",
               "body_markdown": "[``Pandas``][1] is great for time series in general, and has direct support for date ranges. \r\n\r\nFor example [``pd.date_range()``][2]:\r\n\r\n    import pandas as pd\r\n    datelist = pd.date_range(pd.datetime.today(), periods=100).tolist()\r\n\r\nIt also has lots of options to make life easier. For example if you only wanted weekdays, you would just swap in [`bdate_range`][3].\r\n\r\nSee http://pandas.pydata.org/pandas-docs/stable/timeseries.html#generating-ranges-of-timestamps\r\n\r\nIn addition it fully supports pytz timezones and can smoothly span spring/autumn DST shifts.\r\n\r\n\r\n  [1]: http://pandas.pydata.org/\r\n  [2]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.date_range.html\r\n  [3]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.bdate_range.html",
               "tags": [],
               "creation_date": 1398050173,
               "last_edit_date": 1515435389,
               "is_accepted": false,
               "id": "23190286",
               "down_vote_count": 0,
               "score": 124
            },
            {
               "up_vote_count": 33,
               "answer_id": 24637447,
               "last_activity_date": 1405067601,
               "path": "3.stack.answer",
               "body_markdown": "Get range of dates between specified start and end date (Optimized for time &amp; space complexity):\r\n\r\n    import datetime\r\n\r\n    start = datetime.datetime.strptime(&quot;21-06-2014&quot;, &quot;%d-%m-%Y&quot;)\r\n    end = datetime.datetime.strptime(&quot;07-07-2014&quot;, &quot;%d-%m-%Y&quot;)\r\n    date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\r\n\r\n    for date in date_generated:\r\n        print date.strftime(&quot;%d-%m-%Y&quot;)",
               "tags": [],
               "creation_date": 1404838227,
               "last_edit_date": 1405067601,
               "is_accepted": false,
               "id": "24637447",
               "down_vote_count": 0,
               "score": 33
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 29480840,
               "is_accepted": false,
               "last_activity_date": 1428359912,
               "body_markdown": "Matplotlib related\r\n\r\n    from matplotlib.dates import drange\r\n    import datetime\r\n\r\n    base = datetime.date.today()\r\n    end  = base + datetime.timedelta(days=100)\r\n    delta = datetime.timedelta(days=1)\r\n    l = drange(base, end, delta)",
               "id": "29480840",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1428359912,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 31452467,
               "is_accepted": false,
               "last_activity_date": 1437044875,
               "body_markdown": "\r\n\r\n    import datetime    \r\n    def date_generator():\r\n        cur = base = datetime.date.today()\r\n        end  = base + datetime.timedelta(days=100)\r\n        delta = datetime.timedelta(days=1)\r\n        while(end&gt;base):\r\n            base = base+delta\r\n            print base\r\n            \r\n    date_generator()",
               "id": "31452467",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1437044875,
               "score": 0
            },
            {
               "up_vote_count": 26,
               "answer_id": 32616832,
               "last_activity_date": 1499902790,
               "path": "3.stack.answer",
               "body_markdown": "You can also use the day ordinal to make it simpler:\r\n\r\n    def date_range(start_date, end_date):\r\n        for ordinal in range(start_date.toordinal(), end_date.toordinal()):\r\n            yield datetime.date.fromordinal(ordinal)\r\n\r\nOr as suggested in the comments you can create a list like this:\r\n\r\n    date_range = [\r\n        datetime.date.fromordinal(ordinal) \r\n        for ordinal in range(\r\n            start_date.toordinal(),\r\n            end_date.toordinal(),\r\n        )\r\n    ]",
               "tags": [],
               "creation_date": 1442430769,
               "last_edit_date": 1499902790,
               "is_accepted": false,
               "id": "32616832",
               "down_vote_count": 0,
               "score": 26
            },
            {
               "up_vote_count": 4,
               "answer_id": 34533517,
               "last_activity_date": 1451494583,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s a one liner for bash scripts to get a list of weekdays, this is python 3. Easily modified for whatever, the int at the end is the number of days in the past you want. \r\n\r\n    python -c &quot;import sys,datetime; print(&#39;\\n&#39;.join([(datetime.datetime.today() - datetime.timedelta(days=x)).strftime(\\&quot;%Y/%m/%d\\&quot;) for x in range(0,int(sys.argv[1])) if (datetime.datetime.today() - datetime.timedelta(days=x)).isoweekday()&lt;6]))&quot; 10\r\n\r\nHere is a variant to provide a start (or rather, end) date\r\n\r\n    python -c &quot;import sys,datetime; print(&#39;\\n&#39;.join([(datetime.datetime.strptime(sys.argv[1],\\&quot;%Y/%m/%d\\&quot;) - datetime.timedelta(days=x)).strftime(\\&quot;%Y/%m/%d \\&quot;) for x in range(0,int(sys.argv[2])) if (datetime.datetime.today() - datetime.timedelta(days=x)).isoweekday()&lt;6]))&quot; 2015/12/30 10\r\n\r\nHere is a variant for arbitrary start and end dates. not that this isn&#39;t terribly efficient, but is good for putting in a for loop in a bash script:\r\n\r\n    python -c &quot;import sys,datetime; print(&#39;\\n&#39;.join([(datetime.datetime.strptime(sys.argv[1],\\&quot;%Y/%m/%d\\&quot;) + datetime.timedelta(days=x)).strftime(\\&quot;%Y/%m/%d\\&quot;) for x in range(0,int((datetime.datetime.strptime(sys.argv[2], \\&quot;%Y/%m/%d\\&quot;) - datetime.datetime.strptime(sys.argv[1], \\&quot;%Y/%m/%d\\&quot;)).days)) if (datetime.datetime.strptime(sys.argv[1], \\&quot;%Y/%m/%d\\&quot;) + datetime.timedelta(days=x)).isoweekday()&lt;6]))&quot; 2015/12/15 2015/12/30\r\n",
               "tags": [],
               "creation_date": 1451493815,
               "last_edit_date": 1451494583,
               "is_accepted": false,
               "id": "34533517",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 2,
               "answer_id": 36637717,
               "last_activity_date": 1460692067,
               "path": "3.stack.answer",
               "body_markdown": "    from datetime import datetime, timedelta\r\n    from dateutil import parser\r\n    def getDateRange(begin, end):\r\n        &quot;&quot;&quot;  &quot;&quot;&quot;\r\n        beginDate = parser.parse(begin)\r\n        endDate =  parser.parse(end)\r\n        delta = endDate-beginDate\r\n        numdays = delta.days + 1\r\n        dayList = [datetime.strftime(beginDate + timedelta(days=x), &#39;%Y%m%d&#39;) for x in range(0, numdays)]\r\n        return dayList",
               "tags": [],
               "creation_date": 1460690609,
               "last_edit_date": 1460692067,
               "is_accepted": false,
               "id": "36637717",
               "down_vote_count": 1,
               "score": 1
            },
            {
               "up_vote_count": 15,
               "answer_id": 36651311,
               "last_activity_date": 1460734399,
               "path": "3.stack.answer",
               "body_markdown": "From the title of this question I was expecting to find something like `range()`, that would let me specify two dates and create a list with all the dates in between. That way one does not need to calculate the number of days between those two dates, if one does not know it beforehand.\r\n\r\nSo with the risk of being slightly off-topic, this one-liner does the job:\r\n\r\n    import datetime\r\n    start_date = datetime.date(2011, 01, 01)\r\n    end_date   = datetime.date(2014, 01, 01)\r\n\r\n    dates_2011_2013 = [ start_date + datetime.timedelta(n) for n in range(int ((end_date - start_date).days))]\r\n\r\nAll credits to [this answer][1]! \r\n\r\n\r\n  [1]: https://stackoverflow.com/a/1060330/4041970",
               "tags": [],
               "creation_date": 1460734399,
               "last_edit_date": 1495542893,
               "is_accepted": false,
               "id": "36651311",
               "down_vote_count": 1,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 46244058,
               "is_accepted": false,
               "last_activity_date": 1505493144,
               "body_markdown": "Here&#39;s a slightly different answer building off of S.Lott&#39;s answer that gives a list of dates between two dates `start` and `end`. In the example below, from the start of 2017 to today.\r\n\r\n\r\n    start = datetime.datetime(2017,1,1)\r\n    end = datetime.datetime.today()\r\n    daterange = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\r\n",
               "id": "46244058",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1505493144,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 47671475,
               "is_accepted": false,
               "last_activity_date": 1512554405,
               "body_markdown": "Based on answers I wrote for my self this:\r\n\r\n    import datetime;\r\n    print [(datetime.date.today() - datetime.timedelta(days=x)).strftime(&#39;%Y-%m-%d&#39;) for x in range(-5, 0)]\r\n\r\nOutput\r\n\r\n    [&#39;2017-12-11&#39;, &#39;2017-12-10&#39;, &#39;2017-12-09&#39;, &#39;2017-12-08&#39;, &#39;2017-12-07&#39;]\r\n\r\nThe difference is that I get the &#39;date&#39; object not datetime.&#39;datetime&#39; object.\r\n",
               "id": "47671475",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512554405,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/993358/creating-a-range-of-dates-in-python",
         "id": "858127-2269"
      },
      {
         "up_vote_count": "178",
         "path": "2.stack",
         "body_markdown": "This seems rather obvious, but I can&#39;t seem to figure out how do I convert an index of data frame to a column?\r\n\r\nFor example:\r\n    \r\n    df=\r\n               gi  ptt_loc\r\n     0  384444683      593  \r\n     1  384444684      594 \r\n     2  384444686      596  \r\n\r\nTo,\r\n\r\n    df=\r\n        index1       gi    ptt_loc\r\n     0  0     384444683      593  \r\n     1  1     384444684      594 \r\n     2  2     384444686      596  \r\n\r\n",
         "view_count": "155002",
         "answer_count": "3",
         "tags": "['python', 'pandas']",
         "creation_date": "1386549256",
         "last_edit_date": "1446891576",
         "code_snippet": "['<code>df=\\n           gi  ptt_loc\\n 0  384444683      593  \\n 1  384444684      594 \\n 2  384444686      596  \\n</code>', '<code>df=\\n    index1       gi    ptt_loc\\n 0  0     384444683      593  \\n 1  1     384444684      594 \\n 2  2     384444686      596  \\n</code>', \"<code>df['index1'] = df.index\\n</code>\", '<code>.reset_index</code>', '<code>df.reset_index(level=0, inplace=True)\\n</code>', '<code>&gt;&gt;&gt; df\\n                       val\\ntick       tag obs        \\n2016-02-26 C   2    0.0139\\n2016-02-27 A   2    0.5577\\n2016-02-28 C   6    0.0303\\n</code>', '<code>tick</code>', '<code>obs</code>', \"<code>&gt;&gt;&gt; df.reset_index(level=['tick', 'obs'])\\n          tick  obs     val\\ntag                        \\nC   2016-02-26    2  0.0139\\nA   2016-02-27    2  0.5577\\nC   2016-02-28    6  0.0303\\n</code>\", '<code>df.reset_index(level=df.index.names, inplace=True)</code>', \"<code>df['si_name'] = R.index.get_level_values('si_name') \\n</code>\", '<code>si_name</code>', \"<code>index = pd.MultiIndex.from_product([['TX', 'FL', 'CA'], \\n                                    ['North', 'South']], \\n                                   names=['State', 'Direction'])\\n\\ndf = pd.DataFrame(index=index, \\n                  data=np.random.randint(0, 10, (6,4)), \\n                  columns=list('abcd'))\\n</code>\", '<code>reset_index</code>', '<code>RangeIndex</code>', '<code>df.reset_index()\\n</code>', '<code>level</code>', \"<code>df.reset_index(level='State') # same as df.reset_index(level=0)\\n</code>\", \"<code># for a single level\\ndf.assign(State=df.index.get_level_values('State'))\\n\\n# for all levels\\ndf.assign(**df.index.to_frame())\\n</code>\"]",
         "title": "How to convert pandas index in a dataframe to a column?",
         "_childDocuments_": [
            {
               "up_vote_count": 310,
               "answer_id": 20461206,
               "last_activity_date": 1457266908,
               "path": "3.stack.answer",
               "body_markdown": "either:\r\n \r\n    df[&#39;index1&#39;] = df.index\r\nor, [`.reset_index`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html#pandas.DataFrame.reset_index):\r\n\r\n    df.reset_index(level=0, inplace=True)\r\n\r\n-------------\r\n\r\nso, if you have a multi-index frame with 3 levels of index, like:\r\n\r\n    &gt;&gt;&gt; df\r\n                           val\r\n    tick       tag obs        \r\n    2016-02-26 C   2    0.0139\r\n    2016-02-27 A   2    0.5577\r\n    2016-02-28 C   6    0.0303\r\n\r\nand you want to convert the 1st (`tick`) and 3rd (`obs`) levels in the index into columns, you would do:\r\n\r\n    &gt;&gt;&gt; df.reset_index(level=[&#39;tick&#39;, &#39;obs&#39;])\r\n              tick  obs     val\r\n    tag                        \r\n    C   2016-02-26    2  0.0139\r\n    A   2016-02-27    2  0.5577\r\n    C   2016-02-28    6  0.0303\r\n",
               "tags": [],
               "creation_date": 1386549557,
               "last_edit_date": 1457266908,
               "is_accepted": true,
               "id": "20461206",
               "down_vote_count": 0,
               "score": 310
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 22,
               "answer_id": 31075478,
               "is_accepted": false,
               "last_activity_date": 1435328038,
               "body_markdown": "For MultiIndex you can extract its subindex using \r\n\r\n    df[&#39;si_name&#39;] = R.index.get_level_values(&#39;si_name&#39;) \r\n\r\nwhere `si_name` is the name of the subindex.",
               "id": "31075478",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1435328038,
               "score": 22
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 46920777,
               "is_accepted": false,
               "last_activity_date": 1508882667,
               "body_markdown": "To provide a bit more clarity, let&#39;s look at a DataFrame with two levels in its index (a MultiIndex).\r\n\r\n    index = pd.MultiIndex.from_product([[&#39;TX&#39;, &#39;FL&#39;, &#39;CA&#39;], \r\n                                        [&#39;North&#39;, &#39;South&#39;]], \r\n                                       names=[&#39;State&#39;, &#39;Direction&#39;])\r\n    \r\n    df = pd.DataFrame(index=index, \r\n                      data=np.random.randint(0, 10, (6,4)), \r\n                      columns=list(&#39;abcd&#39;))\r\n\r\n[![enter image description here][1]][1]\r\n\r\nThe `reset_index` method, called with the default parameters, converts all index levels to columns and uses a simple `RangeIndex` as new index.\r\n\r\n    df.reset_index()\r\n[![enter image description here][2]][2]\r\n\r\nUse the `level` parameter to control which index levels are converted into columns. If possible, use the level name, which is more explicit. If there are no level names, you can refer to each level by its integer location, which begin at 0 from the outside. You can use a scalar value here or a list of all the indexes you would like to reset.\r\n\r\n    df.reset_index(level=&#39;State&#39;) # same as df.reset_index(level=0)\r\n\r\n[![enter image description here][3]][3]\r\n\r\nIn the rare event that you want to preserve the index and turn the index into a column, you can do the following:\r\n\r\n    # for a single level\r\n    df.assign(State=df.index.get_level_values(&#39;State&#39;))\r\n\r\n    # for all levels\r\n    df.assign(**df.index.to_frame())\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/SuURU.png\r\n  [2]: https://i.stack.imgur.com/58rRj.png\r\n  [3]: https://i.stack.imgur.com/sxY88.png",
               "id": "46920777",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1508882667,
               "score": 3
            }
         ],
         "link": "https://stackoverflow.com/questions/20461165/how-to-convert-pandas-index-in-a-dataframe-to-a-column",
         "id": "858127-2270"
      },
      {
         "up_vote_count": "561",
         "path": "2.stack",
         "body_markdown": "Is there a built-in that removes duplicates from list in Python, whilst preserving order? I know that I can use a set to remove duplicates, but that destroys the original order. I also know that I can roll my own like this:\r\n\r\n    def uniq(input):\r\n      output = []\r\n      for x in input:\r\n        if x not in output:\r\n          output.append(x)\r\n      return output\r\n\r\n(Thanks to [unwind][1] for that [code sample][2].)\r\n\r\nBut I&#39;d like to avail myself of a built-in or a more Pythonic idiom if possible.\r\n\r\nRelated question: [In Python, what is the fastest algorithm for removing duplicates from a list so that all elements are unique *while preserving order*?][3]\r\n\r\n\r\n  [1]: https://stackoverflow.com/users/28169/unwind\r\n  [2]: https://stackoverflow.com/questions/479897/how-do-you-remove-duplicates-from-a-list-in-python#479921\r\n  [3]: https://stackoverflow.com/questions/89178/in-python-what-is-the-fastest-algorithm-for-removing-duplicates-from-a-list-so-t",
         "view_count": "430190",
         "answer_count": "34",
         "tags": "['python', 'list', 'duplicates', 'unique']",
         "creation_date": "1232984638",
         "last_edit_date": "1495535471",
         "code_snippet": "['<code>def uniq(input):\\n  output = []\\n  for x in input:\\n    if x not in output:\\n      output.append(x)\\n  return output\\n</code>', '<code>def f7(seq):\\n    seen = set()\\n    seen_add = seen.add\\n    return [x for x in seq if not (x in seen or seen_add(x))]\\n</code>', '<code>seen.add</code>', '<code>seen_add</code>', '<code>seen.add</code>', '<code>seen.add</code>', '<code>seen.add</code>', '<code>seen.add</code>', '<code>dis.dis(f)</code>', '<code>LOAD_ATTR</code>', '<code>add</code>', '<code>seen_add</code>', '<code>OrderedDict</code>', '<code>OrderedDict</code>', '<code>OrderedDict</code>', '<code>more_itertools</code>', '<code>pip install more_itertools</code>', '<code>unique_everseen</code>', '<code>not seen.add</code>', '<code>&gt;&gt;&gt; from  more_itertools import unique_everseen\\n&gt;&gt;&gt; items = [1, 2, 0, 1, 3, 2]\\n&gt;&gt;&gt; list(unique_everseen(items))\\n[1, 2, 0, 3]\\n</code>', '<code>unique_everseen</code>', '<code>def unique_everseen(iterable, key=None):\\n    \"List unique elements, preserving order. Remember all elements ever seen.\"\\n    # unique_everseen(\\'AAAABBBCCDAABBB\\') --&gt; A B C D\\n    # unique_everseen(\\'ABBCcAD\\', str.lower) --&gt; A B C D\\n    seen = set()\\n    seen_add = seen.add\\n    if key is None:\\n        for element in filterfalse(seen.__contains__, iterable):\\n            seen_add(element)\\n            yield element\\n    else:\\n        for element in iterable:\\n            k = key(element)\\n            if k not in seen:\\n                seen_add(k)\\n                yield element\\n</code>', '<code>2.7+</code>', '<code>unique_everseen</code>', '<code>collections.OrderedDict</code>', '<code>&gt;&gt;&gt; from collections import OrderedDict\\n&gt;&gt;&gt; items = [1, 2, 0, 1, 3, 2]\\n&gt;&gt;&gt; list(OrderedDict.fromkeys(items))\\n[1, 2, 0, 3]\\n</code>', '<code>seen = set()\\n[x for x in seq if x not in seen and not seen.add(x)]\\n</code>', '<code>not seen.add(x)\\n</code>', '<code>set.add</code>', '<code>None</code>', '<code>not None</code>', '<code>True</code>', \"<code>sequence = ['1', '2', '3', '3', '6', '4', '5', '6']\\nunique = []\\n[unique.append(item) for item in sequence if item not in unique]\\n</code>\", \"<code>['1', '2', '3', '6', '4', '5']</code>\", '<code>n^2</code>', '<code>None</code>', '<code>for</code>', \"<code>&gt;&gt;&gt; from collections import OrderedDict\\n&gt;&gt;&gt; list(OrderedDict.fromkeys('abracadabra'))\\n['a', 'b', 'r', 'c', 'd']\\n</code>\", \"<code>&gt;&gt;&gt; list(dict.fromkeys('abracadabra'))\\n['a', 'b', 'r', 'c', 'd']\\n</code>\", \"<code>&gt;&gt;&gt; list(dict.fromkeys('abracadabra'))\\n['a', 'b', 'r', 'c', 'd']\\n</code>\", '<code>OrderedDict</code>', '<code>from itertools import groupby\\n[ key for key,_ in groupby(sortedList)]\\n</code>', \"<code>list1 = ['b','c','d','b','c','a','a']    \\nlist2 = list(set(list1))    \\nlist2.sort(key=list1.index)    \\nprint list2\\n</code>\", \"<code>list1 = ['b','c','d','b','c','a','a']  \\nlist2 = sorted(set(list1),key=list1.index)  \\nprint list2 \\n</code>\", \"<code>list1 = ['b','c','d','b','c','a','a']    \\nlist2 = []    \\nfor i in list1:    \\n    if not i in list2:  \\n        list2.append(i)`    \\nprint list2\\n</code>\", \"<code>list1 = ['b','c','d','b','c','a','a']    \\nlist2 = []    \\n[list2.append(i) for i in list1 if not i in list2]    \\nprint list2 \\n</code>\", '<code>itertools</code>', '<code>seen</code>', '<code>key</code>', '<code>seen.add</code>', '<code>f7</code>', '<code>ifilterfalse</code>', '<code>ifilterfalse</code>', '<code>f7</code>', '<code>f7</code>', '<code>append</code>', '<code>yield</code>', '<code>list</code>', '<code>more-iterools</code>', '<code>key</code>', '<code>def unique(iterable):\\n    seen = set()\\n    seen_add = seen.add\\n    for element in itertools.ifilterfalse(seen.__contains__, iterable):\\n        seen_add(element)\\n        yield element\\n</code>', '<code>more-itertools</code>', '<code>from more_itertools import unique_everseen</code>', '<code>list(unique_everseen(items))</code>', '<code>def f7_noHash(seq)\\n    seen = set()\\n    return [ x for x in seq if str( x ) not in seen and not seen.add( str( x ) )]\\n</code>', '<code>iteration_utilities.unique_everseen</code>', '<code>&gt;&gt;&gt; from iteration_utilities import unique_everseen\\n&gt;&gt;&gt; lst = [1,1,1,2,3,2,2,2,1,3,4]\\n\\n&gt;&gt;&gt; list(unique_everseen(lst))\\n[1, 2, 3, 4]\\n</code>', '<code>OrderedDict.fromkeys</code>', '<code>f7</code>', '<code>more_itertools.unique_everseen</code>', \"<code>%matplotlib notebook\\n\\nfrom iteration_utilities import unique_everseen\\nfrom collections import OrderedDict\\nfrom more_itertools import unique_everseen as mi_unique_everseen\\n\\ndef f7(seq):\\n    seen = set()\\n    seen_add = seen.add\\n    return [x for x in seq if not (x in seen or seen_add(x))]\\n\\ndef iteration_utilities_unique_everseen(seq):\\n    return list(unique_everseen(seq))\\n\\ndef more_itertools_unique_everseen(seq):\\n    return list(mi_unique_everseen(seq))\\n\\ndef odict(seq):\\n    return list(OrderedDict.fromkeys(seq))\\n\\nfrom simple_benchmark import Benchmark\\n\\nb = Benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],\\n              {2**i: list(range(2**i)) for i in range(1, 20)},\\n              'list size (no duplicates)')\\nb.run()\\n\\nb.plot()\\n</code>\", \"<code>import random\\n\\nb = Benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],\\n              {2**i: [random.randint(0, 2**(i-1)) for _ in range(2**i)] for i in range(1, 20)},\\n              'list size (lots of duplicates)')\\nb.run()\\n\\nb.plot()\\n</code>\", \"<code>b = Benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],\\n              {2**i: [1]*(2**i) for i in range(1, 20)},\\n              'list size (only duplicates)')\\nb.run()\\n\\nb.plot()\\n</code>\", '<code>iteration_utilities.unique_everseen</code>', '<code>iteration_utilities.unique_everseen</code>', '<code>O(n*n)</code>', '<code>O(n)</code>', '<code>&gt;&gt;&gt; lst = [{1}, {1}, {2}, {1}, {3}]\\n\\n&gt;&gt;&gt; list(unique_everseen(lst))\\n[{1}, {2}, {3}]\\n</code>', '<code>import pandas as pd\\n\\nmy_list = range(5) + range(5)  # [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\\n&gt;&gt;&gt; pd.Series(my_list).drop_duplicates().tolist()\\n# Output:\\n# [0, 1, 2, 3, 4]\\n</code>', '<code>nub</code>', '<code>def unique(lst):\\n    return [] if lst==[] else [lst[0]] + unique(filter(lambda x: x!= lst[0], lst[1:]))\\n</code>', '<code>In [118]: unique([1,5,1,1,4,3,4])\\nOut[118]: [1, 5, 4, 3]\\n</code>', '<code>In [122]: %timeit unique(np.random.randint(5, size=(1)))\\n10000 loops, best of 3: 25.3 us per loop\\n\\nIn [123]: %timeit unique(np.random.randint(5, size=(10)))\\n10000 loops, best of 3: 42.9 us per loop\\n\\nIn [124]: %timeit unique(np.random.randint(5, size=(100)))\\n10000 loops, best of 3: 132 us per loop\\n\\nIn [125]: %timeit unique(np.random.randint(5, size=(1000)))\\n1000 loops, best of 3: 1.05 ms per loop\\n\\nIn [126]: %timeit unique(np.random.randint(5, size=(10000)))\\n100 loops, best of 3: 11 ms per loop\\n</code>', '<code>import operator\\ndef unique(lst, cmp_op=operator.ne):\\n    return [] if lst==[] else [lst[0]] + unique(filter(lambda x: cmp_op(x, lst[0]), lst[1:]), cmp_op)\\n</code>', '<code>def test_round(x,y):\\n    return round(x) != round(y)\\n</code>', '<code>In [6]: unique([1.2, 5, 1.9, 1.1, 4.2, 3, 4.8], test_round)\\nOut[6]: [1.2, 5, 1.9, 4.2, 3]\\n</code>', '<code>filter</code>', '<code>&gt;&gt;&gt; l = [5, 6, 6, 1, 1, 2, 2, 3, 4]\\n&gt;&gt;&gt; reduce(lambda r, v: v in r[1] and r or (r[0].append(v) or r[1].add(v)) or r, l, ([], set()))[0]\\n[5, 6, 1, 2, 3, 4]\\n</code>', '<code>default = (list(), set())\\n# use list to keep order\\n# use set to make lookup faster\\n\\ndef reducer(result, item):\\n    if item not in result[1]:\\n        result[0].append(item)\\n        result[1].add(item)\\n    return result\\n\\n&gt;&gt;&gt; reduce(reducer, l, default)[0]\\n[5, 6, 1, 2, 3, 4]\\n</code>', \"<code>def unique(my_list): \\n    return [x for x in my_list if x not in locals()['_[1]']]\\n</code>\", \"<code>l1 = [1, 2, 3, 4, 1, 2, 3, 4, 5]\\nl2 = [x for x in l1 if x not in locals()['_[1]']]\\nprint l2\\n</code>\", '<code>[1, 2, 3, 4, 5]\\n</code>', '<code>mylist = [x for i,x in enumerate(mylist) if x not in mylist[i+1:]]\\n</code>', '<code>O(n)</code>', '<code>O(n^2)</code>', '<code>[l[i] for i in range(len(l)) if l.index(l[i]) == i]\\n</code>', '<code>i,e in enumerate(l)</code>', '<code>l[i] for i in range(len(l))</code>', '<code>l = [1,2,2,3,3,...]\\nn = []\\nn.extend(ele for ele in l if ele not in set(n))\\n</code>', '<code>extend</code>', '<code>set(n)</code>', '<code>ele in n</code>', '<code>_sorted_</code>', '<code>numpy</code>', '<code>b = np.array([1,3,3, 8, 12, 12,12])    \\nnumpy.hstack([b[0], [x[0] for x in zip(b[1:], b[:-1]) if x[0]!=x[1]]])\\n</code>', '<code>array([ 1,  3,  8, 12])\\n</code>', '<code>def uniquefy_list(a):\\n    return uniquefy_list(a[1:]) if a[0] in a[1:] else [a[0]]+uniquefy_list(a[1:]) if len(a)&gt;1 else [a[0]]\\n</code>', '<code>&gt;&gt;&gt; l = [3, 4, 3, 6, 4, 1, 4, 8]\\n\\n&gt;&gt;&gt; l = [l[i] for i in range(len(l)) if i == l.index(l[i])]\\n\\n&gt;&gt;&gt; l = [3, 4, 6, 1, 8]\\n</code>', '<code>reduce(lambda x, y: x + y if y[0] not in x else x, map(lambda x: [x],lst))\\n</code>', '<code>l = list(set(l))\\n</code>', '<code>l = reduce(lambda x, y: x if y in x else x + [y], l, [])\\n</code>', '<code>&gt;&gt;&gt; list1 = [ 1,1,2,2,3,3 ]\\n&gt;&gt;&gt; [ list1.pop(i) for i in range(len(list1))[::-1] if list1.count(list1[i]) &gt; 1 ]\\n[1, 2, 3]\\n</code>', '<code>[::-1]</code>', '<code>yourlist[::-1]</code>', '<code>list1 = [0, 2, 4, 9]\\nfor x in range(0, 7):\\n  list1.append(x)\\n</code>', '<code>list1 = [0, 2, 4, 9]\\nfor x in range(0, 7)\\n  if x not in list1:\\n    list1.append(x)\\n</code>', '<code>seen_add</code>', '<code>get(x,&lt;default&gt;)</code>', '<code>setdefault(x,&lt;default&gt;)</code>', '<code># Explanation of d.get(x,True) != d.setdefault(x,False)\\n#\\n# x in d | d[x]  | A = d.get(x,True) | x in d | B = d.setdefault(x,False) | x in d | d[x]    | A xor B\\n# False  | None  | True          (1) | False  | False                 (2) | True   | False   | True\\n# True   | False | False         (3) | True   | False                 (4) | True   | False   | False\\n#\\n# Notes\\n# (1) x is not in the dictionary, so get(x,&lt;default&gt;) returns True but does __not__ add the value to the dictionary\\n# (2) x is not in the dictionary, so setdefault(x,&lt;default&gt;) adds the {x:False} and returns False\\n# (3) since x is in the dictionary, the &lt;default&gt; argument is ignored, and the value of the key is returned, which was\\n#     set to False in (2)\\n# (4) since the key is already in the dictionary, its value is returned directly and the argument is ignored\\n#\\n# A != B is how to do boolean XOR in Python\\n#\\ndef sort_with_order(s):\\n    d = dict()\\n    return [x for x in s if d.get(x,True) != d.setdefault(x,False)]\\n</code>', '<code>get(x,&lt;default&gt;)</code>', '<code>&lt;default&gt;</code>', '<code>x</code>', '<code>set(x,&lt;default&gt;)</code>', '<code>&lt;default&gt;</code>', '<code>a != b</code>', '<code>__missing__</code>', '<code>d[k]</code>', '<code>class Tracker(dict):\\n    # returns True if missing, otherwise sets the value to False\\n    # so next time d[key] is called, the value False will be returned\\n    # and __missing__ will not be called again\\n    def __missing__(self, key):\\n        self[key] = False\\n        return True\\n\\nt = Tracker()\\nunique_with_order = [x for x in samples if t[x]]\\n</code>', '<code>def uniquify(s):\\n    if len(s) &lt; 2:\\n        return s\\n    return uniquify(s[:-1]) + [s[-1]] * (s[-1] not in s[:-1])\\n</code>', '<code>pandas</code>', '<code>pandas.Series.drop_duplicates</code>', '<code>    import pandas as pd\\n    import numpy as np\\n\\n    uniquifier = lambda alist: pd.Series(alist).drop_duplicates().tolist()\\n\\n    # from the chosen answer \\n    def f7(seq):\\n        seen = set()\\n        seen_add = seen.add\\n        return [ x for x in seq if not (x in seen or seen_add(x))]\\n\\n    alist = np.random.randint(low=0, high=1000, size=10000).tolist()\\n\\n    print uniquifier(alist) == f7(alist)  # True\\n</code>', '<code>    In [104]: %timeit f7(alist)\\n    1000 loops, best of 3: 1.3 ms per loop\\n    In [110]: %timeit uniquifier(alist)\\n    100 loops, best of 3: 4.39 ms per loop\\n</code>', '<code>def deduplicate(l):\\n    count = {}\\n    (read,write) = (0,0)\\n    while read &lt; len(l):\\n        if l[read] in count:\\n            read += 1\\n            continue\\n        count[l[read]] = True\\n        l[write] = l[read]\\n        read += 1\\n        write += 1\\n    return l[0:write]\\n</code>', '<code>text = \"ask not what your country can do for you ask what you can do for your country\"\\nsentence = text.split(\" \")\\nnoduplicates = [(sentence[i]) for i in range (0,len(sentence)) if sentence[i] not in sentence[:i]]\\nprint(noduplicates)\\n</code>', \"<code>['ask', 'not', 'what', 'your', 'country', 'can', 'do', 'for', 'you']\\n</code>\", '<code>def unique(nums):\\n    unique = []\\n    for n in nums:\\n        if n not in unique:\\n            unique.append(n)\\n    return unique\\n</code>', '<code>a_list = [\"a\", \"b\", \"a\", \"c\"]\\n\\nsorted_list = [x[0] for x in (sorted({x:a_list.index(x) for x in set(a_list)}.items(), key=lambda x: x[1]))]\\n\\nprint sorted_list\\n</code>']",
         "title": "How do you remove duplicates from a list in whilst preserving order?",
         "_childDocuments_": [
            {
               "up_vote_count": 610,
               "answer_id": 480227,
               "last_activity_date": 1447662059,
               "path": "3.stack.answer",
               "body_markdown": "Here you have some alternatives: &lt;http://www.peterbe.com/plog/uniqifiers-benchmark&gt;\r\n\r\nFastest one:\r\n\r\n    def f7(seq):\r\n        seen = set()\r\n        seen_add = seen.add\r\n        return [x for x in seq if not (x in seen or seen_add(x))]\r\n\r\nWhy assign `seen.add` to `seen_add` instead of just calling `seen.add`? Python is a dynamic language, and resolving `seen.add` each iteration is more costly than resolving a local variable. `seen.add` could have changed between iterations, and the runtime isn&#39;t smart enough to rule that out. To play it safe, it has to check the object each time.\r\n\r\nIf you plan on using this function a lot on the same dataset, perhaps you would be better off with an ordered set: &lt;http://code.activestate.com/recipes/528878/&gt;\r\n\r\n*O*(1) insertion, deletion and member-check per operation.",
               "tags": [],
               "creation_date": 1232984821,
               "last_edit_date": 1447662059,
               "is_accepted": true,
               "id": "480227",
               "down_vote_count": 3,
               "score": 607
            },
            {
               "up_vote_count": 24,
               "answer_id": 480229,
               "last_activity_date": 1310597147,
               "path": "3.stack.answer",
               "body_markdown": "    from itertools import groupby\r\n    [ key for key,_ in groupby(sortedList)]\r\n\r\nThe list doesn&#39;t even have to be *sorted*, the sufficient condition is that equal values are grouped together.\r\n\r\n**Edit: I assumed that &quot;preserving order&quot; implies that the list is actually ordered. If this is not the case, then the solution from MizardX is the right one.**\r\n\r\nCommunity edit: This is however the most elegant way to &quot;compress duplicate consecutive elements into a single element&quot;.",
               "tags": [],
               "creation_date": 1232984834,
               "last_edit_date": 1310597147,
               "is_accepted": false,
               "id": "480229",
               "down_vote_count": 3,
               "score": 21
            },
            {
               "up_vote_count": 0,
               "answer_id": 1420478,
               "last_activity_date": 1338207440,
               "path": "3.stack.answer",
               "body_markdown": "    for i in range(len(theArray)-1,-1,-1): #get the indexes in reverse\r\n        if theArray.count(theArray[i]) &gt; 1:\r\n            theArray.pop(i)",
               "tags": [],
               "creation_date": 1252919708,
               "last_edit_date": 1338207440,
               "is_accepted": false,
               "id": "1420478",
               "down_vote_count": 2,
               "score": -2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 6959997,
               "is_accepted": false,
               "last_activity_date": 1312563985,
               "body_markdown": "If you need one liner then maybe this would help:\r\n\r\n    reduce(lambda x, y: x + y if y[0] not in x else x, map(lambda x: [x],lst))\r\n\r\n... should work but correct me if i&#39;m wrong",
               "id": "6959997",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1312563985,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 7140776,
               "is_accepted": false,
               "last_activity_date": 1313957052,
               "body_markdown": "For no hashable types (e.g. list of lists), based on MizardX&#39;s:\r\n\r\n    def f7_noHash(seq)\r\n        seen = set()\r\n        return [ x for x in seq if str( x ) not in seen and not seen.add( str( x ) )]",
               "id": "7140776",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1313957052,
               "score": 6
            },
            {
               "up_vote_count": 5,
               "answer_id": 7704123,
               "last_activity_date": 1338207424,
               "path": "3.stack.answer",
               "body_markdown": "MizardX&#39;s answer gives a good collection of multiple approaches.\r\n\r\nThis is what I came up with while thinking aloud:\r\n\r\n    mylist = [x for i,x in enumerate(mylist) if x not in mylist[i+1:]]",
               "tags": [],
               "creation_date": 1318169760,
               "last_edit_date": 1338207424,
               "is_accepted": false,
               "id": "7704123",
               "down_vote_count": 3,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 10485907,
               "is_accepted": false,
               "last_activity_date": 1336409493,
               "body_markdown": "This is fast but...\r\n\r\n    l = list(set(l))\r\n\r\n... it doesn&#39;t work if your list items aren&#39;t hashable.\r\n\r\nA more generic approach is:\r\n\r\n    l = reduce(lambda x, y: x if y in x else x + [y], l, [])\r\n\r\n... it should work for all cases.\r\n\r\n\r\n\r\n",
               "id": "10485907",
               "tags": [],
               "down_vote_count": 9,
               "creation_date": 1336409493,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 11280704,
               "is_accepted": false,
               "last_activity_date": 1341132776,
               "body_markdown": "Pop the duplicate in a list and hold uniques in source list :\r\n\r\n    &gt;&gt;&gt; list1 = [ 1,1,2,2,3,3 ]\r\n    &gt;&gt;&gt; [ list1.pop(i) for i in range(len(list1))[::-1] if list1.count(list1[i]) &gt; 1 ]\r\n    [1, 2, 3]\r\n\r\nI use `[::-1]` for read list in reverse order.",
               "id": "11280704",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1341132776,
               "score": 0
            },
            {
               "up_vote_count": 6,
               "answer_id": 13262682,
               "last_activity_date": 1473374437,
               "path": "3.stack.answer",
               "body_markdown": "You can reference a list comprehension as it is being built by the symbol &#39;_[1]&#39;. &lt;br&gt;For example, the following function unique-ifies a list of elements without changing their order by referencing its list comprehension.\r\n\r\n\r\n    def unique(my_list): \r\n        return [x for x in my_list if x not in locals()[&#39;_[1]&#39;]]\r\nDemo:\r\n\r\n    l1 = [1, 2, 3, 4, 1, 2, 3, 4, 5]\r\n    l2 = [x for x in l1 if x not in locals()[&#39;_[1]&#39;]]\r\n    print l2\r\n\r\nOutput:\r\n\r\n    [1, 2, 3, 4, 5]",
               "tags": [],
               "creation_date": 1352258494,
               "last_edit_date": 1473374437,
               "is_accepted": false,
               "id": "13262682",
               "down_vote_count": 3,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 15686617,
               "is_accepted": false,
               "last_activity_date": 1364486535,
               "body_markdown": "If your situation allows, you might consider removing duplicates as you load:\r\n\r\nSay you have a loop that is pulling in data and uses list1.append(item)...\r\n\r\n    list1 = [0, 2, 4, 9]\r\n    for x in range(0, 7):\r\n      list1.append(x)\r\n\r\nThat gives you some duplicates:\r\n [0, 2, 4, 9, 0, 1, 2, 3, 4, 5, 6]\r\n\r\nBut if you did:\r\n\r\n    list1 = [0, 2, 4, 9]\r\n    for x in range(0, 7)\r\n      if x not in list1:\r\n        list1.append(x)\r\n\r\nYou get no duplicates and the order is preserved:\r\n [0, 2, 4, 9, 1, 3, 5, 6]",
               "id": "15686617",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1364486535,
               "score": 0
            },
            {
               "up_vote_count": 51,
               "answer_id": 15990766,
               "last_activity_date": 1391314292,
               "path": "3.stack.answer",
               "body_markdown": "    \r\n    sequence = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;3&#39;, &#39;6&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;]\r\n    unique = []\r\n    [unique.append(item) for item in sequence if item not in unique]\r\n\r\n\r\nunique \u2192 `[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;6&#39;, &#39;4&#39;, &#39;5&#39;]`",
               "tags": [],
               "creation_date": 1365874339,
               "last_edit_date": 1391314292,
               "is_accepted": false,
               "id": "15990766",
               "down_vote_count": 14,
               "score": 37
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 21,
               "answer_id": 16780848,
               "is_accepted": false,
               "last_activity_date": 1369690643,
               "body_markdown": "I think if you wanna maintain the order,\r\n\r\n##you can try this:\r\n\r\n    list1 = [&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;b&#39;,&#39;c&#39;,&#39;a&#39;,&#39;a&#39;]    \r\n    list2 = list(set(list1))    \r\n    list2.sort(key=list1.index)    \r\n    print list2\r\n\r\n##OR similarly you can do this:\r\n\r\n    list1 = [&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;b&#39;,&#39;c&#39;,&#39;a&#39;,&#39;a&#39;]  \r\n    list2 = sorted(set(list1),key=list1.index)  \r\n    print list2 \r\n\r\n##You can also do this:\r\n\r\n    list1 = [&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;b&#39;,&#39;c&#39;,&#39;a&#39;,&#39;a&#39;]    \r\n    list2 = []    \r\n    for i in list1:    \r\n        if not i in list2:  \r\n            list2.append(i)`    \r\n    print list2\r\n\r\n##It can also be written as this:\r\n\r\n    list1 = [&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;b&#39;,&#39;c&#39;,&#39;a&#39;,&#39;a&#39;]    \r\n    list2 = []    \r\n    [list2.append(i) for i in list1 if not i in list2]    \r\n    print list2 \r\n\r\n",
               "id": "16780848",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1369690643,
               "score": 18
            },
            {
               "up_vote_count": 277,
               "answer_id": 17016257,
               "last_activity_date": 1481747160,
               "path": "3.stack.answer",
               "body_markdown": "**Edit 2016**\r\n\r\nAs Raymond [pointed out](https://stackoverflow.com/a/39835527/336527), in python 3.5+ where `OrderedDict` is implemented in C, the list comprehension approach will be slower than `OrderedDict` (unless you actually need the list at the end - and even then, only if the input is very short). So the best solution for 3.5+ is `OrderedDict`.\r\n\r\n**Important Edit 2015**\r\n\r\nAs [@abarnert][1] notes, the [`more_itertools`][2] library (`pip install more_itertools`) contains a [`unique_everseen`][3] function that is built to solve this problem without any **unreadable** (`not seen.add`) **mutations** in list comprehensions. This is also the fastest solution too:\r\n\r\n    &gt;&gt;&gt; from  more_itertools import unique_everseen\r\n    &gt;&gt;&gt; items = [1, 2, 0, 1, 3, 2]\r\n    &gt;&gt;&gt; list(unique_everseen(items))\r\n    [1, 2, 0, 3]\r\n\r\nJust one simple library import and no hacks. \r\nThis comes from an implementation of the itertools recipe [`unique_everseen`][4] which looks like:\r\n\r\n    def unique_everseen(iterable, key=None):\r\n        &quot;List unique elements, preserving order. Remember all elements ever seen.&quot;\r\n        # unique_everseen(&#39;AAAABBBCCDAABBB&#39;) --&gt; A B C D\r\n        # unique_everseen(&#39;ABBCcAD&#39;, str.lower) --&gt; A B C D\r\n        seen = set()\r\n        seen_add = seen.add\r\n        if key is None:\r\n            for element in filterfalse(seen.__contains__, iterable):\r\n                seen_add(element)\r\n                yield element\r\n        else:\r\n            for element in iterable:\r\n                k = key(element)\r\n                if k not in seen:\r\n                    seen_add(k)\r\n                    yield element\r\n\r\n_____\r\n\r\nIn Python `2.7+` the &lt;strike&gt;accepted common idiom&lt;/strike&gt; (this works but isn&#39;t optimized for speed, i would now use [`unique_everseen`][3]) for this uses [`collections.OrderedDict`](http://docs.python.org/3/library/collections.html#collections.OrderedDict):\r\n\r\nRuntime: **O(N)**\r\n\r\n    &gt;&gt;&gt; from collections import OrderedDict\r\n    &gt;&gt;&gt; items = [1, 2, 0, 1, 3, 2]\r\n    &gt;&gt;&gt; list(OrderedDict.fromkeys(items))\r\n    [1, 2, 0, 3]\r\n\r\n\r\n\r\nThis looks much nicer than:\r\n\r\n    seen = set()\r\n    [x for x in seq if x not in seen and not seen.add(x)]\r\n\r\nand doesn&#39;t utilize the **ugly hack**:\r\n\r\n    not seen.add(x)\r\n\r\nwhich relies on the fact that `set.add` is an in-place method that always returns `None` so `not None` evaluates to `True`. \r\n\r\nNote however that the hack solution is faster in raw speed though it has the same runtime complexity O(N).\r\n\r\n\r\n  [1]: https://stackoverflow.com/a/19279812/1219006\r\n  [2]: https://pythonhosted.org/more-itertools/api.html\r\n  [3]: https://pythonhosted.org/more-itertools/api.html#more_itertools.unique_everseen\r\n  [4]: https://docs.python.org/3/library/itertools.html#itertools-recipes",
               "tags": [],
               "creation_date": 1370832433,
               "last_edit_date": 1495542381,
               "is_accepted": false,
               "id": "17016257",
               "down_vote_count": 1,
               "score": 276
            },
            {
               "up_vote_count": 3,
               "answer_id": 18729193,
               "last_activity_date": 1378909795,
               "path": "3.stack.answer",
               "body_markdown": "Borrowing the recursive idea used in definining Haskell&#39;s `nub` function for lists, this would be a recursive approach:\r\n\r\n    def unique(lst):\r\n        return [] if lst==[] else [lst[0]] + unique(filter(lambda x: x!= lst[0], lst[1:]))\r\n\r\ne.g.:\r\n\r\n    In [118]: unique([1,5,1,1,4,3,4])\r\n    Out[118]: [1, 5, 4, 3]\r\n\r\nI tried it for growing data sizes and saw sub-linear time-complexity (not definitive, but suggests this should be fine for normal data).\r\n\r\n    In [122]: %timeit unique(np.random.randint(5, size=(1)))\r\n    10000 loops, best of 3: 25.3 us per loop\r\n    \r\n    In [123]: %timeit unique(np.random.randint(5, size=(10)))\r\n    10000 loops, best of 3: 42.9 us per loop\r\n    \r\n    In [124]: %timeit unique(np.random.randint(5, size=(100)))\r\n    10000 loops, best of 3: 132 us per loop\r\n    \r\n    In [125]: %timeit unique(np.random.randint(5, size=(1000)))\r\n    1000 loops, best of 3: 1.05 ms per loop\r\n    \r\n    In [126]: %timeit unique(np.random.randint(5, size=(10000)))\r\n    100 loops, best of 3: 11 ms per loop\r\n\r\n\r\nI also think it&#39;s interesting that this could be readily generalized to uniqueness by other operations. Like this:\r\n\r\n    import operator\r\n    def unique(lst, cmp_op=operator.ne):\r\n        return [] if lst==[] else [lst[0]] + unique(filter(lambda x: cmp_op(x, lst[0]), lst[1:]), cmp_op)\r\n\r\nFor example, you could pass in a function that uses the notion of rounding to the same integer as if it was &quot;equality&quot; for uniqueness purposes, like this:\r\n\r\n    def test_round(x,y):\r\n        return round(x) != round(y)\r\n\r\nthen unique(some_list, test_round) would provide the unique elements of the list where uniqueness no longer meant traditional equality (which is implied by using any sort of set-based or dict-key-based approach to this problem) but instead meant to take only the first element that rounds to K for each possible integer K that the elements might round to, e.g.:\r\n\r\n    In [6]: unique([1.2, 5, 1.9, 1.1, 4.2, 3, 4.8], test_round)\r\n    Out[6]: [1.2, 5, 1.9, 4.2, 3]\r\n\r\n",
               "tags": [],
               "creation_date": 1378849258,
               "last_edit_date": 1378909795,
               "is_accepted": false,
               "id": "18729193",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 1,
               "answer_id": 19135774,
               "last_activity_date": 1398406279,
               "path": "3.stack.answer",
               "body_markdown": "Relatively effective approach with `_sorted_` a `numpy` arrays:\r\n\r\n    b = np.array([1,3,3, 8, 12, 12,12])    \r\n    numpy.hstack([b[0], [x[0] for x in zip(b[1:], b[:-1]) if x[0]!=x[1]]])\r\n\r\nOutputs:\r\n\r\n    array([ 1,  3,  8, 12])\r\n\r\n",
               "tags": [],
               "creation_date": 1380713011,
               "last_edit_date": 1398406279,
               "is_accepted": false,
               "id": "19135774",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 11,
               "answer_id": 19279812,
               "last_activity_date": 1381343582,
               "path": "3.stack.answer",
               "body_markdown": "For another very late answer to another very old question:\r\n\r\nThe [`itertools` recipes](http://docs.python.org/3/library/itertools.html#itertools-recipes) have a function that does this, using the `seen` set technique, but:\r\n\r\n * Handles a standard `key` function.\r\n * Uses no unseemly hacks.\r\n * Optimizes the loop by pre-binding `seen.add` instead of looking it up N times. (`f7` also does this, but some versions don&#39;t.)\r\n * Optimizes the loop by using `ifilterfalse`, so you only have to loop over the unique elements in Python, instead of all of them. (You still iterate over all of them inside `ifilterfalse`, of course, but that&#39;s in C, and much faster.)\r\n\r\nIs it actually faster than `f7`? It depends on your data, so you&#39;ll have to test it and see. If you want a list in the end, `f7` uses a listcomp, and there&#39;s no way to do that here. (You can directly `append` instead of `yield`ing, or you can feed the generator into the `list` function, but neither one can be as fast as the LIST_APPEND inside a listcomp.) At any rate, usually, squeezing out a few microseconds is not going to be as important as having an easily-understandable, reusable, already-written function that doesn&#39;t require DSU when you want to decorate.\r\n\r\nAs with all of the recipes, it&#39;s also available in [`more-iterools`](https://pypi.python.org/pypi/more-itertools).\r\n\r\nIf you just want the no-`key` case, you can simplify it as:\r\n\r\n    def unique(iterable):\r\n        seen = set()\r\n        seen_add = seen.add\r\n        for element in itertools.ifilterfalse(seen.__contains__, iterable):\r\n            seen_add(element)\r\n            yield element",
               "tags": [],
               "creation_date": 1381343229,
               "last_edit_date": 1381343582,
               "is_accepted": false,
               "id": "19279812",
               "down_vote_count": 0,
               "score": 11
            },
            {
               "up_vote_count": 1,
               "answer_id": 21119677,
               "last_activity_date": 1389720107,
               "path": "3.stack.answer",
               "body_markdown": "Because I was looking at a [dup](https://stackoverflow.com/q/4459703/1174169) and collected some related but different, related, useful information that isn&#39;t part of the other answers, here are two other possible solutions. \r\n\r\n__.get(True) XOR .setdefault(False)__\r\n\r\nThe first is very much like the accepted `seen_add` soultion but with explicit side effects using dictionary&#39;s `get(x,&lt;default&gt;)` and `setdefault(x,&lt;default&gt;)`:\r\n\r\n    # Explanation of d.get(x,True) != d.setdefault(x,False)\r\n    #\r\n    # x in d | d[x]  | A = d.get(x,True) | x in d | B = d.setdefault(x,False) | x in d | d[x]    | A xor B\r\n    # False  | None  | True          (1) | False  | False                 (2) | True   | False   | True\r\n    # True   | False | False         (3) | True   | False                 (4) | True   | False   | False\r\n    #\r\n    # Notes\r\n    # (1) x is not in the dictionary, so get(x,&lt;default&gt;) returns True but does __not__ add the value to the dictionary\r\n    # (2) x is not in the dictionary, so setdefault(x,&lt;default&gt;) adds the {x:False} and returns False\r\n    # (3) since x is in the dictionary, the &lt;default&gt; argument is ignored, and the value of the key is returned, which was\r\n    #     set to False in (2)\r\n    # (4) since the key is already in the dictionary, its value is returned directly and the argument is ignored\r\n    #\r\n    # A != B is how to do boolean XOR in Python\r\n    #\r\n    def sort_with_order(s):\r\n        d = dict()\r\n        return [x for x in s if d.get(x,True) != d.setdefault(x,False)]\r\n\r\n`get(x,&lt;default&gt;)` returns `&lt;default&gt;` if `x` is not in the dictionary, but does not add the key to the dictionary. `set(x,&lt;default&gt;)` returns the value if the key is in the dictionary,  otherwise sets it to and returns `&lt;default&gt;`.\r\n\r\nAside: [`a != b` is how to do an XOR in python](https://stackoverflow.com/a/433161/1174169)\r\n\r\n__OVERRIDING ___missing_____ (inspired by [this answer](https://stackoverflow.com/a/14507623/1174169))\r\n\r\nThe second technique is overriding the `__missing__` method that gets called when the key doesn&#39;t exist in a dictionary, which is only called when using `d[k]` notation:\r\n\r\n    class Tracker(dict):\r\n        # returns True if missing, otherwise sets the value to False\r\n        # so next time d[key] is called, the value False will be returned\r\n        # and __missing__ will not be called again\r\n        def __missing__(self, key):\r\n            self[key] = False\r\n            return True\r\n        \r\n    t = Tracker()\r\n    unique_with_order = [x for x in samples if t[x]]\r\n\r\nFrom [the docs](http://docs.python.org/2/library/stdtypes.html#mapping-types-dict):\r\n\r\n&gt; New in version 2.5: If a subclass of dict defines a method\r\n&gt; _____missing_____(), if the key key is not present, the d[key] operation calls that method with the key key as argument. The d[key] operation\r\n&gt; then returns or raises whatever is returned or raised by the\r\n&gt; _____missing_____(key) call if the key is not present. No other operations or methods invoke _____missing_____(). If _____missing_____() is not defined,\r\n&gt; KeyError is raised. _____missing_____() must be a method; it cannot be an\r\n&gt; instance variable. For an example, see collections.defaultdict.",
               "tags": [],
               "creation_date": 1389719690,
               "last_edit_date": 1495540035,
               "is_accepted": false,
               "id": "21119677",
               "down_vote_count": 1,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 23282700,
               "is_accepted": false,
               "last_activity_date": 1398389311,
               "body_markdown": "You could do a sort of ugly list comprehension hack.\r\n\r\n    [l[i] for i in range(len(l)) if l.index(l[i]) == i]",
               "id": "23282700",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1398389311,
               "score": 2
            },
            {
               "up_vote_count": 2,
               "answer_id": 23919756,
               "last_activity_date": 1501840020,
               "path": "3.stack.answer",
               "body_markdown": "(This solution may not preserve the order of the list.)\r\n\r\n    l = [1,2,3,4,5,1,2,3,4]\r\n    s = set(l)\r\n    l = list(s)\r\n    print l\r\n\r\n**Output:**\r\n\r\n    [1,2,3,4,5]",
               "tags": [],
               "creation_date": 1401303565,
               "last_edit_date": 1501840020,
               "is_accepted": false,
               "id": "23919756",
               "down_vote_count": 7,
               "score": -5
            },
            {
               "up_vote_count": 5,
               "answer_id": 24914302,
               "last_activity_date": 1472474024,
               "path": "3.stack.answer",
               "body_markdown": "    inpList = [1, 1, 2, 2, 3, 3, 2, 2, 4, 1, 2, 5, 5]\r\n    myList = list(set(inpList))\r\n    print myList\r\n\r\n&gt;output: [1, 2, 3, 4, 5]",
               "tags": [],
               "creation_date": 1406128338,
               "last_edit_date": 1472474024,
               "is_accepted": false,
               "id": "24914302",
               "down_vote_count": 8,
               "score": -3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 26794407,
               "is_accepted": false,
               "last_activity_date": 1415336574,
               "body_markdown": "    l = [1,2,2,3,3,...]\r\n    n = []\r\n    n.extend(ele for ele in l if ele not in set(n))\r\n\r\nA generator expression that uses the O(1) look up of a set to determine whether or not to include an element in the new list.",
               "id": "26794407",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1415336574,
               "score": 2
            },
            {
               "up_vote_count": 0,
               "answer_id": 27466786,
               "last_activity_date": 1432910830,
               "path": "3.stack.answer",
               "body_markdown": "Here is an O(N&lt;sup&gt;2&lt;/sup&gt;) recursive version for fun:\r\n\r\n    def uniquify(s):\r\n        if len(s) &lt; 2:\r\n            return s\r\n        return uniquify(s[:-1]) + [s[-1]] * (s[-1] not in s[:-1])",
               "tags": [],
               "creation_date": 1418536873,
               "last_edit_date": 1432910830,
               "is_accepted": false,
               "id": "27466786",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 3,
               "answer_id": 29898968,
               "last_activity_date": 1430203155,
               "path": "3.stack.answer",
               "body_markdown": "5 x faster reduce variant but more sophisticated\r\n    \r\n    &gt;&gt;&gt; l = [5, 6, 6, 1, 1, 2, 2, 3, 4]\r\n    &gt;&gt;&gt; reduce(lambda r, v: v in r[1] and r or (r[0].append(v) or r[1].add(v)) or r, l, ([], set()))[0]\r\n    [5, 6, 1, 2, 3, 4]\r\n\r\nExplanation:\r\n\r\n    default = (list(), set())\r\n    # use list to keep order\r\n    # use set to make lookup faster\r\n    \r\n    def reducer(result, item):\r\n        if item not in result[1]:\r\n            result[0].append(item)\r\n            result[1].add(item)\r\n        return result\r\n\r\n    &gt;&gt;&gt; reduce(reducer, l, default)[0]\r\n    [5, 6, 1, 2, 3, 4]",
               "tags": [],
               "creation_date": 1430146041,
               "last_edit_date": 1430203155,
               "is_accepted": false,
               "id": "29898968",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 30281633,
               "is_accepted": false,
               "last_activity_date": 1431817508,
               "body_markdown": "A simple recursive solution:\r\n\r\n    def uniquefy_list(a):\r\n        return uniquefy_list(a[1:]) if a[0] in a[1:] else [a[0]]+uniquefy_list(a[1:]) if len(a)&gt;1 else [a[0]]",
               "id": "30281633",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1431817508,
               "score": 1
            },
            {
               "up_vote_count": 0,
               "answer_id": 31486477,
               "last_activity_date": 1437178854,
               "path": "3.stack.answer",
               "body_markdown": "If you routinely use [`pandas`](http://pandas.pydata.org/), and aesthetics is preferred over performance, then consider the built-in function `pandas.Series.drop_duplicates`:\r\n\r\n        import pandas as pd\r\n        import numpy as np\r\n        \r\n        uniquifier = lambda alist: pd.Series(alist).drop_duplicates().tolist()\r\n        \r\n        # from the chosen answer \r\n        def f7(seq):\r\n            seen = set()\r\n            seen_add = seen.add\r\n            return [ x for x in seq if not (x in seen or seen_add(x))]\r\n\r\n        alist = np.random.randint(low=0, high=1000, size=10000).tolist()\r\n        \r\n        print uniquifier(alist) == f7(alist)  # True\r\n\r\nTiming: \r\n\r\n        In [104]: %timeit f7(alist)\r\n        1000 loops, best of 3: 1.3 ms per loop\r\n        In [110]: %timeit uniquifier(alist)\r\n        100 loops, best of 3: 4.39 ms per loop",
               "tags": [],
               "creation_date": 1437178251,
               "last_edit_date": 1437178854,
               "is_accepted": false,
               "id": "31486477",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 1,
               "answer_id": 32291752,
               "last_activity_date": 1463116161,
               "path": "3.stack.answer",
               "body_markdown": "Here is my 2 cents on this:\r\n\r\n    def unique(nums):\r\n        unique = []\r\n        for n in nums:\r\n            if n not in unique:\r\n                unique.append(n)\r\n        return unique\r\n\r\nRegards,\r\nYuriy",
               "tags": [],
               "creation_date": 1440891636,
               "last_edit_date": 1463116161,
               "is_accepted": false,
               "id": "32291752",
               "down_vote_count": 1,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 34749886,
               "is_accepted": false,
               "last_activity_date": 1452618979,
               "body_markdown": "this will preserve order and run in O(n) time. basically the idea is to create a hole wherever there is a duplicate found and sink it down to the bottom. makes use of a read and write pointer. whenever a duplicate is found only the read pointer advances and write pointer stays on the duplicate entry to overwrite it.\r\n\r\n\r\n    def deduplicate(l):\r\n    \tcount = {}\r\n    \t(read,write) = (0,0)\r\n    \twhile read &lt; len(l):\r\n    \t\tif l[read] in count:\r\n    \t\t\tread += 1\r\n    \t\t\tcontinue\r\n    \t\tcount[l[read]] = True\r\n    \t\tl[write] = l[read]\r\n    \t\tread += 1\r\n    \t\twrite += 1\r\n    \treturn l[0:write]",
               "id": "34749886",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1452618979,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 35038274,
               "is_accepted": false,
               "last_activity_date": 1453900088,
               "body_markdown": "A solution without using imported modules or sets:\r\n\r\n    text = &quot;ask not what your country can do for you ask what you can do for your country&quot;\r\n    sentence = text.split(&quot; &quot;)\r\n    noduplicates = [(sentence[i]) for i in range (0,len(sentence)) if sentence[i] not in sentence[:i]]\r\n    print(noduplicates)\r\n\r\nGives output:\r\n\r\n    [&#39;ask&#39;, &#39;not&#39;, &#39;what&#39;, &#39;your&#39;, &#39;country&#39;, &#39;can&#39;, &#39;do&#39;, &#39;for&#39;, &#39;you&#39;]\r\n    ",
               "id": "35038274",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1453900088,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 39456312,
               "is_accepted": false,
               "last_activity_date": 1473703993,
               "body_markdown": "this is the smartes way to remove duplicates from a list in Python whilst preserving its order, you can even do it in one line of code:\r\n\r\n    a_list = [&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;]\r\n    \r\n    sorted_list = [x[0] for x in (sorted({x:a_list.index(x) for x in set(a_list)}.items(), key=lambda x: x[1]))]\r\n    \r\n    print sorted_list\r\n\r\n",
               "id": "39456312",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1473703993,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 39564397,
               "is_accepted": false,
               "last_activity_date": 1474250595,
               "body_markdown": "My buddy Wes gave me this sweet answer using list comprehensions.\r\n\r\nExample Code:\r\n\r\n    &gt;&gt;&gt; l = [3, 4, 3, 6, 4, 1, 4, 8]\r\n\r\n    &gt;&gt;&gt; l = [l[i] for i in range(len(l)) if i == l.index(l[i])]\r\n\r\n    &gt;&gt;&gt; l = [3, 4, 6, 1, 8]",
               "id": "39564397",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1474250595,
               "score": 1
            },
            {
               "up_vote_count": 33,
               "answer_id": 39835527,
               "last_activity_date": 1513931762,
               "path": "3.stack.answer",
               "body_markdown": "**In Python 2.7**, the new way of removing duplicates from an iterable while keeping it in the original order is:\r\n\r\n    &gt;&gt;&gt; from collections import OrderedDict\r\n    &gt;&gt;&gt; list(OrderedDict.fromkeys(&#39;abracadabra&#39;))\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;r&#39;, &#39;c&#39;, &#39;d&#39;]\r\n\r\n**In Python 3.5**, the OrderedDict has a C implementation. My timings show that this is now both the fastest and shortest of the various approaches for Python 3.5.\r\n\r\n**In Python 3.6**, the regular dict became both ordered and compact.  (This feature is holds for CPython and PyPy but may not present in other implementations).  That gives us a new fastest way of deduping while retaining order:\r\n\r\n    &gt;&gt;&gt; list(dict.fromkeys(&#39;abracadabra&#39;))\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;r&#39;, &#39;c&#39;, &#39;d&#39;]\r\n\r\n**In Python 3.7**, the regular dict is guaranteed to both ordered across all implementations.  **So, the shortest and fastest solution is:**\r\n\r\n    &gt;&gt;&gt; list(dict.fromkeys(&#39;abracadabra&#39;))\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;r&#39;, &#39;c&#39;, &#39;d&#39;]\r\n\r\n---------------------\r\n\r\nResponse to @max:  Once you move to 3.6 or 3.7 and use the regular dict instead of *OrderedDict*, you can&#39;t really beat the performance in any other way.  The dictionary is dense and readily converts to a list with almost no overhead.  The target list is pre-sized to len(d) which saves all the resizes that occur in a list comprehension.  Also, since the internal key list is dense, copying the pointers is about almost fast as a list copy.",
               "tags": [],
               "creation_date": 1475509653,
               "last_edit_date": 1513931762,
               "is_accepted": false,
               "id": "39835527",
               "down_vote_count": 0,
               "score": 33
            },
            {
               "up_vote_count": 5,
               "answer_id": 41577279,
               "last_activity_date": 1519078769,
               "path": "3.stack.answer",
               "body_markdown": "Just to add another (very performant) implementation of such a functionality from an external module&lt;sup&gt;1&lt;/sup&gt;: [`iteration_utilities.unique_everseen`](https://iteration-utilities.readthedocs.io/en/latest/generated/unique_everseen.html):\r\n\r\n    &gt;&gt;&gt; from iteration_utilities import unique_everseen\r\n    &gt;&gt;&gt; lst = [1,1,1,2,3,2,2,2,1,3,4]\r\n    \r\n    &gt;&gt;&gt; list(unique_everseen(lst))\r\n    [1, 2, 3, 4]\r\n\r\n# Timings\r\n\r\nI did some timings (Python 3.6) and these show that it&#39;s faster than all other alternatives I tested, including `OrderedDict.fromkeys`, `f7` and `more_itertools.unique_everseen`:\r\n\r\n    %matplotlib notebook\r\n    \r\n    from iteration_utilities import unique_everseen\r\n    from collections import OrderedDict\r\n    from more_itertools import unique_everseen as mi_unique_everseen\r\n    \r\n    def f7(seq):\r\n        seen = set()\r\n        seen_add = seen.add\r\n        return [x for x in seq if not (x in seen or seen_add(x))]\r\n    \r\n    def iteration_utilities_unique_everseen(seq):\r\n        return list(unique_everseen(seq))\r\n    \r\n    def more_itertools_unique_everseen(seq):\r\n        return list(mi_unique_everseen(seq))\r\n    \r\n    def odict(seq):\r\n        return list(OrderedDict.fromkeys(seq))\r\n    \r\n    from simple_benchmark import Benchmark\r\n    \r\n    b = Benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],\r\n                  {2**i: list(range(2**i)) for i in range(1, 20)},\r\n                  &#39;list size (no duplicates)&#39;)\r\n    b.run()\r\n    \r\n    b.plot()\r\n\r\n[![enter image description here][1]][1]\r\n\r\nAnd just to make sure I also did a test with more duplicates just to check if it makes a difference:\r\n\r\n    import random\r\n\r\n    b = Benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],\r\n                  {2**i: [random.randint(0, 2**(i-1)) for _ in range(2**i)] for i in range(1, 20)},\r\n                  &#39;list size (lots of duplicates)&#39;)\r\n    b.run()\r\n    \r\n    b.plot()\r\n\r\n[![enter image description here][2]][2]\r\n\r\nAnd one containing only one value:\r\n\r\n    b = Benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],\r\n                  {2**i: [1]*(2**i) for i in range(1, 20)},\r\n                  &#39;list size (only duplicates)&#39;)\r\n    b.run()\r\n    \r\n    b.plot()\r\n\r\n[![enter image description here][3]][3]\r\n\r\nIn all of these cases the `iteration_utilities.unique_everseen` function is the fastest (on my computer).\r\n\r\n---\r\n\r\nThis `iteration_utilities.unique_everseen` function can also handle unhashable values in the input (however with an `O(n*n)` performance instead of the `O(n)` performance when the values are hashable).\r\n\r\n    &gt;&gt;&gt; lst = [{1}, {1}, {2}, {1}, {3}]\r\n    \r\n    &gt;&gt;&gt; list(unique_everseen(lst))\r\n    [{1}, {2}, {3}]\r\n\r\n---\r\n\r\n&lt;sup&gt;1&lt;/sup&gt; Disclaimer: I&#39;m the author of that package.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/XLrov.png\r\n  [2]: https://i.stack.imgur.com/YCx2c.png\r\n  [3]: https://i.stack.imgur.com/SPCcT.png",
               "tags": [],
               "creation_date": 1484078148,
               "last_edit_date": 1519078769,
               "is_accepted": false,
               "id": "41577279",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 45746808,
               "is_accepted": false,
               "last_activity_date": 1503016508,
               "body_markdown": "Not to kick a dead horse (this question is very old and already has lots of good answers), but here is a solution using pandas that is quite fast in many circumstances and is dead simple to use.  \r\n\r\n    import pandas as pd\r\n\r\n    my_list = range(5) + range(5)  # [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\r\n    &gt;&gt;&gt; pd.Series(my_list).drop_duplicates().tolist()\r\n    # Output:\r\n    # [0, 1, 2, 3, 4]",
               "id": "45746808",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1503016508,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 49065241,
               "is_accepted": false,
               "last_activity_date": 1519979033,
               "body_markdown": "In **Python 3.7** and above, dictionaries are [guaranteed](https://mail.python.org/pipermail/python-dev/2017-December/151283.html) to remember their key insertion order. The answer to [this](https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6) question summarizes the current state of affairs.\r\n\r\nThe `OrderedDict` solution thus becomes obsolete and without any import statements we can simply issue:\r\n\r\n\r\n    &gt;&gt;&gt; list(dict.fromkeys([1, 2, 1, 3, 3, 2, 4]).keys())\r\n    [1, 2, 3, 4]\r\n\r\n",
               "id": "49065241",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519979033,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-in-whilst-preserving-order",
         "id": "858127-2271"
      },
      {
         "up_vote_count": "278",
         "path": "2.stack",
         "body_markdown": "I have a list of dictionaries like this:\r\n\r\n    [{&#39;points&#39;: 50, &#39;time&#39;: &#39;5:00&#39;, &#39;year&#39;: 2010}, \r\n    {&#39;points&#39;: 25, &#39;time&#39;: &#39;6:00&#39;, &#39;month&#39;: &quot;february&quot;}, \r\n    {&#39;points&#39;:90, &#39;time&#39;: &#39;9:00&#39;, &#39;month&#39;: &#39;january&#39;}, \r\n    {&#39;points_h1&#39;:20, &#39;month&#39;: &#39;june&#39;}]\r\n\r\nand I want to turn this into a pandas `DataFrame` like this:\r\n\r\n          month  points  points_h1  time  year\r\n    0       NaN      50        NaN  5:00  2010\r\n    1  february      25        NaN  6:00   NaN\r\n    2   january      90        NaN  9:00   NaN\r\n    3      june     NaN         20   NaN   NaN\r\n\r\n*Note: Order of the columns does not matter.*\r\n\r\nUltimately, the goal is to write this to a text file and this seems like the best solution I could find.  How can I turn the list of dictionaries into a panda DataFrame as shown above?",
         "view_count": "83390",
         "answer_count": "3",
         "tags": "['python', 'dictionary', 'pandas', 'dataframe']",
         "creation_date": "1387293891",
         "last_edit_date": "1450595897",
         "code_snippet": "['<code>[{\\'points\\': 50, \\'time\\': \\'5:00\\', \\'year\\': 2010}, \\n{\\'points\\': 25, \\'time\\': \\'6:00\\', \\'month\\': \"february\"}, \\n{\\'points\\':90, \\'time\\': \\'9:00\\', \\'month\\': \\'january\\'}, \\n{\\'points_h1\\':20, \\'month\\': \\'june\\'}]\\n</code>', '<code>DataFrame</code>', '<code>      month  points  points_h1  time  year\\n0       NaN      50        NaN  5:00  2010\\n1  february      25        NaN  6:00   NaN\\n2   january      90        NaN  9:00   NaN\\n3      june     NaN         20   NaN   NaN\\n</code>', '<code>d</code>', '<code>pd.DataFrame(d)\\n</code>', \"<code>df = df.set_index('time')</code>\", '<code>pandas.DataFrame</code>', '<code>pd.DataFrame.from_records(d)</code>', '<code>deque</code>', '<code>0.17.1</code>', '<code>0.18.1</code>', '<code>from_records</code>', '<code>pd.DataFrame.from_dict(d)</code>', '<code>In [8]: d = [{\\'points\\': 50, \\'time\\': \\'5:00\\', \\'year\\': 2010}, \\n   ...: {\\'points\\': 25, \\'time\\': \\'6:00\\', \\'month\\': \"february\"}, \\n   ...: {\\'points\\':90, \\'time\\': \\'9:00\\', \\'month\\': \\'january\\'}, \\n   ...: {\\'points_h1\\':20, \\'month\\': \\'june\\'}]\\n\\nIn [12]: pd.DataFrame.from_dict(d)\\nOut[12]: \\n      month  points  points_h1  time    year\\n0       NaN    50.0        NaN  5:00  2010.0\\n1  february    25.0        NaN  6:00     NaN\\n2   january    90.0        NaN  9:00     NaN\\n3      june     NaN       20.0   NaN     NaN\\n</code>', '<code>dict</code>', '<code>dict</code>']",
         "title": "Convert list of dictionaries to Dataframe",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 408,
               "answer_id": 20638258,
               "is_accepted": true,
               "last_activity_date": 1387294513,
               "body_markdown": "Supposing `d` is your list of dicts, simply:\r\n\r\n    pd.DataFrame(d)",
               "id": "20638258",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1387294513,
               "score": 406
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 34,
               "answer_id": 33020669,
               "is_accepted": false,
               "last_activity_date": 1444319972,
               "body_markdown": "in pandas 16.2, I had to do `pd.DataFrame.from_records(d)` to get this to work. ",
               "id": "33020669",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1444319972,
               "score": 34
            },
            {
               "up_vote_count": 9,
               "answer_id": 44957133,
               "last_activity_date": 1499407388,
               "path": "3.stack.answer",
               "body_markdown": "You can also use `pd.DataFrame.from_dict(d)` as :\r\n\r\n    In [8]: d = [{&#39;points&#39;: 50, &#39;time&#39;: &#39;5:00&#39;, &#39;year&#39;: 2010}, \r\n       ...: {&#39;points&#39;: 25, &#39;time&#39;: &#39;6:00&#39;, &#39;month&#39;: &quot;february&quot;}, \r\n       ...: {&#39;points&#39;:90, &#39;time&#39;: &#39;9:00&#39;, &#39;month&#39;: &#39;january&#39;}, \r\n       ...: {&#39;points_h1&#39;:20, &#39;month&#39;: &#39;june&#39;}]\r\n    \r\n    In [12]: pd.DataFrame.from_dict(d)\r\n    Out[12]: \r\n          month  points  points_h1  time    year\r\n    0       NaN    50.0        NaN  5:00  2010.0\r\n    1  february    25.0        NaN  6:00     NaN\r\n    2   january    90.0        NaN  9:00     NaN\r\n    3      june     NaN       20.0   NaN     NaN\r\n\r\n",
               "tags": [],
               "creation_date": 1499368424,
               "last_edit_date": 1499407388,
               "is_accepted": false,
               "id": "44957133",
               "down_vote_count": 0,
               "score": 9
            }
         ],
         "link": "https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-dataframe",
         "id": "858127-2272"
      },
      {
         "up_vote_count": "407",
         "body_markdown": "Using pip, is it possible to figure out which version of a package is currently installed?\r\n\r\nI know about `pip install XYZ --upgrade` but I am wondering if there is anything like `pip info XYZ`.  If not what would be the best way to tell what version I am currently using.",
         "view_count": "253433",
         "answer_count": "9",
         "tags": "['python', 'pip']",
         "creation_date": "1334770640",
         "path": "2.stack",
         "code_snippet": "['<code>pip install XYZ --upgrade</code>', '<code>pip info XYZ</code>', '<code>show</code>', '<code>pip show</code>', '<code>$ pip show Jinja2\\n---\\nName: Jinja2\\nVersion: 2.7.3\\nLocation: /path/to/virtualenv/lib/python2.7/site-packages\\nRequires: markupsafe\\n</code>', '<code>pip freeze</code>', '<code>grep</code>', '<code>$ pip freeze | grep Jinja2\\nJinja2==2.7.3\\n</code>', '<code>pip freeze</code>', '<code>save</code>', '<code>pip show pip</code>', '<code>pip --version</code>', '<code>$ pip show specloud\\n\\nPackage: specloud\\nVersion: 0.4.4\\nRequires:\\nnose\\nfigleaf\\npinocchio\\n</code>', '<code>$ pip list\\nargparse (1.2.1)\\npip (1.5.1)\\nsetuptools (2.1)\\nwsgiref (0.1.2)\\n</code>', '<code>$ pip list --outdated\\ndistribute (Current: 0.6.34 Latest: 0.7.3)\\ndjango-bootstrap3 (Current: 1.1.0 Latest: 4.3.0)\\nDjango (Current: 1.5.4 Latest: 1.6.4)\\nJinja2 (Current: 2.6 Latest: 2.8)\\n</code>', '<code>$ pip list --outdated | grep Jinja2\\nJinja2 (Current: 2.6 Latest: 2.8)\\n</code>', '<code>yolk</code>', '<code>yolk -l</code>', '<code>(venv)CWD&gt; /space/vhosts/pyramid.xcode.com/venv/build/unittest \\nproject@pyramid 43&gt; yolk -l\\nChameleon       - 2.8.2        - active \\nJinja2          - 2.6          - active \\nMako            - 0.7.0        - active \\nMarkupSafe      - 0.15         - active \\nPasteDeploy     - 1.5.0        - active \\nPygments        - 1.5          - active \\nPython          - 2.7.3        - active development (/usr/lib/python2.7/lib-dynload)\\nSQLAlchemy      - 0.7.6        - active \\nWebOb           - 1.2b3        - active \\naccount         - 0.0          - active development (/space/vhosts/pyramid.xcode.com/project/account)\\ndistribute      - 0.6.19       - active \\negenix-mx-base  - 3.2.3        - active \\nipython         - 0.12         - active \\nlogilab-astng   - 0.23.1       - active \\nlogilab-common  - 0.57.1       - active \\nnose            - 1.1.2        - active \\npbkdf2          - 1.3          - active \\npip             - 1.0.2        - active \\npyScss          - 1.1.3        - active \\npycrypto        - 2.5          - active \\npylint          - 0.25.1       - active \\npyramid-debugtoolbar - 1.0.1        - active \\npyramid-tm      - 0.4          - active \\npyramid         - 1.3          - active \\nrepoze.lru      - 0.5          - active \\nsimplejson      - 2.5.0        - active \\ntransaction     - 1.2.0        - active \\ntranslationstring - 1.1          - active \\nvenusian        - 1.0a3        - active \\nwaitress        - 0.8.1        - active \\nwsgiref         - 0.1.2        - active development (/usr/lib/python2.7)\\nyolk            - 0.4.3        - active \\nzope.deprecation - 3.5.1        - active \\nzope.interface  - 3.8.0        - active \\nzope.sqlalchemy - 0.7          - active \\n</code>', '<code>pip show &lt;package_name&gt;|grep Version\\n</code>', '<code>pip show urllib3|grep Version\\n</code>', '<code>pip list\\n</code>', '<code>appdirs (1.4.3)\\nBeautifulSoup (3.2.\\nbeautifulsoup4 (4.6\\ncertifi (2017.4.17)\\nchardet (3.0.4)\\ncookies (2.2.1)\\ndj-database-url (0.\\nDjango (1.10.4)\\ndjango-allauth (0.3\\ndjango-filter (1.0.\\ndjango-haystack (2.\\ndjango-oauth-toolki\\ndjango-recaptcha (1\\ndjangorestframework\\ndjangorestframework\\nfuncsigs (1.0.2)\\ngunicorn (19.7.1)\\nidna (2.5)\\nJinja2 (2.9.6)\\nMarkdown (2.6.8)\\nMarkupSafe (1.0)\\nmock (2.0.0)\\nMySQL-python (1.2.5\\nnumpy (1.13.1)\\noauthlib (2.0.2)\\npackaging (16.8)\\npandas (0.20.3)\\npbr (3.1.1)\\npep8 (1.7.0)\\npip (9.0.1)\\npsycopg2 (2.7.1)\\nPyJWT (1.5.2)\\npyparsing (2.2.0)\\npython-dateutil (2.\\npython-decouple (3.\\npython-openid (2.2.\\npytz (2017.2)\\nrequests (2.18.1)\\nrequests-oauthlib (\\nresponses (0.5.1)\\nsetuptools (35.0.1)\\nsix (1.10.0)\\nsocial-auth-app-dja\\nsocial-auth-core (1\\nurllib3 (1.21.1)\\nvirtualenv (15.1.0)\\nwheel (0.29.0)\\nwhitenoise (3.3.0)\\n</code>', '<code>import jinja2\\nprint jinja2.__version__\\n</code>', '<code>__version__</code>', '<code>pip show setuptools | findstr \"Version\"\\n</code>', '<code>Version: 34.1.1\\n</code>']",
         "title": "Find which version of package is installed with pip",
         "_childDocuments_": [
            {
               "up_vote_count": 528,
               "answer_id": 10215100,
               "last_activity_date": 1408590403,
               "path": "3.stack.answer",
               "body_markdown": "As of [pip 1.3](https://github.com/pypa/pip/blob/1.3/CHANGES.txt#L54), there is a `pip show` command.\r\n\r\n    $ pip show Jinja2\r\n    ---\r\n    Name: Jinja2\r\n    Version: 2.7.3\r\n    Location: /path/to/virtualenv/lib/python2.7/site-packages\r\n    Requires: markupsafe\r\n\r\nIn older versions, `pip freeze` and `grep` should do the job nicely.\r\n\r\n    $ pip freeze | grep Jinja2\r\n    Jinja2==2.7.3",
               "tags": [],
               "creation_date": 1334771631,
               "last_edit_date": 1408590403,
               "is_accepted": true,
               "id": "10215100",
               "down_vote_count": 2,
               "score": 526
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 13,
               "answer_id": 10215664,
               "is_accepted": false,
               "last_activity_date": 1334773837,
               "body_markdown": "You can also install `yolk` and then run `yolk -l` which also gives some nice output.  Here is what I get for my little virtualenv:\r\n\r\n    (venv)CWD&gt; /space/vhosts/pyramid.xcode.com/venv/build/unittest \r\n    project@pyramid 43&gt; yolk -l\r\n    Chameleon       - 2.8.2        - active \r\n    Jinja2          - 2.6          - active \r\n    Mako            - 0.7.0        - active \r\n    MarkupSafe      - 0.15         - active \r\n    PasteDeploy     - 1.5.0        - active \r\n    Pygments        - 1.5          - active \r\n    Python          - 2.7.3        - active development (/usr/lib/python2.7/lib-dynload)\r\n    SQLAlchemy      - 0.7.6        - active \r\n    WebOb           - 1.2b3        - active \r\n    account         - 0.0          - active development (/space/vhosts/pyramid.xcode.com/project/account)\r\n    distribute      - 0.6.19       - active \r\n    egenix-mx-base  - 3.2.3        - active \r\n    ipython         - 0.12         - active \r\n    logilab-astng   - 0.23.1       - active \r\n    logilab-common  - 0.57.1       - active \r\n    nose            - 1.1.2        - active \r\n    pbkdf2          - 1.3          - active \r\n    pip             - 1.0.2        - active \r\n    pyScss          - 1.1.3        - active \r\n    pycrypto        - 2.5          - active \r\n    pylint          - 0.25.1       - active \r\n    pyramid-debugtoolbar - 1.0.1        - active \r\n    pyramid-tm      - 0.4          - active \r\n    pyramid         - 1.3          - active \r\n    repoze.lru      - 0.5          - active \r\n    simplejson      - 2.5.0        - active \r\n    transaction     - 1.2.0        - active \r\n    translationstring - 1.1          - active \r\n    venusian        - 1.0a3        - active \r\n    waitress        - 0.8.1        - active \r\n    wsgiref         - 0.1.2        - active development (/usr/lib/python2.7)\r\n    yolk            - 0.4.3        - active \r\n    zope.deprecation - 3.5.1        - active \r\n    zope.interface  - 3.8.0        - active \r\n    zope.sqlalchemy - 0.7          - active ",
               "id": "10215664",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1334773837,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 51,
               "answer_id": 10300036,
               "is_accepted": false,
               "last_activity_date": 1335277723,
               "body_markdown": "I just sent a pull request in pip with the enhancement Hugo Tavares said:\r\n\r\n(specloud as example)\r\n\r\n    $ pip show specloud\r\n\r\n    Package: specloud\r\n    Version: 0.4.4\r\n    Requires:\r\n    nose\r\n    figleaf\r\n    pinocchio",
               "id": "10300036",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1335277723,
               "score": 51
            },
            {
               "up_vote_count": 26,
               "answer_id": 21305641,
               "last_activity_date": 1413464680,
               "path": "3.stack.answer",
               "body_markdown": "Pip 1.3 now also has a [list][1] command:\r\n\r\n    $ pip list\r\n    argparse (1.2.1)\r\n    pip (1.5.1)\r\n    setuptools (2.1)\r\n    wsgiref (0.1.2)\r\n\r\n\r\n  [1]: https://pip.pypa.io/en/latest/reference/pip_list.html",
               "tags": [],
               "creation_date": 1390473382,
               "last_edit_date": 1413464680,
               "is_accepted": false,
               "id": "21305641",
               "down_vote_count": 0,
               "score": 26
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 21,
               "answer_id": 23386328,
               "is_accepted": false,
               "last_activity_date": 1398855648,
               "body_markdown": "and with --outdated as an extra argument, you will get the Current and Latest versions of the packages you are using :\r\n\r\n    $ pip list --outdated\r\n    distribute (Current: 0.6.34 Latest: 0.7.3)\r\n    django-bootstrap3 (Current: 1.1.0 Latest: 4.3.0)\r\n    Django (Current: 1.5.4 Latest: 1.6.4)\r\n    Jinja2 (Current: 2.6 Latest: 2.8)\r\n\r\nSo combining with AdamKG &#39;s answer :\r\n\r\n    $ pip list --outdated | grep Jinja2\r\n    Jinja2 (Current: 2.6 Latest: 2.8)\r\n\r\nCheck **pip-tools** too : https://github.com/nvie/pip-tools",
               "id": "23386328",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1398855648,
               "score": 21
            },
            {
               "up_vote_count": 3,
               "answer_id": 34526609,
               "last_activity_date": 1471774565,
               "path": "3.stack.answer",
               "body_markdown": "The easiest way is this:\r\n\r\n    import jinja2\r\n    print jinja2.__version__",
               "tags": [],
               "creation_date": 1451467651,
               "last_edit_date": 1471774565,
               "is_accepted": false,
               "id": "34526609",
               "down_vote_count": 1,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 34640581,
               "is_accepted": false,
               "last_activity_date": 1452106366,
               "body_markdown": "You can use the grep command to find out.\r\n\r\n    pip show &lt;package_name&gt;|grep Version\r\n\r\nExample:\r\n\r\n    pip show urllib3|grep Version\r\n\r\nwill show only the versions.\r\n\r\n&gt; Metadata-Version: 2.0  \r\nVersion: 1.12\r\n\r\n",
               "id": "34640581",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1452106366,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 45531653,
               "is_accepted": false,
               "last_activity_date": 1502019622,
               "body_markdown": "You can get a list of packages and their versions, e.g.:\r\n\r\n    pip list\r\n\r\n**Output :**\r\n\r\n    appdirs (1.4.3)\r\n    BeautifulSoup (3.2.\r\n    beautifulsoup4 (4.6\r\n    certifi (2017.4.17)\r\n    chardet (3.0.4)\r\n    cookies (2.2.1)\r\n    dj-database-url (0.\r\n    Django (1.10.4)\r\n    django-allauth (0.3\r\n    django-filter (1.0.\r\n    django-haystack (2.\r\n    django-oauth-toolki\r\n    django-recaptcha (1\r\n    djangorestframework\r\n    djangorestframework\r\n    funcsigs (1.0.2)\r\n    gunicorn (19.7.1)\r\n    idna (2.5)\r\n    Jinja2 (2.9.6)\r\n    Markdown (2.6.8)\r\n    MarkupSafe (1.0)\r\n    mock (2.0.0)\r\n    MySQL-python (1.2.5\r\n    numpy (1.13.1)\r\n    oauthlib (2.0.2)\r\n    packaging (16.8)\r\n    pandas (0.20.3)\r\n    pbr (3.1.1)\r\n    pep8 (1.7.0)\r\n    pip (9.0.1)\r\n    psycopg2 (2.7.1)\r\n    PyJWT (1.5.2)\r\n    pyparsing (2.2.0)\r\n    python-dateutil (2.\r\n    python-decouple (3.\r\n    python-openid (2.2.\r\n    pytz (2017.2)\r\n    requests (2.18.1)\r\n    requests-oauthlib (\r\n    responses (0.5.1)\r\n    setuptools (35.0.1)\r\n    six (1.10.0)\r\n    social-auth-app-dja\r\n    social-auth-core (1\r\n    urllib3 (1.21.1)\r\n    virtualenv (15.1.0)\r\n    wheel (0.29.0)\r\n    whitenoise (3.3.0)",
               "id": "45531653",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1502019622,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48959839,
               "is_accepted": false,
               "last_activity_date": 1519451555,
               "body_markdown": "On windows, you can issue command such as:\r\n\r\n    pip show setuptools | findstr &quot;Version&quot;\r\n\r\nOutput:\r\n \r\n    Version: 34.1.1",
               "id": "48959839",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519451555,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/10214827/find-which-version-of-package-is-installed-with-pip",
         "id": "858127-2273"
      },
      {
         "up_vote_count": "185",
         "path": "2.stack",
         "body_markdown": "I have a list of strings like this:\r\n\r\n    X = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;, &quot;i&quot;]\r\n    Y = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]\r\n\r\nWhat is the shortest way of sorting X using values from Y to get the following output?\r\n\r\n        [&quot;a&quot;, &quot;d&quot;, &quot;h&quot;, &quot;b&quot;, &quot;c&quot;, &quot;e&quot;, &quot;i&quot;, &quot;f&quot;, &quot;g&quot;]\r\n\r\nThe order of the elements having the same &quot;key&quot; does not matter. I can resort to use `for` constructs but I am curious if there is a shorter way. Any suggestions?",
         "view_count": "124768",
         "answer_count": "11",
         "tags": "['python', 'sorting']",
         "creation_date": "1310083017",
         "last_edit_date": "1488372396",
         "code_snippet": "['<code>X = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"]\\nY = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]\\n</code>', '<code>    [\"a\", \"d\", \"h\", \"b\", \"c\", \"e\", \"i\", \"f\", \"g\"]\\n</code>', '<code>for</code>', '<code>strip()</code>', '<code>Alex.strip(clothes)</code>', '<code>[x for _,x in sorted(zip(Y,X))]\\n</code>', '<code>X = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"]\\nY = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]\\n\\nZ = [x for _,x in sorted(zip(Y,X))]\\nprint(Z)  # [\"a\", \"d\", \"h\", \"b\", \"c\", \"e\", \"i\", \"f\", \"g\"]\\n</code>', '<code>[x for _, x in sorted(zip(Y,X), key=lambda pair: pair[0])]\\n</code>', '<code>zip</code>', '<code>list</code>', '<code>list</code>', '<code>zip</code>', '<code>sorted()</code>', '<code>list</code>', '<code>key</code>', '<code>sorted</code>', \"<code>&gt;&gt;&gt; yx = zip(Y, X)\\n&gt;&gt;&gt; yx\\n[(0, 'a'), (1, 'b'), (1, 'c'), (0, 'd'), (1, 'e'), (2, 'f'), (2, 'g'), (0, 'h'), (1, 'i')]\\n&gt;&gt;&gt; yx.sort()\\n&gt;&gt;&gt; yx\\n[(0, 'a'), (0, 'd'), (0, 'h'), (1, 'b'), (1, 'c'), (1, 'e'), (1, 'i'), (2, 'f'), (2, 'g')]\\n&gt;&gt;&gt; x_sorted = [x for y, x in yx]\\n&gt;&gt;&gt; x_sorted\\n['a', 'd', 'h', 'b', 'c', 'e', 'i', 'f', 'g']\\n</code>\", '<code>[x for y, x in sorted(zip(Y, X))]\\n</code>', '<code>X</code>', '<code>str</code>', '<code>&lt;</code>', '<code>X</code>', '<code>None</code>', \"<code>AttributeError: 'zip' object has no attribute 'sort'</code>\", '<code>sorted(zip(...))</code>', '<code>them = list(zip(...)); them.sort()</code>', \"<code>people = ['Jim', 'Pam', 'Micheal', 'Dwight']\\nages = [27, 25, 4, 9]\\n\\nimport numpy\\npeople = numpy.array(people)\\nages = numpy.array(ages)\\ninds = ages.argsort()\\nsortedPeople = people[inds]\\n</code>\", '<code>key</code>', '<code>&gt;&gt;&gt; X = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"]\\n&gt;&gt;&gt; Y = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]\\n&gt;&gt;&gt; keydict = dict(zip(X, Y))\\n&gt;&gt;&gt; X.sort(key=keydict.get)\\n&gt;&gt;&gt; X\\n[\\'a\\', \\'d\\', \\'h\\', \\'b\\', \\'c\\', \\'e\\', \\'i\\', \\'f\\', \\'g\\']\\n</code>', '<code>&gt;&gt;&gt; X.sort(key=dict(zip(X, Y)).get)\\n</code>', '<code>    X = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"]\\n    Y = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]\\n\\n    sorted_y_idx_list = sorted(range(len(Y)),key=lambda x:Y[x])\\n    Xs = [X[i] for i in sorted_y_idx_list ]\\n\\n    print( \"Xs:\", Xs )\\n    # prints: Xs: [\"a\", \"d\", \"h\", \"b\", \"c\", \"e\", \"i\", \"f\", \"g\"]\\n</code>', '<code>zip(*sorted(zip(Y,X)))[1]\\n</code>', '<code>zip(*sorted(zip(X,Y), key=operator.itemgetter(1)))[0]\\n</code>', '<code>more_itertools</code>', \"<code>from more_itertools import sort_together\\n\\nsort_together([Y, X])[1]\\n# ('a', 'd', 'h', 'b', 'c', 'e', 'i', 'f', 'g')\\n</code>\", '<code>list_a = [5,4,3,2,1]\\nlist_b = [1,1.5,1.75,2,3,3.5,3.75,4,5]\\n</code>', '<code>orderedList =  sorted(list_a, key=lambda x: list_b.index(x))\\n</code>', '<code>pandas Series</code>', '<code>data</code>', '<code>index</code>', '<code>import pandas as pd\\npd.Series(data=X,index=Y).sort_index().tolist()\\n</code>', \"<code>['a', 'd', 'h', 'b', 'c', 'e', 'i', 'f', 'g']\\n</code>\", '<code>list_set = set(list2)\\n</code>', \"<code>list_str = ''.join(str(s) for s in list2)\\n</code>\", '<code>for i in list_set:\\ncur_loc = list_str.find(str(i))\\n\\nwhile cur_loc &gt;= 0:\\n    cur_loclist.append(cur_loc)\\n    cur_loc = list_str.find(str(i),cur_loc+1)\\n\\nprint(cur_loclist)\\n\\nfor i in range(0,len(cur_loclist)):\\noutput.append(list1[cur_loclist[i]])\\nprint(output)\\n</code>']",
         "title": "Sorting list based on values from another list?",
         "_childDocuments_": [
            {
               "up_vote_count": 242,
               "answer_id": 6618543,
               "last_activity_date": 1503395855,
               "path": "3.stack.answer",
               "body_markdown": "**Shortest Code**\r\n\r\n    [x for _,x in sorted(zip(Y,X))]\r\n\r\n**Example:**\r\n\r\n    X = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;, &quot;i&quot;]\r\n    Y = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]\r\n   \r\n    Z = [x for _,x in sorted(zip(Y,X))]\r\n    print(Z)  # [&quot;a&quot;, &quot;d&quot;, &quot;h&quot;, &quot;b&quot;, &quot;c&quot;, &quot;e&quot;, &quot;i&quot;, &quot;f&quot;, &quot;g&quot;]\r\n\r\n\r\n----------\r\n\r\n\r\n**Generally Speaking**\r\n\r\n    [x for _, x in sorted(zip(Y,X), key=lambda pair: pair[0])]\r\n\r\n**Explained:**\r\n\r\n 1. [`zip`][1] the two `list`s.\r\n 2. create a new, sorted `list` based on the `zip` using [`sorted()`][2].\r\n 3. using a list comprehension _extract_ the first elements of each pair from the sorted, zipped `list`.\r\n\r\n_For more information on how to set\\use the `key` parameter as well as the `sorted` function in general, take a look at [this][3]._\r\n\r\n----------\r\n\r\n\r\n  [1]: https://docs.python.org/3/library/functions.html#zip\r\n  [2]: https://www.programiz.com/python-programming/methods/built-in/sorted\r\n  [3]: http://pythoncentral.io/how-to-sort-a-list-tuple-or-object-with-sorted-in-python/",
               "tags": [],
               "creation_date": 1310083334,
               "last_edit_date": 1503395855,
               "is_accepted": true,
               "id": "6618543",
               "down_vote_count": 3,
               "score": 239
            },
            {
               "up_vote_count": 26,
               "answer_id": 6618548,
               "last_activity_date": 1310084511,
               "path": "3.stack.answer",
               "body_markdown": "The most obvious solution to me is to use the `key` keyword arg.\r\n\r\n    &gt;&gt;&gt; X = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;, &quot;i&quot;]\r\n    &gt;&gt;&gt; Y = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]\r\n    &gt;&gt;&gt; keydict = dict(zip(X, Y))\r\n    &gt;&gt;&gt; X.sort(key=keydict.get)\r\n    &gt;&gt;&gt; X\r\n    [&#39;a&#39;, &#39;d&#39;, &#39;h&#39;, &#39;b&#39;, &#39;c&#39;, &#39;e&#39;, &#39;i&#39;, &#39;f&#39;, &#39;g&#39;]\r\n\r\nNote that you can shorten this to a one-liner if you care to:\r\n\r\n    &gt;&gt;&gt; X.sort(key=dict(zip(X, Y)).get)\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1310083352,
               "last_edit_date": 1310084511,
               "is_accepted": false,
               "id": "6618548",
               "down_vote_count": 0,
               "score": 26
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 82,
               "answer_id": 6618553,
               "is_accepted": false,
               "last_activity_date": 1310083384,
               "body_markdown": "Zip the two lists together, sort it, then take the parts you want:\r\n\r\n    &gt;&gt;&gt; yx = zip(Y, X)\r\n    &gt;&gt;&gt; yx\r\n    [(0, &#39;a&#39;), (1, &#39;b&#39;), (1, &#39;c&#39;), (0, &#39;d&#39;), (1, &#39;e&#39;), (2, &#39;f&#39;), (2, &#39;g&#39;), (0, &#39;h&#39;), (1, &#39;i&#39;)]\r\n    &gt;&gt;&gt; yx.sort()\r\n    &gt;&gt;&gt; yx\r\n    [(0, &#39;a&#39;), (0, &#39;d&#39;), (0, &#39;h&#39;), (1, &#39;b&#39;), (1, &#39;c&#39;), (1, &#39;e&#39;), (1, &#39;i&#39;), (2, &#39;f&#39;), (2, &#39;g&#39;)]\r\n    &gt;&gt;&gt; x_sorted = [x for y, x in yx]\r\n    &gt;&gt;&gt; x_sorted\r\n    [&#39;a&#39;, &#39;d&#39;, &#39;h&#39;, &#39;b&#39;, &#39;c&#39;, &#39;e&#39;, &#39;i&#39;, &#39;f&#39;, &#39;g&#39;]\r\n\r\nCombine these together to get:\r\n\r\n    [x for y, x in sorted(zip(Y, X))]\r\n  \r\n\r\n",
               "id": "6618553",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1310083384,
               "score": 82
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 6620187,
               "is_accepted": false,
               "last_activity_date": 1310101677,
               "body_markdown": "zip, sort by the second column, return the first column.\r\n\r\n    zip(*sorted(zip(X,Y), key=operator.itemgetter(1)))[0]",
               "id": "6620187",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1310101677,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 19382336,
               "is_accepted": false,
               "last_activity_date": 1381843289,
               "body_markdown": "Another alternative, combining several of the answers.\r\n\r\n    zip(*sorted(zip(Y,X)))[1]",
               "id": "19382336",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1381843289,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 39,
               "answer_id": 21077060,
               "is_accepted": false,
               "last_activity_date": 1389543520,
               "body_markdown": "Also, if you don&#39;t mind using numpy arrays (or in fact already are dealing with numpy arrays...), here is another nice solution:\r\n\r\n    people = [&#39;Jim&#39;, &#39;Pam&#39;, &#39;Micheal&#39;, &#39;Dwight&#39;]\r\n    ages = [27, 25, 4, 9]\r\n     \r\n    import numpy\r\n    people = numpy.array(people)\r\n    ages = numpy.array(ages)\r\n    inds = ages.argsort()\r\n    sortedPeople = people[inds]\r\n\r\nI found it here:\r\nhttp://scienceoss.com/sort-one-list-by-another-list/\r\n",
               "id": "21077060",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1389543520,
               "score": 39
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 29066554,
               "is_accepted": false,
               "last_activity_date": 1426456022,
               "body_markdown": "I like having a list of sorted indices. That way, I can sort any list in the same order as the source list. Once you have a list of sorted indices, a simple list comprehension will do the trick:\r\n\r\n        X = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;, &quot;i&quot;]\r\n        Y = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]\r\n\r\n        sorted_y_idx_list = sorted(range(len(Y)),key=lambda x:Y[x])\r\n        Xs = [X[i] for i in sorted_y_idx_list ]\r\n\r\n        print( &quot;Xs:&quot;, Xs )\r\n        # prints: Xs: [&quot;a&quot;, &quot;d&quot;, &quot;h&quot;, &quot;b&quot;, &quot;c&quot;, &quot;e&quot;, &quot;i&quot;, &quot;f&quot;, &quot;g&quot;]\r\n\r\nNote that the sorted index list can also be gotten using numpy argsort().",
               "id": "29066554",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1426456022,
               "score": 8
            },
            {
               "up_vote_count": 3,
               "answer_id": 45514542,
               "last_activity_date": 1501961155,
               "path": "3.stack.answer",
               "body_markdown": "[`more_itertools`][1] has a tool for sorting iterables in parallel:\r\n\r\n    from more_itertools import sort_together\r\n    \r\n    sort_together([Y, X])[1]\r\n    # (&#39;a&#39;, &#39;d&#39;, &#39;h&#39;, &#39;b&#39;, &#39;c&#39;, &#39;e&#39;, &#39;i&#39;, &#39;f&#39;, &#39;g&#39;)\r\n\r\n  [1]: https://more-itertools.readthedocs.io/en/latest/api.html#more_itertools.sort_together",
               "tags": [],
               "creation_date": 1501876439,
               "last_edit_date": 1501961155,
               "is_accepted": false,
               "id": "45514542",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 0,
               "answer_id": 48142660,
               "last_activity_date": 1515369724,
               "path": "3.stack.answer",
               "body_markdown": "You can create a `pandas Series`, using the primary list as `data` and the other list as `index`, and then just sort by the index:\r\n\r\n    import pandas as pd\r\n    pd.Series(data=X,index=Y).sort_index().tolist()\r\n\r\noutput:\r\n\r\n    [&#39;a&#39;, &#39;d&#39;, &#39;h&#39;, &#39;b&#39;, &#39;c&#39;, &#39;e&#39;, &#39;i&#39;, &#39;f&#39;, &#39;g&#39;]",
               "tags": [],
               "creation_date": 1515369180,
               "last_edit_date": 1515369724,
               "is_accepted": false,
               "id": "48142660",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 48176435,
               "is_accepted": false,
               "last_activity_date": 1515530951,
               "body_markdown": "A quick one-liner.\r\n\r\n    list_a = [5,4,3,2,1]\r\n    list_b = [1,1.5,1.75,2,3,3.5,3.75,4,5]\r\n\r\nSay you want list a to match list b.\r\n\r\n    orderedList =  sorted(list_a, key=lambda x: list_b.index(x))\r\n\r\nThis is helpful when needing to order a smaller list to values in larger. Assuming that the larger list contains all values in the smaller list, it can be done.",
               "id": "48176435",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1515530951,
               "score": 1
            },
            {
               "up_vote_count": 1,
               "answer_id": 48820358,
               "last_activity_date": 1518764214,
               "path": "3.stack.answer",
               "body_markdown": "list1 = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;,&#39;h&#39;,&#39;i&#39;]\r\nlist2 = [0,1,1,0,1,2,2,0,1]\r\n\r\noutput=[]\r\ncur_loclist = []\r\n\r\nto get unique values present in list2\r\n\r\n    list_set = set(list2)\r\n\r\nto find the loc of the index in list2 \r\n\r\n    list_str = &#39;&#39;.join(str(s) for s in list2)\r\n\r\nlocation of index in list2 is tracked using cur_loclist\r\n[0, 3, 7, 1, 2, 4, 8, 5, 6]\r\n\r\n    for i in list_set:\r\n    cur_loc = list_str.find(str(i))\r\n    \r\n    while cur_loc &gt;= 0:\r\n        cur_loclist.append(cur_loc)\r\n        cur_loc = list_str.find(str(i),cur_loc+1)\r\n\r\n    print(cur_loclist)\r\n\r\n    for i in range(0,len(cur_loclist)):\r\n    output.append(list1[cur_loclist[i]])\r\n    print(output)",
               "tags": [],
               "creation_date": 1518757434,
               "last_edit_date": 1518764214,
               "is_accepted": false,
               "id": "48820358",
               "down_vote_count": 1,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/6618515/sorting-list-based-on-values-from-another-list",
         "id": "858127-2274"
      },
      {
         "up_vote_count": "227",
         "path": "2.stack",
         "body_markdown": "I&#39;m starting with input data like this\r\n\r\n    df1 = pandas.DataFrame( { \r\n        &quot;Name&quot; : [&quot;Alice&quot;, &quot;Bob&quot;, &quot;Mallory&quot;, &quot;Mallory&quot;, &quot;Bob&quot; , &quot;Mallory&quot;] , \r\n        &quot;City&quot; : [&quot;Seattle&quot;, &quot;Seattle&quot;, &quot;Portland&quot;, &quot;Seattle&quot;, &quot;Seattle&quot;, &quot;Portland&quot;] } )\r\n\r\nWhich when printed appears as this:\r\n\r\n       City     Name\r\n    0   Seattle    Alice\r\n    1   Seattle      Bob\r\n    2  Portland  Mallory\r\n    3   Seattle  Mallory\r\n    4   Seattle      Bob\r\n    5  Portland  Mallory\r\n\r\n\r\nGrouping is simple enough:\r\n\r\n    g1 = df1.groupby( [ &quot;Name&quot;, &quot;City&quot;] ).count()\r\n\r\nand printing yields a `GroupBy` object:\r\n\r\n                      City  Name\r\n    Name    City\r\n    Alice   Seattle      1     1\r\n    Bob     Seattle      2     2\r\n    Mallory Portland     2     2\r\n            Seattle      1     1\r\n\r\n\r\nBut what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:\r\n\r\n\r\n                      City  Name\r\n    Name    City\r\n    Alice   Seattle      1     1\r\n    Bob     Seattle      2     2\r\n    Mallory Portland     2     2\r\n    Mallory Seattle      1     1\r\n\r\nI can&#39;t quite see how to accomplish this in the pandas documentation. Any hints would be welcome.\r\n",
         "view_count": "250802",
         "answer_count": "6",
         "tags": "['python', 'pandas', 'dataframe', 'group-by', 'multi-index']",
         "creation_date": "1335715835",
         "last_edit_date": "1494079775",
         "code_snippet": "['<code>df1 = pandas.DataFrame( { \\n    \"Name\" : [\"Alice\", \"Bob\", \"Mallory\", \"Mallory\", \"Bob\" , \"Mallory\"] , \\n    \"City\" : [\"Seattle\", \"Seattle\", \"Portland\", \"Seattle\", \"Seattle\", \"Portland\"] } )\\n</code>', '<code>   City     Name\\n0   Seattle    Alice\\n1   Seattle      Bob\\n2  Portland  Mallory\\n3   Seattle  Mallory\\n4   Seattle      Bob\\n5  Portland  Mallory\\n</code>', '<code>g1 = df1.groupby( [ \"Name\", \"City\"] ).count()\\n</code>', '<code>GroupBy</code>', '<code>                  City  Name\\nName    City\\nAlice   Seattle      1     1\\nBob     Seattle      2     2\\nMallory Portland     2     2\\n        Seattle      1     1\\n</code>', '<code>                  City  Name\\nName    City\\nAlice   Seattle      1     1\\nBob     Seattle      2     2\\nMallory Portland     2     2\\nMallory Seattle      1     1\\n</code>', '<code>g1</code>', \"<code>In [19]: type(g1)\\nOut[19]: pandas.core.frame.DataFrame\\n\\nIn [20]: g1.index\\nOut[20]: \\nMultiIndex([('Alice', 'Seattle'), ('Bob', 'Seattle'), ('Mallory', 'Portland'),\\n       ('Mallory', 'Seattle')], dtype=object)\\n</code>\", \"<code>In [21]: g1.add_suffix('_Count').reset_index()\\nOut[21]: \\n      Name      City  City_Count  Name_Count\\n0    Alice   Seattle           1           1\\n1      Bob   Seattle           2           2\\n2  Mallory  Portland           2           2\\n3  Mallory   Seattle           1           1\\n</code>\", '<code>In [36]: DataFrame({\\'count\\' : df1.groupby( [ \"Name\", \"City\"] ).size()}).reset_index()\\nOut[36]: \\n      Name      City  count\\n0    Alice   Seattle      1\\n1      Bob   Seattle      2\\n2  Mallory  Portland      2\\n3  Mallory   Seattle      1\\n</code>', '<code>reset.index()</code>', '<code>df1.groupby( [ \"Name\", \"City\"] ).size().to_frame(name = \\'count\\').reset_index()</code>', '<code>.reset_index()</code>', \"<code>df.groupby('some_column').apply(your_custom_func)</code>\", '<code>as_index=False</code>', '<code>as_index=True</code>', '<code>as_index=False</code>', '<code>mean</code>', '<code>sum</code>', '<code>size</code>', '<code>count</code>', '<code>std</code>', '<code>var</code>', '<code>sem</code>', '<code>describe</code>', '<code>first</code>', '<code>last</code>', '<code>nth</code>', '<code>min</code>', '<code>max</code>', '<code>DataFrame.sum()</code>', '<code>Series</code>', '<code>import pandas as pd\\n\\ndf1 = pd.DataFrame({\"Name\":[\"Alice\", \"Bob\", \"Mallory\", \"Mallory\", \"Bob\" , \"Mallory\"],\\n                    \"City\":[\"Seattle\",\"Seattle\",\"Portland\",\"Seattle\",\"Seattle\",\"Portland\"]})\\nprint df1\\n#\\n#       City     Name\\n#0   Seattle    Alice\\n#1   Seattle      Bob\\n#2  Portland  Mallory\\n#3   Seattle  Mallory\\n#4   Seattle      Bob\\n#5  Portland  Mallory\\n#\\ng1 = df1.groupby([\"Name\", \"City\"], as_index=False).count()\\nprint g1\\n#\\n#                  City  Name\\n#Name    City\\n#Alice   Seattle      1     1\\n#Bob     Seattle      2     2\\n#Mallory Portland     2     2\\n#        Seattle      1     1\\n#\\n</code>', '<code>0.17.1</code>', '<code>subset</code>', '<code>count</code>', '<code>reset_index</code>', '<code>name</code>', '<code>size</code>', '<code>print df1.groupby([\"Name\", \"City\"], as_index=False ).count()\\n#IndexError: list index out of range\\n\\nprint df1.groupby([\"Name\", \"City\"]).count()\\n#Empty DataFrame\\n#Columns: []\\n#Index: [(Alice, Seattle), (Bob, Seattle), (Mallory, Portland), (Mallory, Seattle)]\\n\\nprint df1.groupby([\"Name\", \"City\"])[[\\'Name\\',\\'City\\']].count()\\n#                  Name  City\\n#Name    City                \\n#Alice   Seattle      1     1\\n#Bob     Seattle      2     2\\n#Mallory Portland     2     2\\n#        Seattle      1     1\\n\\nprint df1.groupby([\"Name\", \"City\"]).size().reset_index(name=\\'count\\')\\n#      Name      City  count\\n#0    Alice   Seattle      1\\n#1      Bob   Seattle      2\\n#2  Mallory  Portland      2\\n#3  Mallory   Seattle      1\\n</code>', '<code>count</code>', '<code>size</code>', '<code>size</code>', '<code>count</code>', '<code>df1.groupby( [ \"Name\", \"City\"]).size().reset_index(name=\"count\")</code>', \"<code>as_index=False' stopped working in latest versions? I also tried to run </code>\", '<code>Series</code>', '<code>DataFrame</code>', '<code>groupby</code>', '<code>import pandas as pd\\n\\ngrouped_df = df1.groupby( [ \"Name\", \"City\"] )\\n\\npd.DataFrame(grouped_df.size().reset_index(name = \"Group_Count\"))\\n</code>', '<code>import numpy as np\\nimport pandas as pd\\n\\ndf1 = pd.DataFrame({ \\n    \"Name\" : [\"Alice\", \"Bob\", \"Mallory\", \"Mallory\", \"Bob\" , \"Mallory\"] , \\n    \"City\" : [\"Seattle\", \"Seattle\", \"Portland\", \"Seattle\", \"Seattle\", \"Portland\"]})\\n\\ndf1[\\'City_count\\'] = 1\\ndf1[\\'Name_count\\'] = 1\\n\\ndf1.groupby([\\'Name\\', \\'City\\'], as_index=False).count()\\n</code>', '<code>df = df[\\'TIME\\'].groupby(df[\\'Name\\']).min()\\ndf = df.to_frame()\\ndf = df.reset_index(level=[\\'Name\\',\"TIME\"])\\n</code>', \"<code>almo_grp_data = pd.DataFrame({'Qty_cnt' :\\nalmo_slt_models_data.groupby( ['orderDate','Item','State Abv']\\n          )['Qty'].sum()}).reset_index()\\n</code>\"]",
         "title": "Converting a Pandas GroupBy object to DataFrame",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 300,
               "answer_id": 10374456,
               "is_accepted": true,
               "last_activity_date": 1335721833,
               "body_markdown": "`g1` here _is_ a DataFrame. It has a hierarchical index, though:\r\n\r\n    In [19]: type(g1)\r\n    Out[19]: pandas.core.frame.DataFrame\r\n    \r\n    In [20]: g1.index\r\n    Out[20]: \r\n    MultiIndex([(&#39;Alice&#39;, &#39;Seattle&#39;), (&#39;Bob&#39;, &#39;Seattle&#39;), (&#39;Mallory&#39;, &#39;Portland&#39;),\r\n           (&#39;Mallory&#39;, &#39;Seattle&#39;)], dtype=object)\r\n\r\nPerhaps you want something like this?\r\n\r\n    In [21]: g1.add_suffix(&#39;_Count&#39;).reset_index()\r\n    Out[21]: \r\n          Name      City  City_Count  Name_Count\r\n    0    Alice   Seattle           1           1\r\n    1      Bob   Seattle           2           2\r\n    2  Mallory  Portland           2           2\r\n    3  Mallory   Seattle           1           1\r\n\r\nOr something like:\r\n\r\n    In [36]: DataFrame({&#39;count&#39; : df1.groupby( [ &quot;Name&quot;, &quot;City&quot;] ).size()}).reset_index()\r\n    Out[36]: \r\n          Name      City  count\r\n    0    Alice   Seattle      1\r\n    1      Bob   Seattle      2\r\n    2  Mallory  Portland      2\r\n    3  Mallory   Seattle      1\r\n\r\n",
               "id": "10374456",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1335721833,
               "score": 299
            },
            {
               "up_vote_count": 65,
               "answer_id": 32307259,
               "last_activity_date": 1506436957,
               "path": "3.stack.answer",
               "body_markdown": "I want to little bit change answer by Wes, because version 0.16.2 need set `as_index=False`. If you don&#39;t set it, you get empty dataframe.\r\n\r\n[Source](http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation):\r\n&gt;Aggregation functions will not return the groups that you are aggregating over if they are named columns, when `as_index=True`, the default. The grouped columns will be the indices of the returned object.\r\n\r\n&gt;Passing `as_index=False` will return the groups that you are aggregating over, if they are named columns.\r\n\r\n&gt;Aggregating functions are ones that reduce the dimension of the returned objects, for example: `mean`, `sum`, `size`, `count`, `std`, `var`, `sem`, `describe`, `first`, `last`, `nth`, `min`, `max`. This is what happens when you do for example `DataFrame.sum()` and get back a `Series`.  \r\n\r\n&gt;nth can act as a reducer or a filter, see [here](http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-nth).\r\n\r\n\timport pandas as pd\r\n\r\n\tdf1 = pd.DataFrame({&quot;Name&quot;:[&quot;Alice&quot;, &quot;Bob&quot;, &quot;Mallory&quot;, &quot;Mallory&quot;, &quot;Bob&quot; , &quot;Mallory&quot;],\r\n\t                    &quot;City&quot;:[&quot;Seattle&quot;,&quot;Seattle&quot;,&quot;Portland&quot;,&quot;Seattle&quot;,&quot;Seattle&quot;,&quot;Portland&quot;]})\r\n\tprint df1\r\n\t#\r\n\t#       City     Name\r\n\t#0   Seattle    Alice\r\n\t#1   Seattle      Bob\r\n\t#2  Portland  Mallory\r\n\t#3   Seattle  Mallory\r\n\t#4   Seattle      Bob\r\n\t#5  Portland  Mallory\r\n\t#\r\n\tg1 = df1.groupby([&quot;Name&quot;, &quot;City&quot;], as_index=False).count()\r\n\tprint g1\r\n\t#\r\n\t#                  City  Name\r\n\t#Name    City\r\n\t#Alice   Seattle      1     1\r\n\t#Bob     Seattle      2     2\r\n\t#Mallory Portland     2     2\r\n\t#        Seattle      1     1\r\n\t#\r\n\r\nEDIT:\r\n\r\nIn version `0.17.1` and later you can use `subset` in [`count`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.count.html) and [`reset_index`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.reset_index.html) with parameter `name` in [`size`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html):\r\n\r\n    print df1.groupby([&quot;Name&quot;, &quot;City&quot;], as_index=False ).count()\r\n    #IndexError: list index out of range\r\n\r\n    print df1.groupby([&quot;Name&quot;, &quot;City&quot;]).count()\r\n    #Empty DataFrame\r\n    #Columns: []\r\n    #Index: [(Alice, Seattle), (Bob, Seattle), (Mallory, Portland), (Mallory, Seattle)]\r\n    \r\n    print df1.groupby([&quot;Name&quot;, &quot;City&quot;])[[&#39;Name&#39;,&#39;City&#39;]].count()\r\n    #                  Name  City\r\n    #Name    City                \r\n    #Alice   Seattle      1     1\r\n    #Bob     Seattle      2     2\r\n    #Mallory Portland     2     2\r\n    #        Seattle      1     1\r\n    \r\n    print df1.groupby([&quot;Name&quot;, &quot;City&quot;]).size().reset_index(name=&#39;count&#39;)\r\n    #      Name      City  count\r\n    #0    Alice   Seattle      1\r\n    #1      Bob   Seattle      2\r\n    #2  Mallory  Portland      2\r\n    #3  Mallory   Seattle      1\r\n\r\nThe difference between `count` and `size` is that `size` counts NaN values while `count` does not.",
               "tags": [],
               "creation_date": 1441010885,
               "last_edit_date": 1506436957,
               "is_accepted": false,
               "id": "32307259",
               "down_vote_count": 0,
               "score": 65
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 36926844,
               "is_accepted": false,
               "last_activity_date": 1461884202,
               "body_markdown": "I found this worked for me.\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n\r\n    df1 = pd.DataFrame({ \r\n        &quot;Name&quot; : [&quot;Alice&quot;, &quot;Bob&quot;, &quot;Mallory&quot;, &quot;Mallory&quot;, &quot;Bob&quot; , &quot;Mallory&quot;] , \r\n        &quot;City&quot; : [&quot;Seattle&quot;, &quot;Seattle&quot;, &quot;Portland&quot;, &quot;Seattle&quot;, &quot;Seattle&quot;, &quot;Portland&quot;]})\r\n\r\n    df1[&#39;City_count&#39;] = 1\r\n    df1[&#39;Name_count&#39;] = 1\r\n\r\n    df1.groupby([&#39;Name&#39;, &#39;City&#39;], as_index=False).count()",
               "id": "36926844",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1461884202,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 36953019,
               "is_accepted": false,
               "last_activity_date": 1462007795,
               "body_markdown": "Simply, this should do the task:\r\n\r\n    import pandas as pd\r\n\r\n    grouped_df = df1.groupby( [ &quot;Name&quot;, &quot;City&quot;] )\r\n\r\n    pd.DataFrame(grouped_df.size().reset_index(name = &quot;Group_Count&quot;))\r\n\r\nHere, grouped_df.size() pulls up the unique groupby count, and reset_index() method resets the name of the column you want it to be.\r\nFinally, the pandas Dataframe() function is called upon to create DataFrame object. \r\n",
               "id": "36953019",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1462007795,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 43148562,
               "is_accepted": false,
               "last_activity_date": 1490989537,
               "body_markdown": "Maybe I misunderstand the question but if you want to convert the groupby back to a dataframe you can use .to_frame(). I wanted to reset the index when I did this so I included that part as well. \r\n\r\nexample code unrelated to question\r\n\r\n    df = df[&#39;TIME&#39;].groupby(df[&#39;Name&#39;]).min()\r\n    df = df.to_frame()\r\n    df = df.reset_index(level=[&#39;Name&#39;,&quot;TIME&quot;])",
               "id": "43148562",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1490989537,
               "score": 3
            },
            {
               "up_vote_count": 0,
               "answer_id": 47865979,
               "last_activity_date": 1517452065,
               "path": "3.stack.answer",
               "body_markdown": "I have aggregated with Qty wise data and store to dataframe\r\n\r\n    almo_grp_data = pd.DataFrame({&#39;Qty_cnt&#39; :\r\n    almo_slt_models_data.groupby( [&#39;orderDate&#39;,&#39;Item&#39;,&#39;State Abv&#39;]\r\n              )[&#39;Qty&#39;].sum()}).reset_index()",
               "tags": [],
               "creation_date": 1513591373,
               "last_edit_date": 1517452065,
               "is_accepted": false,
               "id": "47865979",
               "down_vote_count": 0,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-object-to-dataframe",
         "id": "858127-2275"
      },
      {
         "up_vote_count": "413",
         "body_markdown": "Is there a way using Python&#39;s standard library to easily determine (i.e. one function call) the last day of a given month?\r\n\r\nIf the standard library doesn&#39;t support that, does the dateutil package support this?",
         "view_count": "177473",
         "answer_count": "22",
         "tags": "['python', 'date']",
         "creation_date": "1220489684",
         "path": "2.stack",
         "code_snippet": "['<code>&gt;&gt;&gt; import calendar\\n&gt;&gt;&gt; calendar.monthrange(2002,1)\\n(1, 31)\\n&gt;&gt;&gt; calendar.monthrange(2008,2)\\n(4, 29)\\n&gt;&gt;&gt; calendar.monthrange(2100,2)\\n(0, 28)\\n</code>', '<code>calendar.monthrange(year, month)[1]\\n</code>', '<code>monthrange</code>', '<code>&gt;&gt;&gt; from calendar import monthrange\\n&gt;&gt;&gt; monthrange(2012, 2)\\n(2, 29)\\n</code>', '<code>calendar</code>', '<code>import datetime\\n\\ndef last_day_of_month(any_day):\\n    next_month = any_day.replace(day=28) + datetime.timedelta(days=4)  # this will never fail\\n    return next_month - datetime.timedelta(days=next_month.day)\\n</code>', '<code>&gt;&gt;&gt; for month in range(1, 13):\\n...     print last_day_of_month(datetime.date(2012, month, 1))\\n...\\n2012-01-31\\n2012-02-29\\n2012-03-31\\n2012-04-30\\n2012-05-31\\n2012-06-30\\n2012-07-31\\n2012-08-31\\n2012-09-30\\n2012-10-31\\n2012-11-30\\n2012-12-31\\n</code>', '<code>&gt;&gt;&gt; import datetime\\n&gt;&gt;&gt; datetime.date (2000, 2, 1) - datetime.timedelta (days = 1)\\ndatetime.date(2000, 1, 31)\\n&gt;&gt;&gt; \\n</code>', '<code>today.month + 1 == 13</code>', '<code>ValueError</code>', '<code>def last_day_of_month(date):\\n    if date.month == 12:\\n        return date.replace(day=31)\\n    return date.replace(month=date.month+1, day=1) - datetime.timedelta(days=1)\\n\\n&gt;&gt;&gt; last_day_of_month(datetime.date(2002, 1, 17))\\ndatetime.date(2002, 1, 31)\\n&gt;&gt;&gt; last_day_of_month(datetime.date(2002, 12, 9))\\ndatetime.date(2002, 12, 31)\\n&gt;&gt;&gt; last_day_of_month(datetime.date(2008, 2, 14))\\ndatetime.date(2008, 2, 29)\\n</code>', '<code>today = datetime.date.today(); start_date = today.replace(day=1)</code>', '<code>dateutil.relativedelta</code>', '<code>day=31</code>', '<code>from datetime import datetime\\nfrom dateutil.relativedelta import relativedelta\\n\\ndate_in_feb = datetime.datetime(2013, 2, 21)\\nprint datetime.datetime(2013, 2, 21) + relativedelta(day=31)  # End-of-month\\n&gt;&gt;&gt; datetime.datetime(2013, 2, 28, 0, 0)\\n</code>', '<code>datetime(2014, 2, 1) + relativedelta(days=31)</code>', '<code>datetime(2014, 3, 4, 0, 0)</code>', '<code>relativedelta</code>', '<code>from dateutil.relativedelta import relativedelta\\nlast_date_of_month = datetime(mydate.year,mydate.month,1)+relativedelta(months=1,days=-1)\\n</code>', '<code>relativedelta</code>', '<code>from datetime import datetime\\n\\ndef last_day_of_month(year, month):\\n    \"\"\" Work out the last day of the month \"\"\"\\n    last_days = [31, 30, 29, 28, 27]\\n    for i in last_days:\\n        try:\\n            end = datetime(year, month, i)\\n        except ValueError:\\n            continue\\n        else:\\n            return end.date()\\n    return None\\n</code>', '<code>&gt;&gt;&gt; \\n&gt;&gt;&gt; last_day_of_month(2008, 2)\\ndatetime.date(2008, 2, 29)\\n&gt;&gt;&gt; last_day_of_month(2009, 2)\\ndatetime.date(2009, 2, 28)\\n&gt;&gt;&gt; last_day_of_month(2008, 11)\\ndatetime.date(2008, 11, 30)\\n&gt;&gt;&gt; last_day_of_month(2008, 12)\\ndatetime.date(2008, 12, 31)\\n</code>', '<code>from datetime import timedelta\\n(any_day.replace(day=1) + timedelta(days=32)).replace(day=1) - timedelta(days=1)\\n</code>', '<code>&gt;&gt;&gt; import datetime\\n&gt;&gt;&gt; import calendar\\n&gt;&gt;&gt; date  = datetime.datetime.now()\\n\\n&gt;&gt;&gt; print date\\n2015-03-06 01:25:14.939574\\n\\n&gt;&gt;&gt; print date.replace(day = 1)\\n2015-03-01 01:25:14.939574\\n\\n&gt;&gt;&gt; print date.replace(day = calendar.monthrange(date.year, date.month)[1])\\n2015-03-31 01:25:14.939574\\n</code>', '<code>import datetime\\n\\nnow = datetime.datetime.now()\\nstart_month = datetime.datetime(now.year, now.month, 1)\\ndate_on_next_month = start_month + datetime.timedelta(35)\\nstart_next_month = datetime.datetime(date_on_next_month.year, date_on_next_month.month, 1)\\nlast_day_month = start_next_month - datetime.timedelta(1)\\n</code>', \"<code>import arrow\\narrow.utcnow().ceil('month').date()\\n</code>\", '<code>selected_date = date(some_year, some_month, some_day)\\n\\nif selected_date.month == 12: # December\\n     last_day_selected_month = date(selected_date.year, selected_date.month, 31)\\nelse:\\n     last_day_selected_month = date(selected_date.year, selected_date.month + 1, 1) - timedelta(days=1)\\n</code>', '<code>from datetime import date, timedelta\\nimport calendar\\nlast_day = date.today().replace(day=calendar.monthrange(date.today().year, date.today().month)[1])\\n</code>', '<code>calendar.monthrange(date.today().year, date.today().month)[1]\\n</code>', '<code>&gt;&gt;&gt; date.today()\\ndatetime.date(2017, 1, 3)\\n&gt;&gt;&gt; date.today().replace(day=31)\\ndatetime.date(2017, 1, 31)\\n</code>', '<code>date.replace(day=day)</code>', '<code>import datetime as dt\\nfrom dateutil.relativedelta import relativedelta\\n\\nthisDate = dt.datetime(2017, 11, 17)\\n\\nlast_day_of_the_month = dt.datetime(thisDate.year, (thisDate + relativedelta(months=1)).month, 1) - dt.timedelta(days=1)\\nprint last_day_of_the_month\\n</code>', '<code>datetime.datetime(2017, 11, 30, 0, 0)\\n</code>', '<code>import calendar</code>', \"<code>import datetime as dt\\nimport calendar\\nfrom dateutil.relativedelta import relativedelta\\n\\nsomeDates = [dt.datetime.today() - dt.timedelta(days=x) for x in range(0, 10000)]\\n\\nstart1 = dt.datetime.now()\\nfor thisDate in someDates:\\n    lastDay = dt.datetime(thisDate.year, (thisDate + relativedelta(months=1)).month, 1) - dt.timedelta(days=1)\\n\\nprint ('Time Spent= ', dt.datetime.now() - start1)\\n\\n\\nstart2 = dt.datetime.now()\\nfor thisDate in someDates:\\n    lastDay = dt.datetime(thisDate.year, \\n                          thisDate.month, \\n                          calendar.monthrange(thisDate.year, thisDate.month)[1])\\n\\nprint ('Time Spent= ', dt.datetime.now() - start2)\\n</code>\", '<code>Time Spent=  0:00:00.097814\\nTime Spent=  0:00:00.109791\\n</code>', '<code>import calendar</code>', '<code>calendar.monthcalendar</code>', '<code># Some random date.\\nsome_date = datetime.date(2012, 5, 23)\\n\\n# Get last weekday\\nlast_weekday = np.asarray(calendar.monthcalendar(some_date.year, some_date.month))[:,0:-2].ravel().max()\\n\\nprint last_weekday\\n31\\n</code>', '<code>[0:-2]</code>', '<code>numpy.ravel</code>', '<code>numpy.ndarray.max</code>', \"<code>def isMonthEnd(date):\\n    return date + pd.offsets.MonthEnd(0) == date\\n\\nisMonthEnd(datetime(1999, 12, 31))\\nTrue\\nisMonthEnd(pd.Timestamp('1999-12-31'))\\nTrue\\nisMonthEnd(pd.Timestamp(1965, 1, 10))\\nFalse\\n</code>\", '<code>pd.series.dt.is_month_end</code>', '<code>import datetime\\nimport calendar\\n\\ndate=datetime.datetime.now()\\nmonth_end_date=datetime.datetime(date.year,date.month,1) + datetime.timedelta(days=calendar.monthrange(date.year,date.month)[1] - 1)\\n</code>', '<code>import datetime\\n\\ndef end_date_of_a_month(date):\\n\\n\\n    start_date_of_this_month = date.replace(day=1)\\n\\n    month = start_date_of_this_month.month\\n    year = start_date_of_this_month.year\\n    if month == 12:\\n        month = 1\\n        year += 1\\n    else:\\n        month += 1\\n    next_month_start_date = start_date_of_this_month.replace(month=month, year=year)\\n\\n    this_month_end_date = next_month_start_date - datetime.timedelta(days=1)\\n    return this_month_end_date\\n</code>', '<code>end_date_of_a_month(datetime.datetime.now().date())\\n</code>', '<code>import calendar\\nfrom time import gmtime, strftime\\ncalendar.monthrange(int(strftime(\"%Y\", gmtime())), int(strftime(\"%m\", gmtime())))[1]\\n</code>', '<code>31\\n</code>', '<code>import calendar\\nfrom time import gmtime, strftime\\nlastDay = calendar.monthrange(int(strftime(\"%Y\", gmtime())), int(strftime(\"%m\", gmtime())))[1]\\ntoday = strftime(\"%d\", gmtime())\\nlastDay == today\\n</code>', '<code>False\\n</code>', '<code>def eomday(year, month):\\n    \"\"\"returns the number of days in a given month\"\"\"\\n    days_per_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\\n    d = days_per_month[month - 1]\\n    if month == 2 and (year % 4 == 0 and year % 100 != 0 or year % 400 == 0):\\n        d = 29\\n    return d\\n</code>', \"<code>import time\\nfrom datetime import datetime, date\\nfrom datetime import timedelta\\nfrom dateutil import relativedelta\\n\\n  start_date = fields.Date(\\n        string='Start Date', \\n        required=True,\\n        ) \\n\\n    end_date = fields.Date(\\n        string='End Date', \\n        required=True,\\n        )\\n\\n    _defaults = {\\n        'start_date': lambda *a: time.strftime('%Y-%m-01'),\\n        'end_date': lambda *a: str(datetime.now() + relativedelta.relativedelta(months=+1, day=1, days=-1))[:10],\\n    }\\n</code>\", '<code>import datetime   \\ndatetime.date(2012,2, 1).replace(day=1,month=datetime.date(2012,2,1).month+1)-timedelta(days=1)\\ndatetime.date(2012, 2, 29)\\n</code>', '<code>13</code>']",
         "title": "Get Last Day of the Month in Python",
         "_childDocuments_": [
            {
               "up_vote_count": 62,
               "answer_id": 42997,
               "last_activity_date": 1220574964,
               "path": "3.stack.answer",
               "body_markdown": "EDIT: See @[Blair Conrad&#39;s answer](#43663) for a cleaner solution\r\n\r\n---\r\n\r\n    &gt;&gt;&gt; import datetime\r\n    &gt;&gt;&gt; datetime.date (2000, 2, 1) - datetime.timedelta (days = 1)\r\n    datetime.date(2000, 1, 31)\r\n    &gt;&gt;&gt; \r\n\r\n",
               "tags": [],
               "creation_date": 1220491593,
               "last_edit_date": 1220574964,
               "is_accepted": false,
               "id": "42997",
               "down_vote_count": 5,
               "score": 57
            },
            {
               "up_vote_count": 37,
               "answer_id": 43088,
               "last_activity_date": 1220643895,
               "path": "3.stack.answer",
               "body_markdown": "EDIT: see [my other answer][1]. It has a better implementation than this one, which I leave here just in case someone&#39;s interested in seeing how one might &quot;roll your own&quot; calculator.\r\n\r\n@[John Millikin](#42997) gives a good answer, with the added complication of calculating the first day of the next month.\r\n\r\nThe following isn&#39;t particularly elegant, but to figure out the last day of the month that any given date lives in, you could try:\r\n\r\n    def last_day_of_month(date):\r\n        if date.month == 12:\r\n            return date.replace(day=31)\r\n        return date.replace(month=date.month+1, day=1) - datetime.timedelta(days=1)\r\n\r\n    &gt;&gt;&gt; last_day_of_month(datetime.date(2002, 1, 17))\r\n    datetime.date(2002, 1, 31)\r\n    &gt;&gt;&gt; last_day_of_month(datetime.date(2002, 12, 9))\r\n    datetime.date(2002, 12, 31)\r\n    &gt;&gt;&gt; last_day_of_month(datetime.date(2008, 2, 14))\r\n    datetime.date(2008, 2, 29)\r\n\r\n\r\n  [1]: /questions/42950/get-last-day-of-the-month-in-python#43663",
               "tags": [],
               "creation_date": 1220495150,
               "last_edit_date": 1220643895,
               "is_accepted": false,
               "id": "43088",
               "down_vote_count": 1,
               "score": 36
            },
            {
               "up_vote_count": 755,
               "answer_id": 43663,
               "last_activity_date": 1457278088,
               "path": "3.stack.answer",
               "body_markdown": "I didn&#39;t notice this earlier when I was looking at the [documentation for the calendar module][1], but a method called [monthrange][2] provides this information:\r\n\r\n&gt; &lt;b&gt;monthrange(year, month)&lt;/b&gt;&lt;br&gt;\r\n&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Returns weekday of first day of the month and number of days in month, for the specified year and month. \r\n&gt;\r\n\r\n    &gt;&gt;&gt; import calendar\r\n    &gt;&gt;&gt; calendar.monthrange(2002,1)\r\n    (1, 31)\r\n    &gt;&gt;&gt; calendar.monthrange(2008,2)\r\n    (4, 29)\r\n    &gt;&gt;&gt; calendar.monthrange(2100,2)\r\n    (0, 28)\r\n\r\nso:\r\n\r\n    calendar.monthrange(year, month)[1]\r\n\r\nseems like the simplest way to go.\r\n\r\nJust to be clear, `monthrange` supports leap years as well:\r\n\r\n    &gt;&gt;&gt; from calendar import monthrange\r\n    &gt;&gt;&gt; monthrange(2012, 2)\r\n    (2, 29)\r\n\r\n[My previous answer][3] still works, but is clearly suboptimal.\r\n\r\n\r\n  [1]: https://docs.python.org/2/library/calendar.html\r\n  [2]: http://docs.python.org/library/calendar.html#calendar.monthrange\r\n  [3]: https://stackoverflow.com/questions/42950/get-last-day-of-the-month-in-python#43088",
               "tags": [],
               "creation_date": 1220532252,
               "last_edit_date": 1495540989,
               "is_accepted": true,
               "id": "43663",
               "down_vote_count": 0,
               "score": 755
            },
            {
               "up_vote_count": 20,
               "answer_id": 356535,
               "last_activity_date": 1464811488,
               "path": "3.stack.answer",
               "body_markdown": "Another solution would be to do something like this: \r\n\r\n    from datetime import datetime\r\n\r\n    def last_day_of_month(year, month):\r\n        &quot;&quot;&quot; Work out the last day of the month &quot;&quot;&quot;\r\n        last_days = [31, 30, 29, 28, 27]\r\n        for i in last_days:\r\n            try:\r\n                end = datetime(year, month, i)\r\n            except ValueError:\r\n                continue\r\n            else:\r\n                return end.date()\r\n        return None\r\n\r\nAnd use the function like this:\r\n\r\n    &gt;&gt;&gt; \r\n    &gt;&gt;&gt; last_day_of_month(2008, 2)\r\n    datetime.date(2008, 2, 29)\r\n    &gt;&gt;&gt; last_day_of_month(2009, 2)\r\n    datetime.date(2009, 2, 28)\r\n    &gt;&gt;&gt; last_day_of_month(2008, 11)\r\n    datetime.date(2008, 11, 30)\r\n    &gt;&gt;&gt; last_day_of_month(2008, 12)\r\n    datetime.date(2008, 12, 31)\r\n\r\n",
               "tags": [],
               "creation_date": 1228924077,
               "last_edit_date": 1464811488,
               "is_accepted": false,
               "id": "356535",
               "down_vote_count": 9,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 12064327,
               "is_accepted": false,
               "last_activity_date": 1345590597,
               "body_markdown": "i have a simple solution:\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    import datetime   \r\n    datetime.date(2012,2, 1).replace(day=1,month=datetime.date(2012,2,1).month+1)-timedelta(days=1)\r\n    datetime.date(2012, 2, 29)\r\n\r\n\r\n\r\n",
               "id": "12064327",
               "tags": [],
               "down_vote_count": 6,
               "creation_date": 1345590597,
               "score": -6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 13386470,
               "is_accepted": false,
               "last_activity_date": 1352923586,
               "body_markdown": "This does not address the main question, but one nice trick to get the last *weekday* in a month is to use `calendar.monthcalendar`, which returns a matrix of dates, organized with Monday as the first column through Sunday as the last.\r\n\r\n    # Some random date.\r\n    some_date = datetime.date(2012, 5, 23)\r\n    \r\n    # Get last weekday\r\n    last_weekday = np.asarray(calendar.monthcalendar(some_date.year, some_date.month))[:,0:-2].ravel().max()\r\n\r\n    print last_weekday\r\n    31\r\n\r\nThe whole `[0:-2]` thing is to shave off the weekend columns and throw them out. Dates that fall outside of the month are indicated by 0, so the max effectively ignores them.\r\n\r\nThe use of `numpy.ravel` is not strictly necessary, but I hate relying on the *mere convention* that `numpy.ndarray.max` will flatten the array if not told which axis to calculate over.",
               "id": "13386470",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1352923586,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 80,
               "answer_id": 13565185,
               "is_accepted": false,
               "last_activity_date": 1353934120,
               "body_markdown": "If you don&#39;t want to import the `calendar` module, a simple two-step function can also be:\r\n\r\n    import datetime\r\n\r\n    def last_day_of_month(any_day):\r\n        next_month = any_day.replace(day=28) + datetime.timedelta(days=4)  # this will never fail\r\n        return next_month - datetime.timedelta(days=next_month.day)\r\n\r\nOutputs:\r\n\r\n    &gt;&gt;&gt; for month in range(1, 13):\r\n    ...     print last_day_of_month(datetime.date(2012, month, 1))\r\n    ...\r\n    2012-01-31\r\n    2012-02-29\r\n    2012-03-31\r\n    2012-04-30\r\n    2012-05-31\r\n    2012-06-30\r\n    2012-07-31\r\n    2012-08-31\r\n    2012-09-30\r\n    2012-10-31\r\n    2012-11-30\r\n    2012-12-31",
               "id": "13565185",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1353934120,
               "score": 80
            },
            {
               "up_vote_count": 24,
               "answer_id": 14994380,
               "last_activity_date": 1440420379,
               "path": "3.stack.answer",
               "body_markdown": "This is actually pretty easy with `dateutil.relativedelta` (package python-datetutil for pip). `day=31` will always always return the last day of the month.\r\n\r\nExample:\r\n\r\n    from datetime import datetime\r\n    from dateutil.relativedelta import relativedelta\r\n\r\n    date_in_feb = datetime.datetime(2013, 2, 21)\r\n    print datetime.datetime(2013, 2, 21) + relativedelta(day=31)  # End-of-month\r\n    &gt;&gt;&gt; datetime.datetime(2013, 2, 28, 0, 0)\r\n",
               "tags": [],
               "creation_date": 1361419749,
               "last_edit_date": 1440420379,
               "is_accepted": false,
               "id": "14994380",
               "down_vote_count": 1,
               "score": 23
            },
            {
               "up_vote_count": 6,
               "answer_id": 17135571,
               "last_activity_date": 1371401852,
               "path": "3.stack.answer",
               "body_markdown": "    import datetime\r\n\r\n    now = datetime.datetime.now()\r\n    start_month = datetime.datetime(now.year, now.month, 1)\r\n    date_on_next_month = start_month + datetime.timedelta(35)\r\n    start_next_month = datetime.datetime(date_on_next_month.year, date_on_next_month.month, 1)\r\n    last_day_month = start_next_month - datetime.timedelta(1)",
               "tags": [],
               "creation_date": 1371401502,
               "last_edit_date": 1371401852,
               "is_accepted": false,
               "id": "17135571",
               "down_vote_count": 1,
               "score": 5
            },
            {
               "up_vote_count": 1,
               "answer_id": 23383345,
               "last_activity_date": 1504513605,
               "path": "3.stack.answer",
               "body_markdown": "If you want to make your own small function, this is a good starting point:\r\n\r\n    def eomday(year, month):\r\n        &quot;&quot;&quot;returns the number of days in a given month&quot;&quot;&quot;\r\n        days_per_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\r\n        d = days_per_month[month - 1]\r\n        if month == 2 and (year % 4 == 0 and year % 100 != 0 or year % 400 == 0):\r\n            d = 29\r\n        return d\r\n\r\nFor this you have to know the rules for the leap years:\r\n\r\n - every fourth year\r\n - with the exception of every 100 year\r\n - but again every 400 years",
               "tags": [],
               "creation_date": 1398846837,
               "last_edit_date": 1504513605,
               "is_accepted": false,
               "id": "23383345",
               "down_vote_count": 1,
               "score": 0
            },
            {
               "up_vote_count": 10,
               "answer_id": 23447523,
               "last_activity_date": 1409748247,
               "path": "3.stack.answer",
               "body_markdown": "    from datetime import timedelta\r\n    (any_day.replace(day=1) + timedelta(days=32)).replace(day=1) - timedelta(days=1)",
               "tags": [],
               "creation_date": 1399137418,
               "last_edit_date": 1409748247,
               "is_accepted": false,
               "id": "23447523",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 3,
               "answer_id": 24929705,
               "last_activity_date": 1406734148,
               "path": "3.stack.answer",
               "body_markdown": "For me it&#39;s the simplest way:\r\n\r\n    selected_date = date(some_year, some_month, some_day)\r\n\r\n    if selected_date.month == 12: # December\r\n         last_day_selected_month = date(selected_date.year, selected_date.month, 31)\r\n    else:\r\n         last_day_selected_month = date(selected_date.year, selected_date.month + 1, 1) - timedelta(days=1)",
               "tags": [],
               "creation_date": 1406193385,
               "last_edit_date": 1406734148,
               "is_accepted": false,
               "id": "24929705",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 11,
               "answer_id": 27667421,
               "last_activity_date": 1464810291,
               "path": "3.stack.answer",
               "body_markdown": "Using `relativedelta` you would get last date of month like this:\r\n\r\n    from dateutil.relativedelta import relativedelta\r\n    last_date_of_month = datetime(mydate.year,mydate.month,1)+relativedelta(months=1,days=-1)\r\n\r\n\r\nThe idea is to get the fist day of month and use `relativedelta` to go 1 month ahead and 1 day back so you would get the last day of the month you wanted.",
               "tags": [],
               "creation_date": 1419684891,
               "last_edit_date": 1464810291,
               "is_accepted": false,
               "id": "27667421",
               "down_vote_count": 0,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 28886669,
               "is_accepted": false,
               "last_activity_date": 1425585738,
               "body_markdown": "    &gt;&gt;&gt; import datetime\r\n    &gt;&gt;&gt; import calendar\r\n    &gt;&gt;&gt; date  = datetime.datetime.now()\r\n    \r\n    &gt;&gt;&gt; print date\r\n    2015-03-06 01:25:14.939574\r\n    \r\n    &gt;&gt;&gt; print date.replace(day = 1)\r\n    2015-03-01 01:25:14.939574\r\n    \r\n    &gt;&gt;&gt; print date.replace(day = calendar.monthrange(date.year, date.month)[1])\r\n    2015-03-31 01:25:14.939574\r\n\r\n\r\n\r\n\r\n",
               "id": "28886669",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1425585738,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 31618852,
               "is_accepted": false,
               "last_activity_date": 1437767993,
               "body_markdown": "Use pandas!\r\n\r\n    def isMonthEnd(date):\r\n        return date + pd.offsets.MonthEnd(0) == date\r\n\r\n    isMonthEnd(datetime(1999, 12, 31))\r\n    True\r\n    isMonthEnd(pd.Timestamp(&#39;1999-12-31&#39;))\r\n    True\r\n    isMonthEnd(pd.Timestamp(1965, 1, 10))\r\n    False",
               "id": "31618852",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1437767993,
               "score": 1
            },
            {
               "up_vote_count": 0,
               "answer_id": 37246666,
               "last_activity_date": 1463374316,
               "path": "3.stack.answer",
               "body_markdown": "    import calendar\r\n    from time import gmtime, strftime\r\n    calendar.monthrange(int(strftime(&quot;%Y&quot;, gmtime())), int(strftime(&quot;%m&quot;, gmtime())))[1]\r\nOutput:&lt;br&gt;\r\n\r\n    31\r\n&lt;br&gt;&lt;br&gt;\r\nThis will print the last day of whatever the current month is. In this example it was 15th May, 2016.  So your output may be different, however the output will be as many days that the current month is.  Great if you want to check the last day of the month by running a daily cron job.\r\n&lt;p&gt;\r\nSo: &lt;br&gt;\r\n    \r\n    import calendar\r\n    from time import gmtime, strftime\r\n    lastDay = calendar.monthrange(int(strftime(&quot;%Y&quot;, gmtime())), int(strftime(&quot;%m&quot;, gmtime())))[1]\r\n    today = strftime(&quot;%d&quot;, gmtime())\r\n    lastDay == today\r\nOutput:&lt;br&gt;\r\n\r\n    False\r\nUnless it IS the last day of the month.\r\n\r\n",
               "tags": [],
               "creation_date": 1463372538,
               "last_edit_date": 1463374316,
               "is_accepted": false,
               "id": "37246666",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 39141916,
               "is_accepted": false,
               "last_activity_date": 1472118485,
               "body_markdown": "I prefer this way\r\n\r\n    import datetime\r\n    import calendar\r\n    \t\r\n    date=datetime.datetime.now()\r\n    month_end_date=datetime.datetime(date.year,date.month,1) + datetime.timedelta(days=calendar.monthrange(date.year,date.month)[1] - 1)",
               "id": "39141916",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1472118485,
               "score": 1
            },
            {
               "up_vote_count": 2,
               "answer_id": 39223365,
               "last_activity_date": 1483437294,
               "path": "3.stack.answer",
               "body_markdown": "To get the last date of the month we do something like this:\r\n\r\n    from datetime import date, timedelta\r\n    import calendar\r\n    last_day = date.today().replace(day=calendar.monthrange(date.today().year, date.today().month)[1])\r\n\r\nNow to explain what we are doing here we will break it into two parts:\r\n\r\nfirst is getting the number of days of the current month for which we use [monthrange][1] which Blair Conrad has already mentioned his solution:\r\n\r\n    calendar.monthrange(date.today().year, date.today().month)[1]\r\nsecond is getting the last date itself which we do with the help of [replace][2] e.g\r\n\r\n    &gt;&gt;&gt; date.today()\r\n    datetime.date(2017, 1, 3)\r\n    &gt;&gt;&gt; date.today().replace(day=31)\r\n    datetime.date(2017, 1, 31)\r\n\r\nand when we combine them as mentioned on the top we get a dynamic solution.\r\n  [1]: https://docs.python.org/3/library/calendar.html#calendar.monthrange\r\n  [2]: https://docs.python.org/3/library/datetime.html#datetime.datetime.replace\r\n",
               "tags": [],
               "creation_date": 1472548469,
               "last_edit_date": 1483437294,
               "is_accepted": false,
               "id": "39223365",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 39288092,
               "is_accepted": false,
               "last_activity_date": 1472806031,
               "body_markdown": "You can calculate the end date yourself. the simple logic is to subtract a day from the start_date of next month. :) \r\n\r\nSo write a custom method,\r\n\r\n    import datetime\r\n    \r\n    def end_date_of_a_month(date):\r\n\r\n\r\n        start_date_of_this_month = date.replace(day=1)\r\n\r\n        month = start_date_of_this_month.month\r\n        year = start_date_of_this_month.year\r\n        if month == 12:\r\n            month = 1\r\n            year += 1\r\n        else:\r\n            month += 1\r\n        next_month_start_date = start_date_of_this_month.replace(month=month, year=year)\r\n\r\n        this_month_end_date = next_month_start_date - datetime.timedelta(days=1)\r\n        return this_month_end_date\r\n\r\nCalling, \r\n\r\n    end_date_of_a_month(datetime.datetime.now().date())\r\n\r\nIt will return the end date of this month. Pass any date to this function. returns you the end date of that month. \r\n\r\n\r\n",
               "id": "39288092",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1472806031,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 40827688,
               "is_accepted": false,
               "last_activity_date": 1480240866,
               "body_markdown": "if you are willing to use an external library, check out http://crsmithdev.com/arrow/\r\n\r\nU can then get the last day of the month with:\r\n\r\n    import arrow\r\n    arrow.utcnow().ceil(&#39;month&#39;).date()\r\nThis returns a date object which you can then do your manipulation.",
               "id": "40827688",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1480240866,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 46447774,
               "is_accepted": false,
               "last_activity_date": 1506514983,
               "body_markdown": "I hope,It&#39;s usefull for very much..Try it on this way..we must need import some package\r\n\r\n    import time\r\n    from datetime import datetime, date\r\n    from datetime import timedelta\r\n    from dateutil import relativedelta\r\n    \r\n      start_date = fields.Date(\r\n            string=&#39;Start Date&#39;, \r\n            required=True,\r\n            ) \r\n            \r\n        end_date = fields.Date(\r\n            string=&#39;End Date&#39;, \r\n            required=True,\r\n            )\r\n        \r\n        _defaults = {\r\n            &#39;start_date&#39;: lambda *a: time.strftime(&#39;%Y-%m-01&#39;),\r\n            &#39;end_date&#39;: lambda *a: str(datetime.now() + relativedelta.relativedelta(months=+1, day=1, days=-1))[:10],\r\n        }",
               "id": "46447774",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1506514983,
               "score": -2
            },
            {
               "up_vote_count": 2,
               "answer_id": 47358096,
               "last_activity_date": 1511897993,
               "path": "3.stack.answer",
               "body_markdown": "The easiest way (without having to import calendar), is to get the first day of the next month, and then subtract a day from it.\r\n\r\n    import datetime as dt\r\n    from dateutil.relativedelta import relativedelta\r\n\r\n    thisDate = dt.datetime(2017, 11, 17)\r\n    \r\n    last_day_of_the_month = dt.datetime(thisDate.year, (thisDate + relativedelta(months=1)).month, 1) - dt.timedelta(days=1)\r\n    print last_day_of_the_month\r\n\r\nOutput:\r\n\r\n    datetime.datetime(2017, 11, 30, 0, 0)\r\n\r\n-----------------------------------------------------\r\n\r\n**PS:** This code runs faster as compared to the `import calendar`approach; see below:\r\n\r\n    import datetime as dt\r\n    import calendar\r\n    from dateutil.relativedelta import relativedelta\r\n    \r\n    someDates = [dt.datetime.today() - dt.timedelta(days=x) for x in range(0, 10000)]\r\n    \r\n    start1 = dt.datetime.now()\r\n    for thisDate in someDates:\r\n        lastDay = dt.datetime(thisDate.year, (thisDate + relativedelta(months=1)).month, 1) - dt.timedelta(days=1)\r\n    \r\n    print (&#39;Time Spent= &#39;, dt.datetime.now() - start1)\r\n    \r\n    \r\n    start2 = dt.datetime.now()\r\n    for thisDate in someDates:\r\n        lastDay = dt.datetime(thisDate.year, \r\n                              thisDate.month, \r\n                              calendar.monthrange(thisDate.year, thisDate.month)[1])\r\n    \r\n    print (&#39;Time Spent= &#39;, dt.datetime.now() - start2)\r\n\r\nOUTPUT:\r\n\r\n    Time Spent=  0:00:00.097814\r\n    Time Spent=  0:00:00.109791\r\n\r\nThis code assumes that you want the date of the last day of the month (i.e., not just the DD part, but the entire YYYYMMDD date)\r\n\r\n",
               "tags": [],
               "creation_date": 1510947489,
               "last_edit_date": 1511897993,
               "is_accepted": false,
               "id": "47358096",
               "down_vote_count": 0,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/42950/get-last-day-of-the-month-in-python",
         "id": "858127-2276"
      },
      {
         "up_vote_count": "193",
         "path": "2.stack",
         "body_markdown": "Is there a way to widen the display of output in either interactive or script-execution mode?\r\n\r\nSpecifically, I am using the describe() function on a Pandas `dataframe`.  When the `dataframe` is 5 columns (labels) wide, I get the descriptive statistics that I want.  However, if the `dataframe` has any more columns, the statistics are suppressed and something like this is returned:\r\n\r\n    &gt;&gt; Index: 8 entries, count to max  \r\n    &gt;&gt; Data columns:  \r\n    &gt;&gt; x1          8  non-null values  \r\n    &gt;&gt; x2          8  non-null values  \r\n    &gt;&gt; x3          8  non-null values  \r\n    &gt;&gt; x4          8  non-null values  \r\n    &gt;&gt; x5          8  non-null values  \r\n    &gt;&gt; x6          8  non-null values  \r\n    &gt;&gt; x7          8  non-null values  \r\n\r\nThe &quot;8&quot; value is given whether there are 6 or 7 columns.  What does the &quot;8&quot; refer to?\r\n\r\nI have already tried dragging the IDLE window larger, as well as increasing the &quot;Configure IDLE&quot; width options, to no avail.\r\n\r\nMy purpose in using Pandas and describe() is to avoid using a second program like **STATA** to do basic data manipulation and investigation.\r\n\r\nThanks.\r\n\r\nPython/IDLE 2.7.3  \r\nPandas 0.8.1  \r\nNotepad++ 6.1.4 (UNICODE)  \r\nWindows Vista SP2  ",
         "view_count": "130459",
         "answer_count": "8",
         "tags": "['python', 'pandas', 'options', 'display', 'column-width']",
         "creation_date": "1343547891",
         "last_edit_date": "1511274558",
         "code_snippet": "['<code>dataframe</code>', '<code>dataframe</code>', '<code>dataframe</code>', '<code>&gt;&gt; Index: 8 entries, count to max  \\n&gt;&gt; Data columns:  \\n&gt;&gt; x1          8  non-null values  \\n&gt;&gt; x2          8  non-null values  \\n&gt;&gt; x3          8  non-null values  \\n&gt;&gt; x4          8  non-null values  \\n&gt;&gt; x5          8  non-null values  \\n&gt;&gt; x6          8  non-null values  \\n&gt;&gt; x7          8  non-null values  \\n</code>', '<code>print df.describe().to_string()</code>', '<code>pandas.util.terminal.get_terminal_size()</code>', '<code>(width, height)</code>', '<code>pandas.set_printoptions(max_rows=200, max_columns=10)</code>', '<code>pandas.set_printoptions(...)</code>', '<code>pandas.set_option</code>', \"<code>import pandas as pd\\npd.set_option('display.height', 1000)\\npd.set_option('display.max_rows', 500)\\npd.set_option('display.max_columns', 500)\\npd.set_option('display.width', 1000)\\n</code>\", '<code>pandas.set_printoptions(max_rows=200, max_columns=10)</code>', \"<code>pandas.set_option('max_rows',200)</code>\", \"<code>pandas.set_option('max_columns',10)</code>\", '<code>display.height</code>', \"<code>pd.set_option('display.expand_frame_repr', False)\\n</code>\", \"<code>with pd.option_context('display.max_rows', -1, 'display.max_columns', 5):\\n    print df\\n</code>\", '<code>with</code>', '<code>None</code>', \"<code>with pd.option_context('display.max_rows', None, 'display.max_columns', None):     print(energy)</code>\", '<code>-1</code>', '<code>500</code>', '<code>set_printoptions</code>', \"<code>In [3]: df.describe()\\nOut[3]: \\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nIndex: 8 entries, count to max\\nData columns:\\nx1    8  non-null values\\nx2    8  non-null values\\nx3    8  non-null values\\nx4    8  non-null values\\nx5    8  non-null values\\nx6    8  non-null values\\nx7    8  non-null values\\ndtypes: float64(7)\\n\\nIn [4]: pd.set_printoptions(precision=2)\\n\\nIn [5]: df.describe()\\nOut[5]: \\n            x1       x2       x3       x4       x5       x6       x7\\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\\nmean   69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\\nstd       17.1     17.1     17.1     17.1     17.1     17.1     17.1\\nmin    69000.0  69001.0  69002.0  69003.0  69004.0  69005.0  69006.0\\n25%    69012.2  69013.2  69014.2  69015.2  69016.2  69017.2  69018.2\\n50%    69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\\n75%    69036.8  69037.8  69038.8  69039.8  69040.8  69041.8  69042.8\\nmax    69049.0  69050.0  69051.0  69052.0  69053.0  69054.0  69055.0\\n</code>\", '<code>to_string</code>', '<code>set_printoptions</code>', '<code>to_string</code>', '<code>In [3]: df.describe()\\nOut[3]: \\n                 x1            x2            x3            x4            x5  \\\\\\ncount      8.000000      8.000000      8.000000      8.000000      8.000000   \\nmean   59832.361578  27356.711336  49317.281222  51214.837838  51254.839690   \\nstd    22600.723536  26867.192716  28071.737509  21012.422793  33831.515761   \\nmin    31906.695474   1648.359160     56.378115  16278.322271     43.745574   \\n25%    45264.625201  12799.540572  41429.628749  40374.273582  29789.643875   \\n50%    56340.214856  18666.456293  51995.661512  54894.562656  47667.684422   \\n75%    75587.003417  31375.610322  61069.190523  67811.893435  76014.884048   \\nmax    98136.474782  84544.484627  91743.983895  75154.587156  99012.695717   \\n\\n                 x6            x7  \\ncount      8.000000      8.000000  \\nmean   41863.000717  33950.235126  \\nstd    38709.468281  29075.745673  \\nmin     3590.990740   1833.464154  \\n25%    15145.759625   6879.523949  \\n50%    22139.243042  33706.029946  \\n75%    72038.983496  51449.893980  \\nmax    98601.190488  83309.051963  \\n</code>', \"<code>In [4]: pd.set_option('display.precision', 2)\\n\\nIn [5]: df.describe()\\nOut[5]: \\n            x1       x2       x3       x4       x5       x6       x7\\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\\nmean   59832.4  27356.7  49317.3  51214.8  51254.8  41863.0  33950.2\\nstd    22600.7  26867.2  28071.7  21012.4  33831.5  38709.5  29075.7\\nmin    31906.7   1648.4     56.4  16278.3     43.7   3591.0   1833.5\\n25%    45264.6  12799.5  41429.6  40374.3  29789.6  15145.8   6879.5\\n50%    56340.2  18666.5  51995.7  54894.6  47667.7  22139.2  33706.0\\n75%    75587.0  31375.6  61069.2  67811.9  76014.9  72039.0  51449.9\\nmax    98136.5  84544.5  91744.0  75154.6  99012.7  98601.2  83309.1\\n</code>\", '<code>display.precision</code>', \"<code>pd.set_option('display.width', pd.util.terminal.get_terminal_size()[0])\\n</code>\", \"<code>pd.set_option('display.width', None)</code>\", \"<code>import pandas as pd; pd.set_option('display.width', None); pd.DataFrame(1.00001, index=range(10), columns=range(100))</code>\", '<code>print df.describe().to_string()</code>', '<code>to_string()</code>', '<code>describe</code>', '<code>describe</code>', \"<code>pd.set_option('max_colwidth', 800)\\n</code>\", \"<code>pd.set_option('display.large_repr', 'truncate')\\npd.set_option('display.max_columns', 0)\\n</code>\"]",
         "title": "Python pandas, how to widen output display to see more columns?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 14,
               "answer_id": 11707706,
               "is_accepted": false,
               "last_activity_date": 1343549015,
               "body_markdown": "You can use `print df.describe().to_string()` to force it to show the whole table.  (You can use `to_string()` like this for any DataFrame.  The result of `describe` is just a DataFrame itself.)\r\n\r\nThe 8 is the number of rows in the DataFrame holding the &quot;description&quot; (because `describe` computes 8 statistics, min, max, mean, etc.).",
               "id": "11707706",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1343549015,
               "score": 14
            },
            {
               "up_vote_count": 20,
               "answer_id": 11708664,
               "last_activity_date": 1357812419,
               "path": "3.stack.answer",
               "body_markdown": "You can adjust pandas print options with `set_printoptions`.\r\n\r\n    In [3]: df.describe()\r\n    Out[3]: \r\n    &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\r\n    Index: 8 entries, count to max\r\n    Data columns:\r\n    x1    8  non-null values\r\n    x2    8  non-null values\r\n    x3    8  non-null values\r\n    x4    8  non-null values\r\n    x5    8  non-null values\r\n    x6    8  non-null values\r\n    x7    8  non-null values\r\n    dtypes: float64(7)\r\n\r\n    In [4]: pd.set_printoptions(precision=2)\r\n\r\n    In [5]: df.describe()\r\n    Out[5]: \r\n                x1       x2       x3       x4       x5       x6       x7\r\n    count      8.0      8.0      8.0      8.0      8.0      8.0      8.0\r\n    mean   69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\r\n    std       17.1     17.1     17.1     17.1     17.1     17.1     17.1\r\n    min    69000.0  69001.0  69002.0  69003.0  69004.0  69005.0  69006.0\r\n    25%    69012.2  69013.2  69014.2  69015.2  69016.2  69017.2  69018.2\r\n    50%    69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\r\n    75%    69036.8  69037.8  69038.8  69039.8  69040.8  69041.8  69042.8\r\n    max    69049.0  69050.0  69051.0  69052.0  69053.0  69054.0  69055.0\r\n\r\nHowever this will not work in all cases as pandas detects your console width and it will only use `to_string` if the output fits in the console (see the docstring of `set_printoptions`). \r\nIn this case you can explicitly call `to_string` as answered by [BrenBarn](https://stackoverflow.com/a/11707706/1301710).\r\n\r\n**Update**\r\n\r\nWith version 0.10 the way wide dataframes are printed [changed](http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#wide-dataframe-print):\r\n\r\n    In [3]: df.describe()\r\n    Out[3]: \r\n                     x1            x2            x3            x4            x5  \\\r\n    count      8.000000      8.000000      8.000000      8.000000      8.000000   \r\n    mean   59832.361578  27356.711336  49317.281222  51214.837838  51254.839690   \r\n    std    22600.723536  26867.192716  28071.737509  21012.422793  33831.515761   \r\n    min    31906.695474   1648.359160     56.378115  16278.322271     43.745574   \r\n    25%    45264.625201  12799.540572  41429.628749  40374.273582  29789.643875   \r\n    50%    56340.214856  18666.456293  51995.661512  54894.562656  47667.684422   \r\n    75%    75587.003417  31375.610322  61069.190523  67811.893435  76014.884048   \r\n    max    98136.474782  84544.484627  91743.983895  75154.587156  99012.695717   \r\n    \r\n                     x6            x7  \r\n    count      8.000000      8.000000  \r\n    mean   41863.000717  33950.235126  \r\n    std    38709.468281  29075.745673  \r\n    min     3590.990740   1833.464154  \r\n    25%    15145.759625   6879.523949  \r\n    50%    22139.243042  33706.029946  \r\n    75%    72038.983496  51449.893980  \r\n    max    98601.190488  83309.051963  \r\n\r\nFurther more the API for setting pandas options changed:\r\n\r\n    In [4]: pd.set_option(&#39;display.precision&#39;, 2)\r\n    \r\n    In [5]: df.describe()\r\n    Out[5]: \r\n                x1       x2       x3       x4       x5       x6       x7\r\n    count      8.0      8.0      8.0      8.0      8.0      8.0      8.0\r\n    mean   59832.4  27356.7  49317.3  51214.8  51254.8  41863.0  33950.2\r\n    std    22600.7  26867.2  28071.7  21012.4  33831.5  38709.5  29075.7\r\n    min    31906.7   1648.4     56.4  16278.3     43.7   3591.0   1833.5\r\n    25%    45264.6  12799.5  41429.6  40374.3  29789.6  15145.8   6879.5\r\n    50%    56340.2  18666.5  51995.7  54894.6  47667.7  22139.2  33706.0\r\n    75%    75587.0  31375.6  61069.2  67811.9  76014.9  72039.0  51449.9\r\n    max    98136.5  84544.5  91744.0  75154.6  99012.7  98601.2  83309.1\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1343559361,
               "last_edit_date": 1495540519,
               "is_accepted": false,
               "id": "11708664",
               "down_vote_count": 0,
               "score": 20
            },
            {
               "up_vote_count": 288,
               "answer_id": 11711637,
               "last_activity_date": 1436901881,
               "path": "3.stack.answer",
               "body_markdown": "As @bmu [mentioned](https://stackoverflow.com/a/11708664/623735), pandas auto detects (by default) the size of the display area, a summary view will be used when an object repr does not fit on the display. You mentioned resizing the IDLE window, to no effect. If you do `print df.describe().to_string()` does it fit on the IDLE window?\r\n\r\nThe terminal size is determined by `pandas.util.terminal.get_terminal_size()`, this returns a tuple containing the `(width, height)` of the display. Does the output match the size of your IDLE window? There might be an issue (there was one before when running a terminal in emacs).\r\n\r\nNote that it is possible to bypass the autodetect, `pandas.set_printoptions(max_rows=200, max_columns=10)` will never switch to summary view if number of rows, columns does not exceed the given limits.\r\n\r\n----\r\n**Update: Pandas 0.11.0 onwards**\r\n\r\n`pandas.set_printoptions(...)` is depracted. Instead, use `pandas.set_option`. Like:\r\n\r\n    import pandas as pd\r\n    pd.set_option(&#39;display.height&#39;, 1000)\r\n    pd.set_option(&#39;display.max_rows&#39;, 500)\r\n    pd.set_option(&#39;display.max_columns&#39;, 500)\r\n    pd.set_option(&#39;display.width&#39;, 1000)\r\n\r\nHere is the help:\r\n\r\n&lt;pre&gt;\r\nset_option(pat,value) - Sets the value of the specified option\r\n\r\nAvailable options:\r\ndisplay.[chop_threshold, colheader_justify, column_space, date_dayfirst,\r\n         date_yearfirst, encoding, expand_frame_repr, float_format, height,\r\n         line_width, max_columns, max_colwidth, max_info_columns, max_info_rows,\r\n         max_rows, max_seq_items, mpl_style, multi_sparse, notebook_repr_html,\r\n         pprint_nest_depth, precision, width]\r\nmode.[sim_interactive, use_inf_as_null]\r\n\r\nParameters\r\n----------\r\npat - str/regexp which should match a single option.\r\n\r\nNote: partial matches are supported for convenience, but unless you use the\r\nfull option name (e.g. x.y.z.option_name), your code may break in future\r\nversions if new options with similar names are introduced.\r\n\r\nvalue - new value of option.\r\n\r\nReturns\r\n-------\r\nNone\r\n\r\nRaises\r\n------\r\nKeyError if no such option exists\r\n\r\ndisplay.chop_threshold: [default: None] [currently: None]\r\n: float or None\r\n        if set to a float value, all float values smaller then the given threshold\r\n        will be displayed as exactly 0 by repr and friends.\r\ndisplay.colheader_justify: [default: right] [currently: right]\r\n: &#39;left&#39;/&#39;right&#39;\r\n        Controls the justification of column headers. used by DataFrameFormatter.\r\ndisplay.column_space: [default: 12] [currently: 12]No description available.\r\n\r\ndisplay.date_dayfirst: [default: False] [currently: False]\r\n: boolean\r\n        When True, prints and parses dates with the day first, eg 20/01/2005\r\ndisplay.date_yearfirst: [default: False] [currently: False]\r\n: boolean\r\n        When True, prints and parses dates with the year first, eg 2005/01/20\r\ndisplay.encoding: [default: UTF-8] [currently: UTF-8]\r\n: str/unicode\r\n        Defaults to the detected encoding of the console.\r\n        Specifies the encoding to be used for strings returned by to_string,\r\n        these are generally strings meant to be displayed on the console.\r\ndisplay.expand_frame_repr: [default: True] [currently: True]\r\n: boolean\r\n        Whether to print out the full DataFrame repr for wide DataFrames\r\n        across multiple lines, `max_columns` is still respected, but the output will\r\n        wrap-around across multiple &quot;pages&quot; if it&#39;s width exceeds `display.width`.\r\ndisplay.float_format: [default: None] [currently: None]\r\n: callable\r\n        The callable should accept a floating point number and return\r\n        a string with the desired format of the number. This is used\r\n        in some places like SeriesFormatter.\r\n        See core.format.EngFormatter for an example.\r\ndisplay.height: [default: 60] [currently: 1000]\r\n: int\r\n        Deprecated.\r\n        (Deprecated, use `display.height` instead.)\r\n\r\ndisplay.line_width: [default: 80] [currently: 1000]\r\n: int\r\n        Deprecated.\r\n        (Deprecated, use `display.width` instead.)\r\n\r\ndisplay.max_columns: [default: 20] [currently: 500]\r\n: int\r\n        max_rows and max_columns are used in __repr__() methods to decide if\r\n        to_string() or info() is used to render an object to a string.  In case\r\n        python/IPython is running in a terminal this can be set to 0 and pandas\r\n        will correctly auto-detect the width the terminal and swap to a smaller\r\n        format in case all columns would not fit vertically. The IPython notebook,\r\n        IPython qtconsole, or IDLE do not run in a terminal and hence it is not\r\n        possible to do correct auto-detection.\r\n        &#39;None&#39; value means unlimited.\r\ndisplay.max_colwidth: [default: 50] [currently: 50]\r\n: int\r\n        The maximum width in characters of a column in the repr of\r\n        a pandas data structure. When the column overflows, a &quot;...&quot;\r\n        placeholder is embedded in the output.\r\ndisplay.max_info_columns: [default: 100] [currently: 100]\r\n: int\r\n        max_info_columns is used in DataFrame.info method to decide if\r\n        per column information will be printed.\r\ndisplay.max_info_rows: [default: 1690785] [currently: 1690785]\r\n: int or None\r\n        max_info_rows is the maximum number of rows for which a frame will\r\n        perform a null check on its columns when repr&#39;ing To a console.\r\n        The default is 1,000,000 rows. So, if a DataFrame has more\r\n        1,000,000 rows there will be no null check performed on the\r\n        columns and thus the representation will take much less time to\r\n        display in an interactive session. A value of None means always\r\n        perform a null check when repr&#39;ing.\r\ndisplay.max_rows: [default: 60] [currently: 500]\r\n: int\r\n        This sets the maximum number of rows pandas should output when printing\r\n        out various output. For example, this value determines whether the repr()\r\n        for a dataframe prints out fully or just a summary repr.\r\n        &#39;None&#39; value means unlimited.\r\ndisplay.max_seq_items: [default: None] [currently: None]\r\n: int or None\r\n    \r\n        when pretty-printing a long sequence, no more then `max_seq_items`\r\n        will be printed. If items are ommitted, they will be denoted by the addition\r\n        of &quot;...&quot; to the resulting string.\r\n    \r\n        If set to None, the number of items to be printed is unlimited.\r\ndisplay.mpl_style: [default: None] [currently: None]\r\n: bool\r\n    \r\n        Setting this to &#39;default&#39; will modify the rcParams used by matplotlib\r\n        to give plots a more pleasing visual style by default.\r\n        Setting this to None/False restores the values to their initial value.\r\ndisplay.multi_sparse: [default: True] [currently: True]\r\n: boolean\r\n        &quot;sparsify&quot; MultiIndex display (don&#39;t display repeated\r\n        elements in outer levels within groups)\r\ndisplay.notebook_repr_html: [default: True] [currently: True]\r\n: boolean\r\n        When True, IPython notebook will use html representation for\r\n        pandas objects (if it is available).\r\ndisplay.pprint_nest_depth: [default: 3] [currently: 3]\r\n: int\r\n        Controls the number of nested levels to process when pretty-printing\r\ndisplay.precision: [default: 7] [currently: 7]\r\n: int\r\n        Floating point output precision (number of significant digits). This is\r\n        only a suggestion\r\ndisplay.width: [default: 80] [currently: 1000]\r\n: int\r\n        Width of the display in characters. In case python/IPython is running in\r\n        a terminal this can be set to None and pandas will correctly auto-detect the\r\n        width.\r\n        Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a\r\n        terminal and hence it is not possible to correctly detect the width.\r\nmode.sim_interactive: [default: False] [currently: False]\r\n: boolean\r\n        Whether to simulate interactive mode for purposes of testing\r\nmode.use_inf_as_null: [default: False] [currently: False]\r\n: boolean\r\n        True means treat None, NaN, INF, -INF as null (old way),\r\n        False means None and NaN are null, but INF, -INF are not null\r\n        (new way).\r\nCall def:   pd.set_option(self, *args, **kwds)\r\n&lt;/pre&gt;",
               "tags": [],
               "creation_date": 1343584922,
               "last_edit_date": 1495539204,
               "is_accepted": true,
               "id": "11711637",
               "down_vote_count": 2,
               "score": 286
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 76,
               "answer_id": 25415404,
               "is_accepted": false,
               "last_activity_date": 1408573164,
               "body_markdown": "Try this:\r\n\r\n    pd.set_option(&#39;display.expand_frame_repr&#39;, False)\r\n\r\nFrom the documentation:\r\n\r\n&gt; display.expand_frame_repr : boolean\r\n&gt; \r\n&gt; Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple \u201cpages\u201d if it\u2019s width exceeds display.width. [default: True] [currently: True]\r\n\r\nSee: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html\r\n\r\n",
               "id": "25415404",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1408573164,
               "score": 76
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 17,
               "answer_id": 29074073,
               "is_accepted": false,
               "last_activity_date": 1426500296,
               "body_markdown": "You can set the output display to match your current terminal width:\r\n\r\n    pd.set_option(&#39;display.width&#39;, pd.util.terminal.get_terminal_size()[0])",
               "id": "29074073",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1426500296,
               "score": 17
            },
            {
               "up_vote_count": 31,
               "answer_id": 33375383,
               "last_activity_date": 1507026363,
               "path": "3.stack.answer",
               "body_markdown": "If you want to set options temporarily to display one large DataFrame, you can use [option_context](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.option_context.html):\r\n\r\n\twith pd.option_context(&#39;display.max_rows&#39;, -1, &#39;display.max_columns&#39;, 5):\r\n\t\tprint df\r\n\r\nOption values are restored automatically when you exit the `with` block. ",
               "tags": [],
               "creation_date": 1445969074,
               "last_edit_date": 1507026363,
               "is_accepted": false,
               "id": "33375383",
               "down_vote_count": 0,
               "score": 31
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 13,
               "answer_id": 33798922,
               "is_accepted": false,
               "last_activity_date": 1447922615,
               "body_markdown": "Set column max width using:\r\n\r\n    pd.set_option(&#39;max_colwidth&#39;, 800)\r\n\r\nThis particular statement sets max width to 800px, per column.",
               "id": "33798922",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1447922615,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 11,
               "answer_id": 36373866,
               "is_accepted": false,
               "last_activity_date": 1459603599,
               "body_markdown": "According to the [docs for v0.18.0][1], if you&#39;re running on a terminal (ie not iPython notebook, qtconsole or IDLE), it&#39;s a 2-liner to have Pandas auto-detect your screen width and adapt on the fly with how many columns it shows:\r\n\r\n    pd.set_option(&#39;display.large_repr&#39;, &#39;truncate&#39;)\r\n    pd.set_option(&#39;display.max_columns&#39;, 0)\r\n\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html",
               "id": "36373866",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1459603599,
               "score": 11
            }
         ],
         "link": "https://stackoverflow.com/questions/11707586/python-pandas-how-to-widen-output-display-to-see-more-columns",
         "id": "858127-2277"
      },
      {
         "up_vote_count": "789",
         "path": "2.stack",
         "body_markdown": "I tried to install the Python package [dulwich](https://pypi.python.org/pypi/dulwich):\r\n\r\n&lt;!-- language-all: lang-none --&gt;\r\n\r\n    pip install dulwich\r\n\r\nBut I get a cryptic error message:\r\n\r\n    error: Unable to find vcvarsall.bat\r\n\r\nThe same happens if I try installing the package manually:\r\n\r\n    &gt; python setup.py install\r\n    running build_ext\r\n    building &#39;dulwich._objects&#39; extension\r\n    error: Unable to find vcvarsall.bat",
         "view_count": "772019",
         "answer_count": "39",
         "tags": "['python', 'windows', 'pip', 'setup.py', 'failed-installation']",
         "creation_date": "1273659875",
         "last_edit_date": "1508776321",
         "code_snippet": "['<code>pip install dulwich\\n</code>', '<code>error: Unable to find vcvarsall.bat\\n</code>', \"<code>&gt; python setup.py install\\nrunning build_ext\\nbuilding 'dulwich._objects' extension\\nerror: Unable to find vcvarsall.bat\\n</code>\", '<code>C:\\\\Users\\\\User\\\\libfreenect\\\\wrappers\\\\python&gt;python setup.py install</code>', \"<code>running install running build running build_ext building 'freenect' extension error: Unable to find vcvarsall.bat</code>\", '<code>VS90COMNTOOLS</code>', '<code>setup.py</code>', '<code>SET VS90COMNTOOLS=%VS100COMNTOOLS%</code>', '<code>SET VS90COMNTOOLS=%VS110COMNTOOLS%</code>', '<code>SET VS90COMNTOOLS=%VS120COMNTOOLS%</code>', '<code>SET VS90COMNTOOLS=%VS140COMNTOOLS%</code>', '<code>easy_install binary_installer_built_with_distutils.exe</code>', '<code>wheel convert ....exe</code>', '<code>python -m pip install &lt;filename&gt;</code>', '<code>C:\\\\programs\\\\mingw\\\\</code>', '<code>c:\\\\programs\\\\MinGW\\\\bin;</code>', '<code>C:\\\\Python26\\\\Lib\\\\distutils\\\\distutils.cfg</code>', '<code>[build]\\ncompiler=mingw32\\n</code>', '<code>easy_install.exe amara</code>', '<code>cmd.exe</code>', '<code>C:\\\\msysgit\\\\mingw\\\\mingw32\\\\bin</code>', '<code>C:\\\\msysgit\\\\mingw\\\\bin</code>', '<code>pip install</code>', \"<code>gcc: error: unrecognized command line option '-mno-cygwin' error: Setup script exited with error: command 'gcc' failed with exit status 1</code>\", '<code>easy_install.exe amara</code>', '<code>C:\\\\Python27\\\\Lib\\\\distutils\\\\cygwincompiler.py</code>', '<code>\u2013mno-cygwin</code>', '<code>\u2013mno-cygwin</code>', '<code>python.exe</code>', '<code>get_build_version</code>', '<code>MSC v.1000 -&gt; Visual C++ 4.x        \\nMSC v.1100 -&gt; Visual C++ 5          \\nMSC v.1200 -&gt; Visual C++ 6          \\nMSC v.1300 -&gt; Visual C++ .NET       \\nMSC v.1310 -&gt; Visual C++ .NET 2003  \\nMSC v.1400 -&gt; Visual C++ 2005  (8.0)\\nMSC v.1500 -&gt; Visual C++ 2008  (9.0)\\nMSC v.1600 -&gt; Visual C++ 2010 (10.0)\\nMSC v.1700 -&gt; Visual C++ 2012 (11.0)\\nMSC v.1800 -&gt; Visual C++ 2013 (12.0)\\nMSC v.1900 -&gt; Visual C++ 2015 (14.0)\\nMSC v.1910 -&gt; Visual C++ 2017 (15.0)      \\n</code>', '<code>Developer Tools &gt;&gt; Visual C++ Compilers</code>', '<code>cmd.exe</code>', '<code>C:\\\\Program Files\\\\</code>', '<code>\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 9.0\\\\Common7\\\\Tools\\\\vsvars32.bat\"</code>', '<code>\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 9.0\\\\Common7\\\\Tools\\\\vsvars64.bat\"</code>', '<code>Program Files (x86)</code>', '<code>vcvars64.bat</code>', '<code>vcvarsx86_amd64.bat</code>', '<code>amd64</code>', '<code>x86_amd64</code>', '<code>error: ... was unexpected at this time.</code>', '<code>...</code>', '<code>Setting environment for using Microsoft Visual Studio 20xx x86 tools.</code>', '<code>Setting environment for using Microsoft Visual Studio 20xx x64 tools.</code>', '<code>python setup.py install</code>', '<code>pip install pkg-name</code>', '<code>...\\\\VC\\\\bin\\\\vcvarsamd64.bat</code>', '<code>...\\\\VC\\\\bin\\\\amd64\\\\vcvarsamd64.bat</code>', '<code>setup.py</code>', '<code>setuptools</code>', '<code>6.0</code>', '<code>pip install setuptools --upgrade</code>', '<code>sys.version</code>', '<code>MSC</code>', '<code>MSC v.1500</code>', '<code>setuptools</code>', '<code>setuptools.setup()</code>', '<code>distutils</code>', '<code>setuptools&gt;=6.0</code>', '<code>setuptools</code>', '<code>pip install setuptools --upgrade</code>', '<code>setuptools</code>', '<code>distutils</code>', '<code>distitutils</code>', '<code>error: Unable to find vcvarsall.bat\\n</code>', '<code>setup.py install build --compiler=mingw32\\n</code>', '<code>configure: error: cannot run C compiled programs.\\n</code>', \"<code>cc1.exe: error: unrecognized command line option '-mno-cygwin' \\nerror: command 'gcc' failed with exit status 1\\n</code>\", '<code>-c mingw32</code>', '<code>python setup.py install -c mingw32\\n</code>', '<code>python setup.py build --compiler=mingw32</code>', '<code>python setup.py install</code>', '<code>C:\\\\programs\\\\mingw\\\\bin;</code>', '<code>C:\\\\Python27\\\\Lib\\\\distutils\\\\distutils.cfg</code>', '<code>[build]\\ncompiler=mingw32\\n</code>', \"<code>  self.set_executables(compiler='gcc -O -Wall',\\n                         compiler_so='gcc -mdll -O -Wall',\\n                         compiler_cxx='g++ -O -Wall',\\n                         linker_exe='gcc',\\n                         linker_so='%s %s %s'\\n                                    % (self.linker_dll, shared_option,\\n                                       entry_point))\\n</code>\", '<code>setup.py</code>', '<code>distutils.core.setup()</code>', '<code>setuptools.setup()</code>', '<code>distutils.core.setup()</code>', '<code>python setup.py build</code>', '<code>pip</code>', '<code>setup.py</code>', '<code>python setup.py build</code>', '<code>distutils.core.setup()</code>', '<code>SET DISTUTILS_USE_SDK=1\\nSET MSSdk=1\\n</code>', '<code>cd</code>', '<code>python setup.py build</code>', '<code>python setup.py install</code>', '<code>set VS90COMNTOOLS=%VS80COMNTOOLS%\\n</code>', '<code>python.exe setup.py install\\n</code>', '<code>vcvars64.bat</code>', '<code>C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 10.0\\\\VC\\\\bin\\\\amd64</code>', '<code>CALL \"C:\\\\Program Files\\\\Microsoft SDKs\\\\Windows\\\\v7.1\\\\Bin\\\\SetEnv.cmd\" /x64</code>', '<code>pip install numpy</code>', '<code>File \"numpy\\\\core\\\\setup.py\", line 686, in get_mathlib_info\\nraise RuntimeError(\"Broken toolchain: cannot link a simple C program\")\\nRuntimeError: Broken toolchain: cannot link a simple C program\\n</code>', '<code>mfinfo</code>', '<code>None</code>', '<code>C:\\\\Python34\\\\Lib\\\\distutils\\\\msvc9compiler.py</code>', '<code>pip install numpy</code>', '<code>numpy</code>', '<code>numpy\u20111.9.2+mkl\u2011cp34\u2011none\u2011win_amd64.whl</code>', '<code>pip install numpy\u20111.9.2+mkl\u2011cp34\u2011none\u2011win_amd64.whl</code>', '<code>vcvarsall.bat</code>', '<code>C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 11.0\\\\VC</code>', '<code>C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 11.0\\\\Common7\\\\Tools</code>', \"<code>query_vcvarsall    raise ValueError(str(list(result.keys())))ValueError: [u'path']</code>\", '<code>installation failed with return code 5100</code>', '<code>C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 10.0\\\\VC\\\\bin</code>', '<code>CALL setenv /x64</code>', '<code>C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 10.0\\\\VC\\\\bin\\\\amd64\\\\vcvars64.bat</code>', '<code>CALL setenv /x64</code>', '<code>C:\\\\Users\\\\Andreas\\\\AppData\\\\Local\\\\Programs\\\\Common\\\\Microsoft\\\\Visual C++ for Python\\\\9.0\\n</code>', '<code>C:/Python27/lib/distutils\\n</code>', '<code>productdir = os.path.join(toolsdir, os.pardir, os.pardir, \"VC\")\\n</code>', '<code>productdir = os.path.join(toolsdir)\\n</code>', '<code>%userprofile%\\\\Appdata\\\\Local\\\\Programs\\\\Common</code>', '<code>pip install --global-option build_ext --global-option --compiler=mingw32 packagename\\n</code>', '<code>error: Microsoft Visual C++ 9.0 is required (Unable to find vcvarsall.bat). Get it from http://aka.ms/vcpython27</code>', '<code>vcpython27</code>', \"<code>&gt; iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\\n&gt; choco install python2 -y\\n&gt; choco install vcpython27 -y\\n</code>\", '<code>error: Setup script exited with error: Unable to find vcvarsall.bat</code>', '<code>\\n    #productdir = os.path.join(toolsdir, os.pardir, os.pardir, \"VC\")\\n    productdir = os.path.join(toolsdir, os.pardir, os.pardir)\\n</code>', '<code>[python installation directory or virtualenv]\\\\Lib\\\\site-packages</code>', '<code>C:\\\\Programs\\\\MinGW\\\\bin;C:\\\\Programs\\\\MinGW\\\\msys\\\\1.0\\\\bin;</code>', '<code>distutils.cfg</code>', '<code>C:\\\\Python26\\\\Lib\\\\distutils\\\\distutils.cfg</code>', '<code>[build]\\ncompiler=mingw32\\n</code>', '<code>x86_64-w64-mingw32-gcc</code>', '<code>i686-w64-mingw32-gcc</code>']",
         "title": "error: Unable to find vcvarsall.bat",
         "_childDocuments_": [
            {
               "up_vote_count": 25,
               "answer_id": 2818009,
               "last_activity_date": 1273665666,
               "path": "3.stack.answer",
               "body_markdown": "Looks like its looking for VC compilers, so you could try to mention compiler type with `-c mingw32`, since you have msys\r\n\r\n    python setup.py install -c mingw32\r\n\r\n",
               "tags": [],
               "creation_date": 1273661061,
               "last_edit_date": 1273665666,
               "is_accepted": false,
               "id": "2818009",
               "down_vote_count": 2,
               "score": 23
            },
            {
               "up_vote_count": 226,
               "answer_id": 2838827,
               "last_activity_date": 1384510019,
               "path": "3.stack.answer",
               "body_markdown": "I found the solution. \r\nI had the exact same problem, and error, installing &#39;amara&#39;. I had mingw32 installed, but distutils needed to be configured.\r\n\r\n 1. I have Python 2.6 that was already installed.\r\n 2. I installed mingw32 to `C:\\programs\\mingw\\`\r\n 3. Add mingw32&#39;s bin directory to your environment variable: append `c:\\programs\\MinGW\\bin;` to the **PATH**\r\n 4. Edit (create if not existing) **distutils.cfg** file located at `C:\\Python26\\Lib\\distutils\\distutils.cfg` to be:  \r\n\r\n        [build]\r\n        compiler=mingw32\r\n\r\n 5. Now run `easy_install.exe amara`.\r\n\r\nMake sure environment is set by opening a new `cmd.exe`.\r\n",
               "tags": [],
               "creation_date": 1273894651,
               "last_edit_date": 1384510019,
               "is_accepted": false,
               "id": "2838827",
               "down_vote_count": 5,
               "score": 221
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 223,
               "answer_id": 5983696,
               "is_accepted": false,
               "last_activity_date": 1305230138,
               "body_markdown": "You can install compiled version from http://www.lfd.uci.edu/~gohlke/pythonlibs/",
               "id": "5983696",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1305230138,
               "score": 221
            },
            {
               "up_vote_count": 55,
               "answer_id": 10501736,
               "last_activity_date": 1380110396,
               "path": "3.stack.answer",
               "body_markdown": "I just had this same problem, so I&#39;ll tell my story here hoping it helps someone else with the same issues and save them the couple of hours I just spent:\r\n\r\nI have mingw (g++ (GCC) 4.6.1) and python 2.7.3 in a windows 7 box and I&#39;m trying to install PyCrypto.\r\n\r\nIt all started with this error when running setup.py install: \r\n\r\n    error: Unable to find vcvarsall.bat\r\n\r\nEasily solved after googling the error by specifying mingw as the compiler of choice: \r\n\r\n    setup.py install build --compiler=mingw32\r\n\r\nThe problem is that then I got a different error: \r\n\r\n    configure: error: cannot run C compiled programs.\r\n\r\nIt turns out that my anti-virus was blocking the execution of a freshly compiled .exe. I just disabled the anti-virus &quot;resident shield&quot; and went to the next error:\r\n\r\n    cc1.exe: error: unrecognized command line option &#39;-mno-cygwin&#39; \r\n    error: command &#39;gcc&#39; failed with exit status 1\r\n\r\nThis solved it: &quot;Either install a slightly older version of MinGW, or edit distutils\\cygwinccompiler.py in your Python directory to remove all instances of -mno-cygwin.&quot; (from [here][1])\r\n\r\nNow, I can finally start working.\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/6034390/compiling-with-cython-and-mingw-produces-gcc-error-unrecognized-command-line-o",
               "tags": [],
               "creation_date": 1336491107,
               "last_edit_date": 1495541454,
               "is_accepted": false,
               "id": "10501736",
               "down_vote_count": 1,
               "score": 54
            },
            {
               "up_vote_count": 718,
               "answer_id": 10558328,
               "last_activity_date": 1481641790,
               "path": "3.stack.answer",
               "body_markdown": "***Update**: Comments point out that the instructions here may be dangerous. Consider using the Visual C++ 2008 Express edition or the purpose-built [Microsoft Visual C++ Compiler for Python][1] ([details][2]) and **NOT** using the original answer below. Original error message means the required version of Visual C++ is not installed.*\r\n\r\n\r\n----------\r\n\r\n\r\nFor Windows installations:\r\n\r\nWhile running setup.py for package installations, Python 2.7 searches for an installed Visual Studio 2008. You can trick Python to use a newer Visual Studio by setting the correct path in `VS90COMNTOOLS` environment variable before calling `setup.py`.\r\n\r\nExecute the following command based on the version of Visual Studio installed:\r\n\r\n- Visual Studio 2010 (VS10): `SET VS90COMNTOOLS=%VS100COMNTOOLS%`\r\n- Visual Studio 2012 (VS11): `SET VS90COMNTOOLS=%VS110COMNTOOLS%`\r\n- Visual Studio 2013 (VS12): `SET VS90COMNTOOLS=%VS120COMNTOOLS%`\r\n- Visual Studio 2015 (VS14): `SET VS90COMNTOOLS=%VS140COMNTOOLS%`\r\n\r\n\r\n  [1]: http://www.microsoft.com/en-us/download/details.aspx?id=44266\r\n  [2]: /a/26127562/2778484\r\n\r\n----\r\n\r\nWARNING: As noted below, this answer is unlikely to work if you are trying to compile python modules.\r\n\r\nSee https://stackoverflow.com/questions/3047542 for details.",
               "tags": [],
               "creation_date": 1336768797,
               "last_edit_date": 1495540989,
               "is_accepted": false,
               "id": "10558328",
               "down_vote_count": 47,
               "score": 671
            },
            {
               "up_vote_count": 20,
               "answer_id": 15718810,
               "last_activity_date": 1367852981,
               "path": "3.stack.answer",
               "body_markdown": "I have python 2.73 and windows 7 .The solution that worked for me was:\r\n\r\n1. Added mingw32&#39;s bin directory to environment variable: append **PATH** with `C:\\programs\\mingw\\bin;`\r\n2. Created **distutils.cfg** located at `C:\\Python27\\Lib\\distutils\\distutils.cfg` containing:  \r\n\r\n        [build]\r\n        compiler=mingw32\r\n\r\nTo deal with MinGW not recognizing the -mno-cygwin flag anymore, remove the flag in C:\\Python27\\Lib\\distutils\\cygwincompiler.py line 322 to 326, so it looks like this:\r\n\r\n      self.set_executables(compiler=&#39;gcc -O -Wall&#39;,\r\n                             compiler_so=&#39;gcc -mdll -O -Wall&#39;,\r\n                             compiler_cxx=&#39;g++ -O -Wall&#39;,\r\n                             linker_exe=&#39;gcc&#39;,\r\n                             linker_so=&#39;%s %s %s&#39;\r\n                                        % (self.linker_dll, shared_option,\r\n                                           entry_point))\r\n",
               "tags": [],
               "creation_date": 1364648456,
               "last_edit_date": 1367852981,
               "is_accepted": false,
               "id": "15718810",
               "down_vote_count": 0,
               "score": 20
            },
            {
               "up_vote_count": 11,
               "answer_id": 15832595,
               "last_activity_date": 1380109924,
               "path": "3.stack.answer",
               "body_markdown": "Maybe somebody can be interested, the following worked for me for the py2exe package.\r\n(I have windows 7 64 bit and portable python 2.7, Visual Studio 2005 Express with Windows SDK for Windows 7 and .NET Framework 4)\r\n\r\n    set VS90COMNTOOLS=%VS80COMNTOOLS%\r\n\r\nthen:\r\n\r\n    python.exe setup.py install\r\n\r\n",
               "tags": [],
               "creation_date": 1365160839,
               "last_edit_date": 1380109924,
               "is_accepted": false,
               "id": "15832595",
               "down_vote_count": 2,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 16716775,
               "is_accepted": false,
               "last_activity_date": 1369319682,
               "body_markdown": "If you&#39;re looking to install pyodbc on a Windows box that *doesn&#39;t* have Visual Studio installed another option is to manually install pyodbc using the binary distribution.\r\n\r\nThis is particularly useful if you do not have administrator privileges on the machine you&#39;re working with and are trying to set up a [virtualenv][1].\r\n\r\nSteps:\r\n\r\n1. Download the latest Windows installer from [here][2] (pyodbc-X.X.X.win-Y-py2.7.exe)\r\n2. Open the installer executable using 7-Zip (or WinRAR or whatever)\r\n3. Extract pyodbc.pyd and pyodbc-X.X.X-py2.7.egg-info and place them in `[python installation directory or virtualenv]\\Lib\\site-packages`\r\n4. There is no step 4 :)\r\n\r\n\r\n  [1]: http://www.virtualenv.org/en/latest/\r\n  [2]: https://code.google.com/p/pyodbc/downloads/list",
               "id": "16716775",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1369319682,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 17065122,
               "is_accepted": false,
               "last_activity_date": 1371038948,
               "body_markdown": "I tried all the above answers, and found all of them not to work, this was perhaps I was using Windows 8 and had installed Visual Studio 2012. In this case, this is what you do.\r\n\r\nThe `vcvarsall.bat` file is located here:\r\n`C:\\Program Files (x86)\\Microsoft Visual Studio 11.0\\VC`\r\n\r\nSimply select the file, and copy it.\r\n\r\nThen go to this directory:\r\n`C:\\Program Files (x86)\\Microsoft Visual Studio 11.0\\Common7\\Tools`\r\n\r\nand paste the file. And then, all should be well.",
               "id": "17065122",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1371038948,
               "score": 7
            },
            {
               "up_vote_count": 4,
               "answer_id": 17265608,
               "last_activity_date": 1402216407,
               "path": "3.stack.answer",
               "body_markdown": "I tried many solutions but only one worked for me, the install of Microsoft Visual Studio **2008** Express C++.\r\n\r\nI got this issue with a Python 2.7 module written in C (yEnc, which has other issues with MS VS). Note that Python 2.7 is built with MS VS 2008 version, not 2010!\r\n\r\nDespite the fact it&#39;s free, it is quite hard to find since MS is promoting VS 2010.\r\nStill, the MSDN official very direct links are still working: check https://stackoverflow.com/a/15319069/2227298 for download links.",
               "tags": [],
               "creation_date": 1372023648,
               "last_edit_date": 1495539206,
               "is_accepted": false,
               "id": "17265608",
               "down_vote_count": 1,
               "score": 3
            },
            {
               "up_vote_count": 3,
               "answer_id": 17904968,
               "last_activity_date": 1412689827,
               "path": "3.stack.answer",
               "body_markdown": "The answer given by @monkey is one of the correct ones, but it is incomplete.\r\n\r\nIn case you&#39;d like to use [MinGW][1], you should select the C, C++ and also other development tools suggested during the MinGW installation process to also get &quot;make.exe.&quot;\r\n\r\nYou must also have the path set to make.exe in the env.\r\n\r\nTo complete his answer, here are the steps:\r\n\r\n1. Add mingw32&#39;s bin directory to your environment variables\r\n2. Append `C:\\Programs\\MinGW\\bin;C:\\Programs\\MinGW\\msys\\1.0\\bin;` to the PATH \r\n3. Edit (create if it doesn&#39;t exist) the `distutils.cfg` file located at `C:\\Python26\\Lib\\distutils\\distutils.cfg` to be:\r\n\r\n        [build]\r\n        compiler=mingw32\r\n\r\nMake sure the environment variables is set by opening a new cmd.exe.\r\n\r\n  [1]: http://en.wikipedia.org/wiki/MinGW\r\n",
               "tags": [],
               "creation_date": 1374985408,
               "last_edit_date": 1412689827,
               "is_accepted": false,
               "id": "17904968",
               "down_vote_count": 2,
               "score": 1
            },
            {
               "up_vote_count": 70,
               "answer_id": 18018539,
               "last_activity_date": 1419332389,
               "path": "3.stack.answer",
               "body_markdown": "What&#39;s going on? Python modules can be [part written in C or C++](https://docs.python.org/3/extending/extending.html) (typically for speed). If you try to install such a package with Pip (or `setup.py`), it has to compile that C/C++ from source. Out the box, Pip will brazenly assume you the compiler Microsoft Visual C++ installed. If you don&#39;t have it, you&#39;ll see this cryptic error message &quot;Error: Unable to find vcvarsall.bat&quot;.\r\n\r\nThe prescribed solution is to install a C/C++ compiler, either Microsoft Visual C++, or [MinGW](http://www.mingw.org/wiki/Getting_Started) (an open-source project). However, installing and configuring either is prohibitively difficult. (Edit 2014: Microsoft have published a special [C++ compiler](http://www.microsoft.com/en-us/download/details.aspx?id=44266) for Python 2.7)\r\n\r\nThe easiest solution is to use Christoph Gohlke&#39;s Windows installers (.msi) for popular Python packages. He builds installers for Python 2.x and 3.x, 32 bit and 64 bit. You can download them from http://www.lfd.uci.edu/~gohlke/pythonlibs/\r\n\r\n* * *\r\n\r\nIf you too think &quot;Error: Unable to find vcvarsall.bat&quot; is a ludicrously cryptic and unhelpful message, then please comment on the bug at http://bugs.python.org/issue2943 to replace it with a more helpful and user-friendly message. \r\n\r\nFor comparison, Ruby ships with a package manager Gem and offers a quasi-official C/C++ compiler, DevKit. If you try to install a package without it, you see this helpful friendly useful message:\r\n\r\n&gt; Please update your PATH to include build tools or download the DevKit from http://rubyinstaller.org/downloads and follow the instructions at http://github.com/oneclick/rubyinstaller/wiki/Development-Kit\r\n\r\nYou can read a longer rant about Python packaging at https://stackoverflow.com/a/13445719/284795\r\n",
               "tags": [],
               "creation_date": 1375451326,
               "last_edit_date": 1495539206,
               "is_accepted": false,
               "id": "18018539",
               "down_vote_count": 1,
               "score": 69
            },
            {
               "up_vote_count": 142,
               "answer_id": 18045219,
               "last_activity_date": 1495100464,
               "path": "3.stack.answer",
               "body_markdown": "## If you want to compile with Visual Studio C++ instead of mingw...\r\n\r\n1. Run `python.exe` to display which version of VC++ it was compiled with (example shown below).\r\n\r\n &gt; It is **important** to use the corresponding version of the Visual C++ compiler that Python was compiled with since [distilutils][1]&#39;s `get_build_version` prevents mixing versions ([per Piotr&#39;s warning][2]).\r\n \r\n  - Yellow (top) is Python 2.7, compiled with MSC v.1500 (Visual Studio C++ 2008)\r\n  - Red (bottom) is Python 3.4.1, compiled with MSC v.1600 (Visual Studio C++ 2010)\r\n\r\n ![Example from the command line showing Python 2.7 compiled with MSC v.1500 and Python 3.4.1 compiled with MSC v.1600][3]\r\n\r\n2. Use the table below&lt;sup&gt;[\\[1\\]][4]&lt;/sup&gt; to match the internal VC++ version with the corresponding Visual Studio release:\r\n\r\n        MSC v.1000 -&gt; Visual C++ 4.x        \r\n        MSC v.1100 -&gt; Visual C++ 5          \r\n        MSC v.1200 -&gt; Visual C++ 6          \r\n        MSC v.1300 -&gt; Visual C++ .NET       \r\n        MSC v.1310 -&gt; Visual C++ .NET 2003  \r\n        MSC v.1400 -&gt; Visual C++ 2005  (8.0)\r\n        MSC v.1500 -&gt; Visual C++ 2008  (9.0)\r\n        MSC v.1600 -&gt; Visual C++ 2010 (10.0)\r\n        MSC v.1700 -&gt; Visual C++ 2012 (11.0)\r\n        MSC v.1800 -&gt; Visual C++ 2013 (12.0)\r\n        MSC v.1900 -&gt; Visual C++ 2015 (14.0)\r\n        MSC v.1910 -&gt; Visual C++ 2017 (15.0)      \r\n\r\n1. Download and install the corresponding version of Visual Studio C++ from the previous step.    \r\n Additional notes for specific versions of VC++ are listed below.  \r\n\r\n  ### Notes for Visual Studio C++ 2008\r\n  \r\n  For *only* the 32-bit compilers, download [Visual Studio C++ 2008 Express Edition][5].\r\n\r\n  For the 64-bit compilers&lt;sup&gt;[\\[2\\]][7]&lt;/sup&gt;&lt;sup&gt;[\\[3\\]][8]&lt;/sup&gt;, download [Windows SDK for Windows 7 and .NET Framework 3.5 SP1][6].    \r\n\r\n   &gt; * Uncheck everything except `Developer Tools &gt;&gt; Visual C++ Compilers` to save time and disk space from installing SDK tools you otherwise don&#39;t need.  \r\n\r\n  ### Notes for Visual Studio C++ 2010\r\n\r\n  According to Microsoft, if you installed Visual Studio 2010 SP1, it may have removed the compilers and libraries for VC++.    \r\n  If that is the case, download [Visual C++ 2010 SP1 Compiler Update][9].\r\n\r\n  ### Notes for Visual Studio C++ 2015\r\n  \r\n  If you don&#39;t need the Visual Studio IDE, download [Visual Studio C++ 2015 Build Tools][12].\r\n\r\n  ### Notes for Visual Studio C++ 2017\r\n\r\n  If you don&#39;t need the Visual Studio IDE, download [Build Tools for Visual Studio 2017][13].\r\n\r\n   **Suggestion**: If you have both a 32- and 64-bit Python installation, you may also want to use [virtualenv][14] to create separate Python environments so you can use one or the other at a time without messing with your path to choose which Python version to use.\r\n\r\n&gt; According to [@srodriguex][10], you may be able to skip manually loading the   bath file (Steps 2-4) by instead copying a few batch files to where Python is searching by following [this answer][11]. If that doesn&#39;t work, here are the following steps that originally worked for me.\r\n\r\n2. Open up a `cmd.exe`\r\n\r\n3. *Before* you try installing something which requires C extensions, run the following batch file to load the VC++ compiler&#39;s environment into the session (i.e. environment variables, the path to the compiler, etc).  \r\n\r\n    **Execute:**    \r\n     * 32-bit Compilers:\r\n\r\n          *Note*: 32-bit Windows installs will only have `C:\\Program Files\\` as expected\r\n\r\n          &gt; `&quot;C:\\Program Files (x86)\\Microsoft Visual Studio 9.0\\Common7\\Tools\\vsvars32.bat&quot;`\r\n     * 64-bit Compilers:\r\n\r\n          &gt; `&quot;C:\\Program Files (x86)\\Microsoft Visual Studio 9.0\\Common7\\Tools\\vsvars64.bat&quot;`\r\n\r\n          *Note*: Yes, the native 64-bit compilers are in `Program Files (x86)`. Don&#39;t ask me why.  \r\n          Additionally, if you are wondering what the difference between `vcvars64.bat` and `vcvarsx86_amd64.bat` or more importantly the difference between `amd64` and `x86_amd64`, the former are for the native 64-bit compiler tools and the latter are the 64-bit cross compilers that can run on a 32-bit Windows installation.\r\n\r\n\r\n    **Update:**  \r\n  If for some reason you are getting ``error: ... was unexpected at this time.`` where the `...` is some series of characters, then you need to check that you path variable does not have any extraneous characters like extra quotations or stray characters. The batch file is not going to be able to update your session path if it can&#39;t make sense of it in the first place.\r\n\r\n4. If that went well, you should get one of the following messages depending on which version of VC++ and which command you ran:  \r\n   \r\n   For the 32-bit compiler tools:  \r\n``Setting environment for using Microsoft Visual Studio 20xx x86 tools.``  \r\n\r\n   For the 64-bit compiler tools:  \r\n``Setting environment for using Microsoft Visual Studio 20xx x64 tools.``\r\n\r\n5. *Now*, run the setup via `python setup.py install` or `pip install pkg-name`\r\n\r\n6. Hope and cross your fingers that the planets are aligned correctly for VC++ to cooperate.\r\n\r\n\r\n  [1]: http://svn.python.org/projects/python/branches/release27-maint/Lib/distutils/msvc9compiler.py\r\n  [2]: https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat#comment-23311731\r\n  [3]: https://i.stack.imgur.com/jzygw.png\r\n  [4]: https://stackoverflow.com/a/2676904/809572\r\n  [5]: http://download.microsoft.com/download/A/5/4/A54BADB6-9C3F-478D-8657-93B3FC9FE62D/vcsetup.exe\r\n  [6]: http://www.microsoft.com/en-us/download/details.aspx?id=3138\r\n  [7]: http://blog.victorjabur.com/2011/06/05/compiling-python-2-7-modules-on-windows-32-and-64-using-msvc-2008-express/\r\n  [8]: http://smaudet.wordpress.com/2014/01/26/building-pyaudio-on-windows-7-x64-using-the-free-msvc-toolchains/\r\n  [9]: https://www.microsoft.com/en-us/download/details.aspx?id=4422\r\n  [10]: https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat#comment35207766_18045219\r\n  [11]: https://stackoverflow.com/questions/13596407/errors-while-building-installing-c-module-for-python-2-7/21898585#21898585\r\n  [12]: http://landinghub.visualstudio.com/visual-cpp-build-tools\r\n  [13]: https://www.visualstudio.com/downloads/#build-tools-for-visual-studio-2017\r\n  [14]: http://www.virtualenv.org/en/latest/virtualenv.html",
               "tags": [],
               "creation_date": 1375634915,
               "last_edit_date": 1495541914,
               "is_accepted": false,
               "id": "18045219",
               "down_vote_count": 1,
               "score": 141
            },
            {
               "up_vote_count": 7,
               "answer_id": 21271388,
               "last_activity_date": 1390348343,
               "path": "3.stack.answer",
               "body_markdown": "You can download the free Visual C++ 2008 Express Edition from http://go.microsoft.com/?linkid=7729279, which will set the VS90COMNTOOLS environment variable during installation and therefore build with a compatible compiler.\r\n\r\nAs @PiotrDobrogost mentioned in a comment, his answer to this other question goes into details about why Visual C++ 2008 is the right thing to build with, but this can change as the Windows build of Python moves to newer versions of Visual Studio: https://stackoverflow.com/questions/3047542/building-lxml-for-python-2-7-on-windows/5122521#5122521",
               "tags": [],
               "creation_date": 1390347827,
               "last_edit_date": 1495540989,
               "is_accepted": false,
               "id": "21271388",
               "down_vote_count": 1,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 22658820,
               "is_accepted": false,
               "last_activity_date": 1395831425,
               "body_markdown": "With Python 3.4, the dependency is on Visual Studio 2010. Installing Visual C++ 2010 Express fixed the problem for me. \r\n\r\nTricking it into using the VS 2008 or 2013 installs that I happened to have didn&#39;t work.",
               "id": "22658820",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1395831425,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 25753004,
               "is_accepted": false,
               "last_activity_date": 1410294134,
               "body_markdown": "You can use easy_install instead of pip it works for me.",
               "id": "25753004",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1410294134,
               "score": 2
            },
            {
               "up_vote_count": 63,
               "answer_id": 26127562,
               "last_activity_date": 1418671864,
               "path": "3.stack.answer",
               "body_markdown": "You&#39;ll need to install a Microsoft compiler, compatible with the compiler used to build Python. This means you need Visual C++ 2008 (or newer, with [some tweaking](/a/10558328)).\r\n\r\nMicrosoft now supplies a bundled compiler and headers *just* to be able to compile Python extensions, at the memorable URL: \r\n\r\n&gt; ### Microsoft Visual C++ Compiler for Python 2.7\r\n&gt;\r\n&gt; http://aka.ms/vcpython27\r\n\r\nThis is a relatively small package; 85MB to download, installable without admin privileges, no reboot required. The name is a little misleading, the compiler will work for any Python version originally compiled with Visual C++ 2008, not just Python 2.7.\r\n\r\nIf you start a Python interactive prompt or print `sys.version`, look for the `MSC` version string; if it is `MSC v.1500` you can use this tool.\r\n\r\nFrom the [original announcement to the distutils list](https://mail.python.org/pipermail/distutils-sig/2014-September/024885.html):\r\n\r\n&gt; Microsoft has released a compiler package for Python 2.7 to make it easier for people to build and distribute their C extension modules on Windows. The Microsoft Visual C++ Compiler for Python 2.7 (a.k.a. VC9) is available from: http://aka.ms/vcpython27 \r\n&gt;\r\n&gt; This package contains all the tools and headers required to build C extension modules for Python 2.7 32-bit and 64-bit (note that some extension modules require 3rd party dependencies such as OpenSSL or libxml2 that are not included). Other versions of Python built with Visual C++ 2008 are also supported, so &quot;Python 2.7&quot; is just advertising - it&#39;ll work fine with 2.6 and 3.2.\r\n\r\nNote that you need to have [`setuptools` 6.0 or newer](https://pypi.python.org/pypi/setuptools) installed (listed in the system requirements on the download page). The project you are installing must use `setuptools.setup()`, not `distutils` or the auto-detection won&#39;t work.\r\n\r\nMicrosoft has stated that they want to keep the URL stable, so that automated scripts can reference it easily.",
               "tags": [],
               "creation_date": 1412101388,
               "last_edit_date": 1418671864,
               "is_accepted": false,
               "id": "26127562",
               "down_vote_count": 0,
               "score": 63
            },
            {
               "up_vote_count": 6,
               "answer_id": 27033824,
               "last_activity_date": 1416467663,
               "path": "3.stack.answer",
               "body_markdown": "I had this problem using **Python 3.4.1 on Windows 7 x64**, and unfortunately the packages I needed didn&#39;t have suitable exe or wheels that I could use. This system requires a few &#39;workarounds&#39;, which are detailed below (and **TLDR at bottom**).\r\n\r\nUsing the info in [Jaxrtech&#39;s answer above](https://stackoverflow.com/a/18045219/3800244), I determined I needed Visual Studio C++ 2010 (sys.version return MSC v.1600), so I installed Visual C++ 2010 Express from the link in his answer, which is http://go.microsoft.com/?linkid=9709949. I installed everything with updates, but as you can read below, this was a mistake. Only the original version of Express should be installed at this time (no updated anything).\r\n\r\nvcvarsall.bat was now present, but there was a new error when installing the package, `query_vcvarsall    raise ValueError(str(list(result.keys())))ValueError: [u&#39;path&#39;]`. There are other stackoverflow questions with this error, such as https://stackoverflow.com/questions/13596407/errors-while-building-installing-c-module-for-python-2-7\r\n\r\nI determined from that answer that 2010 Express only installs 32-bit compilers. To get 64-bit (and other) compilers, you need to install Windows 7.1 SDK. See http://msdn.microsoft.com/en-us/windowsserver/bb980924.aspx\r\n\r\nThis would not install for me though, and the installer returned the error `installation failed with return code 5100`. I found the solution at the following link: http://support.microsoft.com/kb/2717426. In short, if newer versions of x86 and x64 Microsoft Visual C++ 2010 Redistributable&#39;s are installed, they conflict with the ones in SDK installer, and need uninstalling first.\r\n\r\nThe SDK then installed, but I noticed vcvars64.bat still did not exist in `C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin`, nor its subfolders. vcvarsall.bat runs the vcvars64 batch file, so without it, the python package still wouldn&#39;t install (I forgot the error that was shown at this time).\r\n\r\nI then found some instructions here: http://www.cryptohaze.com/wiki/index.php/Windows_7_Build_Setup#Download_VS_2010_and_Windows_SDK_7.1\r\nFollowing the instructions, I had already installed Express and 7.1 SDK, so installed SDK 7.1 SP1, and did the missing header file fix. I then manually created vcvars64.bat with the content `CALL setenv /x64`. I will paste all those instructions here, so they don&#39;t get lost.\r\n\r\n&gt; Step 1 is to download Visual Studio Express 2010.\r\n&gt; \r\n&gt; http://www.microsoft.com/visualstudio/en-us/products/2010-editions/express\r\n&gt; is a good place to start. Download the installer, and run it\r\n&gt; (vc_web.exe). You don&#39;t need the SQL 2008 additional download.\r\n&gt; \r\n&gt; You&#39;ll also need the Windows SDK (currently 7.1) for the 64-bit\r\n&gt; compilers - unless you want to do 32-bit only builds, which are not\r\n&gt; fully supported...\r\n&gt; \r\n&gt; http://www.microsoft.com/en-us/download/details.aspx?id=8279 is a good\r\n&gt; starting point to download this - you&#39;ll want to run winsdk_web.exe\r\n&gt; when downloaded!\r\n&gt; \r\n&gt; The default install here is just fine.\r\n&gt; \r\n&gt; Finally, download and install the Windows SDK 7.1 SP1 update:\r\n&gt; http://www.microsoft.com/en-us/download/details.aspx?id=4422\r\n&gt; \r\n&gt; And, to fix missing header file, VS2010 SP1.\r\n&gt; http://www.microsoft.com/downloads/en/confirmation.aspx?FamilyID=75568aa6-8107-475d-948a-ef22627e57a5\r\n&gt; \r\n&gt; And, bloody hell, fix the missing batch file for VS2010 Express. This\r\n&gt; is getting downright absurd.\r\n&gt; \r\n&gt; In C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64,\r\n&gt; create &quot;vcvars64.bat&quot; with the following (you will need to be running\r\n&gt; as administrator):\r\n&gt; \r\n&gt; CALL setenv /x64\r\n\r\nMy python package still did not install (can&#39;t recall error). I then found some instructions (copied below) to use the special SDK 7.1 Command Prompt, see: https://mail.python.org/pipermail/distutils-sig/2012-February/018300.html\r\n\r\n&gt; Never mind this question.  Somebody here noticed this item on the menu:  Start-&gt;All Programs-&gt;Microsoft Windows SDK v7.1 -&gt;Windows SDK 7.1 Command Prompt\r\n&gt; \r\n&gt; This runs a batch job that appears to set up a working environment for the compiler.  From that prompt, you can type &quot;setup.py build&quot; or &quot;setup.py install&quot;.\r\n\r\nI opened the Windows SDK 7.1 Command Prompt as instructed, and used it to run easy_install on the python package. And at last, success!\r\n\r\n----------\r\n\r\n**TLDR**;\r\n\r\n 1. Install Visual Studio Express 2010 (preferably without updated redistributables or SQL server).\r\n 2. Install Windows 7.1 SDK\r\n 3. Instal SDK 7.1 SP1 update, and VS2010 SP1 header file fix (this step may not be required).\r\n 4. Manually create `C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64\\vcvars64.bat` with content `CALL setenv /x64`\r\n 5. Start-&gt;All Programs-&gt;Microsoft Windows SDK v7.1 -&gt;Windows SDK 7.1 Command Prompt to open special x64 command prompt, which can then be used with python/easy_install/pip/etc (including those in virtual_envs).",
               "tags": [],
               "creation_date": 1416467663,
               "last_edit_date": 1495542403,
               "is_accepted": false,
               "id": "27033824",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "up_vote_count": 2,
               "answer_id": 27572520,
               "last_activity_date": 1419026055,
               "path": "3.stack.answer",
               "body_markdown": "I had the same error (which I find silly and not really helpful whatsoever as error messages go) and continued having problems, despite having a C compiler available. \r\n\r\nSurprising, what ended up working for me was simply **upgrading pip and setuptools to the most recent version**. Hope this helps someone else out there.",
               "tags": [],
               "creation_date": 1419016570,
               "last_edit_date": 1419026055,
               "is_accepted": false,
               "id": "27572520",
               "down_vote_count": 1,
               "score": 1
            },
            {
               "up_vote_count": 14,
               "answer_id": 28617830,
               "last_activity_date": 1424386332,
               "path": "3.stack.answer",
               "body_markdown": "Look in the `setup.py` file of the package you are trying to install. If it is an older package it may be importing `distutils.core.setup()` rather than `setuptools.setup()`.\r\n\r\nI ran in to this (in 2015) with a combination of these factors:\r\n\r\n1. The Microsoft Visual C++ Compiler for Python 2.7 from http://aka.ms/vcpython27\r\n\r\n1. An older package that uses `distutils.core.setup()`\r\n\r\n1. Trying to do `python setup.py build` rather than using `pip`.\r\n\r\nIf you use a recent version of pip, it will force (monkeypatch) the package to use setuptools, even if its `setup.py` calls for distutils. However, if you are not using pip, and instead are just doing `python setup.py build`, the build process will use `distutils.core.setup()`, which does not know about the compiler install location.\r\n\r\n------------------------\r\n\r\nSolution\r\n--------\r\n\r\n**Step 1:** Open the appropriate Visual C++ 2008 Command Prompt\r\n\r\n\r\nOpen the Start menu or Start screen, and search for &quot;Visual C++ 2008 32-bit Command Prompt&quot; (if your python is 32-bit) or &quot;Visual C++ 2008 64-bit Command Prompt&quot; (if your python is 64-bit). Run it. The command prompt should say Visual C++ 2008 ... in the title bar.\r\n\r\n\r\n\r\n**Step 2:** Set environment variables\r\n\r\n\r\nSet these environment variables in the command prompt you just opened.\r\n\r\n\r\n&lt;!-- language: lang-none --&gt;\r\n\r\n    SET DISTUTILS_USE_SDK=1\r\n    SET MSSdk=1\r\n\r\nReference http://bugs.python.org/issue23246\r\n\r\n**Step 3:** Build and install\r\n\r\n`cd` to the package you want to build, and run `python setup.py build`, then `python setup.py install`. If you want to install in to a virtualenv, activate it before you build. ",
               "tags": [],
               "creation_date": 1424383253,
               "last_edit_date": 1424386332,
               "is_accepted": false,
               "id": "28617830",
               "down_vote_count": 0,
               "score": 14
            },
            {
               "up_vote_count": 0,
               "answer_id": 28727016,
               "last_activity_date": 1424890993,
               "path": "3.stack.answer",
               "body_markdown": "I followed the instructions http://springflex.blogspot.ru/2014/02/how-to-fix-valueerror-when-trying-to.html. but nothing happened.\r\nThen I installed 2010 Visual Studio Express (http://www.microsoft.com/visualstudio/en-us/products/2010-editions/visual-cpp-express) following the advice http://blog.python.org/2012/05/recent-windows-changes-in-python-33.html\r\nit helped me",
               "tags": [],
               "creation_date": 1424889516,
               "last_edit_date": 1424890993,
               "is_accepted": false,
               "id": "28727016",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 28908976,
               "is_accepted": false,
               "last_activity_date": 1425681934,
               "body_markdown": "**fastest solution**:\r\n\r\nIf you have **python 3.4.x**, the solution is simply to install \r\nVC++ 2010 since it is used to compile itself into.\r\n\r\nhttps://www.visualstudio.com/en-us/downloads#d-2010-express\r\n\r\nmy python version is\r\nMSC v.1600 32 bit (intel)] on win32\r\n\r\n*worked fine on Windows8*",
               "id": "28908976",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1425681934,
               "score": 1
            },
            {
               "up_vote_count": 8,
               "answer_id": 29062159,
               "last_activity_date": 1441969234,
               "path": "3.stack.answer",
               "body_markdown": "I spent almost 2 days figuring out how to fix this problem in my python 3.4 64 bit version: Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)] on win32\r\n\r\n**Solution 1, hard:** (before reading this, read first Solution 2 below)\r\nFinally, this is what helped me:\r\n\r\n1. install [Visual C++ 2010 Express](http://go.microsoft.com/?linkid=9709949)\r\n2. install [Microsoft Windows SDK v7.1 for Windows 7](http://www.microsoft.com/en-us/download/details.aspx?id=8279)\r\n3. create manually file `vcvars64.bat` in `C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64` which contains `CALL &quot;C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin\\SetEnv.cmd&quot; /x64` or other path depending on where you have yours installed\r\n4. (this seems to be optional) install [Microsoft Visual Studio 2010 Service Pack 1](http://www.microsoft.com/en-us/download/details.aspx?id=23691) together with [Microsoft Visual C++ 2010 Service Pack 1 Compiler Update for the Windows SDK 7.1](http://www.microsoft.com/en-us/download/details.aspx?id=4422)\r\n5. after that I tried to `pip install numpy` but received the following error:\r\n\r\n        File &quot;numpy\\core\\setup.py&quot;, line 686, in get_mathlib_info\r\n        raise RuntimeError(&quot;Broken toolchain: cannot link a simple C program&quot;)\r\n        RuntimeError: Broken toolchain: cannot link a simple C program\r\nI changed `mfinfo` to `None` in `C:\\Python34\\Lib\\distutils\\msvc9compiler.py` per this https://stackoverflow.com/a/23099820/4383472\r\n6. finally after `pip install numpy` command my avast antivirus tried to interfere into the installation process, but i quickly disabled it\r\n\r\nIt took very long - several minutes for numpy to compile, I even thought that there was an error, but finally everything was ok.\r\n\r\n**Solution 2, easy:**\r\n(I know this approach has already been mentioned in a highly voted [answer](https://stackoverflow.com/a/5983696/4383472), but let me repeat since it really is easier)\r\nAfter going through all of this work I understood that the best way for me is just to use already precompiled binaries from http://www.lfd.uci.edu/~gohlke/pythonlibs/ in future. There is very small chance that I will ever need some package (or a version of a package) which this site doesn&#39;t contain. The installation process is also much quicker this way. For example, to install `numpy`:\r\n\r\n 1. donwload `numpy\u20111.9.2+mkl\u2011cp34\u2011none\u2011win_amd64.whl` (if you have Python 3.4 64-bit) from that site\r\n 2. in command prompt or powershell install it with pip `pip install numpy\u20111.9.2+mkl\u2011cp34\u2011none\u2011win_amd64.whl` (or full path to the file depending how command prompt is opened)\r\n",
               "tags": [],
               "creation_date": 1426432091,
               "last_edit_date": 1495542899,
               "is_accepted": false,
               "id": "29062159",
               "down_vote_count": 0,
               "score": 8
            },
            {
               "up_vote_count": 1,
               "answer_id": 30180088,
               "last_activity_date": 1431393392,
               "path": "3.stack.answer",
               "body_markdown": "Go here: [http://docs.continuum.io/anaconda/install.html#windows-install][1]\r\n\r\nThere are instructions to install anaconda which will provide a GUI and a silent install of a majority of the packages that seem to be causing this issue from [http://www.scipy.org/][2].  I am aware of the solution for 2.7 here [https://www.microsoft.com/en-us/download/details.aspx?id=44266][3] but I did not see an option for Python 3.4.  After downloading and installing Anaconda you should be able to import a majority of the packages you need from scipy.\r\n\r\nHope this helps some people.  Took me 45 minutes of scouring posts and sites.\r\n\r\nEDIT: Just wanted to note there is a Python34 link on the GUI page next to the OS symbols.\r\n\r\n  [1]: http://docs.continuum.io/anaconda/install.html#windows-install\r\n  [2]: http://www.scipy.org/\r\n  [3]: https://www.microsoft.com/en-us/download/details.aspx?id=44266",
               "tags": [],
               "creation_date": 1431392900,
               "last_edit_date": 1431393392,
               "is_accepted": false,
               "id": "30180088",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 30384847,
               "is_accepted": false,
               "last_activity_date": 1432244423,
               "body_markdown": "If you have mingw installed\r\n\r\n    pip install --global-option build_ext --global-option --compiler=mingw32 packagename\r\n\r\nworks, forcing pip to build using the mingw compiler instead of Microsoft&#39;s. See here https://github.com/pypa/pip/issues/18 for details (last post).",
               "id": "30384847",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1432244423,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 32124031,
               "is_accepted": false,
               "last_activity_date": 1440089704,
               "body_markdown": "Install Visual Studio 2015 Community Edition from https://www.visualstudio.com,\r\nthen\r\n\r\nfor Python 3.4\r\n\r\n```set VS100COMNTOOLS=%VS140COMNTOOLS% &amp;&amp; pip install XX```",
               "id": "32124031",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1440089704,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 33156982,
               "is_accepted": false,
               "last_activity_date": 1444937571,
               "body_markdown": "I don&#39;t know if it is too late, but I found [Microsoft Visual C++ Compiler for Python 2.7][1] which reads\r\n\r\n&gt; The typical error message you will receive if you need this compiler package is **Unable to find vcvarsall.bat**\r\n\r\n\r\nHope this helps!\r\n\r\n\r\n  [1]: http://www.microsoft.com/en-us/download/details.aspx?id=44266",
               "id": "33156982",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1444937571,
               "score": 2
            },
            {
               "up_vote_count": 7,
               "answer_id": 33444371,
               "last_activity_date": 1454781576,
               "path": "3.stack.answer",
               "body_markdown": "I encountered this issue when I tried to install numpy library on my python 3.5. The solution is to install VS2015. I had VS2008, 2012, 2013, none of which is compatible with python 3.5. Apparently newer version of python has dependency on newer versions of VS.\r\n\r\nAlso make sure C++ Common Tools are is installed with Visual Studio.\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/EvXfE.png",
               "tags": [],
               "creation_date": 1446236716,
               "last_edit_date": 1454781576,
               "is_accepted": false,
               "id": "33444371",
               "down_vote_count": 0,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 33939484,
               "is_accepted": false,
               "last_activity_date": 1448543077,
               "body_markdown": "Is Microsoft Visual C++ Compiler for Python 2.7 at http://www.microsoft.com/en-us/download/details.aspx?id=44266 not a solution? ",
               "id": "33939484",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1448543077,
               "score": 3
            },
            {
               "up_vote_count": 6,
               "answer_id": 34152660,
               "last_activity_date": 1449658857,
               "path": "3.stack.answer",
               "body_markdown": "I wanted to run pysph on Windows 10 under Python 2.7 and got vcvarsall.bat was not found (from distutils)\r\n\r\nMy solution was the following:\r\n\r\nInstall Microsoft Visual C++ for Python 2.7 (like @Michael suggested)\r\n\r\nOn Windows 10 it was installed into (my username is Andreas):\r\n\r\n    C:\\Users\\Andreas\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\r\n\r\nSet environment variable **VS90COMNTOOLS** to the installation path of Visual C++ for Python 2.7 (see above path).\r\n\r\nIf it still doesn&#39;t work, then modifiy in the module\r\n\r\n    C:/Python27/lib/distutils\r\n\r\nthe file **msvc9compiler.py**. Find in it the function **find_vcvarsall** and do following modification.\r\n\r\nReplace the line:\r\n\r\n    productdir = os.path.join(toolsdir, os.pardir, os.pardir, &quot;VC&quot;)\r\n\r\nwith\r\n\r\n    productdir = os.path.join(toolsdir)\r\n\r\nThis is where vcvarsall.bat resides in my case (check, where vcvarsall.bat is in your installation).\r\n",
               "tags": [],
               "creation_date": 1449568092,
               "last_edit_date": 1449658857,
               "is_accepted": false,
               "id": "34152660",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 35021094,
               "is_accepted": false,
               "last_activity_date": 1453832774,
               "body_markdown": "I had the same error when I tried to install pandas in windows 10. After searching several solutions, I ended up with the use of wheel.\r\n\r\nFirst of all, upgrade pip to the newest version:\r\n\r\n    easy_install install -U pip\r\n\r\nSecond, install wheel:\r\n\r\n    pip install wheel\r\n\r\nThird, download the whl file for your package and install it:\r\n\r\n    pip install [xxx].whl\r\n\r\nSo far, I believe wheel is the best way to install Python packages on windows.\r\n\r\n",
               "id": "35021094",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1453832774,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 35484698,
               "is_accepted": false,
               "last_activity_date": 1455807070,
               "body_markdown": "The easiest way to solve this in 2016 is to install Chocolatey and then the  `vcpython27` package. Open Powershell:\r\n\r\n    &gt; iex ((new-object net.webclient).DownloadString(&#39;https://chocolatey.org/install.ps1&#39;))\r\n    &gt; choco install python2 -y\r\n    &gt; choco install vcpython27 -y\r\n\r\n",
               "id": "35484698",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1455807070,
               "score": 3
            },
            {
               "up_vote_count": 1,
               "answer_id": 36558208,
               "last_activity_date": 1460406874,
               "path": "3.stack.answer",
               "body_markdown": "**An exhaustive list of MS V\u0421++ versions** and installation variations **officially supported by `distutils`** (and some by `setuptools`) **and how to use them** can be found at\r\n\r\n**https://wiki.python.org/moin/WindowsCompilers**\r\n\r\nIt also specifies which VC++ version is required for which official Win32 Python release. **Note that MinGW is not officially supported** (see below for details).\r\n\r\nIn brief:\r\n\r\n* For each release, there&#39;s a specific Visual Studio release that works out of the box.\r\n    * Note that Express editions before 2012 [do not support x64][1]. Trying to use one for x64 results in an [obscure error][2].\r\n* For standalone SDKs, it&#39;s generally required to run the installation from the SDK prompt (=set environment variables).\r\n    * For some, this requirement can be lifted by a one-time tweaking which doesn&#39;t have adversal side effects\r\n* Some unusual VC++ releases are supported by `setuptools` and not `distutils`.\r\n* **MinGW** is said to &quot;work&quot;, but **only for x86.** https://bugs.python.org/issue4709 says that problems with its support lie in MinGW itself and **even x86 is bound to have problems since Python 3.5.**\r\n\r\n\r\n  [1]: https://msdn.microsoft.com/en-us/library/hs24szh9%28v=vs.100%29.aspx\r\n  [2]: https://stackoverflow.com/questions/4676728/value-error-trying-to-install-python-for-windows-extensions",
               "tags": [],
               "creation_date": 1460404983,
               "last_edit_date": 1495540519,
               "is_accepted": false,
               "id": "36558208",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 37853161,
               "is_accepted": false,
               "last_activity_date": 1466063851,
               "body_markdown": "I am tried all the above answers, but not worked for me. \r\nI was using Windows 10 and had installed Visual Studio 2010\r\nIn my case need add `vcvars64.bat` to `C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64`\r\n\r\nbelow is vcvars64.bat:\r\n\r\n    CALL &quot;C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin\\SetEnv.cmd&quot; /x64\r\n\r\nInstall Microsoft SDK 7.1 if you not install, and rerun `pip install dulwich` ",
               "id": "37853161",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1466063851,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 38053451,
               "is_accepted": false,
               "last_activity_date": 1467028897,
               "body_markdown": "I got the same problem and have solved it at the moment.\r\n\r\n&quot;Google&quot; told me that I need to install &quot;Microsoft Visual C++ Compiler for Python 2.7&quot;. I install not only the tool, but also Visual C++ 2008 Reditributable, but it didn&#39;t help. I then tried to install Visual C++ 2008 Express Edition. And the problem has gone!\r\n\r\nJust try to install Visual C++ 2008 Express Edition!",
               "id": "38053451",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1467028897,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 38477649,
               "is_accepted": false,
               "last_activity_date": 1469007587,
               "body_markdown": "http://www.stickpeople.com/projects/python/win-psycopg/\r\n\r\nInstalling Appropriate file from above link fixed my issue.\r\n\r\nMention: Jason Erickson [jerickso@stickpeople.com]. He manages this page fairly well for Windows users.",
               "id": "38477649",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1469007587,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 43672298,
               "is_accepted": false,
               "last_activity_date": 1493355731,
               "body_markdown": "I find a much easier way to do this. Just download binaries packages from website:http://www.lfd.uci.edu/~gohlke/pythonlibs&#39;\r\nFor example:\r\nautopy3\u20110.51.1\u2011cp36\u2011cp36m\u2011win32.whl(cp36 means Python 3.6)\r\nDownload it\r\nAnd install by pip install location of file",
               "id": "43672298",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1493355731,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 44475503,
               "is_accepted": false,
               "last_activity_date": 1497111507,
               "body_markdown": "calling `import setuptools` will monkey patch distutils to force compatibility with Visual Studio. Calling `vcvars32.bat` manually will setup the virtual environment and prevent other common errors the compiler will throw. The file is located at\r\n\r\n&gt;  &quot;C:\\Program Files (x86)\\Microsoft Visual\r\n&gt; Studio\\2017\\Community\\VC\\Auxiliary\\Build\\vcvars32.bat&quot;\r\n\r\n for VS 2017.",
               "id": "44475503",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1497111507,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 47748008,
               "is_accepted": false,
               "last_activity_date": 1512976466,
               "body_markdown": "Below steps fixed this issue for me, I was trying to create setup with cython extension.\r\n\r\n 1. Install Microsoft Visual C++ Compiler for Python 2.7 \r\n 2. The default install location would be @\r\n    C:\\Users\\PC-user\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++\r\n    for Python This might actually fix the issue, test once before proceeding. \r\n 3. If it fails, Check where in VC++\r\n    for python vcvarsall.bat file is located\r\n 4. Open the msvc9compiler.py\r\n    file of distutils package in notepad. \r\n 5. In my box this was @\r\n    C:\\Anaconda2\\Lib\\distutils\\msvc9compiler.py find_vcvarsall function\r\n    in this file, determine the version of VC by printing out version\r\n    argument. For Python 2.7 it&#39;s likely to be 9.0\r\n 6. Now create an\r\n    environment variable VS90COMNTOOLS, Pointing to\r\n    C:\\Users\\PC-user\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++\r\n    for Python\\9.0\\VC\\bin\r\n 7. For some reason distutils expects the\r\n    vcvarsall.bat file to be within VC dir, but VC++ for python tools\r\n    has it in the root of 9.0 To fix this, remove &quot;VC&quot; from the\r\n    path.join (roughly around line 247)\r\n\r\n    &lt;code&gt;\r\n        #productdir = os.path.join(toolsdir, os.pardir, os.pardir, &quot;VC&quot;)\r\n        productdir = os.path.join(toolsdir, os.pardir, os.pardir)\r\n    &lt;/code&gt;\r\n\r\nThe above steps fixed the issue for me.",
               "id": "47748008",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512976466,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat",
         "id": "858127-2278"
      },
      {
         "up_vote_count": "756",
         "path": "2.stack",
         "body_markdown": "Is there a method like `isiterable`? The only solution I have found so far is to call\r\n\r\n    hasattr(myObj, &#39;__iter__&#39;)\r\n\r\nBut I am not sure how fool-proof this is.\r\n\r\n",
         "view_count": "271187",
         "answer_count": "17",
         "tags": "['python', 'iterable']",
         "creation_date": "1261570435",
         "last_edit_date": "1384220303",
         "code_snippet": "['<code>isiterable</code>', \"<code>hasattr(myObj, '__iter__')\\n</code>\", '<code>__getitem__</code>', '<code>iter(myObj)</code>', '<code>isinstance(myObj, dict)</code>', '<code>myObj</code>', '<code>dict</code>', '<code>dict</code>', '<code>__getitem__</code>', '<code>__iter__</code>', \"<code>try:\\n    some_object_iterator = iter(some_object)\\nexcept TypeError, te:\\n    print some_object, 'is not iterable'\\n</code>\", '<code>iter</code>', '<code>__iter__</code>', '<code>__getitem__</code>', \"<code>try:\\n   _ = (e for e in my_object)\\nexcept TypeError:\\n   print my_object, 'is not iterable'\\n</code>\", '<code>collections</code>', '<code>import collections\\n\\nif isinstance(e, collections.Iterable):\\n    # e is iterable\\n</code>', '<code>__getitem__</code>', '<code>[e for e in my_object]</code>', '<code>my_object</code>', '<code>my_object</code>', \"<code>isinstance('', Sequence) == True</code>\", \"<code>isinstance('', Iterable)</code>\", \"<code>hasattr('', '__iter__') == False</code>\", '<code>my_object</code>', '<code>itertools.count()</code>', '<code>hasattr(u\"hello\", \\'__iter__\\')</code>', '<code>True</code>', '<code>try:\\n    iterator = iter(theElement)\\nexcept TypeError:\\n    # not iterable\\nelse:\\n    # iterable\\n\\n# for obj in iterator:\\n#     pass\\n</code>', '<code>import collections\\n\\nif isinstance(theElement, collections.Iterable):\\n    # iterable\\nelse:\\n    # not iterable\\n</code>', '<code>collections.Iterable</code>', '<code>__getitem__</code>', '<code>iter</code>', '<code>iter(x)</code>', '<code>x.__iter__()</code>', '<code>x.__iter__()</code>', '<code>iter()</code>', '<code>x.__getitem__()</code>', '<code>iter</code>', '<code>__iter__</code>', '<code>__getitem__</code>', \"<code>try:\\n    iter(maybe_iterable)\\n    print('iteration will probably work')\\nexcept TypeError:\\n    print('not iterable')\\n</code>\", '<code>for</code>', '<code>o</code>', '<code>iter(o)</code>', '<code>o</code>', '<code>__iter__</code>', '<code>__iter__</code>', '<code>__next__</code>', '<code>next</code>', '<code>o</code>', '<code>__getitem__</code>', '<code>Iterable</code>', '<code>Sequence</code>', '<code>__iter__</code>', '<code>o</code>', '<code>__getitem__</code>', '<code>__iter__</code>', '<code>iter(o)</code>', '<code>o</code>', '<code>IndexError</code>', '<code>StopIteration</code>', '<code>iter</code>', '<code>o</code>', '<code>__iter__</code>', '<code>iter</code>', '<code>__iter__</code>', '<code>__getitem__</code>', '<code>__iter__</code>', '<code>o</code>', '<code>__iter__</code>', '<code>__getitem__</code>', '<code>iter(o)</code>', '<code>__iter__</code>', '<code>__iter__</code>', '<code>for</code>', '<code>for</code>', '<code>for item in o</code>', '<code>o</code>', '<code>iter(o)</code>', '<code>__next__</code>', '<code>next</code>', '<code>__iter__</code>', '<code>__iter__</code>', '<code>return self</code>', '<code>next</code>', '<code>StopIteration</code>', \"<code>import random\\n\\nclass DemoIterable(object):\\n    def __iter__(self):\\n        print('__iter__ called')\\n        return DemoIterator()\\n\\nclass DemoIterator(object):\\n    def __iter__(self):\\n        return self\\n\\n    def __next__(self):\\n        print('__next__ called')\\n        r = random.randint(1, 10)\\n        if r == 5:\\n            print('raising StopIteration')\\n            raise StopIteration\\n        return r\\n</code>\", '<code>DemoIterable</code>', '<code>&gt;&gt;&gt; di = DemoIterable()\\n&gt;&gt;&gt; for x in di:\\n...     print(x)\\n...\\n__iter__ called\\n__next__ called\\n9\\n__next__ called\\n8\\n__next__ called\\n10\\n__next__ called\\n3\\n__next__ called\\n10\\n__next__ called\\nraising StopIteration\\n</code>', '<code>class BasicIterable(object):\\n    def __getitem__(self, item):\\n        if item == 3:\\n            raise IndexError\\n        return item\\n</code>', '<code>iter</code>', '<code>BasicIterable</code>', '<code>BasicIterable</code>', '<code>__getitem__</code>', '<code>&gt;&gt;&gt; b = BasicIterable()\\n&gt;&gt;&gt; iter(b)\\n&lt;iterator object at 0x7f1ab216e320&gt;\\n</code>', '<code>b</code>', '<code>__iter__</code>', '<code>Iterable</code>', '<code>Sequence</code>', \"<code>&gt;&gt;&gt; from collections import Iterable, Sequence\\n&gt;&gt;&gt; hasattr(b, '__iter__')\\nFalse\\n&gt;&gt;&gt; isinstance(b, Iterable)\\nFalse\\n&gt;&gt;&gt; isinstance(b, Sequence)\\nFalse\\n</code>\", '<code>iter</code>', '<code>TypeError</code>', '<code>x</code>', '<code>iter(x)</code>', '<code>TypeError</code>', '<code>isinstance(x, abc.Iterable)</code>', '<code>iter(x)</code>', '<code>__getitem__</code>', '<code>Iterable</code>', '<code>__getitem__</code>', '<code>__iter__</code>', '<code>BasicIterable</code>', '<code>IndexError</code>', '<code>__getitem__</code>', '<code>item</code>', '<code>__getitem__(self, item)</code>', '<code>iter</code>', '<code>&gt;&gt;&gt; b = BasicIterable()\\n&gt;&gt;&gt; it = iter(b)\\n&gt;&gt;&gt; next(it)\\n0\\n&gt;&gt;&gt; next(it)\\n1\\n&gt;&gt;&gt; next(it)\\n2\\n&gt;&gt;&gt; next(it)\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nStopIteration\\n</code>', '<code>StopIteration</code>', '<code>IndexError</code>', '<code>item == 3</code>', '<code>BasicIterable</code>', '<code>for</code>', '<code>&gt;&gt;&gt; for x in b:\\n...     print(x)\\n...\\n0\\n1\\n2\\n</code>', '<code>iter</code>', '<code>WrappedDict</code>', '<code>dict</code>', '<code>__iter__</code>', '<code>class WrappedDict(object): # note: no inheritance from dict!\\n    def __init__(self, dic):\\n        self._dict = dic\\n\\n    def __getitem__(self, item):\\n        try:\\n            return self._dict[item] # delegate to dict.__getitem__\\n        except KeyError:\\n            raise IndexError\\n</code>', '<code>__getitem__</code>', '<code>dict.__getitem__</code>', \"<code>&gt;&gt;&gt; w = WrappedDict({-1: 'not printed',\\n...                   0: 'hi', 1: 'StackOverflow', 2: '!',\\n...                   4: 'not printed', \\n...                   'x': 'not printed'})\\n&gt;&gt;&gt; for x in w:\\n...     print(x)\\n... \\nhi\\nStackOverflow\\n!\\n</code>\", '<code>iter</code>', '<code>__iter__</code>', '<code>iter(o)</code>', '<code>o</code>', '<code>iter</code>', '<code>__iter__</code>', '<code>__next__</code>', '<code>next</code>', '<code>__iter__</code>', '<code>iter</code>', '<code>__getitem__</code>', '<code>class FailIterIterable(object):\\n    def __iter__(self):\\n        return object() # not an iterator\\n\\nclass FailGetitemIterable(object):\\n    def __getitem__(self, item):\\n        raise Exception\\n</code>', '<code>FailIterIterable</code>', '<code>FailGetItemIterable</code>', '<code>__next__</code>', '<code>&gt;&gt;&gt; fii = FailIterIterable()\\n&gt;&gt;&gt; iter(fii)\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nTypeError: iter() returned non-iterator of type \\'object\\'\\n&gt;&gt;&gt;\\n&gt;&gt;&gt; fgi = FailGetitemIterable()\\n&gt;&gt;&gt; it = iter(fgi)\\n&gt;&gt;&gt; next(it)\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\n  File \"/path/iterdemo.py\", line 42, in __getitem__\\n    raise Exception\\nException\\n</code>', '<code>__iter__</code>', '<code>__iter__</code>', '<code>__getitem__</code>', '<code>iter</code>', '<code>__iter__</code>', \"<code>class IterWinsDemo(object):\\n    def __iter__(self):\\n        return iter(['__iter__', 'wins'])\\n\\n    def __getitem__(self, item):\\n        return ['__getitem__', 'wins'][item]\\n</code>\", '<code>&gt;&gt;&gt; iwd = IterWinsDemo()\\n&gt;&gt;&gt; for x in iwd:\\n...     print(x)\\n...\\n__iter__\\nwins\\n</code>', '<code>__iter__</code>', '<code>list</code>', '<code>__iter__</code>', '<code>__getitem__</code>', '<code>class WrappedList(object): # note: no inheritance from list!\\n    def __init__(self, lst):\\n        self._list = lst\\n\\n    def __getitem__(self, item):\\n        return self._list[item]\\n</code>', '<code>__getitem__</code>', '<code>list.__getitem__</code>', \"<code>&gt;&gt;&gt; wl = WrappedList(['A', 'B', 'C'])\\n&gt;&gt;&gt; for x in wl:\\n...     print(x)\\n... \\nA\\nB\\nC\\n</code>\", '<code>__iter__</code>', '<code>__iter__</code>', '<code>isinstance(o, collections.Iterable)</code>', '<code>True</code>', '<code>__iter__</code>', '<code>iter</code>', '<code>TypeError</code>', '<code>__getitem__</code>', '<code>__getitem__</code>', '<code>__iter__</code>', '<code>__getitem__</code>', '<code>__iter__</code>', '<code>next</code>', '<code>try:\\n  #treat object as iterable\\nexcept TypeError, e:\\n  #object is not actually iterable\\n</code>', '<code>TypeError</code>', \"<code>hasattr(obj, '__contains__')</code>\", '<code>in</code>', '<code>__iter__</code>', '<code>from collections import Iterable\\n\\nclass MyObject(object):\\n    pass\\n\\nmo = MyObject()\\nprint isinstance(mo, Iterable)\\nIterable.register(MyObject)\\nprint isinstance(mo, Iterable)\\n\\nprint isinstance(\"abc\", Iterable)\\n</code>', '<code>from collections import Iterable\\nfrom traceback import print_exc\\n\\ndef check_and_raise(x):\\n    if not isinstance(x, Iterable):\\n        raise TypeError, \"%s is not iterable\" % x\\n    else:\\n        for i in x:\\n            print i\\n\\ndef just_iter(x):\\n    for i in x:\\n        print i\\n\\n\\nclass NotIterable(object):\\n    pass\\n\\nif __name__ == \"__main__\":\\n    try:\\n        check_and_raise(5)\\n    except:\\n        print_exc()\\n        print\\n\\n    try:\\n        just_iter(5)\\n    except:\\n        print_exc()\\n        print\\n\\n    try:\\n        Iterable.register(NotIterable)\\n        ni = NotIterable()\\n        check_and_raise(ni)\\n    except:\\n        print_exc()\\n        print\\n</code>', '<code>__iter__</code>', '<code>except:</code>', \"<code>isiterable = lambda obj: isinstance(obj, basestring) \\\\\\n    or getattr(obj, '__iter__', False)\\n</code>\", '<code>def iterable(a):\\n    try:\\n        (x for x in a)\\n        return True\\n    except TypeError:\\n        return False\\n</code>', '<code>iterable(itertools.repeat(0))</code>', '<code>(x for x in a)</code>', '<code>(x for x in a)</code>', '<code>iterator = iter(a)</code>', '<code>for _ in a: break</code>', '<code>list</code>', '<code>str</code>', '<code>tuple</code>', '<code>dict</code>', '<code>file</code>', '<code>__iter__()</code>', '<code>__getitem__()</code>', '<code>try:\\n    for i in object_in_question:\\n        do_something\\nexcept TypeError:\\n    do_something_for_non_iterable\\n</code>', '<code>hasattr(object_in_question, \"__iter__\") or hasattr(object_in_question, \"__getitem__\")</code>', '<code>str</code>', '<code>__iter__</code>', '<code>generator</code>', '<code>__getitem__</code>', '<code>iterable</code>', '<code>import collections\\n\\ndef iterable(obj):\\n    return isinstance(obj, collections.Iterable):\\n</code>', '<code>if iterable(obj):\\n    # act on iterable\\nelse:\\n    # not iterable\\n</code>', '<code>callable</code>', '<code>numpy import iterable</code>', '<code>def iterable(obj):\\n    try: iter(obj)\\n    except: return False\\n    return True\\n</code>', '<code>if x: return True</code>', '<code>else: return False</code>', '<code>x</code>', '<code>return x</code>', '<code>return isinstance(\u2026)</code>', '<code>if</code>', '<code>from pandas.util.testing import isiterable\\n</code>', '<code>class A(object):\\n    def __getitem__(self, item):\\n        return something\\n\\nclass B(object):\\n    def __iter__(self):\\n        # Return a compliant iterator. Just an example\\n        return iter([])\\n\\nclass C(object):\\n    def __iter__(self):\\n        # Return crap\\n        return 1\\n\\nclass D(object): pass\\n\\ndef iterable(obj):\\n    try:\\n        iter(obj)\\n        return True\\n    except:\\n        return False\\n\\nassert iterable(A())\\nassert iterable(B())\\nassert iterable(C())\\nassert not iterable(D())\\n</code>', '<code>__iter__</code>', '<code>callable</code>', '<code>AttributeError</code>', '<code>__call__</code>', '<code>TypeError</code>', '<code>isiterable</code>', '<code>True</code>', '<code>False</code>', '<code>def isiterable(object_):\\n    return hasattr(type(object_), \"__iter__\")\\n</code>', '<code>fruits = (\"apple\", \"banana\", \"peach\")\\nisiterable(fruits) # returns True\\n\\nnum = 345\\nisiterable(num) # returns False\\n\\nisiterable(str) # returns False because str type is type class and it\\'s not iterable.\\n\\nhello = \"hello dude !\"\\nisiterable(hello) # returns True because as you know string objects are iterable\\n</code>', '<code>def is_iterable(x):\\n    try:\\n        0 in x\\n    except TypeError:\\n        return False\\n    else:\\n        return True\\n</code>', '<code>import numpy\\n\\nclass Yes:\\n    def __iter__(self):\\n        yield 1;\\n        yield 2;\\n        yield 3;\\n\\nclass No:\\n    pass\\n\\nclass Nope:\\n    def __iter__(self):\\n        return \\'nonsense\\'\\n\\nassert is_iterable(Yes())\\nassert is_iterable(range(3))\\nassert is_iterable((1,2,3))   # tuple\\nassert is_iterable([1,2,3])   # list\\nassert is_iterable({1,2,3})   # set\\nassert is_iterable({1:\\'one\\', 2:\\'two\\', 3:\\'three\\'})   # dictionary\\nassert is_iterable(numpy.array([1,2,3]))\\nassert is_iterable(bytearray(\"not really a string\", \\'utf-8\\'))\\n\\nassert not is_iterable(No())\\nassert not is_iterable(Nope())\\nassert not is_iterable(\"string\")\\nassert not is_iterable(42)\\nassert not is_iterable(True)\\nassert not is_iterable(None)\\n</code>', '<code>import collections\\nimport numpy\\n\\nassert isinstance(\"string\", collections.Iterable)\\nassert isinstance(\"string\", collections.Sequence)\\nassert numpy.iterable(\"string\")\\nassert iter(\"string\")\\nassert hasattr(\"string\", \\'__getitem__\\')\\n</code>', '<code>bytes</code>', '<code>bytearray</code>', '<code>bytes</code>', '<code>True == is_iterable(b\"string\") == is_iterable(\"string\".encode(\\'utf-8\\'))</code>', '<code>bytearray</code>', '<code>True == is_iterable(bytearray(b\"abc\"))</code>', \"<code>hasattr(x, '__iter__')</code>\", \"<code>''</code>\", \"<code>b''</code>\", \"<code>u''</code>\", '<code>__iter__</code>', '<code>temp= [1,2,3,4]\\nhelp(temp)\\n</code>', '<code>from typing import Iterable\\n\\nmy_list = []\\n\\nif isinstance(my_item, Iterable):\\n    print(True)\\n</code>']",
         "title": "In Python, how do I determine if an object is iterable?",
         "_childDocuments_": [
            {
               "up_vote_count": 607,
               "answer_id": 1952481,
               "last_activity_date": 1506983002,
               "path": "3.stack.answer",
               "body_markdown": "1. Checking for ``__iter__`` works on sequence types, but it would fail on e.g. strings **in Python 2**. I would like to know the right answer too, until then, here is one possibility (which would work on strings, too):\r\n\r\n        try:\r\n            some_object_iterator = iter(some_object)\r\n        except TypeError, te:\r\n            print some_object, &#39;is not iterable&#39;\r\n\r\n The `iter` built-in checks for the `__iter__` method or in the case of strings the `__getitem__` method.\r\n\r\n2. Another general pythonic approach is to assume an iterable, then fail gracefully if it does not work on the given object. The Python glossary:\r\n\r\n &gt; Pythonic programming style that determines an object&#39;s type by inspection of its method or attribute signature rather than by explicit relationship to some type object (&quot;If it looks like a **duck** and quacks like a **duck**, it must be a **duck**.&quot;) By emphasizing interfaces rather than specific types, well-designed code improves its flexibility by allowing polymorphic substitution. Duck-typing avoids tests using type() or isinstance(). **Instead, it typically employs the EAFP (Easier to Ask Forgiveness than Permission) style of programming.**\r\n\r\n &gt; ...\r\n\r\n &gt;     try:\r\n &gt;        _ = (e for e in my_object)\r\n &gt;     except TypeError:\r\n &gt;        print my_object, &#39;is not iterable&#39;\r\n\r\n3. The [`collections`][1] module provides some abstract base classes, which allow to ask classes or instances if they provide particular functionality, for example:\r\n\r\n        import collections\r\n\r\n        if isinstance(e, collections.Iterable):\r\n            # e is iterable\r\n\r\n However, this does not check for classes that are iterable through `__getitem__`.\r\n\r\n  [1]: http://docs.python.org/library/collections.html#abcs-abstract-base-classes",
               "tags": [],
               "creation_date": 1261570603,
               "last_edit_date": 1506983002,
               "is_accepted": true,
               "id": "1952481",
               "down_vote_count": 4,
               "score": 603
            },
            {
               "up_vote_count": 28,
               "answer_id": 1952485,
               "last_activity_date": 1485536462,
               "path": "3.stack.answer",
               "body_markdown": "This isn&#39;t sufficient: the object returned by `__iter__` must implement the iteration protocol (i.e. `next` method). See the relevant section in the [documentation][1].\r\n\r\nIn Python, a good practice is to &quot;try and see&quot; instead of &quot;checking&quot;.\r\n\r\n\r\n  [1]: http://docs.python.org/library/stdtypes.html#iterator-types",
               "tags": [],
               "creation_date": 1261570659,
               "last_edit_date": 1485536462,
               "is_accepted": false,
               "id": "1952485",
               "down_vote_count": 0,
               "score": 28
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 1952507,
               "is_accepted": false,
               "last_activity_date": 1261570872,
               "body_markdown": "You could try this:\r\n\r\n    def iterable(a):\r\n        try:\r\n            (x for x in a)\r\n            return True\r\n        except TypeError:\r\n            return False\r\n\r\nIf we can make a generator that iterates over it (but never use the generator so it doesn&#39;t take up space), it&#39;s iterable. Seems like a &quot;duh&quot; kind of thing. Why do you need to determine if a variable is iterable in the first place?",
               "id": "1952507",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1261570872,
               "score": 10
            },
            {
               "up_vote_count": 19,
               "answer_id": 1952508,
               "last_activity_date": 1261571280,
               "path": "3.stack.answer",
               "body_markdown": "    try:\r\n      #treat object as iterable\r\n    except TypeError, e:\r\n      #object is not actually iterable\r\n\r\nDon&#39;t run checks to see &lt;s&gt;if your duck really is a duck&lt;/s&gt; to see if it is iterable or not, treat it as if it was and complain if it wasn&#39;t.",
               "tags": [],
               "creation_date": 1261570895,
               "last_edit_date": 1261571280,
               "is_accepted": false,
               "id": "1952508",
               "down_vote_count": 2,
               "score": 17
            },
            {
               "up_vote_count": 485,
               "answer_id": 1952655,
               "last_activity_date": 1506983264,
               "path": "3.stack.answer",
               "body_markdown": "Duck typing\r\n-----------\r\n\r\n    try:\r\n        iterator = iter(theElement)\r\n    except TypeError:\r\n        # not iterable\r\n    else:\r\n        # iterable\r\n\r\n    # for obj in iterator:\r\n    #     pass\r\n\r\nType checking\r\n-------------\r\n\r\nUse the [Abstract Base Classes][abc]. They need at least Python 2.6 and work only for new-style classes.\r\n\r\n    import collections\r\n\r\n    if isinstance(theElement, collections.Iterable):\r\n        # iterable\r\n    else:\r\n        # not iterable\r\n\r\nHowever, `collections.Iterable` does not check for `__getitem__`, so `iter` is more reliable.\r\n\r\n  [abc]: http://docs.python.org/library/abc.html",
               "tags": [],
               "creation_date": 1261572818,
               "last_edit_date": 1506983264,
               "is_accepted": false,
               "id": "1952655",
               "down_vote_count": 1,
               "score": 484
            },
            {
               "up_vote_count": 11,
               "answer_id": 1952735,
               "last_activity_date": 1470865975,
               "path": "3.stack.answer",
               "body_markdown": "I found a nice solution [here][1]:\r\n\r\n    isiterable = lambda obj: isinstance(obj, basestring) \\\r\n        or getattr(obj, &#39;__iter__&#39;, False)\r\n\r\n  [1]: http://bytes.com/topic/python/answers/514838-how-test-if-object-sequence-iterable\r\n",
               "tags": [],
               "creation_date": 1261574021,
               "last_edit_date": 1470865975,
               "is_accepted": false,
               "id": "1952735",
               "down_vote_count": 0,
               "score": 11
            },
            {
               "up_vote_count": 13,
               "answer_id": 1952851,
               "last_activity_date": 1470866126,
               "path": "3.stack.answer",
               "body_markdown": "In Python &lt;= 2.5, you can&#39;t and shouldn&#39;t - iterable was an &quot;informal&quot; interface.\r\n\r\nBut since Python 2.6 and 3.0 you can leverage the new ABC (abstract base class) infrastructure along with some builtin ABCs which are available in the collections module:\r\n\r\n    from collections import Iterable\r\n\r\n    class MyObject(object):\r\n        pass\r\n\r\n    mo = MyObject()\r\n    print isinstance(mo, Iterable)\r\n    Iterable.register(MyObject)\r\n    print isinstance(mo, Iterable)\r\n\r\n    print isinstance(&quot;abc&quot;, Iterable)\r\n\r\nNow, whether this is desirable or actually works, is just a matter of conventions. As you can see, you *can* register a non-iterable object as Iterable - and it will raise an exception at runtime. Hence, isinstance acquires a &quot;new&quot; meaning - it just checks for &quot;declared&quot; type compatibility, which is a good way to go in Python.\r\n\r\nOn the other hand, if your object does not satisfy the interface you need, what are you going to do? Take the following example:\r\n\r\n    from collections import Iterable\r\n    from traceback import print_exc\r\n\r\n    def check_and_raise(x):\r\n        if not isinstance(x, Iterable):\r\n            raise TypeError, &quot;%s is not iterable&quot; % x\r\n        else:\r\n            for i in x:\r\n                print i\r\n\r\n    def just_iter(x):\r\n        for i in x:\r\n            print i\r\n\r\n\r\n    class NotIterable(object):\r\n        pass\r\n\r\n    if __name__ == &quot;__main__&quot;:\r\n        try:\r\n            check_and_raise(5)\r\n        except:\r\n            print_exc()\r\n            print\r\n\r\n        try:\r\n            just_iter(5)\r\n        except:\r\n            print_exc()\r\n            print\r\n\r\n        try:\r\n            Iterable.register(NotIterable)\r\n            ni = NotIterable()\r\n            check_and_raise(ni)\r\n        except:\r\n            print_exc()\r\n            print\r\n\r\nIf the object doesn&#39;t satisfy what you expect, you just throw a TypeError, but if the proper ABC has been registered, your check is unuseful. On the contrary, if the `__iter__` method is available Python will automatically recognize object of that class as being Iterable.\r\n\r\nSo, if you just expect an iterable, iterate over it and forget it. On the other hand, if you need to do different things depending on input type, you might find the ABC infrastructure pretty useful.\r\n",
               "tags": [],
               "creation_date": 1261575347,
               "last_edit_date": 1470866126,
               "is_accepted": false,
               "id": "1952851",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 28,
               "answer_id": 4136141,
               "last_activity_date": 1505749620,
               "path": "3.stack.answer",
               "body_markdown": "The best solution I&#39;ve found so far:\r\n\r\n`hasattr(obj, &#39;__contains__&#39;)`\r\n\r\nwhich basically checks if the object implements the `in` operator.\r\n\r\n**Advantages** (none of the other solutions has all three):\r\n\r\n - it is an expression (works as a **lambda**, as opposed to the **try...except** variant)\r\n - it is (should be) implemented by all iterables, including **strings** (as opposed to `__iter__`)\r\n - works on any Python &gt;= 2.5\r\n\r\nNotes: \r\n\r\n - the Python philosophy of &quot;ask for forgiveness, not permission&quot; doesn&#39;t work well when e.g. in a list you have both iterables and non-iterables and you need to treat each element differently according to it&#39;s type (treating iterables on try and non-iterables on except *would* work, but it would look butt-ugly and misleading)\r\n - solutions to this problem which attempt to actually iterate over the object (e.g. [x for x in obj]) to check if it&#39;s iterable may induce significant performance penalties for large iterables (especially if you just need the first few elements of the iterable, for example) and should be avoided\r\n",
               "tags": [],
               "creation_date": 1289320845,
               "last_edit_date": 1505749620,
               "is_accepted": false,
               "id": "4136141",
               "down_vote_count": 13,
               "score": 15
            },
            {
               "up_vote_count": 6,
               "answer_id": 10664278,
               "last_activity_date": 1416910490,
               "path": "3.stack.answer",
               "body_markdown": "According to the [Python 2 Glossary][1], iterables are\r\n\r\n&gt; all sequence types (such as `list`, `str`, and `tuple`) and some non-sequence types like `dict` and `file` and objects of any classes you define with an `__iter__()` or `__getitem__()` method. Iterables can be used in a for loop and in many other places where a sequence is needed (zip(), map(), ...). When an iterable object is passed as an argument to the built-in function iter(), it returns an iterator for the object.\r\n\r\nOf course, given the general coding style for Python based on the fact that it&#39;s \u201cEasier to ask for forgiveness than permission.\u201d, the general expectation is to use\r\n\r\n    try:\r\n        for i in object_in_question:\r\n            do_something\r\n    except TypeError:\r\n        do_something_for_non_iterable\r\n\r\nBut if you need to check it explicitly, you can test for an iterable by `hasattr(object_in_question, &quot;__iter__&quot;) or hasattr(object_in_question, &quot;__getitem__&quot;)`. You need to check for both, because `str`s don&#39;t have an `__iter__` method (at least not in Python 2, in Python 3 they do) and because `generator` objects don&#39;t have a `__getitem__` method.\r\n\r\n\r\n  [1]: https://docs.python.org/2/glossary.html#term-iterable",
               "tags": [],
               "creation_date": 1337422668,
               "last_edit_date": 1416910490,
               "is_accepted": false,
               "id": "10664278",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "up_vote_count": 5,
               "answer_id": 15598574,
               "last_activity_date": 1390751574,
               "path": "3.stack.answer",
               "body_markdown": "I often find convenient, inside my scripts, to define an `iterable` function.\r\n(Now incorporates Alfe&#39;s suggested simplification):\r\n\r\n    import collections\r\n\r\n    def iterable(obj):\r\n        return isinstance(obj, collections.Iterable):\r\n\r\n\r\nso you can test if any object is iterable in the very readable form\r\n\r\n    if iterable(obj):\r\n        # act on iterable\r\n    else:\r\n        # not iterable\r\n\r\nas you would do with the`callable` function\r\n\r\nEDIT: if you have numpy installed, you can simply do: from `numpy import iterable`, \r\nwhich is simply something like\r\n\r\n    def iterable(obj):\r\n        try: iter(obj)\r\n        except: return False\r\n        return True\r\nIf you do not have numpy, you can simply implement this code, or the one above.",
               "tags": [],
               "creation_date": 1364128860,
               "last_edit_date": 1390751574,
               "is_accepted": false,
               "id": "15598574",
               "down_vote_count": 1,
               "score": 4
            },
            {
               "up_vote_count": 1,
               "answer_id": 33897506,
               "last_activity_date": 1470866235,
               "path": "3.stack.answer",
               "body_markdown": "The easiest way, respecting the Python&#39;s [duck typing][1], is to catch the error (Python knows perfectly what does it expect from an object to become an iterator):\r\n\r\n    class A(object):\r\n        def __getitem__(self, item):\r\n            return something\r\n\r\n    class B(object):\r\n        def __iter__(self):\r\n            # Return a compliant iterator. Just an example\r\n            return iter([])\r\n\r\n    class C(object):\r\n        def __iter__(self):\r\n            # Return crap\r\n            return 1\r\n\r\n    class D(object): pass\r\n\r\n    def iterable(obj):\r\n        try:\r\n            iter(obj)\r\n            return True\r\n        except:\r\n            return False\r\n\r\n    assert iterable(A())\r\n    assert iterable(B())\r\n    assert iterable(C())\r\n    assert not iterable(D())\r\n\r\n**Notes**:\r\n\r\n 1. It is irrelevant the distinction whether the object is not iterable, or a buggy `__iter__` has been implemented, if the exception type is the same: anyway you will not be able to iterate the object.\r\n 2. I think I understand your concern: How does `callable` exists as a check if I could also rely on duck typing to raise an `AttributeError` if `__call__` is not defined for my object, but that&#39;s not the case for iterable checking?\r\n\r\n    I don&#39;t know the answer, but you can either implement the function I (and other users) gave, or just catch the exception in your code (your implementation in that part will be like the function I wrote - just ensure you isolate the iterator creation from the rest of the code so you can capture the exception and distinguish it from another `TypeError`.\r\n\r\n  [1]: https://en.wikipedia.org/wiki/Duck_typing\r\n",
               "tags": [],
               "creation_date": 1448378740,
               "last_edit_date": 1470866235,
               "is_accepted": false,
               "id": "33897506",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 1,
               "answer_id": 36154791,
               "last_activity_date": 1506369737,
               "path": "3.stack.answer",
               "body_markdown": "    def is_iterable(x):\r\n        try:\r\n            0 in x\r\n        except TypeError:\r\n            return False\r\n        else:\r\n            return True\r\n\r\nThis will say yes to all manner of iterable objects, but it will **say no to strings in Python 2**. (That&#39;s what I want for example when a recursive function could take a string or a container of strings.  In that situation, [asking forgiveness](https://stackoverflow.com/a/1952481/673991) may lead to obfuscode, and it&#39;s better to ask permission first.)\r\n\r\n    import numpy\r\n\r\n    class Yes:\r\n        def __iter__(self):\r\n            yield 1;\r\n            yield 2;\r\n            yield 3;\r\n\r\n    class No:\r\n        pass\r\n\r\n    class Nope:\r\n        def __iter__(self):\r\n            return &#39;nonsense&#39;\r\n\r\n    assert is_iterable(Yes())\r\n    assert is_iterable(range(3))\r\n    assert is_iterable((1,2,3))   # tuple\r\n    assert is_iterable([1,2,3])   # list\r\n    assert is_iterable({1,2,3})   # set\r\n    assert is_iterable({1:&#39;one&#39;, 2:&#39;two&#39;, 3:&#39;three&#39;})   # dictionary\r\n    assert is_iterable(numpy.array([1,2,3]))\r\n    assert is_iterable(bytearray(&quot;not really a string&quot;, &#39;utf-8&#39;))\r\n\r\n    assert not is_iterable(No())\r\n    assert not is_iterable(Nope())\r\n    assert not is_iterable(&quot;string&quot;)\r\n    assert not is_iterable(42)\r\n    assert not is_iterable(True)\r\n    assert not is_iterable(None)\r\n\r\nMany other strategies here will say yes to strings. Use them if that&#39;s what you want.\r\n\r\n    import collections\r\n    import numpy\r\n\r\n    assert isinstance(&quot;string&quot;, collections.Iterable)\r\n    assert isinstance(&quot;string&quot;, collections.Sequence)\r\n    assert numpy.iterable(&quot;string&quot;)\r\n    assert iter(&quot;string&quot;)\r\n    assert hasattr(&quot;string&quot;, &#39;__getitem__&#39;)\r\n\r\nNote: is_iterable() will say yes to strings of type `bytes` and `bytearray`.\r\n\r\n - `bytes` objects in Python 3 are iterable `True == is_iterable(b&quot;string&quot;) == is_iterable(&quot;string&quot;.encode(&#39;utf-8&#39;))` There is no such type in Python 2.\r\n - `bytearray` objects in Python 2 and 3 are iterable `True == is_iterable(bytearray(b&quot;abc&quot;))`\r\n\r\nThe O.P. `hasattr(x, &#39;__iter__&#39;)` approach will say yes to strings in Python 3 and no in Python 2 (no matter whether `&#39;&#39;` or `b&#39;&#39;` or `u&#39;&#39;`). Thanks to @LuisMasuelli for noticing it will also let you down on a buggy `__iter__`.\r\n",
               "tags": [],
               "creation_date": 1458650774,
               "last_edit_date": 1506369737,
               "is_accepted": false,
               "id": "36154791",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 39,
               "answer_id": 36407550,
               "is_accepted": false,
               "last_activity_date": 1459785841,
               "body_markdown": "I&#39;d like to shed a little bit more light on the interplay of `iter`, `__iter__` and `__getitem__` and what happens behind the curtains. Armed with that knowledge, you will be able to understand why the best you can do is\r\n\r\n    try:\r\n        iter(maybe_iterable)\r\n        print(&#39;iteration will probably work&#39;)\r\n    except TypeError:\r\n        print(&#39;not iterable&#39;)\r\n\r\nI will list the facts first and then follow up with a quick reminder of what happens when you employ a `for` loop in python, followed by a discussion to illustrate the facts.\r\n\r\n#Facts\r\n\r\n1. You can get an iterator from any object `o` by calling `iter(o)` if at least one of the following conditions holds true: &lt;br&gt;&lt;br&gt;a) `o` has an `__iter__` method which returns an iterator object. An iterator is any object with an `__iter__` and a `__next__` (Python 2: `next`) method. &lt;br&gt;&lt;br&gt;b) `o` has a `__getitem__` method.\r\n\r\n2. Checking for an instance of `Iterable` or `Sequence`, or checking for the\r\nattribute `__iter__` is not enough.\r\n\r\n3. If an object `o` implements only `__getitem__`, but not `__iter__`, `iter(o)` will construct\r\nan iterator that tries to fetch items from `o` by integer index, starting at index 0. The iterator will catch any `IndexError` (but no other errors) that is raised and then raises `StopIteration` itself.\r\n\r\n4. In the most general sense, there&#39;s no way to check whether the iterator returned by `iter` is sane other than to try it out.\r\n\r\n5. If an object `o` implements `__iter__`, the `iter` function will make sure\r\nthat the object returned by `__iter__` is an iterator. There is no sanity check\r\nif an object only implements `__getitem__`.\r\n\r\n6. `__iter__` wins. If an object `o` implements both `__iter__` and `__getitem__`, `iter(o)` will call `__iter__`.\r\n\r\n7. If you want to make your own objects iterable, always implement the `__iter__` method.\r\n\r\n#`for` loops\r\n\r\nIn order to follow along, you need an understanding of what happens when you employ a `for` loop in Python. Feel free to skip right to the next section if you already know.\r\n\r\nWhen you use `for item in o` for some iterable object `o`, Python calls `iter(o)` and expects an iterator object as the return value. An iterator is any object which implements a `__next__` (or `next` in Python 2) method and an `__iter__` method. \r\n\r\nBy convention, the `__iter__` method of an iterator should return the object itself (i.e. `return self`). Python then calls `next` on the iterator until `StopIteration` is raised. All of this happens implicitly, but the following demonstration makes it visible:\r\n\r\n    import random\r\n\r\n    class DemoIterable(object):\r\n        def __iter__(self):\r\n            print(&#39;__iter__ called&#39;)\r\n            return DemoIterator()\r\n\r\n    class DemoIterator(object):\r\n        def __iter__(self):\r\n            return self\r\n\r\n        def __next__(self):\r\n            print(&#39;__next__ called&#39;)\r\n            r = random.randint(1, 10)\r\n            if r == 5:\r\n                print(&#39;raising StopIteration&#39;)\r\n                raise StopIteration\r\n            return r\r\n\r\nIteration over a `DemoIterable`:\r\n\r\n    &gt;&gt;&gt; di = DemoIterable()\r\n    &gt;&gt;&gt; for x in di:\r\n    ...     print(x)\r\n    ...\r\n    __iter__ called\r\n    __next__ called\r\n    9\r\n    __next__ called\r\n    8\r\n    __next__ called\r\n    10\r\n    __next__ called\r\n    3\r\n    __next__ called\r\n    10\r\n    __next__ called\r\n    raising StopIteration\r\n\r\n\r\n#Discussion and illustrations\r\n\r\n**On point 1 and 2: getting an iterator and unreliable checks**\r\n\r\nConsider the following class:\r\n\r\n    class BasicIterable(object):\r\n        def __getitem__(self, item):\r\n            if item == 3:\r\n                raise IndexError\r\n            return item\r\n\r\nCalling `iter` with an instance of `BasicIterable` will return an iterator without any problems because `BasicIterable` implements `__getitem__`.\r\n\r\n    &gt;&gt;&gt; b = BasicIterable()\r\n    &gt;&gt;&gt; iter(b)\r\n    &lt;iterator object at 0x7f1ab216e320&gt;\r\n\r\nHowever, it is important to note that `b` does not have the `__iter__` attribute and is not considered an instance of `Iterable` or `Sequence`:\r\n\r\n    &gt;&gt;&gt; from collections import Iterable, Sequence\r\n    &gt;&gt;&gt; hasattr(b, &#39;__iter__&#39;)\r\n    False\r\n    &gt;&gt;&gt; isinstance(b, Iterable)\r\n    False\r\n    &gt;&gt;&gt; isinstance(b, Sequence)\r\n    False\r\n\r\nThis is why [Fluent Python](http://shop.oreilly.com/product/0636920032519.do) by Luciano Ramalho recommends calling `iter` and handling the potential `TypeError` as the most accurate way to check whether an object is iterable. Quoting directly from the book:\r\n\r\n&gt;As of Python 3.4, the most accurate way to check whether an object `x` is iterable is to call `iter(x)` and handle a `TypeError` exception if it isn\u2019t. This is more accurate than using `isinstance(x, abc.Iterable)` , because `iter(x)` also considers the legacy `__getitem__` method, while the `Iterable` ABC does not.\r\n\r\n**On point 3: Iterating over objects which only provide `__getitem__`, but not `__iter__`**\r\n\r\nIterating over an instance of `BasicIterable` works as expected: Python\r\nconstructs an iterator that tries to fetch items by index, starting at zero, until an `IndexError` is raised. The demo object&#39;s `__getitem__` method simply returns the `item` which was supplied as the argument to `__getitem__(self, item)` by the iterator returned by `iter`.\r\n\r\n    &gt;&gt;&gt; b = BasicIterable()\r\n    &gt;&gt;&gt; it = iter(b)\r\n    &gt;&gt;&gt; next(it)\r\n    0\r\n    &gt;&gt;&gt; next(it)\r\n    1\r\n    &gt;&gt;&gt; next(it)\r\n    2\r\n    &gt;&gt;&gt; next(it)\r\n    Traceback (most recent call last):\r\n      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\n    StopIteration\r\n\r\nNote that the iterator raises `StopIteration` when it cannot return the next item and that the `IndexError` which is raised for `item == 3` is handled internally. This is why looping over a `BasicIterable` with a `for` loop works as expected:\r\n\r\n    &gt;&gt;&gt; for x in b:\r\n    ...     print(x)\r\n    ...\r\n    0\r\n    1\r\n    2\r\n\r\nHere&#39;s another example in order to drive home the concept of how the iterator returned by `iter` tries to access items by index. `WrappedDict` does not inherit from `dict`, which means instances won&#39;t have an `__iter__` method.\r\n\r\n    class WrappedDict(object): # note: no inheritance from dict!\r\n        def __init__(self, dic):\r\n            self._dict = dic\r\n\r\n        def __getitem__(self, item):\r\n            try:\r\n                return self._dict[item] # delegate to dict.__getitem__\r\n            except KeyError:\r\n                raise IndexError\r\n\r\nNote that calls to `__getitem__` are delegated to `dict.__getitem__` for which the square bracket notation is simply a shorthand.\r\n\r\n    &gt;&gt;&gt; w = WrappedDict({-1: &#39;not printed&#39;,\r\n    ...                   0: &#39;hi&#39;, 1: &#39;StackOverflow&#39;, 2: &#39;!&#39;,\r\n    ...                   4: &#39;not printed&#39;, \r\n    ...                   &#39;x&#39;: &#39;not printed&#39;})\r\n    &gt;&gt;&gt; for x in w:\r\n    ...     print(x)\r\n    ... \r\n    hi\r\n    StackOverflow\r\n    !\r\n\r\n**On point 4 and 5: `iter` checks for an iterator when it calls `__iter__`**:\r\n\r\nWhen `iter(o)` is called for an object `o`, `iter` will make sure that the return value of `__iter__`, if the method is present, is an iterator. This means that the returned object\r\nmust implement `__next__` (or `next` in Python 2) and `__iter__`. `iter` cannot perform any sanity checks for objects which only\r\nprovide `__getitem__`, because it has no way to check whether the items of the object are accessible by integer index.\r\n\r\n    class FailIterIterable(object):\r\n        def __iter__(self):\r\n            return object() # not an iterator\r\n\r\n    class FailGetitemIterable(object):\r\n        def __getitem__(self, item):\r\n            raise Exception\r\n\r\nNote that constructing an iterator from `FailIterIterable` instances fails immediately, while constructing an iterator from `FailGetItemIterable` succeeds, but will throw an Exception on the first call to `__next__`.\r\n\r\n    &gt;&gt;&gt; fii = FailIterIterable()\r\n    &gt;&gt;&gt; iter(fii)\r\n    Traceback (most recent call last):\r\n      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\n    TypeError: iter() returned non-iterator of type &#39;object&#39;\r\n    &gt;&gt;&gt;\r\n    &gt;&gt;&gt; fgi = FailGetitemIterable()\r\n    &gt;&gt;&gt; it = iter(fgi)\r\n    &gt;&gt;&gt; next(it)\r\n    Traceback (most recent call last):\r\n      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\n      File &quot;/path/iterdemo.py&quot;, line 42, in __getitem__\r\n        raise Exception\r\n    Exception\r\n\r\n**On point 6: `__iter__` wins**\r\n\r\nThis one is straightforward. If an object implements `__iter__` and `__getitem__`, `iter` will call `__iter__`. Consider the following class\r\n\r\n    class IterWinsDemo(object):\r\n        def __iter__(self):\r\n            return iter([&#39;__iter__&#39;, &#39;wins&#39;])\r\n\r\n        def __getitem__(self, item):\r\n            return [&#39;__getitem__&#39;, &#39;wins&#39;][item]\r\n\r\nand the output when looping over an instance:\r\n\r\n    &gt;&gt;&gt; iwd = IterWinsDemo()\r\n    &gt;&gt;&gt; for x in iwd:\r\n    ...     print(x)\r\n    ...\r\n    __iter__\r\n    wins\r\n\r\n**On point 7: your iterable classes should implement `__iter__`**\r\n\r\nYou might ask yourself why most builtin sequences like `list` implement an `__iter__` method when `__getitem__` would be sufficient.\r\n\r\n    class WrappedList(object): # note: no inheritance from list!\r\n        def __init__(self, lst):\r\n            self._list = lst\r\n\r\n        def __getitem__(self, item):\r\n            return self._list[item]\r\n\r\nAfter all, iteration over instances of the class above, which delegates calls to `__getitem__` to `list.__getitem__` (using the square bracket notation), will work fine:\r\n\r\n    &gt;&gt;&gt; wl = WrappedList([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;])\r\n    &gt;&gt;&gt; for x in wl:\r\n    ...     print(x)\r\n    ... \r\n    A\r\n    B\r\n    C\r\n\r\nThe reasons your custom iterables should implement `__iter__` are as follows:\r\n\r\n1. If you implement `__iter__`, instances will be considered iterables, and `isinstance(o, collections.Iterable)` will return `True`.\r\n2. If the the object returned by `__iter__` is not an iterator, `iter` will fail immediately and raise a `TypeError`.\r\n3. The special handling of `__getitem__` exists for backwards compatibility reasons. Quoting again from Fluent Python:\r\n\r\n&gt;That is why any Python sequence is iterable: they all implement `__getitem__` . In fact,\r\nthe standard sequences also implement `__iter__`, and yours should too, because the\r\nspecial handling of `__getitem__` exists for backward compatibility reasons and may be\r\ngone in the future (although it is not deprecated as I write this).",
               "id": "36407550",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1459785841,
               "score": 39
            },
            {
               "up_vote_count": 3,
               "answer_id": 40557985,
               "last_activity_date": 1494592039,
               "path": "3.stack.answer",
               "body_markdown": "[tag:Pandas] has a built-in function like that:\r\n\r\n    from pandas.util.testing import isiterable\r\n",
               "tags": [],
               "creation_date": 1478908177,
               "last_edit_date": 1494592039,
               "is_accepted": false,
               "id": "40557985",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 1,
               "answer_id": 46245228,
               "last_activity_date": 1505526254,
               "path": "3.stack.answer",
               "body_markdown": "The `isiterable` func at the following code returns `True` if object is iterable. if it&#39;s not iterable returns `False`\r\n    \r\n    def isiterable(object_):\r\n        return hasattr(type(object_), &quot;__iter__&quot;)\r\n\r\nexample\r\n\r\n    fruits = (&quot;apple&quot;, &quot;banana&quot;, &quot;peach&quot;)\r\n    isiterable(fruits) # returns True\r\n    \r\n    num = 345\r\n    isiterable(num) # returns False\r\n    \r\n    isiterable(str) # returns False because str type is type class and it&#39;s not iterable.\r\n    \r\n    hello = &quot;hello dude !&quot;\r\n    isiterable(hello) # returns True because as you know string objects are iterable",
               "tags": [],
               "creation_date": 1505498232,
               "last_edit_date": 1505526254,
               "is_accepted": false,
               "id": "46245228",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47577312,
               "is_accepted": false,
               "last_activity_date": 1512056836,
               "body_markdown": "Apart from regular try and except, you could run help.\r\n\r\n    temp= [1,2,3,4]\r\n    help(temp)\r\nhelp would give all the methods that could be run on that object(it could be any object and may not be a list as per example), which is temp in this case. \r\n\r\nNote: This would be something you would manually do.",
               "id": "47577312",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512056836,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48830589,
               "is_accepted": false,
               "last_activity_date": 1518797132,
               "body_markdown": "Since **Python 3.5** you can use the [typing](https://docs.python.org/3/library/typing.html) module from the standard library for type related things:\r\n\r\n    from typing import Iterable\r\n\r\n    my_list = []\r\n\r\n    if isinstance(my_item, Iterable):\r\n        print(True)\r\n",
               "id": "48830589",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1518797132,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/1952464/in-python-how-do-i-determine-if-an-object-is-iterable",
         "id": "858127-2279"
      },
      {
         "up_vote_count": "177",
         "path": "2.stack",
         "body_markdown": "&lt;br&gt;\r\nI&#39;ve created a pandas DataFrame\r\n\r\n    df=DataFrame(index=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;], columns=[&#39;x&#39;,&#39;y&#39;])\r\n\r\n\r\nand got this\r\n&lt;pre&gt;\r\n    x    y\r\nA  NaN  NaN\r\nB  NaN  NaN\r\nC  NaN  NaN\r\n&lt;/pre&gt;\r\n&lt;br&gt;\r\nThen I want to assign value to particular cell, for example for row &#39;C&#39; and column &#39;x&#39;.\r\nI&#39;ve expected to get such result:\r\n&lt;pre&gt;\r\n    x    y\r\nA  NaN  NaN\r\nB  NaN  NaN\r\nC  10  NaN\r\n&lt;/pre&gt;\r\n\r\nwith this code:\r\n\r\n    df.xs(&#39;C&#39;)[&#39;x&#39;]=10\r\n\r\n\r\nbut contents of &lt;b&gt;df&lt;/b&gt; haven&#39;t changed. It&#39;s again only Nan&#39;s in dataframe. \r\n\r\nAny suggestions?",
         "view_count": "257862",
         "answer_count": "8",
         "tags": "['python', 'pandas']",
         "creation_date": "1355323245",
         "last_edit_date": "1519467514",
         "code_snippet": "[\"<code>df=DataFrame(index=['A','B','C'], columns=['x','y'])\\n</code>\", \"<code>df.xs('C')['x']=10\\n</code>\", \"<code>df['x']['C']</code>\", \"<code>df.ix['x','C']</code>\", '<code>dataframe[column (series)] [row (Series index)]</code>', '<code>dataframe[row][column]</code>', \"<code>df.set_value('C', 'x', 10)</code>\", '<code>.iat/.at</code>', \"<code>df.xs('C')['x']=10</code>\", \"<code>df.xs('C')</code>\", \"<code>df.xs('C')['x']=10\\n</code>\", \"<code>df['x']</code>\", '<code>df</code>', \"<code>df['x']['C'] = 10\\n</code>\", '<code>df</code>', \"<code>df.at['C', 'x'] = 10\\n</code>\", '<code>df</code>', \"<code>In [18]: %timeit df.set_value('C', 'x', 10)\\n100000 loops, best of 3: 2.9 \u00b5s per loop\\n\\nIn [20]: %timeit df['x']['C'] = 10\\n100000 loops, best of 3: 6.31 \u00b5s per loop\\n\\nIn [81]: %timeit df.at['C', 'x'] = 10\\n100000 loops, best of 3: 9.2 \u00b5s per loop\\n</code>\", '<code>df.x</code>', \"<code>'x'</code>\", '<code>df</code>', '<code>df.x</code>', '<code>Series</code>', '<code>x</code>', \"<code>df['x']</code>\", '<code>df.x</code>', '<code>df.xs, df.ix</code>', '<code>.ix</code>', \"<code>df.set_value('C', 'x', 10)</code>\", \"<code>df['x']['C'] = 10 </code>\", '<code>.loc</code>', '<code>df.loc[df[&lt;some_column_name&gt;] == &lt;condition&gt;, &lt;another_column_name&gt;] = &lt;value_to_add&gt;\\n</code>', '<code>&lt;some_column_name</code>', '<code>&lt;condition&gt;</code>', '<code>&lt;another_column_name&gt;</code>', '<code>&lt;value_to_add&gt;</code>', \"<code>df.ix['x','C']=10\\n</code>\", \"<code>df['x']['C']</code>\", '<code>df.loc[row_index,col_indexer] = value</code>', \"<code>df.loc['C', 'x'] = 10\\n</code>\", '<code>.loc</code>', '<code>.iloc</code>', '<code>df.iloc[[2], [0]] = 10\\n</code>', '<code>df.iloc[[2:8], [0]] = [2,3,4,5,6,7]</code>', '<code>df.loc()</code>', \"<code>src_df = pd.read_sql_query(src_sql,src_connection)\\nfor index1, row1 in src_df.iterrows():\\n    for index, row in vertical_df.iterrows():\\n        src_df.set_value(index=index1,col=u'etl_load_key',value=etl_load_key)\\n        if (row1[u'src_id'] == row['SRC_ID']) is True:\\n            src_df.set_value(index=index1,col=u'vertical',value=row['VERTICAL'])\\n</code>\"]",
         "title": "Set value for particular cell in pandas DataFrame using index",
         "_childDocuments_": [
            {
               "up_vote_count": 212,
               "answer_id": 13842286,
               "last_activity_date": 1505529313,
               "path": "3.stack.answer",
               "body_markdown": "[RukTech&#39;s answer](https://stackoverflow.com/a/24517695/190597), `df.set_value(&#39;C&#39;, &#39;x&#39;, 10)`, is far and away faster than the options I&#39;ve suggested below. However, it has been [**slated for deprecation**](https://github.com/pandas-dev/pandas/issues/15269).\r\n\r\nGoing forward, the [recommended method is `.iat/.at`](https://github.com/pandas-dev/pandas/issues/15269#issuecomment-276382712).\r\n\r\n----------\r\n\r\n\r\n**Why `df.xs(&#39;C&#39;)[&#39;x&#39;]=10` does not work:**\r\n\r\n`df.xs(&#39;C&#39;)` by default, returns a new dataframe [with a copy][2] of the data, so \r\n\r\n    df.xs(&#39;C&#39;)[&#39;x&#39;]=10\r\n\r\nmodifies this new dataframe only.\r\n\r\n`df[&#39;x&#39;]` returns a view of the `df` dataframe, so \r\n\r\n    df[&#39;x&#39;][&#39;C&#39;] = 10\r\n\r\nmodifies `df` itself.\r\n\r\n**Warning**: It is sometimes difficult to predict if an operation returns a copy or a view. For this reason the [docs recommend avoiding assignments with &quot;chained indexing&quot;][1].  \r\n\r\n----------\r\n\r\n\r\nSo the recommended alternative is\r\n\r\n    df.at[&#39;C&#39;, &#39;x&#39;] = 10\r\n\r\nwhich *does* modify `df`.\r\n\r\n----------\r\n\r\n    In [18]: %timeit df.set_value(&#39;C&#39;, &#39;x&#39;, 10)\r\n    100000 loops, best of 3: 2.9 &#181;s per loop\r\n\r\n    In [20]: %timeit df[&#39;x&#39;][&#39;C&#39;] = 10\r\n    100000 loops, best of 3: 6.31 &#181;s per loop\r\n\r\n    In [81]: %timeit df.at[&#39;C&#39;, &#39;x&#39;] = 10\r\n    100000 loops, best of 3: 9.2 &#181;s per loop\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/indexing.html#returning-a-view-versus-a-copy\r\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.xs.html#pandas.DataFrame.xsY",
               "tags": [],
               "creation_date": 1355323862,
               "last_edit_date": 1505529313,
               "is_accepted": true,
               "id": "13842286",
               "down_vote_count": 2,
               "score": 210
            },
            {
               "up_vote_count": 28,
               "answer_id": 21287539,
               "last_activity_date": 1390405705,
               "path": "3.stack.answer",
               "body_markdown": "The recommended way (according to the maintainers) to set a value is:\r\n\r\n    df.ix[&#39;x&#39;,&#39;C&#39;]=10\r\n\r\nUsing &#39;chained indexing&#39; (`df[&#39;x&#39;][&#39;C&#39;]`) may lead to problems.\r\n\r\nSee:\r\n\r\n - https://stackoverflow.com/a/21287235/1579844\r\n - http://pandas.pydata.org/pandas-docs/dev/indexing.html#indexing-view-versus-copy\r\n - https://github.com/pydata/pandas/pull/6031",
               "tags": [],
               "creation_date": 1390405705,
               "last_edit_date": 1495542396,
               "is_accepted": false,
               "id": "21287539",
               "down_vote_count": 1,
               "score": 27
            },
            {
               "up_vote_count": 165,
               "answer_id": 24517695,
               "last_activity_date": 1516107797,
               "path": "3.stack.answer",
               "body_markdown": "Update: The .set_value method is going to be [deprecated][2]. .iat/.at are good replacements, unfortunately pandas provides little documentation\r\n\r\n\r\n----------\r\n\r\n\r\nThe fastest way to do this is using [set_value][1]. This method is ~100 times faster than `.ix` method. For example: \r\n\r\n`df.set_value(&#39;C&#39;, &#39;x&#39;, 10)`\r\n\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html\r\n  [2]: https://github.com/pandas-dev/pandas/issues/15269\r\n",
               "tags": [],
               "creation_date": 1404242200,
               "last_edit_date": 1516107797,
               "is_accepted": false,
               "id": "24517695",
               "down_vote_count": 0,
               "score": 165
            },
            {
               "up_vote_count": 11,
               "answer_id": 33149986,
               "last_activity_date": 1445002292,
               "path": "3.stack.answer",
               "body_markdown": "Try using `df.loc[row_index,col_indexer] = value`",
               "tags": [],
               "creation_date": 1444915975,
               "last_edit_date": 1445002292,
               "is_accepted": false,
               "id": "33149986",
               "down_vote_count": 2,
               "score": 9
            },
            {
               "up_vote_count": 6,
               "answer_id": 33401452,
               "last_activity_date": 1485164768,
               "path": "3.stack.answer",
               "body_markdown": "This is the only thing that worked for me!\r\n\r\n    df.loc[&#39;C&#39;, &#39;x&#39;] = 10\r\n\r\nLearn more about `.loc` [here][1].\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy",
               "tags": [],
               "creation_date": 1446065767,
               "last_edit_date": 1485164768,
               "is_accepted": false,
               "id": "33401452",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 35024529,
               "is_accepted": false,
               "last_activity_date": 1453844421,
               "body_markdown": "I too was searching for this topic and I put together a way to iterate through a DataFrame and update it with lookup values from a second DataFrame.  Here is my code.\r\n\r\n    src_df = pd.read_sql_query(src_sql,src_connection)\r\n    for index1, row1 in src_df.iterrows():\r\n        for index, row in vertical_df.iterrows():\r\n            src_df.set_value(index=index1,col=u&#39;etl_load_key&#39;,value=etl_load_key)\r\n            if (row1[u&#39;src_id&#39;] == row[&#39;SRC_ID&#39;]) is True:\r\n                src_df.set_value(index=index1,col=u&#39;vertical&#39;,value=row[&#39;VERTICAL&#39;])",
               "id": "35024529",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1453844421,
               "score": -3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 30,
               "answer_id": 38467449,
               "is_accepted": false,
               "last_activity_date": 1468958390,
               "body_markdown": "You can also use a conditional lookup using `.loc` as seen here:\r\n\r\n    df.loc[df[&lt;some_column_name&gt;] == &lt;condition&gt;, &lt;another_column_name&gt;] = &lt;value_to_add&gt;\r\n\r\nwhere `&lt;some_column_name` is the column you want to check the `&lt;condition&gt;` variable against and `&lt;another_column_name&gt;` is the column you want to add to (can be a new column or one that already exists). `&lt;value_to_add&gt;` is the value you want to add to that column/row.\r\n\r\nThis example doesn&#39;t work precisely with the question at hand, but it might be useful for someone wants to add a specific value based on a condition.",
               "id": "38467449",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1468958390,
               "score": 30
            },
            {
               "up_vote_count": 2,
               "answer_id": 44807048,
               "last_activity_date": 1518596270,
               "path": "3.stack.answer",
               "body_markdown": "you can use `.iloc`.\r\n\r\n&lt;!-- language: lang-python --&gt;\r\n\r\n    df.iloc[[2], [0]] = 10",
               "tags": [],
               "creation_date": 1498664343,
               "last_edit_date": 1518596270,
               "is_accepted": false,
               "id": "44807048",
               "down_vote_count": 0,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index",
         "id": "858127-2280"
      },
      {
         "up_vote_count": "2392",
         "path": "2.stack",
         "body_markdown": "What is the difference between a function decorated with [`@staticmethod`][1] and one decorated with [`@classmethod`][2]?\r\n\r\n\r\n  [1]: http://docs.python.org/library/functions.html#staticmethod\r\n  [2]: http://docs.python.org/library/functions.html#classmethod",
         "view_count": "483437",
         "answer_count": "24",
         "tags": "['python', 'oop', 'methods', 'python-decorators']",
         "creation_date": "1222376517",
         "last_edit_date": "1514813282",
         "code_snippet": "['<code>@staticmethod</code>', '<code>@classmethod</code>', '<code>foo</code>', '<code>class_foo</code>', '<code>static_foo</code>', '<code>class A(object):\\n    def foo(self,x):\\n        print \"executing foo(%s,%s)\"%(self,x)\\n\\n    @classmethod\\n    def class_foo(cls,x):\\n        print \"executing class_foo(%s,%s)\"%(cls,x)\\n\\n    @staticmethod\\n    def static_foo(x):\\n        print \"executing static_foo(%s)\"%x    \\n\\na=A()\\n</code>', '<code>a</code>', '<code>a.foo(1)\\n# executing foo(&lt;__main__.A object at 0xb7dbef0c&gt;,1)\\n</code>', '<code>self</code>', \"<code>a.class_foo(1)\\n# executing class_foo(&lt;class '__main__.A'&gt;,1)\\n</code>\", '<code>class_foo</code>', '<code>A.foo(1)</code>', '<code>A.class_foo(1)</code>', \"<code>A.class_foo(1)\\n# executing class_foo(&lt;class '__main__.A'&gt;,1)\\n</code>\", '<code>self</code>', '<code>cls</code>', \"<code>a.static_foo(1)\\n# executing static_foo(1)\\n\\nA.static_foo('hi')\\n# executing static_foo(hi)\\n</code>\", '<code>foo</code>', '<code>a.foo</code>', '<code>a</code>', '<code>foo</code>', '<code>a.foo</code>', '<code>a</code>', '<code>foo</code>', '<code>print(a.foo)\\n# &lt;bound method A.foo of &lt;__main__.A object at 0xb7d52f0c&gt;&gt;\\n</code>', '<code>a.class_foo</code>', '<code>a</code>', '<code>class_foo</code>', '<code>A</code>', '<code>class_foo</code>', \"<code>print(a.class_foo)\\n# &lt;bound method type.class_foo of &lt;class '__main__.A'&gt;&gt;\\n</code>\", '<code>a.static_foo</code>', '<code>static_foo</code>', '<code>a.static_foo</code>', '<code>print(a.static_foo)\\n# &lt;function static_foo at 0xb7d479cc&gt;\\n</code>', '<code>static_foo</code>', '<code>A</code>', '<code>print(A.static_foo)\\n# &lt;function static_foo at 0xb7d479cc&gt;\\n</code>', '<code>@staticmethod</code>', '<code>However, class methods are still useful in other places, for example, to program inheritable alternate constructors.</code>', '<code>classmethod</code>', '<code>dict.fromkeys()</code>', '<code>&gt;&gt;&gt; class DictSubclass(dict):\\n...     def __repr__(self):\\n...         return \"DictSubclass\"\\n... \\n&gt;&gt;&gt; dict.fromkeys(\"abc\")\\n{\\'a\\': None, \\'c\\': None, \\'b\\': None}\\n&gt;&gt;&gt; DictSubclass.fromkeys(\"abc\")\\nDictSubclass\\n&gt;&gt;&gt; \\n</code>', '<code>@classmethod</code>', '<code>@staticmethod</code>', '<code>class C:\\n    @classmethod\\n    def f(cls, arg1, arg2, ...): ... \\n</code>', '<code>@classmethod</code>', '<code>C.f()</code>', '<code>C().f()</code>', '<code>staticmethod()</code>', '<code>class C:\\n    @staticmethod\\n    def f(arg1, arg2, ...): ... \\n</code>', '<code>@staticmethod</code>', '<code>C.f()</code>', '<code>C().f()</code>', '<code>classmethod()</code>', \"<code>class Foo(object):\\n\\n    def a_normal_instance_method(self, arg_1, kwarg_2=None):\\n        '''\\n        Return a value that is a function of the instance with its\\n        attributes, and other arguments such as arg_1 and kwarg2\\n        '''\\n\\n    @staticmethod\\n    def a_static_method(arg_0):\\n        '''\\n        Return a value that is a function of arg_0. It does not know the \\n        instance or class it is called from.\\n        '''\\n\\n    @classmethod\\n    def a_class_method(cls, arg1):\\n        '''\\n        Return a value that is a function of the class and other arguments.\\n        respects subclassing, it is called with the class it is called from.\\n        '''\\n</code>\", '<code>a_normal_instance_method</code>', \"<code>', '\\n</code>\", '<code>join</code>', \"<code>['a', 'b', 'c']</code>\", \"<code>&gt;&gt;&gt; ', '.join(['a', 'b', 'c'])\\n'a, b, c'\\n</code>\", '<code>str.join</code>', \"<code>':'</code>\", \"<code>&gt;&gt;&gt; join_with_colons = ':'.join \\n</code>\", \"<code>&gt;&gt;&gt; join_with_colons('abcde')\\n'a:b:c:d:e'\\n&gt;&gt;&gt; join_with_colons(['FF', 'FF', 'FF', 'FF', 'FF', 'FF'])\\n'FF:FF:FF:FF:FF:FF'\\n</code>\", '<code>str.maketrans</code>', '<code>string</code>', '<code>str.translate</code>', '<code>string</code>', '<code>str.maketrans</code>', \"<code># demonstrate same function whether called from instance or not:\\n&gt;&gt;&gt; ', '.maketrans('ABC', 'abc')\\n{65: 97, 66: 98, 67: 99}\\n&gt;&gt;&gt; str.maketrans('ABC', 'abc')\\n{65: 97, 66: 98, 67: 99}\\n</code>\", \"<code>&gt;&gt;&gt; import string\\n&gt;&gt;&gt; 'ABCDEFG'.translate(string.maketrans('ABC', 'abc'))\\n'abcDEFG'\\n</code>\", '<code>dict.fromkeys</code>', \"<code>&gt;&gt;&gt; dict.fromkeys(['a', 'b', 'c'])\\n{'c': None, 'b': None, 'a': None}\\n</code>\", \"<code>&gt;&gt;&gt; class MyDict(dict): 'A dict subclass, use to demo classmethods'\\n&gt;&gt;&gt; md = MyDict.fromkeys(['a', 'b', 'c'])\\n&gt;&gt;&gt; md\\n{'a': None, 'c': None, 'b': None}\\n&gt;&gt;&gt; type(md)\\n&lt;class '__main__.MyDict'&gt;\\n</code>\", '<code>classmethod</code>', '<code>staticmethod</code>', \"<code>class Apple:\\n\\n    _counter = 0\\n\\n    @staticmethod\\n    def about_apple():\\n        print('Apple is good for you.')\\n\\n        # note you can still access other member of the class\\n        # but you have to use the class instance \\n        # which is not very nice, because you have repeat yourself\\n        # \\n        # For example:\\n        # @staticmethod\\n        #    print('Number of apples have been juiced: %s' % Apple._counter)\\n        #\\n        # @classmethod\\n        #    print('Number of apples have been juiced: %s' % cls._counter)\\n        #\\n        #    @classmethod is especially useful when you move your function to other class,\\n        #       you don't have to rename the class reference \\n\\n    @classmethod\\n    def make_apple_juice(cls, number_of_apples):\\n        print('Make juice:')\\n        for i in range(number_of_apples):\\n            cls._juice_this(i)\\n\\n    @classmethod\\n    def _juice_this(cls, apple):\\n        print('Juicing %d...' % apple)\\n        cls._counter += 1\\n</code>\", '<code>class Cluster(object):\\n\\n    def _is_cluster_for(cls, name):\\n        \"\"\"\\n        see if this class is the cluster with this name\\n        this is a classmethod\\n        \"\"\" \\n        return cls.__name__ == name\\n    _is_cluster_for = classmethod(_is_cluster_for)\\n\\n    #static method\\n    def getCluster(name):\\n        \"\"\"\\n        static factory method, should be in Cluster class\\n        returns a cluster object for the given name\\n        \"\"\"\\n        for cls in Cluster.__subclasses__():\\n            if cls._is_cluster_for(name):\\n                return cls()\\n    getCluster = staticmethod(getCluster)\\n</code>', '<code>_is_cluster_for</code>', '<code>@staticmethod\\ndef some_static_method(*args, **kwds):\\n    pass\\n</code>', '<code> @classmethod\\n def some_class_method(cls, *args, **kwds):\\n     pass\\n</code>', '<code>@staticmethod</code>', \"<code>&gt;&gt;&gt; class C(object):\\n...  pass\\n... \\n&gt;&gt;&gt; def f():\\n...  pass\\n... \\n&gt;&gt;&gt; staticmethod(f).__get__(None, C)\\n&lt;function f at 0x5c1cf0&gt;\\n&gt;&gt;&gt; classmethod(f).__get__(None, C)\\n&lt;bound method type.f of &lt;class '__main__.C'&gt;&gt;\\n</code>\", '<code>classmethod</code>', \"<code>&gt;&gt;&gt; class CMeta(type):\\n...  def foo(cls):\\n...   print cls\\n... \\n&gt;&gt;&gt; class C(object):\\n...  __metaclass__ = CMeta\\n... \\n&gt;&gt;&gt; C.foo()\\n&lt;class '__main__.C'&gt;\\n</code>\", '<code>c = C(); c.foo()</code>', '<code>type(c).foo()</code>', '<code>@staticmethod</code>', '<code>@classmethod</code>', '<code>@classmethod</code>', '<code>class Foo(object):\\n    @staticmethod\\n    def bar():\\n        return \"In Foo\"\\n</code>', '<code>bar()</code>', '<code>class Foo2(Foo):\\n    @staticmethod\\n    def bar():\\n        return \"In Foo2\"\\n</code>', '<code>bar()</code>', '<code>Foo2</code>', '<code>Foo2</code>', '<code>magic()</code>', '<code>Foo2</code>', '<code>bar()</code>', '<code>class Foo2(Foo):\\n    @staticmethod\\n    def bar():\\n        return \"In Foo2\"\\n    @staticmethod\\n    def magic():\\n        return \"Something useful you\\'d like to use in bar, but now can\\'t\" \\n</code>', '<code>Foo2.magic()</code>', '<code>bar()</code>', '<code>Foo2</code>', '<code>bar()</code>', '<code>Foo</code>', '<code>bar()</code>', '<code>classmethod</code>', '<code>class Foo(object):\\n    @classmethod\\n    def bar(cls):\\n        return \"In Foo\"\\n\\nclass Foo2(Foo):\\n    @classmethod\\n    def bar(cls):\\n        return \"In Foo2 \" + cls.magic()\\n    @classmethod\\n    def magic(cls):\\n        return \"MAGIC\"\\n\\nprint Foo2().bar()\\n</code>', '<code>In Foo2 MAGIC</code>', '<code>class Test(object):\\n    def foo(self, a):\\n        print \"testing (%s,%s)\"%(self,a)\\n\\n    @classmethod\\n    def foo_classmethod(cls, a):\\n        print \"testing foo_classmethod(%s,%s)\"%(cls,a)\\n\\n    @staticmethod\\n    def foo_staticmethod(a):\\n        print \"testing foo_staticmethod(%s)\"%a\\n\\ntest = Test()\\n</code>', '<code>class A(object):\\n    x = 0\\n\\n    def say_hi(self):\\n        pass\\n\\n    @staticmethod\\n    def say_hi_static():\\n        pass\\n\\n    @classmethod\\n    def say_hi_class(cls):\\n        pass\\n\\n    def run_self(self):\\n        self.x += 1\\n        print self.x # outputs 1\\n        self.say_hi()\\n        self.say_hi_static()\\n        self.say_hi_class()\\n\\n    @staticmethod\\n    def run_static():\\n        print A.x  # outputs 0\\n        # A.say_hi() #  wrong\\n        A.say_hi_static()\\n        A.say_hi_class()\\n\\n    @classmethod\\n    def run_class(cls):\\n        print cls.x # outputs 0\\n        # cls.say_hi() #  wrong\\n        cls.say_hi_static()\\n        cls.say_hi_class()\\n</code>', '<code># A.run_self() #  wrong\\nA.run_static()\\nA.run_class()\\n</code>', '<code>class C(object):\\n    @classmethod\\n    def fun(cls, arg1, arg2, ...):\\n       ....\\n\\nfun: function that needs to be converted into a class method\\nreturns: a class method for function.\\n</code>', '<code>class C(object):\\n    @staticmethod\\n    def fun(arg1, arg2, ...):\\n        ...\\nreturns: a static method for function fun.\\n</code>', \"<code># Python program to demonstrate \\n# use of class method and static method.\\nfrom datetime import date\\n\\nclass Person:\\n    def __init__(self, name, age):\\n        self.name = name\\n        self.age = age\\n\\n    # a class method to create a Person object by birth year.\\n    @classmethod\\n    def fromBirthYear(cls, name, year):\\n        return cls(name, date.today().year - year)\\n\\n    # a static method to check if a Person is adult or not.\\n    @staticmethod\\n    def isAdult(age):\\n        return age &gt; 18\\n\\nperson1 = Person('mayank', 21)\\nperson2 = Person.fromBirthYear('mayank', 1996)\\n\\nprint person1.age\\nprint person2.age\\n\\n# print the result\\nprint Person.isAdult(22)\\n</code>\", '<code>21\\n21\\nTrue\\n</code>', '<code>classmethod</code>', '<code>Person</code>', '<code>first_name</code>', '<code>last_name</code>', '<code>class Person(object):\\n\\n    def __init__(self, first_name, last_name):\\n        self.first_name = first_name\\n        self.last_name = last_name\\n</code>', '<code>first_name</code>', '<code>class Person(object):\\n\\n    def __init__(self, first_name, last_name):\\n        self.first_name = first_name\\n        self.last_name = last_name\\n\\n    def __init__(self, first_name):\\n        self.first_name = first_name\\n</code>', '<code>@classmethod</code>', '<code>class Person(object):\\n\\n    def __init__(self, first_name, last_name):\\n        self.first_name = first_name\\n        self.last_name = last_name\\n\\n    @classmethod\\n    def get_person(cls, first_name):\\n        return cls(first_name, \"\")\\n</code>', '<code>first_name</code>', '<code>Person.validate_name(\"Gaurang Shah\")\\n</code>', \"<code>&gt;&gt;&gt; class Klaus:\\n        @classmethod\\n        def classmthd(*args):\\n            return args\\n\\n        @staticmethod\\n        def staticmthd(*args):\\n            return args\\n\\n# 1. Call classmethod without any arg\\n&gt;&gt;&gt; Klaus.classmthd()  \\n(__main__.Klaus,)  # the class gets passed as the first argument\\n\\n# 2. Call classmethod with 1 arg\\n&gt;&gt;&gt; Klaus.classmthd('chumma')\\n(__main__.Klaus, 'chumma')\\n\\n# 3. Call staticmethod without any arg\\n&gt;&gt;&gt; Klaus.staticmthd()  \\n()\\n\\n# 4. Call staticmethod with 1 arg\\n&gt;&gt;&gt; Klaus.staticmthd('chumma')\\n('chumma',)\\n</code>\", '<code>class A(object):\\n    m=54\\n\\n    @classmethod\\n    def class_method(cls):\\n        print \"m is %d\" % cls.m\\n</code>', '<code>class X(object):\\n    m=54 #will not be referenced\\n\\n    @staticmethod\\n    def static_method():\\n        print \"Referencing/calling a variable or function outside this class. E.g. Some global variable/function.\"\\n</code>', \"<code>#!/usr/bin/python\\n#coding:utf-8\\n\\nclass Demo(object):\\n    def __init__(self,x):\\n        self.x = x\\n\\n    @classmethod\\n    def addone(self, x):\\n        return x+1\\n\\n    @staticmethod\\n    def addtwo(x):\\n        return x+2\\n\\n    def addthree(self, x):\\n        return x+3\\n\\ndef main():\\n    print Demo.addone(2)\\n    print Demo.addtwo(2)\\n\\n    #print Demo.addthree(2) #Error\\n    demo = Demo(2)\\n    print demo.addthree(2)\\n\\n\\nif __name__ == '__main__':\\n    main()\\n</code>\", '<code>@staticmethod</code>', '<code>staticmethod()</code>', '<code>@staticmethod</code>']",
         "title": "What is the difference between @staticmethod and @classmethod in Python?",
         "_childDocuments_": [
            {
               "up_vote_count": 639,
               "answer_id": 136138,
               "last_activity_date": 1280189372,
               "path": "3.stack.answer",
               "body_markdown": "A staticmethod is a method that knows nothing about the class or instance it was called on. It just gets the arguments that were passed, no implicit first argument. It is basically useless in Python -- you can just use a module function instead of a staticmethod.\r\n\r\nA classmethod, on the other hand, is a method that gets passed the class it was called on, or the class of the instance it was called on, as first argument. This is useful when you want the method to be a factory for the class: since it gets the actual class it was called on as first argument, you can always instantiate the right class, even when subclasses are involved. Observe for instance how `dict.fromkeys()`, a classmethod, returns an instance of the subclass when called on a subclass:\r\n\r\n    &gt;&gt;&gt; class DictSubclass(dict):\r\n    ...     def __repr__(self):\r\n    ...         return &quot;DictSubclass&quot;\r\n    ... \r\n    &gt;&gt;&gt; dict.fromkeys(&quot;abc&quot;)\r\n    {&#39;a&#39;: None, &#39;c&#39;: None, &#39;b&#39;: None}\r\n    &gt;&gt;&gt; DictSubclass.fromkeys(&quot;abc&quot;)\r\n    DictSubclass\r\n    &gt;&gt;&gt; \r\n\r\n",
               "tags": [],
               "creation_date": 1222376753,
               "last_edit_date": 1280189372,
               "is_accepted": false,
               "id": "136138",
               "down_vote_count": 17,
               "score": 622
            },
            {
               "up_vote_count": 92,
               "answer_id": 136149,
               "last_activity_date": 1349662074,
               "path": "3.stack.answer",
               "body_markdown": "Basically `@classmethod` makes a method whose first argument is the class it&#39;s called from (rather than the class instance), `@staticmethod` does not have any implicit arguments.",
               "tags": [],
               "creation_date": 1222376826,
               "last_edit_date": 1349662074,
               "is_accepted": false,
               "id": "136149",
               "down_vote_count": 0,
               "score": 92
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 22,
               "answer_id": 136246,
               "is_accepted": false,
               "last_activity_date": 1222377853,
               "body_markdown": "`@staticmethod` just disables the default function as method descriptor.  classmethod wraps your function in a container callable that passes a reference to the owning class as first argument:\r\n\r\n    &gt;&gt;&gt; class C(object):\r\n    ...  pass\r\n    ... \r\n    &gt;&gt;&gt; def f():\r\n    ...  pass\r\n    ... \r\n    &gt;&gt;&gt; staticmethod(f).__get__(None, C)\r\n    &lt;function f at 0x5c1cf0&gt;\r\n    &gt;&gt;&gt; classmethod(f).__get__(None, C)\r\n    &lt;bound method type.f of &lt;class &#39;__main__.C&#39;&gt;&gt;\r\n\r\nAs a matter of fact, `classmethod` has a runtime overhead but makes it possible to access the owning class.  Alternatively I recommend using a metaclass and putting the class methods on that metaclass:\r\n\r\n    &gt;&gt;&gt; class CMeta(type):\r\n    ...  def foo(cls):\r\n    ...   print cls\r\n    ... \r\n    &gt;&gt;&gt; class C(object):\r\n    ...  __metaclass__ = CMeta\r\n    ... \r\n    &gt;&gt;&gt; C.foo()\r\n    &lt;class &#39;__main__.C&#39;&gt;",
               "id": "136246",
               "tags": [],
               "down_vote_count": 7,
               "creation_date": 1222377853,
               "score": 15
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 45,
               "answer_id": 1669457,
               "is_accepted": false,
               "last_activity_date": 1257274943,
               "body_markdown": "[Here][1] is a short article on this question\r\n\r\n\r\n  [1]: http://rapd.wordpress.com/2008/07/02/python-staticmethod-vs-classmethod/\r\n\r\n&gt; @staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It\u2019s definition is immutable via inheritance.\r\n\r\n&gt; @classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance. That\u2019s because the first argument for @classmethod function must always be cls (class).",
               "id": "1669457",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1257274943,
               "score": 45
            },
            {
               "up_vote_count": 2046,
               "answer_id": 1669524,
               "last_activity_date": 1455049253,
               "path": "3.stack.answer",
               "body_markdown": "Maybe a bit of example code will help: Notice the difference in the call signatures of `foo`, `class_foo` and `static_foo`:\r\n\r\n    class A(object):\r\n        def foo(self,x):\r\n            print &quot;executing foo(%s,%s)&quot;%(self,x)\r\n    \r\n        @classmethod\r\n        def class_foo(cls,x):\r\n            print &quot;executing class_foo(%s,%s)&quot;%(cls,x)\r\n    \r\n        @staticmethod\r\n        def static_foo(x):\r\n            print &quot;executing static_foo(%s)&quot;%x    \r\n    \r\n    a=A()\r\n    \r\nBelow is the usual way an object instance calls a method. The object instance, `a`, is implicitly passed as the first argument.\r\n    \r\n    a.foo(1)\r\n    # executing foo(&lt;__main__.A object at 0xb7dbef0c&gt;,1)\r\n\r\n\r\n----------\r\n\r\n\r\n    \r\n**With classmethods**, the class of the object instance is implicitly passed as the first argument instead of `self`.\r\n    \r\n    a.class_foo(1)\r\n    # executing class_foo(&lt;class &#39;__main__.A&#39;&gt;,1)\r\n    \r\nYou can also call `class_foo` using the class. In fact, if you define something to be\r\na classmethod, it is probably because you intend to call it from the class rather than from a class instance. `A.foo(1)` would have raised a TypeError, but `A.class_foo(1)` works just fine:\r\n        \r\n    A.class_foo(1)\r\n    # executing class_foo(&lt;class &#39;__main__.A&#39;&gt;,1)\r\n \r\nOne use people have found for class methods is to create [inheritable alternative constructors](https://stackoverflow.com/a/1950927/190597).\r\n\r\n\r\n----------\r\n\r\n\r\n**With staticmethods**, neither `self` (the object instance) nor  `cls` (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class:\r\n    \r\n    a.static_foo(1)\r\n    # executing static_foo(1)\r\n\r\n    A.static_foo(&#39;hi&#39;)\r\n    # executing static_foo(hi)\r\n\r\nStaticmethods are used to group functions which have some logical connection with a class to the class.\r\n\r\n----------\r\n\r\n\r\n`foo` is just a function, but when you call `a.foo` you don&#39;t just get the function,\r\nyou get a &quot;partially applied&quot; version of the function with the object instance `a` bound as the first argument to the function. `foo` expects 2 arguments, while `a.foo` only expects 1 argument.\r\n\r\n`a` is bound to `foo`. That is what is meant by the term &quot;bound&quot; below:\r\n\r\n    print(a.foo)\r\n    # &lt;bound method A.foo of &lt;__main__.A object at 0xb7d52f0c&gt;&gt;\r\n\r\nWith `a.class_foo`, `a` is not bound to `class_foo`, rather the class `A` is bound to `class_foo`.\r\n\r\n    print(a.class_foo)\r\n    # &lt;bound method type.class_foo of &lt;class &#39;__main__.A&#39;&gt;&gt;\r\n\r\nHere, with a staticmethod, even though it is a method, `a.static_foo` just returns\r\na good &#39;ole function with no arguments bound. `static_foo` expects 1 argument, and\r\n`a.static_foo` expects 1 argument too.\r\n\r\n    print(a.static_foo)\r\n    # &lt;function static_foo at 0xb7d479cc&gt;\r\n\r\nAnd of course the same thing happens when you call `static_foo` with the class `A` instead.\r\n\r\n    print(A.static_foo)\r\n    # &lt;function static_foo at 0xb7d479cc&gt;\r\n\r\n\r\n  [1]: http://www.python.org/download/releases/2.2/descrintro/#metaclasses",
               "tags": [],
               "creation_date": 1257275628,
               "last_edit_date": 1495542396,
               "is_accepted": true,
               "id": "1669524",
               "down_vote_count": 2,
               "score": 2044
            },
            {
               "up_vote_count": 66,
               "answer_id": 1669579,
               "last_activity_date": 1355219767,
               "path": "3.stack.answer",
               "body_markdown": "**Official python docs:**\r\n\r\n[@classmethod][1]\r\n\r\n&gt; A class method receives the class as\r\n&gt; implicit first argument, just like an\r\n&gt; instance method receives the instance.\r\n&gt; To declare a class method, use this\r\n&gt; idiom:\r\n&gt; \r\n&gt;     class C:\r\n&gt;         @classmethod\r\n&gt;         def f(cls, arg1, arg2, ...): ... \r\n&gt;\r\n&gt; The `@classmethod` form is a function\r\n&gt; [*decorator*](http://docs.python.org/2/glossary.html#term-decorator) \u2013 see the description of\r\n&gt; function definitions in [*Function\r\n&gt; definitions*](http://docs.python.org/2/reference/compound_stmts.html#function) for details.\r\n&gt; \r\n&gt; It can be called either on the class\r\n&gt; (such as `C.f()`) or on an instance\r\n&gt; (such as `C().f()`). The instance is\r\n&gt; ignored except for its class. If a\r\n&gt; class method is called for a derived\r\n&gt; class, the derived class object is\r\n&gt; passed as the implied first argument.\r\n&gt; \r\n&gt; Class methods are different than C++\r\n&gt; or Java static methods. If you want\r\n&gt; those, see [`staticmethod()`](http://docs.python.org/2/library/functions.html#staticmethod) in this\r\n&gt; section.\r\n\r\n[@staticmethod][2]\r\n\r\n&gt; A static method does not receive an\r\n&gt; implicit first argument. To declare a\r\n&gt; static method, use this idiom:\r\n&gt; \r\n&gt;     class C:\r\n&gt;         @staticmethod\r\n&gt;         def f(arg1, arg2, ...): ... \r\n&gt;\r\n&gt; The `@staticmethod` form is a function\r\n&gt; [*decorator*](http://docs.python.org/2/glossary.html#term-decorator) \u2013 see the description of\r\n&gt; function definitions in [*Function\r\n&gt; definitions*](http://docs.python.org/2/reference/compound_stmts.html#function) for details.\r\n&gt; \r\n&gt; It can be called either on the class\r\n&gt; (such as `C.f()`) or on an instance\r\n&gt; (such as `C().f()`). The instance is\r\n&gt; ignored except for its class.\r\n&gt; \r\n&gt; Static methods in Python are similar\r\n&gt; to those found in Java or C++. For a\r\n&gt; more advanced concept, see\r\n&gt; [`classmethod()`](http://docs.python.org/2/library/functions.html#classmethod) in this section.\r\n\r\n  [1]: http://docs.python.org/library/functions.html#classmethod\r\n  [2]: http://docs.python.org/library/functions.html#staticmethod",
               "tags": [],
               "creation_date": 1257276199,
               "last_edit_date": 1355219767,
               "is_accepted": false,
               "id": "1669579",
               "down_vote_count": 3,
               "score": 63
            },
            {
               "up_vote_count": 25,
               "answer_id": 9428384,
               "last_activity_date": 1487604874,
               "path": "3.stack.answer",
               "body_markdown": "@decorators were added in python 2.4 If you&#39;re using python &lt; 2.4 you can use the classmethod() and staticmethod() function.\r\n\r\nFor example, if you want to create a factory method (A function returning an instance of a different implementation of a class depending on what argument it gets) you can do something like:\r\n    \r\n    class Cluster(object):\r\n\r\n        def _is_cluster_for(cls, name):\r\n            &quot;&quot;&quot;\r\n            see if this class is the cluster with this name\r\n            this is a classmethod\r\n            &quot;&quot;&quot; \r\n            return cls.__name__ == name\r\n        _is_cluster_for = classmethod(_is_cluster_for)\r\n        \r\n        #static method\r\n        def getCluster(name):\r\n            &quot;&quot;&quot;\r\n            static factory method, should be in Cluster class\r\n            returns a cluster object for the given name\r\n            &quot;&quot;&quot;\r\n            for cls in Cluster.__subclasses__():\r\n                if cls._is_cluster_for(name):\r\n                    return cls()\r\n        getCluster = staticmethod(getCluster)\r\n\r\nAlso observe that this is a good example for using a classmethod and a static method,\r\nThe static method clearly belongs to the class, since it uses the class Cluster internally.\r\nThe classmethod only needs information about the class, and no instance of the object.\r\n\r\nAnother benefit of making the `_is_cluster_for` method a classmethod is so a subclass can decide to change it&#39;s implementation, maybe because it is pretty generic and can handle more than one type of cluster, so just checking the name of the class would not be enough.",
               "tags": [],
               "creation_date": 1330075952,
               "last_edit_date": 1487604874,
               "is_accepted": false,
               "id": "9428384",
               "down_vote_count": 1,
               "score": 24
            },
            {
               "up_vote_count": 3,
               "answer_id": 13920259,
               "last_activity_date": 1355771424,
               "path": "3.stack.answer",
               "body_markdown": "A quick hack-up ofotherwise identical methods in iPython reveals that `@staticmethod` yields marginal performance gains (in the nanoseconds), but otherwise it seems to serve no function. Also, any performance gains will probably be wiped out by the additional work of processing the method through `staticmethod()` during compilation (which happens prior to any code execution when you run a script).\r\n\r\nFor the sake of code readability I&#39;d avoid `@staticmethod` unless your method will be used for loads of work, where the nanoseconds count.",
               "tags": [],
               "creation_date": 1355770300,
               "last_edit_date": 1355771424,
               "is_accepted": false,
               "id": "13920259",
               "down_vote_count": 9,
               "score": -6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 20041016,
               "is_accepted": false,
               "last_activity_date": 1384753997,
               "body_markdown": "    #!/usr/bin/python\r\n    #coding:utf-8\r\n    \r\n    class Demo(object):\r\n        def __init__(self,x):\r\n            self.x = x\r\n    \r\n        @classmethod\r\n        def addone(self, x):\r\n            return x+1\r\n    \r\n        @staticmethod\r\n        def addtwo(x):\r\n            return x+2\r\n    \r\n        def addthree(self, x):\r\n            return x+3\r\n    \r\n    def main():\r\n        print Demo.addone(2)\r\n        print Demo.addtwo(2)\r\n    \r\n        #print Demo.addthree(2) #Error\r\n        demo = Demo(2)\r\n        print demo.addthree(2)\r\n    \r\n    \r\n    if __name__ == &#39;__main__&#39;:\r\n        main()\r\n\r\n",
               "id": "20041016",
               "tags": [],
               "down_vote_count": 10,
               "creation_date": 1384753997,
               "score": -5
            },
            {
               "up_vote_count": 35,
               "answer_id": 28117800,
               "last_activity_date": 1499874448,
               "path": "3.stack.answer",
               "body_markdown": "&gt; ##What is the difference between @staticmethod and @classmethod in Python?\r\n\r\nYou may have seen Python code like this pseudocode, which demonstrates the signatures of the various method types and provides a docstring to explain each:\r\n\r\n    class Foo(object):\r\n    \r\n        def a_normal_instance_method(self, arg_1, kwarg_2=None):\r\n            &#39;&#39;&#39;\r\n            Return a value that is a function of the instance with its\r\n            attributes, and other arguments such as arg_1 and kwarg2\r\n            &#39;&#39;&#39;\r\n    \r\n        @staticmethod\r\n        def a_static_method(arg_0):\r\n            &#39;&#39;&#39;\r\n            Return a value that is a function of arg_0. It does not know the \r\n            instance or class it is called from.\r\n            &#39;&#39;&#39;\r\n    \r\n        @classmethod\r\n        def a_class_method(cls, arg1):\r\n            &#39;&#39;&#39;\r\n            Return a value that is a function of the class and other arguments.\r\n            respects subclassing, it is called with the class it is called from.\r\n            &#39;&#39;&#39;\r\n\r\n# The Normal Instance Method\r\n\r\nFirst I&#39;ll explain `a_normal_instance_method`. This is precisely called an &quot;**instance method**&quot;. When an instance method is used, it is used as a partial function (as opposed to a total function, defined for all values when viewed in source code) that is, when used, the first of the arguments is predefined as the instance of the object, with all of its given attributes. It has the instance of the object bound to it, and it must be called from an instance of the object. Typically, it will access various attributes of the instance.\r\n\r\nFor example, this is an instance of a string:\r\n\r\n    &#39;, &#39;\r\n\r\nif we use the instance method, `join` on this string, to join another iterable,\r\nit quite obviously is a function of the instance, in addition to being a function of the iterable list, `[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]`:\r\n\r\n    &gt;&gt;&gt; &#39;, &#39;.join([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\r\n    &#39;a, b, c&#39;\r\n\r\n### Bound methods\r\n\r\nInstance methods can be bound via a dotted lookup for use later.\r\n\r\nFor example, this binds the `str.join` method to the `&#39;:&#39;` instance:\r\n\r\n    &gt;&gt;&gt; join_with_colons = &#39;:&#39;.join \r\n\r\nAnd later we can use this as a function that already has the first argument bound to it. In this way, it works like a partial function on the instance:\r\n\r\n    &gt;&gt;&gt; join_with_colons(&#39;abcde&#39;)\r\n    &#39;a:b:c:d:e&#39;\r\n    &gt;&gt;&gt; join_with_colons([&#39;FF&#39;, &#39;FF&#39;, &#39;FF&#39;, &#39;FF&#39;, &#39;FF&#39;, &#39;FF&#39;])\r\n    &#39;FF:FF:FF:FF:FF:FF&#39;\r\n \r\n# Static Method\r\n\r\nThe static method does *not* take the instance as an argument. \r\n\r\nIt is very similar to a module level function. \r\n\r\nHowever, a module level function must live in the module and be specially imported to other places where it is used. \r\n\r\nIf it is attached to the object, however, it will follow the object conveniently through importing and inheritance as well.\r\n\r\nAn example of a static method is `str.maketrans`, moved from the `string` module in Python 3.  It makes a translation table suitable for consumption by `str.translate`. It does seem rather silly when used from an instance of a string, as demonstrated below, but importing the function from the `string` module is rather clumsy, and it&#39;s nice to be able to call it from the class, as in `str.maketrans`\r\n\r\n    # demonstrate same function whether called from instance or not:\r\n    &gt;&gt;&gt; &#39;, &#39;.maketrans(&#39;ABC&#39;, &#39;abc&#39;)\r\n    {65: 97, 66: 98, 67: 99}\r\n    &gt;&gt;&gt; str.maketrans(&#39;ABC&#39;, &#39;abc&#39;)\r\n    {65: 97, 66: 98, 67: 99}\r\n\r\nIn python 2, you have to import this function from the increasingly less useful string module:\r\n\r\n    &gt;&gt;&gt; import string\r\n    &gt;&gt;&gt; &#39;ABCDEFG&#39;.translate(string.maketrans(&#39;ABC&#39;, &#39;abc&#39;))\r\n    &#39;abcDEFG&#39;\r\n\r\n# Class Method\r\n\r\nA class method is a similar to an instance method in that it takes an implicit first argument, but instead of taking the instance, it takes the class. Frequently these are used as alternative constructors for better semantic usage and it will support inheritance.\r\n\r\nThe most canonical example of a builtin classmethod is `dict.fromkeys`. It is used as an alternative constructor of dict, (well suited for when you know what your keys are and want a default value for them.)\r\n\r\n    &gt;&gt;&gt; dict.fromkeys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\r\n    {&#39;c&#39;: None, &#39;b&#39;: None, &#39;a&#39;: None}\r\n\r\nWhen we subclass dict, we can use the same constructor, which creates an instance of the subclass.\r\n\r\n    &gt;&gt;&gt; class MyDict(dict): &#39;A dict subclass, use to demo classmethods&#39;\r\n    &gt;&gt;&gt; md = MyDict.fromkeys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\r\n    &gt;&gt;&gt; md\r\n    {&#39;a&#39;: None, &#39;c&#39;: None, &#39;b&#39;: None}\r\n    &gt;&gt;&gt; type(md)\r\n    &lt;class &#39;__main__.MyDict&#39;&gt;\r\n\r\nSee the [pandas source code][1] for other similar examples of alternative constructors, and see also the official Python documentation on [`classmethod`][2] and [`staticmethod`][3].\r\n\r\n\r\n  [1]: https://github.com/pydata/pandas/blob/master/pandas/core/frame.py\r\n  [2]: https://docs.python.org/library/functions.html#classmethod\r\n  [3]: https://docs.python.org/library/functions.html#staticmethod",
               "tags": [],
               "creation_date": 1422043280,
               "last_edit_date": 1499874448,
               "is_accepted": false,
               "id": "28117800",
               "down_vote_count": 0,
               "score": 35
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 23,
               "answer_id": 30329887,
               "is_accepted": false,
               "last_activity_date": 1432049233,
               "body_markdown": "I think a better question is &quot;When would you use @classmethod vs @staticmethod?&quot;\r\n\r\n@classmethod allows you easy access to private members that are associated to the class definition. this is a great way to do singletons, or factory classes that control the number of instances of the created objects exist.\r\n\r\n@staticmethod provides marginal performance gains, but I have yet to see a productive use of a static method within a class that couldn&#39;t be achieved as a standalone function outside the class.",
               "id": "30329887",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1432049233,
               "score": 23
            },
            {
               "up_vote_count": 12,
               "answer_id": 33727452,
               "last_activity_date": 1499872901,
               "path": "3.stack.answer",
               "body_markdown": "[The definitive guide on how to use static, class or abstract methods in Python](https://julien.danjou.info/blog/2013/guide-python-static-class-abstract-methods) is one good link for this topic, and summary it as following.\r\n\r\n**`@staticmethod`** function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It\u2019s definition is immutable via inheritance.\r\n\r\n- Python does not have to instantiate a bound-method for object.\r\n- It eases the readability of the code, and it does not depend on the state of object itself;\r\n\r\n**`@classmethod`** function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance, can be overridden by subclass. That\u2019s because the first argument for `@classmethod` function must always be _cls_ (class).\r\n\r\n- _Factory methods_, that are used to create an instance for a class using for example some sort of pre-processing.\r\n- _Static methods calling static methods_: if you split a static methods in several static methods, you shouldn&#39;t hard-code the class name but use class methods",
               "tags": [],
               "creation_date": 1447639250,
               "last_edit_date": 1499872901,
               "is_accepted": false,
               "id": "33727452",
               "down_vote_count": 0,
               "score": 12
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 13,
               "answer_id": 34255425,
               "is_accepted": false,
               "last_activity_date": 1450035463,
               "body_markdown": "**@classmethod means**: when this method is called, we pass the class as the first argument instead of the instance of that class (as we normally do with methods). This means you can use the class and its properties inside that method rather than a particular instance.\r\n\r\n**@staticmethod means:** when this method is called, we don&#39;t pass an instance of the class to it (as we normally do with methods). This means you can put a function inside a class but you can&#39;t access the instance of that class (this is useful when your method does not use the instance).",
               "id": "34255425",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1450035463,
               "score": 13
            },
            {
               "up_vote_count": 32,
               "answer_id": 36798076,
               "last_activity_date": 1517841930,
               "path": "3.stack.answer",
               "body_markdown": "To decide whether to use [@staticmethod](https://docs.python.org/3/library/functions.html?highlight=staticmethod#staticmethod) or [@classmethod](https://docs.python.org/3.5/library/functions.html?highlight=classmethod#classmethod) you have to look inside your method. **If your method accesses other variables/methods in your class then use @classmethod**. On the other hand, if your method does not touches any other parts of the class then use @staticmethod.\r\n\r\n    class Apple:\r\n    \r\n        _counter = 0\r\n    \r\n        @staticmethod\r\n        def about_apple():\r\n            print(&#39;Apple is good for you.&#39;)\r\n\r\n            # note you can still access other member of the class\r\n            # but you have to use the class instance \r\n            # which is not very nice, because you have repeat yourself\r\n            # \r\n            # For example:\r\n            # @staticmethod\r\n            #    print(&#39;Number of apples have been juiced: %s&#39; % Apple._counter)\r\n            #\r\n            # @classmethod\r\n            #    print(&#39;Number of apples have been juiced: %s&#39; % cls._counter)\r\n            #\r\n            #    @classmethod is especially useful when you move your function to other class,\r\n            #       you don&#39;t have to rename the class reference \r\n    \r\n        @classmethod\r\n        def make_apple_juice(cls, number_of_apples):\r\n            print(&#39;Make juice:&#39;)\r\n            for i in range(number_of_apples):\r\n                cls._juice_this(i)\r\n    \r\n        @classmethod\r\n        def _juice_this(cls, apple):\r\n            print(&#39;Juicing %d...&#39; % apple)\r\n            cls._counter += 1",
               "tags": [],
               "creation_date": 1461339620,
               "last_edit_date": 1517841930,
               "is_accepted": false,
               "id": "36798076",
               "down_vote_count": 0,
               "score": 32
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 38219891,
               "is_accepted": false,
               "last_activity_date": 1467795182,
               "body_markdown": "In [Python][1], a classmethod receives a class as the implicit first argument. The class of the object instance is implicitly passed as the first argument. This can be useful\r\nwhen one wants the method to be a factory of the class as it gets the actual class (which called the method) as the first argument, one can instantiate the right class, even if subclasses are also concerned.\r\n\r\nA staticmethod is just a function defined inside a class. It does not  know anything about the class or instance it was called on and only gets  the arguments that were passed without any implicit first argument.\r\nExample:\r\n\r\n    class Test(object):\r\n        def foo(self, a):\r\n            print &quot;testing (%s,%s)&quot;%(self,a)\r\n     \r\n        @classmethod\r\n        def foo_classmethod(cls, a):\r\n            print &quot;testing foo_classmethod(%s,%s)&quot;%(cls,a)\r\n     \r\n        @staticmethod\r\n        def foo_staticmethod(a):\r\n            print &quot;testing foo_staticmethod(%s)&quot;%a\r\n     \r\n    test = Test()\r\n\r\nstaticmethods are used to group functions which have some logical connection with a class to the class.\r\n\r\n\r\n  [1]: https://www.eduonix.com/courses/Software-Development/the-developers-guide-to-python-3-programming?coupon_code=edusk5",
               "id": "38219891",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1467795182,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 39589894,
               "is_accepted": false,
               "last_activity_date": 1474362209,
               "body_markdown": "I will try to explain the basic difference using an example.\r\n\r\n    class A(object):\r\n        x = 0\r\n    \r\n        def say_hi(self):\r\n            pass\r\n    \r\n        @staticmethod\r\n        def say_hi_static():\r\n            pass\r\n    \r\n        @classmethod\r\n        def say_hi_class(cls):\r\n            pass\r\n    \r\n        def run_self(self):\r\n            self.x += 1\r\n            print self.x # outputs 1\r\n            self.say_hi()\r\n            self.say_hi_static()\r\n            self.say_hi_class()\r\n    \r\n        @staticmethod\r\n        def run_static():\r\n            print A.x  # outputs 0\r\n            # A.say_hi() #  wrong\r\n            A.say_hi_static()\r\n            A.say_hi_class()\r\n    \r\n        @classmethod\r\n        def run_class(cls):\r\n            print cls.x # outputs 0\r\n            # cls.say_hi() #  wrong\r\n            cls.say_hi_static()\r\n            cls.say_hi_class()\r\n\r\n1 - we can directly call static and classmethods without initializing\r\n\r\n    # A.run_self() #  wrong\r\n    A.run_static()\r\n    A.run_class()\r\n\r\n2- Static method cannot call self method but can call other static and classmethod\r\n\r\n3- Static method belong to class and will not use object at all.\r\n\r\n4- Class method are not bound to an object but to a class.",
               "id": "39589894",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1474362209,
               "score": 5
            },
            {
               "up_vote_count": 8,
               "answer_id": 39776104,
               "last_activity_date": 1475597383,
               "path": "3.stack.answer",
               "body_markdown": "Another consideration with respect to staticmethod vs classmethod comes up with inheritance.  Say you have the following class:\r\n\r\n    class Foo(object):\r\n        @staticmethod\r\n        def bar():\r\n            return &quot;In Foo&quot;\r\n\r\nAnd you then want to override ```bar()``` in a child class:\r\n\r\n    class Foo2(Foo):\r\n        @staticmethod\r\n        def bar():\r\n            return &quot;In Foo2&quot;\r\n\r\nThis works, but note that now the ```bar()``` implementation in the child class (```Foo2```) can no longer take advantage of anything specific to that class.  For example, say ```Foo2``` had a method called ```magic()``` that you want to use in the ```Foo2``` implementation of ```bar()```:\r\n\r\n    class Foo2(Foo):\r\n        @staticmethod\r\n        def bar():\r\n            return &quot;In Foo2&quot;\r\n        @staticmethod\r\n        def magic():\r\n            return &quot;Something useful you&#39;d like to use in bar, but now can&#39;t&quot; \r\n\r\nThe workaround here would be to call ```Foo2.magic()``` in ```bar()```, but then you&#39;re repeating yourself (if the name of ```Foo2``` changes, you&#39;ll have to remember to update that ```bar()``` method).\r\n\r\nTo me, this is a slight violation of the [open/closed principle](https://en.wikipedia.org/wiki/Open/closed_principle), since a decision made in ```Foo``` is impacting your ability to refactor common code in a derived class (ie it&#39;s less open to extension).  If ```bar()``` were a ```classmethod``` we&#39;d be fine:\r\n\r\n    class Foo(object):\r\n        @classmethod\r\n        def bar(cls):\r\n            return &quot;In Foo&quot;\r\n    \r\n    class Foo2(Foo):\r\n        @classmethod\r\n        def bar(cls):\r\n            return &quot;In Foo2 &quot; + cls.magic()\r\n        @classmethod\r\n        def magic(cls):\r\n            return &quot;MAGIC&quot;\r\n    \r\n    print Foo2().bar()\r\n\r\nGives: ```In Foo2 MAGIC```",
               "tags": [],
               "creation_date": 1475168571,
               "last_edit_date": 1475597383,
               "is_accepted": false,
               "id": "39776104",
               "down_vote_count": 0,
               "score": 8
            },
            {
               "up_vote_count": 20,
               "answer_id": 39829692,
               "last_activity_date": 1500001825,
               "path": "3.stack.answer",
               "body_markdown": "**Static Methods:**\r\n\r\n - Simple functions with no self argument. \r\n - Work on class attributes; not on instance attributes.\r\n - Can be called through both class and instance.\r\n - The built-in function staticmethod()is used to create them.\r\n\r\n**Benefits of Static Methods:**\r\n\r\n - It localizes the function name in the classscope\r\n - It moves the function code closer to where it is used\r\n - More convenient to import versus module-level functions since each method does not have to be specially imported\r\n\r\n        @staticmethod\r\n        def some_static_method(*args, **kwds):\r\n            pass\r\n \r\n**Class Methods:**\r\n\r\n - Functions that have first argument as classname.\r\n - Can be called through both class and instance.\r\n - These are created with classmethod in-built function.\r\n\r\n         @classmethod\r\n         def some_class_method(cls, *args, **kwds):\r\n             pass\r\n \r\n",
               "tags": [],
               "creation_date": 1475491278,
               "last_edit_date": 1500001825,
               "is_accepted": false,
               "id": "39829692",
               "down_vote_count": 0,
               "score": 20
            },
            {
               "up_vote_count": 1,
               "answer_id": 46327819,
               "last_activity_date": 1505927221,
               "path": "3.stack.answer",
               "body_markdown": "@classmethod : can be used to create a shared global access to all the instances created of that class..... like updating a record by multiple users....\r\nI particulary found it use ful when creating singletons as well..:)\r\n\r\n\r\n@static method:  has nothing to do with the class or instance being associated with ...but for readability can use static method",
               "tags": [],
               "creation_date": 1505926640,
               "last_edit_date": 1505927221,
               "is_accepted": false,
               "id": "46327819",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 46664125,
               "is_accepted": false,
               "last_activity_date": 1507630250,
               "body_markdown": "I started learning programming language with C++ and then Java and then Python and so this question bothered me a lot as well, until I understand the simple usage of each. \r\n\r\n**Class Method:** Python unlike Java and C++ doesn&#39;t have constructor overloading.  And so so to achieve this you could use `classmethod`. following example will explain this \r\n\r\nLet&#39;s consider we have a `Person` class which takes two argument `first_name` and `last_name` and creates the instance of Person. \r\n\r\n    class Person(object):\r\n    \r\n        def __init__(self, first_name, last_name):\r\n            self.first_name = first_name\r\n            self.last_name = last_name\r\n\r\nNow, if the requirement comes where you need to create a class using a single name only, just a `first_name`. you **can&#39;t** do something like this in python. \r\n\r\nThis will give you an error when you will try to create an object (instance).\r\n\r\n    class Person(object):\r\n    \r\n        def __init__(self, first_name, last_name):\r\n            self.first_name = first_name\r\n            self.last_name = last_name\r\n    \r\n        def __init__(self, first_name):\r\n            self.first_name = first_name\r\n\r\nHowever, you could achieve the same thing using `@classmethod` as mentioned below \r\n\r\n    class Person(object):\r\n    \r\n        def __init__(self, first_name, last_name):\r\n            self.first_name = first_name\r\n            self.last_name = last_name\r\n    \r\n        @classmethod\r\n        def get_person(cls, first_name):\r\n            return cls(first_name, &quot;&quot;)\r\n\r\n**Static Method:**: This&#39;s a rather simple, it&#39;s not bound to instance or class and you can simple call that using class name. \r\n\r\nSo let&#39;s say in above example you need a validation that `first_name` should not exceed 20 characters, you can simple do this. \r\n\r\n@staticmethod\r\ndef validate_name(name):\r\n    return len(name) &lt;= 20\r\n\r\nand you could simple call using Class Name \r\n\r\n    Person.validate_name(&quot;Gaurang Shah&quot;)\r\n\r\n",
               "id": "46664125",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1507630250,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47591541,
               "is_accepted": false,
               "last_activity_date": 1512124682,
               "body_markdown": "Class methods, as the name suggests, are used to make changes to classes and not the objects. To make changes to classes, they will modify the class attributes(not object attributes), since that is how you update classes.\r\nThis is the reason that class methods take the class(conventionally denoted by &#39;cls&#39;) as the first argument.\r\n\r\n    class A(object):\r\n        m=54\r\n\r\n        @classmethod\r\n        def class_method(cls):\r\n            print &quot;m is %d&quot; % cls.m\r\n\r\nStatic methods on the other hand, are used to perform functionalities that are not bound to the class i.e. they will not read or write class variables. Hence, static methods do not take classes as arguments. They are used so that classes can perform functionalities that are not directly related to the purpose of the class.\r\n\r\n    class X(object):\r\n        m=54 #will not be referenced\r\n\r\n        @staticmethod\r\n        def static_method():\r\n            print &quot;Referencing/calling a variable or function outside this class. E.g. Some global variable/function.&quot;",
               "id": "47591541",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512124682,
               "score": 0
            },
            {
               "up_vote_count": 0,
               "answer_id": 47769396,
               "last_activity_date": 1513077499,
               "path": "3.stack.answer",
               "body_markdown": "Analyze @staticmethod __literally__ providing different insights.\r\n\r\nA normal method of a class is an implicit **dynamic** method which takes the instance as first argument.  \r\nIn contrast, a staticmethod does not take the instance as first argument, so is called **&#39;static&#39;**.\r\n\r\nA staticmethod is indeed such a normal function the same as those outside a class definition.  \r\nIt is luckily grouped into the class just in order to stand closer where it is applied, or you might scroll around to find it.",
               "tags": [],
               "creation_date": 1513071650,
               "last_edit_date": 1513077499,
               "is_accepted": false,
               "id": "47769396",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 1,
               "answer_id": 48250384,
               "last_activity_date": 1519289959,
               "path": "3.stack.answer",
               "body_markdown": "Let me first tell the similarity between a method decorated with @classmethod vs @staticmethod first.\r\n\r\n**Similarity:** Both of them can be called on the *Class* itself, rather than just the *instance* of the class. So, both of them in a sense are *Class&#39;s methods*. \r\n\r\n**Difference:** A classmethod will receive the class itself as the first argument, while a staticmethod does not.\r\n\r\nSo a static method is, in a sense, not bound to the Class itself and is just hanging in there just because it may have a related functionality. \r\n\r\n\r\n    &gt;&gt;&gt; class Klaus:\r\n            @classmethod\r\n            def classmthd(*args):\r\n                return args\r\n    \r\n            @staticmethod\r\n            def staticmthd(*args):\r\n                return args\r\n\r\n    # 1. Call classmethod without any arg\r\n    &gt;&gt;&gt; Klaus.classmthd()  \r\n    (__main__.Klaus,)  # the class gets passed as the first argument\r\n\r\n    # 2. Call classmethod with 1 arg\r\n    &gt;&gt;&gt; Klaus.classmthd(&#39;chumma&#39;)\r\n    (__main__.Klaus, &#39;chumma&#39;)\r\n\r\n    # 3. Call staticmethod without any arg\r\n    &gt;&gt;&gt; Klaus.staticmthd()  \r\n    ()\r\n\r\n    # 4. Call staticmethod with 1 arg\r\n    &gt;&gt;&gt; Klaus.staticmthd(&#39;chumma&#39;)\r\n    (&#39;chumma&#39;,)\r\n\r\n",
               "tags": [],
               "creation_date": 1515938859,
               "last_edit_date": 1519289959,
               "is_accepted": false,
               "id": "48250384",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 48389508,
               "is_accepted": false,
               "last_activity_date": 1516651877,
               "body_markdown": "**class method vs static method in Python**\r\n\r\n**Class Method**\r\n\r\nThe @classmethod decorator, is a builtin function decorator that is an expression that gets evaluated after your function is defined. The result of that evaluation shadows your function definition.\r\n\r\nA class method receives the class as implicit first argument, just like an instance method receives the instance\r\n\r\n**Syntax:**\r\n\r\n    class C(object):\r\n        @classmethod\r\n        def fun(cls, arg1, arg2, ...):\r\n           ....\r\n\r\n    fun: function that needs to be converted into a class method\r\n    returns: a class method for function.\r\n\r\n - A class method is a method which is bound to the class and not the\r\n   object of the class.\r\n - They have the access to the state of the class as it takes a class\r\n   parameter that points to the class and not the object instance.\r\n - It can modify a class state that would apply across all the instances\r\n   of the class.  For example it can modify a class variable that will\r\n   be applicable to all the instances.\r\n\r\n**Static Method**\r\n\r\nA static method does not receive an implicit first argument.\r\n\r\n**Syntax:**\r\n\r\n    class C(object):\r\n        @staticmethod\r\n        def fun(arg1, arg2, ...):\r\n            ...\r\n    returns: a static method for function fun.\r\n\r\n - A static method is also a method which is bound to the class and not\r\n   the object of the class.\r\n - A static method can\u2019t access or modify class state.\r\n - It is present in a class because it makes sense for the method to be\r\n   present in class.\r\n\r\n**Class method vs Static Method**\r\n   \r\n\r\n - A class method takes cls as first parameter while a static method\r\n   needs no specific parameters.\r\n - A class method can access or modify class state while a static method\r\n   can\u2019t access or modify it.\r\n - We use @classmethod decorator in python to create a class method and\r\n   we use @staticmethod decorator to create a static method in python.\r\n\r\n**When to use what?**\r\n\r\n - We generally use class method to create factory methods. Factory\r\n   methods return class object ( similar to a constructor ) for\r\n   different use cases.\r\n - We generally use static methods to create utility functions.\r\n\r\n**How to define a class method and a static method?**\r\n\r\nTo define a class method in python, we use @classmethod decorator and to define a static method we use @staticmethod decorator.\r\n\r\n\r\nLet us look at an example to understand the difference between both of them. \r\nLet us say we want to create a class Person. Now, python doesn\u2019t support method overloading like C++ or Java so we use class methods to create factory methods. In the below example we use a class method to create a person object from birth year.\r\n\r\nAs explained above we use static methods to create utility functions. In the below example we use a static method to check if a person is adult or not.\r\n\r\n*Implementation*\r\n\r\n    # Python program to demonstrate \r\n    # use of class method and static method.\r\n    from datetime import date\r\n     \r\n    class Person:\r\n        def __init__(self, name, age):\r\n            self.name = name\r\n            self.age = age\r\n         \r\n        # a class method to create a Person object by birth year.\r\n        @classmethod\r\n        def fromBirthYear(cls, name, year):\r\n            return cls(name, date.today().year - year)\r\n         \r\n        # a static method to check if a Person is adult or not.\r\n        @staticmethod\r\n        def isAdult(age):\r\n            return age &gt; 18\r\n     \r\n    person1 = Person(&#39;mayank&#39;, 21)\r\n    person2 = Person.fromBirthYear(&#39;mayank&#39;, 1996)\r\n     \r\n    print person1.age\r\n    print person2.age\r\n     \r\n    # print the result\r\n    print Person.isAdult(22)\r\n\r\n\r\n**Output**\r\n\r\n    21\r\n    21\r\n    True\r\n\r\n[Reference][1]\r\n\r\n\r\n  [1]: https://www.geeksforgeeks.org/class-method-vs-static-method-python/",
               "id": "48389508",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1516651877,
               "score": 3
            }
         ],
         "link": "https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python",
         "id": "858127-2281"
      },
      {
         "up_vote_count": "229",
         "path": "2.stack",
         "body_markdown": "I have the following DataFrame:\r\n\r\n                 daysago  line_race rating        rw    wrating\r\n     line_date                                                 \r\n     2007-03-31       62         11     56  1.000000  56.000000\r\n     2007-03-10       83         11     67  1.000000  67.000000\r\n     2007-02-10      111          9     66  1.000000  66.000000\r\n     2007-01-13      139         10     83  0.880678  73.096278\r\n     2006-12-23      160         10     88  0.793033  69.786942\r\n     2006-11-09      204          9     52  0.636655  33.106077\r\n     2006-10-22      222          8     66  0.581946  38.408408\r\n     2006-09-29      245          9     70  0.518825  36.317752\r\n     2006-09-16      258         11     68  0.486226  33.063381\r\n     2006-08-30      275          8     72  0.446667  32.160051\r\n     2006-02-11      475          5     65  0.164591  10.698423\r\n     2006-01-13      504          0     70  0.142409   9.968634\r\n     2006-01-02      515          0     64  0.134800   8.627219\r\n     2005-12-06      542          0     70  0.117803   8.246238\r\n     2005-11-29      549          0     70  0.113758   7.963072\r\n     2005-11-22      556          0     -1  0.109852  -0.109852\r\n     2005-11-01      577          0     -1  0.098919  -0.098919\r\n     2005-10-20      589          0     -1  0.093168  -0.093168\r\n     2005-09-27      612          0     -1  0.083063  -0.083063\r\n     2005-09-07      632          0     -1  0.075171  -0.075171\r\n     2005-06-12      719          0     69  0.048690   3.359623\r\n     2005-05-29      733          0     -1  0.045404  -0.045404\r\n     2005-05-02      760          0     -1  0.039679  -0.039679\r\n     2005-04-02      790          0     -1  0.034160  -0.034160\r\n     2005-03-13      810          0     -1  0.030915  -0.030915\r\n     2004-11-09      934          0     -1  0.016647  -0.016647\r\n\r\nI need to remove the rows where `line_race` is equal to `0`. What&#39;s the most efficient way to do this?",
         "view_count": "334140",
         "answer_count": "4",
         "tags": "['python', 'pandas']",
         "creation_date": "1376230497",
         "last_edit_date": "1469826000",
         "code_snippet": "['<code>             daysago  line_race rating        rw    wrating\\n line_date                                                 \\n 2007-03-31       62         11     56  1.000000  56.000000\\n 2007-03-10       83         11     67  1.000000  67.000000\\n 2007-02-10      111          9     66  1.000000  66.000000\\n 2007-01-13      139         10     83  0.880678  73.096278\\n 2006-12-23      160         10     88  0.793033  69.786942\\n 2006-11-09      204          9     52  0.636655  33.106077\\n 2006-10-22      222          8     66  0.581946  38.408408\\n 2006-09-29      245          9     70  0.518825  36.317752\\n 2006-09-16      258         11     68  0.486226  33.063381\\n 2006-08-30      275          8     72  0.446667  32.160051\\n 2006-02-11      475          5     65  0.164591  10.698423\\n 2006-01-13      504          0     70  0.142409   9.968634\\n 2006-01-02      515          0     64  0.134800   8.627219\\n 2005-12-06      542          0     70  0.117803   8.246238\\n 2005-11-29      549          0     70  0.113758   7.963072\\n 2005-11-22      556          0     -1  0.109852  -0.109852\\n 2005-11-01      577          0     -1  0.098919  -0.098919\\n 2005-10-20      589          0     -1  0.093168  -0.093168\\n 2005-09-27      612          0     -1  0.083063  -0.083063\\n 2005-09-07      632          0     -1  0.075171  -0.075171\\n 2005-06-12      719          0     69  0.048690   3.359623\\n 2005-05-29      733          0     -1  0.045404  -0.045404\\n 2005-05-02      760          0     -1  0.039679  -0.039679\\n 2005-04-02      790          0     -1  0.034160  -0.034160\\n 2005-03-13      810          0     -1  0.030915  -0.030915\\n 2004-11-09      934          0     -1  0.016647  -0.016647\\n</code>', '<code>line_race</code>', '<code>0</code>', '<code>df = df[df.line_race != 0]\\n</code>', '<code>df</code>', '<code>df</code>', \"<code>df = df[df['line race'] != 0]</code>\", '<code>df = df[df.line_race != 0]</code>', '<code>None</code>', '<code>df = df[df.line_race != 0]\\n</code>', '<code>df = df[df.line_race != None]\\n</code>', '<code>df = df[df.line_race.notnull()]\\n</code>', '<code>In [56]: df\\nOut[56]:\\n     line_date  daysago  line_race  rating    raw  wrating\\n0   2007-03-31       62         11      56  1.000   56.000\\n1   2007-03-10       83         11      67  1.000   67.000\\n2   2007-02-10      111          9      66  1.000   66.000\\n3   2007-01-13      139         10      83  0.881   73.096\\n4   2006-12-23      160         10      88  0.793   69.787\\n5   2006-11-09      204          9      52  0.637   33.106\\n6   2006-10-22      222          8      66  0.582   38.408\\n7   2006-09-29      245          9      70  0.519   36.318\\n8   2006-09-16      258         11      68  0.486   33.063\\n9   2006-08-30      275          8      72  0.447   32.160\\n10  2006-02-11      475          5      65  0.165   10.698\\n11  2006-01-13      504          0      70  0.142    9.969\\n12  2006-01-02      515          0      64  0.135    8.627\\n13  2005-12-06      542          0      70  0.118    8.246\\n14  2005-11-29      549          0      70  0.114    7.963\\n15  2005-11-22      556          0      -1  0.110   -0.110\\n16  2005-11-01      577          0      -1  0.099   -0.099\\n17  2005-10-20      589          0      -1  0.093   -0.093\\n18  2005-09-27      612          0      -1  0.083   -0.083\\n19  2005-09-07      632          0      -1  0.075   -0.075\\n20  2005-06-12      719          0      69  0.049    3.360\\n21  2005-05-29      733          0      -1  0.045   -0.045\\n22  2005-05-02      760          0      -1  0.040   -0.040\\n23  2005-04-02      790          0      -1  0.034   -0.034\\n24  2005-03-13      810          0      -1  0.031   -0.031\\n25  2004-11-09      934          0      -1  0.017   -0.017\\n\\nIn [57]: df[df.line_race != 0]\\nOut[57]:\\n     line_date  daysago  line_race  rating    raw  wrating\\n0   2007-03-31       62         11      56  1.000   56.000\\n1   2007-03-10       83         11      67  1.000   67.000\\n2   2007-02-10      111          9      66  1.000   66.000\\n3   2007-01-13      139         10      83  0.881   73.096\\n4   2006-12-23      160         10      88  0.793   69.787\\n5   2006-11-09      204          9      52  0.637   33.106\\n6   2006-10-22      222          8      66  0.582   38.408\\n7   2006-09-29      245          9      70  0.519   36.318\\n8   2006-09-16      258         11      68  0.486   33.063\\n9   2006-08-30      275          8      72  0.447   32.160\\n10  2006-02-11      475          5      65  0.165   10.698\\n</code>', \"<code>df.query('line_race != 0')</code>\", '<code>query</code>', \"<code>df.query('variable in var_list')</code>\", '<code>query</code>', \"<code>df = df.rename(columns=lambda x: x.strip().replace(' ','_'))</code>\", \"<code>df.query('line_race != 0')</code>\"]",
         "title": "Deleting DataFrame row in Pandas based on column value",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 430,
               "answer_id": 18173074,
               "is_accepted": true,
               "last_activity_date": 1376231911,
               "body_markdown": "If I&#39;m understanding correctly, it should be as simple as:\r\n\r\n    df = df[df.line_race != 0]",
               "id": "18173074",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1376231911,
               "score": 428
            },
            {
               "up_vote_count": 30,
               "answer_id": 18173088,
               "last_activity_date": 1392685233,
               "path": "3.stack.answer",
               "body_markdown": "The best way to do this is with boolean masking:\r\n\r\n    In [56]: df\r\n    Out[56]:\r\n         line_date  daysago  line_race  rating    raw  wrating\r\n    0   2007-03-31       62         11      56  1.000   56.000\r\n    1   2007-03-10       83         11      67  1.000   67.000\r\n    2   2007-02-10      111          9      66  1.000   66.000\r\n    3   2007-01-13      139         10      83  0.881   73.096\r\n    4   2006-12-23      160         10      88  0.793   69.787\r\n    5   2006-11-09      204          9      52  0.637   33.106\r\n    6   2006-10-22      222          8      66  0.582   38.408\r\n    7   2006-09-29      245          9      70  0.519   36.318\r\n    8   2006-09-16      258         11      68  0.486   33.063\r\n    9   2006-08-30      275          8      72  0.447   32.160\r\n    10  2006-02-11      475          5      65  0.165   10.698\r\n    11  2006-01-13      504          0      70  0.142    9.969\r\n    12  2006-01-02      515          0      64  0.135    8.627\r\n    13  2005-12-06      542          0      70  0.118    8.246\r\n    14  2005-11-29      549          0      70  0.114    7.963\r\n    15  2005-11-22      556          0      -1  0.110   -0.110\r\n    16  2005-11-01      577          0      -1  0.099   -0.099\r\n    17  2005-10-20      589          0      -1  0.093   -0.093\r\n    18  2005-09-27      612          0      -1  0.083   -0.083\r\n    19  2005-09-07      632          0      -1  0.075   -0.075\r\n    20  2005-06-12      719          0      69  0.049    3.360\r\n    21  2005-05-29      733          0      -1  0.045   -0.045\r\n    22  2005-05-02      760          0      -1  0.040   -0.040\r\n    23  2005-04-02      790          0      -1  0.034   -0.034\r\n    24  2005-03-13      810          0      -1  0.031   -0.031\r\n    25  2004-11-09      934          0      -1  0.017   -0.017\r\n    \r\n    In [57]: df[df.line_race != 0]\r\n    Out[57]:\r\n         line_date  daysago  line_race  rating    raw  wrating\r\n    0   2007-03-31       62         11      56  1.000   56.000\r\n    1   2007-03-10       83         11      67  1.000   67.000\r\n    2   2007-02-10      111          9      66  1.000   66.000\r\n    3   2007-01-13      139         10      83  0.881   73.096\r\n    4   2006-12-23      160         10      88  0.793   69.787\r\n    5   2006-11-09      204          9      52  0.637   33.106\r\n    6   2006-10-22      222          8      66  0.582   38.408\r\n    7   2006-09-29      245          9      70  0.519   36.318\r\n    8   2006-09-16      258         11      68  0.486   33.063\r\n    9   2006-08-30      275          8      72  0.447   32.160\r\n    10  2006-02-11      475          5      65  0.165   10.698\r\n\r\n**UPDATE:** Now that pandas 0.13 is out, another way to do this is `df.query(&#39;line_race != 0&#39;)`.\r\n\r\n",
               "tags": [],
               "creation_date": 1376232032,
               "last_edit_date": 1392685233,
               "is_accepted": false,
               "id": "18173088",
               "down_vote_count": 0,
               "score": 30
            },
            {
               "up_vote_count": 111,
               "answer_id": 24489602,
               "last_activity_date": 1450867558,
               "path": "3.stack.answer",
               "body_markdown": "But for any future bypassers you could mention that `df = df[df.line_race != 0]` doesn&#39;t do anything when trying to filter for `None`/missing values.\r\n\r\nDoes work:\r\n\r\n    df = df[df.line_race != 0]\r\n\r\nDoesn&#39;t do anything:\r\n\r\n    df = df[df.line_race != None]\r\nDoes work:\r\n\r\n    df = df[df.line_race.notnull()]\r\n\r\n",
               "tags": [],
               "creation_date": 1404129403,
               "last_edit_date": 1450867558,
               "is_accepted": false,
               "id": "24489602",
               "down_vote_count": 1,
               "score": 110
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 42626123,
               "is_accepted": false,
               "last_activity_date": 1488804653,
               "body_markdown": "The given answer is correct nontheless as someone above said you can use `df.query(&#39;line_race != 0&#39;)` which depending on your problem is much faster. Highly recommend.\r\n\r\n",
               "id": "42626123",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1488804653,
               "score": 6
            }
         ],
         "link": "https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value",
         "id": "858127-2282"
      },
      {
         "up_vote_count": "312",
         "path": "2.stack",
         "body_markdown": "&gt; **Possible Duplicate:**  \r\n&gt; [how to filter the dataframe rows of pandas by &amp;ldquo;within&amp;rdquo;/&amp;ldquo;in&amp;rdquo;?](https://stackoverflow.com/questions/12065885/how-to-filter-the-dataframe-rows-of-pandas-by-within-in)  \r\n\r\n&lt;!-- End of automatically inserted text --&gt;\r\n\r\nLets say I have the following pandas dataframe:\r\n\r\n    df = DataFrame({&#39;A&#39; : [5,6,3,4], &#39;B&#39; : [1,2,3, 5]})\r\n    df\r\n    \r\n         A   B\r\n    0\t 5\t 1\r\n    1\t 6\t 2\r\n    2\t 3\t 3\r\n    3\t 4\t 5\r\n\r\nI can subset based on a specific value:\r\n\r\n    x = df[df[&#39;A&#39;] == 3]\r\n    x\r\n    \r\n         A   B\r\n    2\t 3\t 3\r\n\r\nBut how can I subset based on a list of values? - something like this:\r\n\r\n    list_of_values = [3,6]\r\n\r\n    y = df[df[&#39;A&#39;] in list_of_values]\r\n\r\n\r\n",
         "view_count": "241937",
         "answer_count": "1",
         "tags": "['python', 'pandas']",
         "creation_date": "1345739472",
         "last_edit_date": "1506192899",
         "code_snippet": "[\"<code>df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\\ndf\\n\\n     A   B\\n0    5   1\\n1    6   2\\n2    3   3\\n3    4   5\\n</code>\", \"<code>x = df[df['A'] == 3]\\nx\\n\\n     A   B\\n2    3   3\\n</code>\", \"<code>list_of_values = [3,6]\\n\\ny = df[df['A'] in list_of_values]\\n</code>\", \"<code>In [5]: df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\\n\\nIn [6]: df\\nOut[6]:\\n   A  B\\n0  5  1\\n1  6  2\\n2  3  3\\n3  4  5\\n\\nIn [7]: df[df['A'].isin([3, 6])]\\nOut[7]:\\n   A  B\\n1  6  2\\n2  3  3\\n</code>\", '<code>list_of_values</code>']",
         "title": "Use a list of values to select rows from a pandas dataframe",
         "_childDocuments_": [
            {
               "up_vote_count": 473,
               "answer_id": 12098586,
               "last_activity_date": 1345749612,
               "path": "3.stack.answer",
               "body_markdown": "This is indeed a duplicate of [how to filter the dataframe rows of pandas by &quot;within&quot;/&quot;in&quot;?][1], translating the response to your example gives:\r\n\r\n    In [5]: df = DataFrame({&#39;A&#39; : [5,6,3,4], &#39;B&#39; : [1,2,3, 5]})\r\n\r\n    In [6]: df\r\n    Out[6]:\r\n       A  B\r\n    0  5  1\r\n    1  6  2\r\n    2  3  3\r\n    3  4  5\r\n\r\n    In [7]: df[df[&#39;A&#39;].isin([3, 6])]\r\n    Out[7]:\r\n       A  B\r\n    1  6  2\r\n    2  3  3\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/12065885/how-to-filter-the-dataframe-rows-of-pandas-by-within-in",
               "tags": [],
               "creation_date": 1345749612,
               "last_edit_date": 1495541443,
               "is_accepted": true,
               "id": "12098586",
               "down_vote_count": 0,
               "score": 473
            }
         ],
         "link": "https://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe",
         "id": "858127-2283"
      },
      {
         "up_vote_count": "425",
         "path": "2.stack",
         "body_markdown": "I want to get a list of the column headers from a pandas DataFrame.  The DataFrame will come from user input so I won&#39;t know how many columns there will be or what they will be called.\r\n\r\nFor example, if I&#39;m given a DataFrame like this:\r\n\r\n    &gt;&gt;&gt; my_dataframe\r\n        y  gdp  cap\r\n    0   1    2    5\r\n    1   2    3    9\r\n    2   8    7    2\r\n    3   3    4    7\r\n    4   6    7    7\r\n    5   4    8    3\r\n    6   8    2    8\r\n    7   9    9   10\r\n    8   6    6    4\r\n    9  10   10    7\r\n\r\nI would want to get a list like this:\r\n\r\n    &gt;&gt;&gt; header_list\r\n    [y, gdp, cap]",
         "view_count": "645089",
         "answer_count": "14",
         "tags": "['python', 'pandas', 'dataframe']",
         "creation_date": "1382303917",
         "last_edit_date": "1450596207",
         "code_snippet": "['<code>&gt;&gt;&gt; my_dataframe\\n    y  gdp  cap\\n0   1    2    5\\n1   2    3    9\\n2   8    7    2\\n3   3    4    7\\n4   6    7    7\\n5   4    8    3\\n6   8    2    8\\n7   9    9   10\\n8   6    6    4\\n9  10   10    7\\n</code>', '<code>&gt;&gt;&gt; header_list\\n[y, gdp, cap]\\n</code>', '<code>list(my_dataframe.columns.values)\\n</code>', '<code>list(my_dataframe)\\n</code>', '<code>columns</code>', '<code>df.column_names()</code>', '<code>my_dataframe.columns.values.tolist()\\n</code>', '<code>.columns</code>', '<code>Index</code>', '<code>.columns.values</code>', '<code>array</code>', '<code>list</code>', '<code>list(df)\\n</code>', '<code>dataframe.columns.values.tolist()</code>', '<code>In [1]: %timeit [column for column in df]\\n1000 loops, best of 3: 81.6 \u00b5s per loop\\n\\nIn [2]: %timeit df.columns.values.tolist()\\n10000 loops, best of 3: 16.1 \u00b5s per loop\\n\\nIn [3]: %timeit list(df)\\n10000 loops, best of 3: 44.9 \u00b5s per loop\\n\\nIn [4]: % timeit list(df.columns.values)\\n10000 loops, best of 3: 38.4 \u00b5s per loop\\n</code>', '<code>list(dataframe)</code>', '<code>df.columns.tolist()\\n</code>', \"<code>&gt;&gt;&gt; list(my_dataframe)\\n['y', 'gdp', 'cap']\\n</code>\", \"<code>&gt;&gt;&gt; [c for c in my_dataframe]\\n['y', 'gdp', 'cap']\\n</code>\", '<code>sorted</code>', \"<code>&gt;&gt;&gt; sorted(my_dataframe)\\n['cap', 'gdp', 'y']\\n</code>\", '<code>list(df)</code>', '<code>[c for c in df]</code>', '<code>my_dataframe.columns</code>', '<code>header_list = list(my_dataframe.columns)</code>', '<code>df.columns.values.tolist()</code>', '<code>df.columns.tolist()</code>', '<code>In [97]: %timeit df.columns.values.tolist()\\n100000 loops, best of 3: 2.97 \u00b5s per loop\\n\\nIn [98]: %timeit df.columns.tolist()\\n10000 loops, best of 3: 9.67 \u00b5s per loop\\n</code>', '<code>[column for column in my_dataframe]\\n</code>', '<code>sorted(df)\\n</code>', '<code>df.columns\\n</code>', '<code>list(my_dataframe.columns)\\n</code>', '<code>n = []\\nfor i in my_dataframe.columns:\\n    n.append(i)\\nprint n\\n</code>', '<code>[n for n in dataframe.columns]</code>', '<code>pd.__version__</code>', '<code>df.keys().tolist()</code>', '<code>df.columns</code>', '<code>list(my_dataframe.columns.values) \\n</code>', '<code>list(my_dataframe) # for less typing.\\n</code>', '<code>list(my_dataframe.columns)\\n</code>', \"<code>df = pd.DataFrame({'col1' : np.random.randn(3), 'col2' : np.random.randn(3)},\\n                 index=['a', 'b', 'c'])\\n</code>\"]",
         "title": "Get list from pandas DataFrame column headers",
         "_childDocuments_": [
            {
               "up_vote_count": 15,
               "answer_id": 19482988,
               "last_activity_date": 1390503027,
               "path": "3.stack.answer",
               "body_markdown": "That&#39;s available as `my_dataframe.columns`.",
               "tags": [],
               "creation_date": 1382304006,
               "last_edit_date": 1390503027,
               "is_accepted": false,
               "id": "19482988",
               "down_vote_count": 1,
               "score": 14
            },
            {
               "up_vote_count": 733,
               "answer_id": 19483025,
               "last_activity_date": 1483677486,
               "path": "3.stack.answer",
               "body_markdown": "You can get the values as a list by doing:\r\n\r\n    list(my_dataframe.columns.values)\r\n\r\nAlso you can simply use:\r\n\r\n    list(my_dataframe)",
               "tags": [],
               "creation_date": 1382304187,
               "last_edit_date": 1483677486,
               "is_accepted": true,
               "id": "19483025",
               "down_vote_count": 2,
               "score": 731
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 19483232,
               "is_accepted": false,
               "last_activity_date": 1382305410,
               "body_markdown": "    n = []\r\n    for i in my_dataframe.columns:\r\n        n.append(i)\r\n    print n\r\n",
               "id": "19483232",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1382305410,
               "score": 1
            },
            {
               "up_vote_count": 235,
               "answer_id": 19483602,
               "last_activity_date": 1426602847,
               "path": "3.stack.answer",
               "body_markdown": "There is a built in method which is the most performant:\r\n\r\n    my_dataframe.columns.values.tolist()\r\n\r\n`.columns` returns an `Index`, `.columns.values` returns an `array` and this has a helper function to return a `list`.\r\n\r\n**EDIT**\r\n\r\nFor those who hate typing this is probably the shortest method:\r\n\r\n    list(df)",
               "tags": [],
               "creation_date": 1382307915,
               "last_edit_date": 1426602847,
               "is_accepted": false,
               "id": "19483602",
               "down_vote_count": 3,
               "score": 232
            },
            {
               "up_vote_count": 6,
               "answer_id": 21315199,
               "last_activity_date": 1426239715,
               "path": "3.stack.answer",
               "body_markdown": "    [column for column in my_dataframe]\r\n\r\n[pandas docs][1]: Iteration over dataframes return column labels\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/basics.html#iteration",
               "tags": [],
               "creation_date": 1390497820,
               "last_edit_date": 1426239715,
               "is_accepted": false,
               "id": "21315199",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "up_vote_count": 51,
               "answer_id": 27236748,
               "last_activity_date": 1426262465,
               "path": "3.stack.answer",
               "body_markdown": "Did some quick tests, and perhaps unsurprisingly the built-in version using `dataframe.columns.values.tolist()` is the fastest:\r\n\r\n\r\n    In [1]: %timeit [column for column in df]\r\n    1000 loops, best of 3: 81.6 &#181;s per loop\r\n    \r\n    In [2]: %timeit df.columns.values.tolist()\r\n    10000 loops, best of 3: 16.1 &#181;s per loop\r\n    \r\n    In [3]: %timeit list(df)\r\n    10000 loops, best of 3: 44.9 &#181;s per loop\r\n    \r\n    In [4]: % timeit list(df.columns.values)\r\n    10000 loops, best of 3: 38.4 &#181;s per loop\r\n\r\n(I still really like the `list(dataframe)` though, so thanks EdChum!)\r\n",
               "tags": [],
               "creation_date": 1417465916,
               "last_edit_date": 1426262465,
               "is_accepted": false,
               "id": "27236748",
               "down_vote_count": 0,
               "score": 51
            },
            {
               "up_vote_count": 25,
               "answer_id": 29494537,
               "last_activity_date": 1430079782,
               "path": "3.stack.answer",
               "body_markdown": "Its gets even simpler (by pandas 0.16.0) : \r\n\r\n    df.columns.tolist()\r\n\r\nwill give you the column names in a nice list.\r\n",
               "tags": [],
               "creation_date": 1428418233,
               "last_edit_date": 1430079782,
               "is_accepted": false,
               "id": "29494537",
               "down_vote_count": 0,
               "score": 25
            },
            {
               "up_vote_count": 24,
               "answer_id": 30511605,
               "last_activity_date": 1517005524,
               "path": "3.stack.answer",
               "body_markdown": "    &gt;&gt;&gt; list(my_dataframe)\r\n    [&#39;y&#39;, &#39;gdp&#39;, &#39;cap&#39;]\r\n\r\nTo list the columns of a dataframe while in debugger mode, use a list comprehension:\r\n\r\n    &gt;&gt;&gt; [c for c in my_dataframe]\r\n    [&#39;y&#39;, &#39;gdp&#39;, &#39;cap&#39;]\r\n\r\nBy the way, you can get a sorted list simply by using `sorted`:\r\n\r\n    &gt;&gt;&gt; sorted(my_dataframe)\r\n    [&#39;cap&#39;, &#39;gdp&#39;, &#39;y&#39;]",
               "tags": [],
               "creation_date": 1432828685,
               "last_edit_date": 1517005524,
               "is_accepted": false,
               "id": "30511605",
               "down_vote_count": 0,
               "score": 24
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 34097939,
               "is_accepted": false,
               "last_activity_date": 1449265313,
               "body_markdown": "It&#39;s interesting but `df.columns.values.tolist()` is almost 3 times faster then `df.columns.tolist()` but I thought that they are the same:\r\n\r\n    In [97]: %timeit df.columns.values.tolist()\r\n    100000 loops, best of 3: 2.97 &#181;s per loop\r\n\r\n    In [98]: %timeit df.columns.tolist()\r\n    10000 loops, best of 3: 9.67 &#181;s per loop\r\n",
               "id": "34097939",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1449265313,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 36302134,
               "is_accepted": false,
               "last_activity_date": 1459322375,
               "body_markdown": "##In the Notebook\r\nFor data exploration in the IPython notebook, my preferred way is this:\r\n\r\n    sorted(df)\r\n\r\nWhich will produce an easy to read alphabetically ordered list.\r\n\r\n##In a code repository\r\nIn code I find it more explicit to do\r\n\r\n    df.columns\r\n\r\nBecause it tells others reading your code what you are doing.",
               "id": "36302134",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1459322375,
               "score": 5
            },
            {
               "up_vote_count": 0,
               "answer_id": 40913027,
               "last_activity_date": 1480602187,
               "path": "3.stack.answer",
               "body_markdown": " can use index attributes\r\n\r\n    df = pd.DataFrame({&#39;col1&#39; : np.random.randn(3), &#39;col2&#39; : np.random.randn(3)},\r\n                     index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])",
               "tags": [],
               "creation_date": 1480602088,
               "last_edit_date": 1480602187,
               "is_accepted": false,
               "id": "40913027",
               "down_vote_count": 1,
               "score": -1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 42459921,
               "is_accepted": false,
               "last_activity_date": 1488047049,
               "body_markdown": "simplest way is:\r\n\r\n    list(my_dataframe.columns)",
               "id": "42459921",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1488047049,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47795845,
               "is_accepted": false,
               "last_activity_date": 1513176456,
               "body_markdown": "I feel question deserves additional explanation.\r\n\r\nAs @fixxxer noted, the answer depends on the pandas version you are using in your project.\r\nWhich you can get with `pd.__version__` command.\r\n\r\nIf you are for some reason like me (on debian jessie I use 0.14.1) using older version of pandas than 0.16.0, then you need to use:\r\n\r\n`df.keys().tolist()` because there is no `df.columns` method implemented yet.\r\n\r\nThe advantage of this keys method is, that it works even in newer version of pandas, so it&#39;s more universal.",
               "id": "47795845",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1513176456,
               "score": 0
            },
            {
               "up_vote_count": 0,
               "answer_id": 48832928,
               "last_activity_date": 1518808500,
               "path": "3.stack.answer",
               "body_markdown": "as answered by Simeon Visser...you could do\r\n\r\n    list(my_dataframe.columns.values) \r\nor    \r\n\r\n    list(my_dataframe) # for less typing.\r\n\r\nBut I think most the sweet spot is:\r\n\r\n    list(my_dataframe.columns)\r\nIt is explicit, at the same time not unnecessarily long.",
               "tags": [],
               "creation_date": 1518806168,
               "last_edit_date": 1518808500,
               "is_accepted": false,
               "id": "48832928",
               "down_vote_count": 0,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/19482970/get-list-from-pandas-dataframe-column-headers",
         "id": "858127-2284"
      },
      {
         "up_vote_count": "975",
         "path": "2.stack",
         "body_markdown": "Given an item, how can I count its occurrences in a list in Python?",
         "view_count": "878526",
         "answer_count": "17",
         "tags": "['python', 'list', 'count']",
         "creation_date": "1270733400",
         "last_edit_date": "1510436347",
         "code_snippet": "['<code>count</code>', '<code>&gt;&gt;&gt; [1, 2, 3, 4, 1, 4, 1].count(1)\\n3\\n</code>', '<code>count</code>', '<code>count</code>', '<code>Counter</code>', '<code>mylist = [1,7,7,7,3,9,9,9,7,9,10,0]   print sorted(set([i for i in mylist if mylist.count(i)&gt;2]))</code>', \"<code>&gt;&gt;&gt; from collections import Counter\\n&gt;&gt;&gt; z = ['blue', 'red', 'blue', 'yellow', 'blue', 'red']\\n&gt;&gt;&gt; Counter(z)\\nCounter({'blue': 3, 'red': 2, 'yellow': 1})\\n</code>\", '<code>count()</code>', '<code>&gt;&gt;&gt; l = [\"a\",\"b\",\"b\"]\\n&gt;&gt;&gt; l.count(\"a\")\\n1\\n&gt;&gt;&gt; l.count(\"b\")\\n2\\n</code>', '<code>l</code>', '<code>count()</code>', '<code>[[x,l.count(x)] for x in set(l)]\\n</code>', '<code>dict((x,l.count(x)) for x in set(l))</code>', '<code>&gt;&gt;&gt; l = [\"a\",\"b\",\"b\"]\\n&gt;&gt;&gt; [[x,l.count(x)] for x in set(l)]\\n[[\\'a\\', 1], [\\'b\\', 2]]\\n&gt;&gt;&gt; dict((x,l.count(x)) for x in set(l))\\n{\\'a\\': 1, \\'b\\': 2}\\n</code>', '<code>Counter</code>', '<code>collections</code>', '<code>Counter(l)\\n</code>', '<code>&gt;&gt;&gt; l = [\"a\",\"b\",\"b\"]\\n&gt;&gt;&gt; from collections import Counter\\n&gt;&gt;&gt; Counter(l)\\nCounter({\\'b\\': 2, \\'a\\': 1})\\n</code>', '<code>Counter</code>', '<code>n</code>', '<code>Counter</code>', '<code>from __future__ import print_function\\nimport timeit\\n\\nt1=timeit.Timer(\\'Counter(l)\\', \\\\\\n                \\'import random;import string;from collections import Counter;n=1000;l=[random.choice(string.ascii_letters) for x in range(n)]\\'\\n                )\\n\\nt2=timeit.Timer(\\'[[x,l.count(x)] for x in set(l)]\\',\\n                \\'import random;import string;n=1000;l=[random.choice(string.ascii_letters) for x in range(n)]\\'\\n                )\\n\\nprint(\"Counter(): \", t1.repeat(repeat=3,number=10000))\\nprint(\"count():   \", t2.repeat(repeat=3,number=10000)\\n</code>', '<code>Counter():  [0.46062711701961234, 0.4022796869976446, 0.3974247490405105]\\ncount():    [7.779430688009597, 7.962715800967999, 8.420845870045014]\\n</code>', '<code>Counter</code>', '<code>Counter</code>', '<code>dict((i, a.count(i)) for i in a)\\n</code>', '<code>n * (number of different items)</code>', '<code>collections.Counter</code>', '<code>list.count(x)</code>', '<code>x</code>', '<code>bincount</code>', '<code>import numpy as np\\na = np.array([1, 2, 3, 4, 1, 4, 1])\\nnp.bincount(a)\\n</code>', '<code>&gt;&gt;&gt; array([0, 3, 1, 1, 2])\\n</code>', \"<code>&gt;&gt;&gt; l = list('aaaaabbbbcccdde')\\n&gt;&gt;&gt; l\\n['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'e']\\n</code>\", '<code>list.count</code>', '<code>list.count</code>', \"<code>&gt;&gt;&gt; l.count('b')\\n4\\n</code>\", \"<code>&gt;&gt;&gt; t = tuple('aabbbffffff')\\n&gt;&gt;&gt; t\\n('a', 'a', 'b', 'b', 'b', 'f', 'f', 'f', 'f', 'f', 'f')\\n&gt;&gt;&gt; t.count('f')\\n6\\n</code>\", '<code>collections.Counter</code>', \"<code>&gt;&gt;&gt; from collections import Counter\\n&gt;&gt;&gt; c = Counter(l)\\n&gt;&gt;&gt; c['b']\\n4\\n</code>\", '<code>collections.Counter</code>', \"<code>&gt;&gt;&gt; c.update(list('bbb'))\\n&gt;&gt;&gt; c['b']\\n7\\n&gt;&gt;&gt; c.subtract(list('bbb'))\\n&gt;&gt;&gt; c['b']\\n4\\n</code>\", \"<code>&gt;&gt;&gt; c2 = Counter(list('aabbxyz'))\\n&gt;&gt;&gt; c - c2                   # set difference\\nCounter({'a': 3, 'c': 3, 'b': 2, 'd': 2, 'e': 1})\\n&gt;&gt;&gt; c + c2                   # addition of all elements\\nCounter({'a': 7, 'b': 6, 'c': 3, 'd': 2, 'e': 1, 'y': 1, 'x': 1, 'z': 1})\\n&gt;&gt;&gt; c | c2                   # set union\\nCounter({'a': 5, 'b': 4, 'c': 3, 'd': 2, 'e': 1, 'y': 1, 'x': 1, 'z': 1})\\n&gt;&gt;&gt; c &amp; c2                   # set intersection\\nCounter({'a': 2, 'b': 2})\\n</code>\", \"<code>import pandas as pd\\n\\nl = ['a', 'b', 'c', 'd', 'a', 'd', 'a']\\n\\n# converting the list to a Series and counting the values\\nmy_count = pd.Series(l).value_counts()\\nmy_count\\n</code>\", '<code>a    3\\nd    2\\nb    1\\nc    1\\ndtype: int64\\n</code>', \"<code>my_count['a']\\n</code>\", '<code>3\\n</code>', '<code>dict((i,a.count(i)) for i in a)\\n</code>', '<code>def occurDict(items):\\n    d = {}\\n    for i in items:\\n        if i in d:\\n            d[i] = d[i]+1\\n        else:\\n            d[i] = 1\\nreturn d\\n</code>', '<code># Python &gt;= 2.6 (defaultdict) &amp;&amp; &lt; 2.7 (Counter, OrderedDict)\\nfrom collections import defaultdict\\ndef count_unsorted_list_items(items):\\n    \"\"\"\\n    :param items: iterable of hashable items to count\\n    :type items: iterable\\n\\n    :returns: dict of counts like Py2.7 Counter\\n    :rtype: dict\\n    \"\"\"\\n    counts = defaultdict(int)\\n    for item in items:\\n        counts[item] += 1\\n    return dict(counts)\\n\\n\\n# Python &gt;= 2.2 (generators)\\ndef count_sorted_list_items(items):\\n    \"\"\"\\n    :param items: sorted iterable of items to count\\n    :type items: sorted iterable\\n\\n    :returns: generator of (item, count) tuples\\n    :rtype: generator\\n    \"\"\"\\n    if not items:\\n        return\\n    elif len(items) == 1:\\n        yield (items[0], 1)\\n        return\\n    prev_item = items[0]\\n    count = 1\\n    for item in items[1:]:\\n        if prev_item == item:\\n            count += 1\\n        else:\\n            yield (prev_item, count)\\n            count = 1\\n            prev_item = item\\n    yield (item, count)\\n    return\\n\\n\\nimport unittest\\nclass TestListCounters(unittest.TestCase):\\n    def test_count_unsorted_list_items(self):\\n        D = (\\n            ([], []),\\n            ([2], [(2,1)]),\\n            ([2,2], [(2,2)]),\\n            ([2,2,2,2,3,3,5,5], [(2,4), (3,2), (5,2)]),\\n            )\\n        for inp, exp_outp in D:\\n            counts = count_unsorted_list_items(inp) \\n            print inp, exp_outp, counts\\n            self.assertEqual(counts, dict( exp_outp ))\\n\\n        inp, exp_outp = UNSORTED_WIN = ([2,2,4,2], [(2,3), (4,1)])\\n        self.assertEqual(dict( exp_outp ), count_unsorted_list_items(inp) )\\n\\n\\n    def test_count_sorted_list_items(self):\\n        D = (\\n            ([], []),\\n            ([2], [(2,1)]),\\n            ([2,2], [(2,2)]),\\n            ([2,2,2,2,3,3,5,5], [(2,4), (3,2), (5,2)]),\\n            )\\n        for inp, exp_outp in D:\\n            counts = list( count_sorted_list_items(inp) )\\n            print inp, exp_outp, counts\\n            self.assertEqual(counts, exp_outp)\\n\\n        inp, exp_outp = UNSORTED_FAIL = ([2,2,4,2], [(2,3), (4,1)])\\n        self.assertEqual(exp_outp, list( count_sorted_list_items(inp) ))\\n        # ... [(2,2), (4,1), (2,1)]\\n</code>', '<code>numpy.sum(numpy.array(a) == 1) \\n</code>', '<code>numpy.bincount(a)\\n</code>', '<code>from collections import Counter\\nfrom collections import defaultdict\\nimport numpy\\nimport operator\\nimport pandas\\nimport perfplot\\n\\n\\ndef counter(a):\\n    return Counter(a)\\n\\n\\ndef count(a):\\n    return dict((i, a.count(i)) for i in set(a))\\n\\n\\ndef bincount(a):\\n    return numpy.bincount(a)\\n\\n\\ndef pandas_value_counts(a):\\n    return pandas.Series(a).value_counts()\\n\\n\\ndef occur_dict(a):\\n    d = {}\\n    for i in a:\\n        if i in d:\\n            d[i] = d[i]+1\\n        else:\\n            d[i] = 1\\n    return d\\n\\n\\ndef count_unsorted_list_items(items):\\n    counts = defaultdict(int)\\n    for item in items:\\n        counts[item] += 1\\n    return dict(counts)\\n\\n\\ndef operator_countof(a):\\n    return dict((i, operator.countOf(a, i)) for i in set(a))\\n\\n\\nperfplot.show(\\n    setup=lambda n: list(numpy.random.randint(0, 100, n)),\\n    n_range=[2**k for k in range(20)],\\n    kernels=[\\n        counter, count, bincount, pandas_value_counts, occur_dict,\\n        count_unsorted_list_items, operator_countof\\n        ],\\n    equality_check=None,\\n    logx=True,\\n    logy=True,\\n    )\\n</code>', '<code>from collections import Counter\\nfrom collections import defaultdict\\nimport numpy\\nimport operator\\nimport pandas\\nimport perfplot\\n\\n\\ndef counter(a):\\n    return Counter(a)\\n\\n\\ndef count(a):\\n    return dict((i, a.count(i)) for i in set(a))\\n\\n\\ndef bincount(a):\\n    return numpy.bincount(a)\\n\\n\\ndef pandas_value_counts(a):\\n    return pandas.Series(a).value_counts()\\n\\n\\ndef occur_dict(a):\\n    d = {}\\n    for i in a:\\n        if i in d:\\n            d[i] = d[i]+1\\n        else:\\n            d[i] = 1\\n    return d\\n\\n\\ndef count_unsorted_list_items(items):\\n    counts = defaultdict(int)\\n    for item in items:\\n        counts[item] += 1\\n    return dict(counts)\\n\\n\\ndef operator_countof(a):\\n    return dict((i, operator.countOf(a, i)) for i in set(a))\\n\\n\\nperfplot.show(\\n    setup=lambda n: list(numpy.random.randint(0, 100, n)),\\n    n_range=[2**k for k in range(20)],\\n    kernels=[\\n        counter, count, bincount, pandas_value_counts, occur_dict,\\n        count_unsorted_list_items, operator_countof\\n        ],\\n    equality_check=None,\\n    logx=True,\\n    logy=True,\\n    )\\n</code>', \"<code>li = ['A0','c5','A8','A2','A5','c2','A3','A9']\\n\\nprint sum(1 for el in li if el[0]=='A' and el[1] in '01234')\\n</code>\", '<code>3</code>', \"<code>from collections import Counter\\ncountry=['Uruguay', 'Mexico', 'Uruguay', 'France', 'Mexico']\\ncount_country = Counter(country)\\noutput_list= [] \\n\\nfor i in count_country:\\n    output_list.append([i,count_country[i]])\\nprint output_list\\n</code>\", \"<code>[['Mexico', 2], ['France', 1], ['Uruguay', 2]]\\n</code>\", '<code>Counter()</code>', '<code>countOf</code>', '<code>operator</code>', '<code>&gt;&gt;&gt; import operator\\n&gt;&gt;&gt; operator.countOf([1, 2, 3, 4, 1, 4, 1], 1)\\n3\\n</code>', '<code>countOf</code>', '<code>list.count</code>', '<code>sum([1 for elem in &lt;yourlist&gt; if elem==&lt;your_value&gt;])\\n</code>', '<code>pandas</code>', '<code>value_counts</code>', '<code>&gt;&gt;&gt; import pandas as pd\\n&gt;&gt;&gt; a = [1, 2, 3, 4, 1, 4, 1]\\n&gt;&gt;&gt; pd.Series(a).value_counts()\\n1    3\\n4    2\\n3    1\\n2    1\\ndtype: int64\\n</code>', '<code>&gt;&gt;&gt; pd.Series(a).value_counts().reset_index().values.tolist()\\n[[1, 3], [4, 2], [3, 1], [2, 1]]\\n</code>', \"<code>arr = np.array(['a','a','b','b','b','c'])\\nprint(set(map(lambda x  : (x , list(arr).count(x)) , arr)))\\n</code>\", \"<code>{('c', 1), ('b', 3), ('a', 2)}\\n</code>\", '<code>dict</code>', '<code>print(dict(map(lambda x  : (x , list(arr).count(x)) , arr)))\\n</code>', \"<code>{'b': 3, 'c': 1, 'a': 2}\\n</code>\"]",
         "title": "How to count the occurrences of a list item?",
         "_childDocuments_": [
            {
               "up_vote_count": 1302,
               "answer_id": 2600208,
               "last_activity_date": 1510782495,
               "path": "3.stack.answer",
               "body_markdown": "If you only want one item&#39;s count, use the `count` method:\r\n\r\n    &gt;&gt;&gt; [1, 2, 3, 4, 1, 4, 1].count(1)\r\n    3\r\n\r\n**Don&#39;t** use this if you want to count multiple items. Calling `count` in a loop requires a separate pass over the list for every `count` call, which can be catastrophic for performance. If you want to count all items, or even just multiple items, use `Counter`, as explained in the other answers.",
               "tags": [],
               "creation_date": 1270733512,
               "last_edit_date": 1510782495,
               "is_accepted": true,
               "id": "2600208",
               "down_vote_count": 4,
               "score": 1298
            },
            {
               "up_vote_count": 38,
               "answer_id": 2600231,
               "last_activity_date": 1450963202,
               "path": "3.stack.answer",
               "body_markdown": "`list.count(x)` returns the number of times `x` appears in a list\r\n\r\nsee:\r\nhttp://docs.python.org/tutorial/datastructures.html#more-on-lists",
               "tags": [],
               "creation_date": 1270733655,
               "last_edit_date": 1450963202,
               "is_accepted": false,
               "id": "2600231",
               "down_vote_count": 0,
               "score": 38
            },
            {
               "up_vote_count": 1250,
               "answer_id": 5829377,
               "last_activity_date": 1312653817,
               "path": "3.stack.answer",
               "body_markdown": "If you are using Python 2.7 or 3 and you want number of occurrences for each element:\r\n\r\n    &gt;&gt;&gt; from collections import Counter\r\n    &gt;&gt;&gt; z = [&#39;blue&#39;, &#39;red&#39;, &#39;blue&#39;, &#39;yellow&#39;, &#39;blue&#39;, &#39;red&#39;]\r\n    &gt;&gt;&gt; Counter(z)\r\n    Counter({&#39;blue&#39;: 3, &#39;red&#39;: 2, &#39;yellow&#39;: 1})",
               "tags": [],
               "creation_date": 1304063062,
               "last_edit_date": 1312653817,
               "is_accepted": false,
               "id": "5829377",
               "down_vote_count": 0,
               "score": 1250
            },
            {
               "up_vote_count": 13,
               "answer_id": 7055873,
               "last_activity_date": 1437763289,
               "path": "3.stack.answer",
               "body_markdown": "&lt;!-- language: lang-py --&gt;\r\n\r\n    # Python &gt;= 2.6 (defaultdict) &amp;&amp; &lt; 2.7 (Counter, OrderedDict)\r\n    from collections import defaultdict\r\n    def count_unsorted_list_items(items):\r\n        &quot;&quot;&quot;\r\n        :param items: iterable of hashable items to count\r\n        :type items: iterable\r\n        \r\n        :returns: dict of counts like Py2.7 Counter\r\n        :rtype: dict\r\n        &quot;&quot;&quot;\r\n        counts = defaultdict(int)\r\n        for item in items:\r\n            counts[item] += 1\r\n        return dict(counts)\r\n\r\n\r\n    # Python &gt;= 2.2 (generators)\r\n    def count_sorted_list_items(items):\r\n        &quot;&quot;&quot;\r\n        :param items: sorted iterable of items to count\r\n        :type items: sorted iterable\r\n        \r\n        :returns: generator of (item, count) tuples\r\n        :rtype: generator\r\n        &quot;&quot;&quot;\r\n        if not items:\r\n            return\r\n        elif len(items) == 1:\r\n            yield (items[0], 1)\r\n            return\r\n        prev_item = items[0]\r\n        count = 1\r\n        for item in items[1:]:\r\n            if prev_item == item:\r\n                count += 1\r\n            else:\r\n                yield (prev_item, count)\r\n                count = 1\r\n                prev_item = item\r\n        yield (item, count)\r\n        return\r\n\r\n\r\n    import unittest\r\n    class TestListCounters(unittest.TestCase):\r\n        def test_count_unsorted_list_items(self):\r\n            D = (\r\n                ([], []),\r\n                ([2], [(2,1)]),\r\n                ([2,2], [(2,2)]),\r\n                ([2,2,2,2,3,3,5,5], [(2,4), (3,2), (5,2)]),\r\n                )\r\n            for inp, exp_outp in D:\r\n                counts = count_unsorted_list_items(inp) \r\n                print inp, exp_outp, counts\r\n                self.assertEqual(counts, dict( exp_outp ))\r\n\r\n            inp, exp_outp = UNSORTED_WIN = ([2,2,4,2], [(2,3), (4,1)])\r\n            self.assertEqual(dict( exp_outp ), count_unsorted_list_items(inp) )\r\n\r\n\r\n        def test_count_sorted_list_items(self):\r\n            D = (\r\n                ([], []),\r\n                ([2], [(2,1)]),\r\n                ([2,2], [(2,2)]),\r\n                ([2,2,2,2,3,3,5,5], [(2,4), (3,2), (5,2)]),\r\n                )\r\n            for inp, exp_outp in D:\r\n                counts = list( count_sorted_list_items(inp) )\r\n                print inp, exp_outp, counts\r\n                self.assertEqual(counts, exp_outp)\r\n\r\n            inp, exp_outp = UNSORTED_FAIL = ([2,2,4,2], [(2,3), (4,1)])\r\n            self.assertEqual(exp_outp, list( count_sorted_list_items(inp) ))\r\n            # ... [(2,2), (4,1), (2,1)]\r\n",
               "tags": [],
               "creation_date": 1313311659,
               "last_edit_date": 1437763289,
               "is_accepted": false,
               "id": "7055873",
               "down_vote_count": 3,
               "score": 10
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 7057878,
               "is_accepted": false,
               "last_activity_date": 1313337119,
               "body_markdown": "To count the number of diverse elements having a common type:\r\n\r\n    li = [&#39;A0&#39;,&#39;c5&#39;,&#39;A8&#39;,&#39;A2&#39;,&#39;A5&#39;,&#39;c2&#39;,&#39;A3&#39;,&#39;A9&#39;]\r\n    \r\n    print sum(1 for el in li if el[0]==&#39;A&#39; and el[1] in &#39;01234&#39;)\r\n\r\ngives\r\n\r\n``3``  , not 6",
               "id": "7057878",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1313337119,
               "score": 5
            },
            {
               "up_vote_count": 60,
               "answer_id": 7843090,
               "last_activity_date": 1438345280,
               "path": "3.stack.answer",
               "body_markdown": "Another way to get the number of occurrences of each item, in a dictionary:\r\n\r\n    dict((i, a.count(i)) for i in a)\r\n\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1319150288,
               "last_edit_date": 1438345280,
               "is_accepted": false,
               "id": "7843090",
               "down_vote_count": 9,
               "score": 51
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 8041395,
               "is_accepted": false,
               "last_activity_date": 1320693534,
               "body_markdown": "I had this problem today and rolled my own solution before I thought to check SO.  This:\r\n\r\n    dict((i,a.count(i)) for i in a)\r\n\r\nis really, really slow for large lists.  My solution\r\n\r\n    def occurDict(items):\r\n        d = {}\r\n        for i in items:\r\n            if i in d:\r\n                d[i] = d[i]+1\r\n            else:\r\n                d[i] = 1\r\n    return d\r\n\r\nis actually a bit faster than the Counter solution, at least for Python 2.7.",
               "id": "8041395",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1320693534,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 25,
               "answer_id": 20069518,
               "is_accepted": false,
               "last_activity_date": 1384858387,
               "body_markdown": "If you want to **count all values at once** you can do it very fast using numpy arrays and `bincount` as follows\r\n\r\n    import numpy as np\r\n    a = np.array([1, 2, 3, 4, 1, 4, 1])\r\n    np.bincount(a)\r\n\r\nwhich gives\r\n\r\n    &gt;&gt;&gt; array([0, 3, 1, 1, 2])\r\n",
               "id": "20069518",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1384858387,
               "score": 24
            },
            {
               "up_vote_count": 161,
               "answer_id": 23909767,
               "last_activity_date": 1495534229,
               "path": "3.stack.answer",
               "body_markdown": "**Counting the occurrences of one item in a list**\r\n\r\nFor counting the occurrences of just one list item you can use `count()`\r\n\r\n    &gt;&gt;&gt; l = [&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]\r\n    &gt;&gt;&gt; l.count(&quot;a&quot;)\r\n    1\r\n    &gt;&gt;&gt; l.count(&quot;b&quot;)\r\n    2\r\n\r\nCounting the occurrences of *all* items in a list is also known as &quot;tallying&quot; a list, or creating a tally counter.\r\n\r\n**Counting all items with count()**\r\n\r\nTo count the occurrences of items in `l` one can simply use a list comprehension and the `count()` method\r\n\r\n    [[x,l.count(x)] for x in set(l)]\r\n\r\n(or similarly with a dictionary `dict((x,l.count(x)) for x in set(l))`)\r\n\r\nExample: \r\n\r\n    &gt;&gt;&gt; l = [&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]\r\n    &gt;&gt;&gt; [[x,l.count(x)] for x in set(l)]\r\n    [[&#39;a&#39;, 1], [&#39;b&#39;, 2]]\r\n    &gt;&gt;&gt; dict((x,l.count(x)) for x in set(l))\r\n    {&#39;a&#39;: 1, &#39;b&#39;: 2}\r\n\r\n**Counting all items with Counter()**\r\n\r\nAlternatively, there&#39;s the faster `Counter` class from the `collections` library\r\n\r\n    Counter(l)\r\n\r\nExample:\r\n\r\n    &gt;&gt;&gt; l = [&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]\r\n    &gt;&gt;&gt; from collections import Counter\r\n    &gt;&gt;&gt; Counter(l)\r\n    Counter({&#39;b&#39;: 2, &#39;a&#39;: 1})\r\n\r\n**How much faster is Counter?**\r\n\r\nI checked how much faster `Counter` is for tallying lists. I tried both methods out with a few values of `n` and it appears that `Counter` is faster by a constant factor of approximately 2.\r\n\r\nHere is the script I used:\r\n\r\n    from __future__ import print_function\r\n    import timeit\r\n    \r\n    t1=timeit.Timer(&#39;Counter(l)&#39;, \\\r\n                    &#39;import random;import string;from collections import Counter;n=1000;l=[random.choice(string.ascii_letters) for x in range(n)]&#39;\r\n                    )\r\n    \r\n    t2=timeit.Timer(&#39;[[x,l.count(x)] for x in set(l)]&#39;,\r\n                    &#39;import random;import string;n=1000;l=[random.choice(string.ascii_letters) for x in range(n)]&#39;\r\n                    )\r\n    \r\n    print(&quot;Counter(): &quot;, t1.repeat(repeat=3,number=10000))\r\n    print(&quot;count():   &quot;, t2.repeat(repeat=3,number=10000)\r\n\r\nAnd the output:\r\n\r\n    Counter():  [0.46062711701961234, 0.4022796869976446, 0.3974247490405105]\r\n    count():    [7.779430688009597, 7.962715800967999, 8.420845870045014]\r\n",
               "tags": [],
               "creation_date": 1401274717,
               "last_edit_date": 1495534229,
               "is_accepted": false,
               "id": "23909767",
               "down_vote_count": 0,
               "score": 161
            },
            {
               "up_vote_count": 22,
               "answer_id": 36598953,
               "last_activity_date": 1494451306,
               "path": "3.stack.answer",
               "body_markdown": "&gt;#Given an item, how can I count its occurrences in a list in Python?\r\n\r\nHere&#39;s an example list:\r\n\r\n    &gt;&gt;&gt; l = list(&#39;aaaaabbbbcccdde&#39;)\r\n    &gt;&gt;&gt; l\r\n    [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;, &#39;d&#39;, &#39;e&#39;]\r\n\r\n## `list.count`\r\n\r\nThere&#39;s the `list.count` method\r\n\r\n    &gt;&gt;&gt; l.count(&#39;b&#39;)\r\n    4\r\n\r\nThis works fine for any list. Tuples have this method as well:\r\n\r\n    &gt;&gt;&gt; t = tuple(&#39;aabbbffffff&#39;)\r\n    &gt;&gt;&gt; t\r\n    (&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;f&#39;, &#39;f&#39;, &#39;f&#39;, &#39;f&#39;, &#39;f&#39;, &#39;f&#39;)\r\n    &gt;&gt;&gt; t.count(&#39;f&#39;)\r\n    6\r\n\r\n## `collections.Counter`\r\n\r\nAnd then there&#39;s collections.Counter. You can dump any iterable into a Counter, not just a list, and the Counter will retain a data structure of the counts of the elements.\r\n\r\n\r\nUsage:\r\n\r\n    &gt;&gt;&gt; from collections import Counter\r\n    &gt;&gt;&gt; c = Counter(l)\r\n    &gt;&gt;&gt; c[&#39;b&#39;]\r\n    4\r\n\r\n Counters are based on Python dictionaries, their keys are the elements, so the keys need to be hashable. They are basically like sets that allow redundant elements into them.\r\n\r\n\r\n### Further usage of `collections.Counter`\r\n\r\nYou can add or subtract with iterables from your counter:\r\n\r\n    &gt;&gt;&gt; c.update(list(&#39;bbb&#39;))\r\n    &gt;&gt;&gt; c[&#39;b&#39;]\r\n    7\r\n    &gt;&gt;&gt; c.subtract(list(&#39;bbb&#39;))\r\n    &gt;&gt;&gt; c[&#39;b&#39;]\r\n    4\r\n\r\nAnd you can do multi-set operations with the counter as well:\r\n\r\n    &gt;&gt;&gt; c2 = Counter(list(&#39;aabbxyz&#39;))\r\n    &gt;&gt;&gt; c - c2                   # set difference\r\n    Counter({&#39;a&#39;: 3, &#39;c&#39;: 3, &#39;b&#39;: 2, &#39;d&#39;: 2, &#39;e&#39;: 1})\r\n    &gt;&gt;&gt; c + c2                   # addition of all elements\r\n    Counter({&#39;a&#39;: 7, &#39;b&#39;: 6, &#39;c&#39;: 3, &#39;d&#39;: 2, &#39;e&#39;: 1, &#39;y&#39;: 1, &#39;x&#39;: 1, &#39;z&#39;: 1})\r\n    &gt;&gt;&gt; c | c2                   # set union\r\n    Counter({&#39;a&#39;: 5, &#39;b&#39;: 4, &#39;c&#39;: 3, &#39;d&#39;: 2, &#39;e&#39;: 1, &#39;y&#39;: 1, &#39;x&#39;: 1, &#39;z&#39;: 1})\r\n    &gt;&gt;&gt; c &amp; c2                   # set intersection\r\n    Counter({&#39;a&#39;: 2, &#39;b&#39;: 2})\r\n\r\n\r\n## Why not pandas?\r\n\r\nAnother answer suggests:\r\n\r\n&gt; Why not use pandas? \r\n\r\nPandas is a common library, but it&#39;s not in the standard library. Adding it as a dependency is non-trivial.\r\n\r\nThere are builtin solutions for this use-case in the list object itself as well as in the standard library.\r\n\r\nIf your project does not already require pandas, it would be foolish to make it a requirement just for this functionality.\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1460551827,
               "last_edit_date": 1494451306,
               "is_accepted": false,
               "id": "36598953",
               "down_vote_count": 0,
               "score": 22
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 39562168,
               "is_accepted": false,
               "last_activity_date": 1474229291,
               "body_markdown": "You can also use [`countOf`](https://docs.python.org/3/library/operator.html#operator.countOf) method of a built-in module [`operator`](https://docs.python.org/3/library/operator.html).\r\n\r\n    &gt;&gt;&gt; import operator\r\n    &gt;&gt;&gt; operator.countOf([1, 2, 3, 4, 1, 4, 1], 1)\r\n    3",
               "id": "39562168",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1474229291,
               "score": 3
            },
            {
               "up_vote_count": 12,
               "answer_id": 40092132,
               "last_activity_date": 1480454382,
               "path": "3.stack.answer",
               "body_markdown": "Why not using Pandas?\r\n\r\n    import pandas as pd\r\n\r\n    l = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;a&#39;, &#39;d&#39;, &#39;a&#39;]\r\n\r\n    # converting the list to a Series and counting the values\r\n    my_count = pd.Series(l).value_counts()\r\n    my_count\r\n\r\nOutput:\r\n\r\n    a    3\r\n    d    2\r\n    b    1\r\n    c    1\r\n    dtype: int64\r\n\r\nIf you are looking for a count of a particular element, say *a*, try:\r\n\r\n    my_count[&#39;a&#39;]\r\n\r\nOutput:\r\n\r\n    3",
               "tags": [],
               "creation_date": 1476724538,
               "last_edit_date": 1480454382,
               "is_accepted": false,
               "id": "40092132",
               "down_vote_count": 0,
               "score": 12
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 43678170,
               "is_accepted": false,
               "last_activity_date": 1493375816,
               "body_markdown": "\tfrom collections import Counter\r\n\tcountry=[&#39;Uruguay&#39;, &#39;Mexico&#39;, &#39;Uruguay&#39;, &#39;France&#39;, &#39;Mexico&#39;]\r\n    count_country = Counter(country)\r\n\toutput_list= [] \r\n\t\r\n\tfor i in count_country:\r\n\t\toutput_list.append([i,count_country[i]])\r\n\tprint output_list\r\n\r\nOutput list:\r\n    \r\n    [[&#39;Mexico&#39;, 2], [&#39;France&#39;, 1], [&#39;Uruguay&#39;, 2]]",
               "id": "43678170",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1493375816,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 45829195,
               "is_accepted": false,
               "last_activity_date": 1503451252,
               "body_markdown": "    sum([1 for elem in &lt;yourlist&gt; if elem==&lt;your_value&gt;])\r\n\r\nThis will return the amount of occurences of your_value",
               "id": "45829195",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1503451252,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 46195192,
               "is_accepted": false,
               "last_activity_date": 1505298768,
               "body_markdown": "I&#39;ve compared all suggested solutions (and a few new ones) with [perfplot](https://github.com/nschloe/perfplot) (a small project of mine).\r\n\r\n### Counting _one_ item\r\n\r\nFor large enough arrays, it turns out that\r\n\r\n&lt;!--language: python--&gt;\r\n\r\n    numpy.sum(numpy.array(a) == 1) \r\n\r\nis slightly faster than the other solutions.\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\n### Counting _all_ items\r\n\r\n[As established before](https://stackoverflow.com/a/43096495/353337),\r\n\r\n&lt;!--language: python--&gt;\r\n\r\n    numpy.bincount(a)\r\n\r\nis what you want.\r\n\r\n[![enter image description here][2]][2]\r\n\r\n---\r\n\r\nCode to reproduce the plots:\r\n\r\n&lt;!--language: python--&gt;\r\n \r\n    from collections import Counter\r\n    from collections import defaultdict\r\n    import numpy\r\n    import operator\r\n    import pandas\r\n    import perfplot\r\n\r\n\r\n    def counter(a):\r\n        return Counter(a)\r\n\r\n\r\n    def count(a):\r\n        return dict((i, a.count(i)) for i in set(a))\r\n\r\n\r\n    def bincount(a):\r\n        return numpy.bincount(a)\r\n\r\n\r\n    def pandas_value_counts(a):\r\n        return pandas.Series(a).value_counts()\r\n\r\n\r\n    def occur_dict(a):\r\n        d = {}\r\n        for i in a:\r\n            if i in d:\r\n                d[i] = d[i]+1\r\n            else:\r\n                d[i] = 1\r\n        return d\r\n\r\n\r\n    def count_unsorted_list_items(items):\r\n        counts = defaultdict(int)\r\n        for item in items:\r\n            counts[item] += 1\r\n        return dict(counts)\r\n\r\n\r\n    def operator_countof(a):\r\n        return dict((i, operator.countOf(a, i)) for i in set(a))\r\n\r\n\r\n    perfplot.show(\r\n        setup=lambda n: list(numpy.random.randint(0, 100, n)),\r\n        n_range=[2**k for k in range(20)],\r\n        kernels=[\r\n            counter, count, bincount, pandas_value_counts, occur_dict,\r\n            count_unsorted_list_items, operator_countof\r\n            ],\r\n        equality_check=None,\r\n        logx=True,\r\n        logy=True,\r\n        )\r\n\r\n\r\n2.\r\n\r\n&lt;!--language: python--&gt;\r\n\r\n    from collections import Counter\r\n    from collections import defaultdict\r\n    import numpy\r\n    import operator\r\n    import pandas\r\n    import perfplot\r\n\r\n\r\n    def counter(a):\r\n        return Counter(a)\r\n\r\n\r\n    def count(a):\r\n        return dict((i, a.count(i)) for i in set(a))\r\n\r\n\r\n    def bincount(a):\r\n        return numpy.bincount(a)\r\n\r\n\r\n    def pandas_value_counts(a):\r\n        return pandas.Series(a).value_counts()\r\n\r\n\r\n    def occur_dict(a):\r\n        d = {}\r\n        for i in a:\r\n            if i in d:\r\n                d[i] = d[i]+1\r\n            else:\r\n                d[i] = 1\r\n        return d\r\n\r\n\r\n    def count_unsorted_list_items(items):\r\n        counts = defaultdict(int)\r\n        for item in items:\r\n            counts[item] += 1\r\n        return dict(counts)\r\n\r\n\r\n    def operator_countof(a):\r\n        return dict((i, operator.countOf(a, i)) for i in set(a))\r\n\r\n\r\n    perfplot.show(\r\n        setup=lambda n: list(numpy.random.randint(0, 100, n)),\r\n        n_range=[2**k for k in range(20)],\r\n        kernels=[\r\n            counter, count, bincount, pandas_value_counts, occur_dict,\r\n            count_unsorted_list_items, operator_countof\r\n            ],\r\n        equality_check=None,\r\n        logx=True,\r\n        logy=True,\r\n        )\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/vTw7i.png\r\n  [2]: https://i.stack.imgur.com/yE0Dy.png",
               "id": "46195192",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1505298768,
               "score": 8
            },
            {
               "up_vote_count": 1,
               "answer_id": 48296129,
               "last_activity_date": 1517393048,
               "path": "3.stack.answer",
               "body_markdown": "If you can use `pandas`, then `value_counts` is there for rescue. \r\n\r\n    &gt;&gt;&gt; import pandas as pd\r\n    &gt;&gt;&gt; a = [1, 2, 3, 4, 1, 4, 1]\r\n    &gt;&gt;&gt; pd.Series(a).value_counts()\r\n    1    3\r\n    4    2\r\n    3    1\r\n    2    1\r\n    dtype: int64\r\n\r\nIt automatically sorts the result based on frequency as well. \r\n\r\nIf you want the result to be in a list of list, do as below\r\n\r\n    &gt;&gt;&gt; pd.Series(a).value_counts().reset_index().values.tolist()\r\n    [[1, 3], [4, 2], [3, 1], [2, 1]]\r\n\r\n",
               "tags": [],
               "creation_date": 1516175782,
               "last_edit_date": 1517393048,
               "is_accepted": false,
               "id": "48296129",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 49047825,
               "is_accepted": false,
               "last_activity_date": 1519900688,
               "body_markdown": "May not be the most efficient, requires an extra pass to remove duplicates. \r\n\r\nFunctional implementation : \r\n\r\n    arr = np.array([&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;])\r\n    print(set(map(lambda x  : (x , list(arr).count(x)) , arr)))\r\n\r\nreturns : \r\n\r\n    {(&#39;c&#39;, 1), (&#39;b&#39;, 3), (&#39;a&#39;, 2)}\r\n\r\nor return as `dict` :\r\n\r\n    print(dict(map(lambda x  : (x , list(arr).count(x)) , arr)))\r\n\r\nreturns : \r\n\r\n    {&#39;b&#39;: 3, &#39;c&#39;: 1, &#39;a&#39;: 2}\r\n\r\n",
               "id": "49047825",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519900688,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/2600191/how-to-count-the-occurrences-of-a-list-item",
         "id": "858127-2285"
      },
      {
         "up_vote_count": "638",
         "path": "2.stack",
         "body_markdown": "I am writing a quick-and-dirty script to generate plots on the fly. I am using the code below (from [Matplotlib][1] documentation) as a starting point:\r\n\r\n    from pylab import figure, axes, pie, title, show\r\n\r\n    # Make a square figure and axes\r\n    figure(1, figsize=(6, 6))\r\n    ax = axes([0.1, 0.1, 0.8, 0.8])\r\n\r\n    labels = &#39;Frogs&#39;, &#39;Hogs&#39;, &#39;Dogs&#39;, &#39;Logs&#39;\r\n    fracs = [15, 30, 45, 10]\r\n\r\n    explode = (0, 0.05, 0, 0)\r\n    pie(fracs, explode=explode, labels=labels, autopct=&#39;%1.1f%%&#39;, shadow=True)\r\n    title(&#39;Raining Hogs and Dogs&#39;, bbox={&#39;facecolor&#39;: &#39;0.8&#39;, &#39;pad&#39;: 5})\r\n\r\n    show()  # Actually, don&#39;t show, just save to foo.png\r\n\r\nI don&#39;t want to display the plot on a GUI, instead, I want to save the plot to a file (say foo.png), so that, for example, it can be used in batch scripts. How do I do that?\r\n\r\n  [1]: http://en.wikipedia.org/wiki/Matplotlib\r\n",
         "view_count": "773874",
         "answer_count": "12",
         "tags": "['python', 'matplotlib']",
         "creation_date": "1331228290",
         "last_edit_date": "1494272655",
         "code_snippet": "[\"<code>from pylab import figure, axes, pie, title, show\\n\\n# Make a square figure and axes\\nfigure(1, figsize=(6, 6))\\nax = axes([0.1, 0.1, 0.8, 0.8])\\n\\nlabels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\\nfracs = [15, 30, 45, 10]\\n\\nexplode = (0, 0.05, 0, 0)\\npie(fracs, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True)\\ntitle('Raining Hogs and Dogs', bbox={'facecolor': '0.8', 'pad': 5})\\n\\nshow()  # Actually, don't show, just save to foo.png\\n</code>\", '<code>savefig</code>', '<code>fig = plt.figure()</code>', '<code>fig.savefig(...)</code>', '<code>plt.close(fig)</code>', '<code>plt.show()</code>', \"<code>savefig('foo.png')\\nsavefig('foo.pdf')\\n</code>\", '<code>pylab</code>', \"<code>savefig('foo.png', bbox_inches='tight')\\n</code>\", '<code>plt.savefig</code>', '<code>figsize</code>', '<code>plt.ioff()</code>', '<code>matplotlib.pyplot</code>', \"<code>pylab.savefig('foo.png')\\n</code>\", '<code>plt.savefig()</code>', '<code>fig1.savefig()</code>', '<code>plt.ion()</code>', \"<code>import matplotlib.pyplot as plt\\nfig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure &amp; 1 axis\\nax.plot([0,1,2], [10,20,3])\\nfig.savefig('path/to/save/image/to.png')   # save the figure to file\\nplt.close(fig)    # close the figure\\n</code>\", '<code>plt.ioff() # turn of interactive plotting mode</code>', '<code>matplotib.use(&lt;backend&gt;)</code>', \"<code>import matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nplt.plot([1,2,3])\\nplt.savefig('myfig')\\n</code>\", '<code>plt.close( fig )</code>', '<code>import matplotlib.image as mpimg\\n\\nimg = mpimg.imread(\"src.png\")\\nmpimg.imsave(\"out.png\", img)\\n</code>', '<code>src.png</code>', '<code>out.png</code>', '<code>img</code>', '<code>.imsave()</code>', '<code>with open(\\'some_file.pkl\\', \"wb\") as fp:\\n    pickle.dump(fig, fp, protocol=4)\\n</code>', '<code>locals()</code>', \"<code>import datetime\\nimport numpy as np\\nfrom matplotlib.backends.backend_pdf import PdfPages\\nimport matplotlib.pyplot as plt\\n\\n# Create the PdfPages object to which we will save the pages:\\n# The with statement makes sure that the PdfPages object is closed properly at\\n# the end of the block, even if an Exception occurs.\\nwith PdfPages('multipage_pdf.pdf') as pdf:\\n    plt.figure(figsize=(3, 3))\\n    plt.plot(range(7), [3, 1, 4, 1, 5, 9, 2], 'r-o')\\n    plt.title('Page One')\\n    pdf.savefig()  # saves the current figure into a pdf page\\n    plt.close()\\n\\n    plt.rc('text', usetex=True)\\n    plt.figure(figsize=(8, 6))\\n    x = np.arange(0, 5, 0.1)\\n    plt.plot(x, np.sin(x), 'b-')\\n    plt.title('Page Two')\\n    pdf.savefig()\\n    plt.close()\\n\\n    plt.rc('text', usetex=False)\\n    fig = plt.figure(figsize=(4, 5))\\n    plt.plot(x, x*x, 'ko')\\n    plt.title('Page Three')\\n    pdf.savefig(fig)  # or you can pass a Figure object to pdf.savefig\\n    plt.close()\\n\\n    # We can also set the file's metadata via the PdfPages object:\\n    d = pdf.infodict()\\n    d['Title'] = 'Multipage PDF Example'\\n    d['Author'] = u'Jouni K. Sepp\\\\xe4nen'\\n    d['Subject'] = 'How to create a multipage pdf file and set its metadata'\\n    d['Keywords'] = 'PdfPages multipage keywords author title subject'\\n    d['CreationDate'] = datetime.datetime(2009, 11, 13)\\n    d['ModDate'] = datetime.datetime.today()\\n</code>\", '<code>import matplotlib.pyplot as plt\\n\\nfig = plt.figure(figuresize=4, 5)\\n# use plot(), etc. to create your plot.\\n\\n# Pick one of the following lines to uncomment\\n# save_file = None\\n# save_file = os.path.join(your_directory, your_file_name)  \\n\\nif save_file:\\n    plt.savefig(save_file)\\n    plt.close(fig)\\nelse:\\n    plt.show()\\n</code>', '<code>import matplotlib.pyplot as plt\\n\\np1 = plt.plot(dates, temp, \\'r-\\', label=\"Temperature (celsius)\")  \\np2 = plt.plot(dates, psal, \\'b-\\', label=\"Salinity (psu)\")  \\nplt.legend(loc=\\'upper center\\', numpoints=1, bbox_to_anchor=(0.5, -0.05),        ncol=2, fancybox=True, shadow=True)\\n\\nplt.savefig(\\'data.png\\')  \\nplt.show()  \\nf.close()\\nplt.close()\\n</code>', '<code>plt.ioff()</code>', '<code>plt.ion()</code>', '<code>import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport matplotlib\\nmatplotlib.style.use(\\'ggplot\\')\\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\\'1/1/2000\\', periods=1000))\\nts = ts.cumsum()\\nplt.figure()\\nts.plot()\\nplt.savefig(\"foo.png\", bbox_inches=\\'tight\\')\\n</code>', '<code>%matplotlib inline\\n</code>', '<code>import matplotlib</code>', \"<code>plt.show(hold=False)\\nplt.savefig('name.pdf')\\n</code>\", '<code>plt.show()</code>', '<code>plt.show()</code>', '<code>plt.savefig()</code>', \"<code>fig, ax = plt.figure(nrows=1, ncols=1)\\nplt.plot(...)\\nplt.show()\\nfig.savefig('out.pdf')\\n</code>\"]",
         "title": "Save plot to image file instead of displaying it using Matplotlib",
         "_childDocuments_": [
            {
               "up_vote_count": 115,
               "answer_id": 9888817,
               "last_activity_date": 1413704272,
               "path": "3.stack.answer",
               "body_markdown": "The solution is: \r\n\r\n    pylab.savefig(&#39;foo.png&#39;)\r\n",
               "tags": [],
               "creation_date": 1332848191,
               "last_edit_date": 1413704272,
               "is_accepted": false,
               "id": "9888817",
               "down_vote_count": 1,
               "score": 114
            },
            {
               "up_vote_count": 777,
               "answer_id": 9890599,
               "last_activity_date": 1391609302,
               "path": "3.stack.answer",
               "body_markdown": "While the question has been answered, I&#39;d like to add some useful tips when using [savefig][1]. The file format can be specified by the extension:\r\n\r\n    savefig(&#39;foo.png&#39;)\r\n    savefig(&#39;foo.pdf&#39;)\r\n\r\nWill give a rasterized or vectorized output respectively, both which could be useful. In addition, you&#39;ll find that `pylab` leaves a generous, often undesirable, whitespace around the image. Remove it with:\r\n\r\n    savefig(&#39;foo.png&#39;, bbox_inches=&#39;tight&#39;)\r\n\r\n\r\n\r\n  [1]: http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.savefig",
               "tags": [],
               "creation_date": 1332855344,
               "last_edit_date": 1391609302,
               "is_accepted": true,
               "id": "9890599",
               "down_vote_count": 1,
               "score": 776
            },
            {
               "up_vote_count": 29,
               "answer_id": 21464691,
               "last_activity_date": 1413704300,
               "path": "3.stack.answer",
               "body_markdown": "If you don&#39;t like the concept of the &quot;current&quot; figure, do:\r\n\r\n    import matplotlib.image as mpimg\r\n\r\n    img = mpimg.imread(&quot;src.png&quot;)\r\n    mpimg.imsave(&quot;out.png&quot;, img)\r\n",
               "tags": [],
               "creation_date": 1391106637,
               "last_edit_date": 1413704300,
               "is_accepted": false,
               "id": "21464691",
               "down_vote_count": 1,
               "score": 28
            },
            {
               "up_vote_count": 90,
               "answer_id": 29931148,
               "last_activity_date": 1445321571,
               "path": "3.stack.answer",
               "body_markdown": "As others have said, `plt.savefig()` or `fig1.savefig()` is indeed the way to save an image.\r\n\r\nHowever I&#39;ve found that in certain cases (eg. with Spyder having `plt.ion()`: interactive mode = On) the figure is always shown.  I work around this by forcing the closing of the figure window in my giant loop, so I don&#39;t have a million open figures during the loop:\r\n\r\n    import matplotlib.pyplot as plt\r\n    fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure &amp; 1 axis\r\n    ax.plot([0,1,2], [10,20,3])\r\n    fig.savefig(&#39;path/to/save/image/to.png&#39;)   # save the figure to file\r\n    plt.close(fig)    # close the figure\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1430260511,
               "last_edit_date": 1445321571,
               "is_accepted": false,
               "id": "29931148",
               "down_vote_count": 1,
               "score": 89
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 17,
               "answer_id": 31133453,
               "is_accepted": false,
               "last_activity_date": 1435653517,
               "body_markdown": "    import datetime\r\n    import numpy as np\r\n    from matplotlib.backends.backend_pdf import PdfPages\r\n    import matplotlib.pyplot as plt\r\n\r\n    # Create the PdfPages object to which we will save the pages:\r\n    # The with statement makes sure that the PdfPages object is closed properly at\r\n    # the end of the block, even if an Exception occurs.\r\n    with PdfPages(&#39;multipage_pdf.pdf&#39;) as pdf:\r\n        plt.figure(figsize=(3, 3))\r\n        plt.plot(range(7), [3, 1, 4, 1, 5, 9, 2], &#39;r-o&#39;)\r\n        plt.title(&#39;Page One&#39;)\r\n        pdf.savefig()  # saves the current figure into a pdf page\r\n        plt.close()\r\n\r\n        plt.rc(&#39;text&#39;, usetex=True)\r\n        plt.figure(figsize=(8, 6))\r\n        x = np.arange(0, 5, 0.1)\r\n        plt.plot(x, np.sin(x), &#39;b-&#39;)\r\n        plt.title(&#39;Page Two&#39;)\r\n        pdf.savefig()\r\n        plt.close()\r\n\r\n        plt.rc(&#39;text&#39;, usetex=False)\r\n        fig = plt.figure(figsize=(4, 5))\r\n        plt.plot(x, x*x, &#39;ko&#39;)\r\n        plt.title(&#39;Page Three&#39;)\r\n        pdf.savefig(fig)  # or you can pass a Figure object to pdf.savefig\r\n        plt.close()\r\n\r\n        # We can also set the file&#39;s metadata via the PdfPages object:\r\n        d = pdf.infodict()\r\n        d[&#39;Title&#39;] = &#39;Multipage PDF Example&#39;\r\n        d[&#39;Author&#39;] = u&#39;Jouni K. Sepp\\xe4nen&#39;\r\n        d[&#39;Subject&#39;] = &#39;How to create a multipage pdf file and set its metadata&#39;\r\n        d[&#39;Keywords&#39;] = &#39;PdfPages multipage keywords author title subject&#39;\r\n        d[&#39;CreationDate&#39;] = datetime.datetime(2009, 11, 13)\r\n        d[&#39;ModDate&#39;] = datetime.datetime.today()",
               "id": "31133453",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1435653517,
               "score": 17
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 32287336,
               "is_accepted": false,
               "last_activity_date": 1440859847,
               "body_markdown": "If, like me, you use Spyder IDE, you have to disable the interactive mode with :\r\n \r\n```plt.ioff()```\r\n\r\n(this command is automatically launched with the scientific startup)\r\n\r\nIf you want to enable it again, use :\r\n\r\n```plt.ion()```\r\n\r\n",
               "id": "32287336",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1440859847,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 46,
               "answer_id": 34583288,
               "is_accepted": false,
               "last_activity_date": 1451867756,
               "body_markdown": "Just found this link on the MatPlotLib documentation addressing exactly this issue:\r\nhttp://matplotlib.org/faq/howto_faq.html#generate-images-without-having-a-window-appear\r\n\r\nThey say that the easiest way to prevent the figure from popping up is to use a non-interactive backend (eg. Agg), via `matplotib.use(&lt;backend&gt;)`, eg:\r\n\r\n    import matplotlib\r\n    matplotlib.use(&#39;Agg&#39;)\r\n    import matplotlib.pyplot as plt\r\n    plt.plot([1,2,3])\r\n    plt.savefig(&#39;myfig&#39;)\r\n\r\nI still personally prefer using `plt.close( fig )`, since then you have the option to hide certain figures (during a loop), but still display figures for post-loop data processing. It is probably slower than choosing a non-interactive backend though - would be interesting if someone tested that.\r\n\r\n",
               "id": "34583288",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1451867756,
               "score": 46
            },
            {
               "up_vote_count": 8,
               "answer_id": 36245528,
               "last_activity_date": 1459069171,
               "path": "3.stack.answer",
               "body_markdown": "The Solution : \r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    import matplotlib\r\n    matplotlib.style.use(&#39;ggplot&#39;)\r\n    ts = pd.Series(np.random.randn(1000), index=pd.date_range(&#39;1/1/2000&#39;, periods=1000))\r\n    ts = ts.cumsum()\r\n    plt.figure()\r\n    ts.plot()\r\n    plt.savefig(&quot;foo.png&quot;, bbox_inches=&#39;tight&#39;)\r\n\r\nIf you do want to display the image as well as saving the image use:\r\n\r\n    %matplotlib inline\r\nafter \r\n`import matplotlib`",
               "tags": [],
               "creation_date": 1459068577,
               "last_edit_date": 1459069171,
               "is_accepted": false,
               "id": "36245528",
               "down_vote_count": 0,
               "score": 8
            },
            {
               "up_vote_count": 13,
               "answer_id": 36427934,
               "last_activity_date": 1459864691,
               "path": "3.stack.answer",
               "body_markdown": "I used the following:\r\n\r\n    import matplotlib.pyplot as plt\r\n\r\n    p1 = plt.plot(dates, temp, &#39;r-&#39;, label=&quot;Temperature (celsius)&quot;)  \r\n    p2 = plt.plot(dates, psal, &#39;b-&#39;, label=&quot;Salinity (psu)&quot;)  \r\n    plt.legend(loc=&#39;upper center&#39;, numpoints=1, bbox_to_anchor=(0.5, -0.05),        ncol=2, fancybox=True, shadow=True)\r\n\r\n    plt.savefig(&#39;data.png&#39;)  \r\n    plt.show()  \r\n    f.close()\r\n    plt.close()\r\n\r\nI found very important to use plt.show after saving the figure, otherwise it won&#39;t work.[figure exported in png][1]\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/2bUFL.png",
               "tags": [],
               "creation_date": 1459863271,
               "last_edit_date": 1459864691,
               "is_accepted": false,
               "id": "36427934",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 19,
               "answer_id": 37460048,
               "last_activity_date": 1487772875,
               "path": "3.stack.answer",
               "body_markdown": "The other answers are correct.  However, I sometimes find that I want to open the figure *object* later.  For example, I might want to change the label sizes, add a grid, or do other processing.  In a perfect world, I would simply rerun the code generating the plot, and adapt the settings.  Alas, the world is not perfect.  Therefore, in addition to saving to PDF or PNG, I add:\r\n\r\n    with open(&#39;some_file.pkl&#39;, &quot;wb&quot;) as fp:\r\n        pickle.dump(fig, fp, protocol=4)\r\n\r\nLike this, I can later load the figure object and manipulate the settings as I please.\r\n\r\nI also write out the stack with the source-code and `locals()` dictionary for each function/method in the stack, so that I can later tell exactly what generated the figure.\r\n\r\nNB: Be careful, as sometimes this method generates huge files.",
               "tags": [],
               "creation_date": 1464263314,
               "last_edit_date": 1487772875,
               "is_accepted": false,
               "id": "37460048",
               "down_vote_count": 0,
               "score": 19
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 15,
               "answer_id": 39987599,
               "is_accepted": false,
               "last_activity_date": 1476223935,
               "body_markdown": "After using the plot() and other functions to create the content you want, you could use a clause like this to select between plotting to the screen or to file:\r\n\r\n    import matplotlib.pyplot as plt\r\n\r\n    fig = plt.figure(figuresize=4, 5)\r\n    # use plot(), etc. to create your plot.\r\n\r\n    # Pick one of the following lines to uncomment\r\n    # save_file = None\r\n    # save_file = os.path.join(your_directory, your_file_name)  \r\n\r\n    if save_file:\r\n        plt.savefig(save_file)\r\n        plt.close(fig)\r\n    else:\r\n        plt.show()",
               "id": "39987599",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1476223935,
               "score": 15
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 46385142,
               "is_accepted": false,
               "last_activity_date": 1506208803,
               "body_markdown": "You can either do: \r\n\r\n\r\n    plt.show(hold=False)\r\n    plt.savefig(&#39;name.pdf&#39;)\r\n\r\n\r\n\r\nand remember to let savefig finish before closing the GUI plot. This way you can see the image beforehand. \r\n\r\nAlternatively, you can look at it with `plt.show()`\r\nThen close the GUI and run the script again, but this time replace `plt.show()` with `plt.savefig()`. \r\n\r\nAlternatively, you can use \r\n\r\n    fig, ax = plt.figure(nrows=1, ncols=1)\r\n    plt.plot(...)\r\n    plt.show()\r\n    fig.savefig(&#39;out.pdf&#39;)\r\n",
               "id": "46385142",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1506208803,
               "score": 3
            }
         ],
         "link": "https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib",
         "id": "858127-2286"
      },
      {
         "up_vote_count": "1980",
         "path": "2.stack",
         "body_markdown": "For a list `[&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;]` and an item in the list `&quot;bar&quot;`, what&#39;s the cleanest way to get its index (1) in Python?",
         "view_count": "2210232",
         "answer_count": "21",
         "tags": "['python', 'list', 'indexing']",
         "creation_date": "1223343578",
         "last_edit_date": "1509406181",
         "code_snippet": "['<code>[\"foo\", \"bar\", \"baz\"]</code>', '<code>\"bar\"</code>', '<code>&gt;&gt;&gt; [\"foo\", \"bar\", \"baz\"].index(\"bar\")\\n1\\n</code>', '<code>&gt;&gt;&gt; help([\"foo\", \"bar\", \"baz\"])\\nHelp on list object:\\n\\nclass list(object)\\n ...\\n\\n |\\n |  index(...)\\n |      L.index(value, [start, [stop]]) -&gt; integer -- return first index of value\\n |\\n</code>', '<code>enumerate()</code>', \"<code>for i, j in enumerate(['foo', 'bar', 'baz']):\\n    if j == 'bar':\\n        print(i)\\n</code>\", '<code>index()</code>', '<code>enumerate()</code>', \"<code>[i for i, j in enumerate(['foo', 'bar', 'baz']) if j == 'bar']\\n</code>\", '<code>itertools.count()</code>', \"<code>from itertools import izip as zip, count # izip for maximum efficiency\\n[i for i, j in zip(count(), ['foo', 'bar', 'baz']) if j == 'bar']\\n</code>\", '<code>enumerate()</code>', '<code>$ python -m timeit -s \"from itertools import izip as zip, count\" \"[i for i, j in zip(count(), [\\'foo\\', \\'bar\\', \\'baz\\']*500) if j == \\'bar\\']\"\\n10000 loops, best of 3: 174 usec per loop\\n$ python -m timeit \"[i for i, j in enumerate([\\'foo\\', \\'bar\\', \\'baz\\']*500) if j == \\'bar\\']\"\\n10000 loops, best of 3: 196 usec per loop\\n</code>', \"<code> indexes = [i for i,x in enumerate(xs) if x == 'foo']\\n</code>\", '<code>index()</code>', '<code>def all_indices(value, qlist):\\n    indices = []\\n    idx = -1\\n    while True:\\n        try:\\n            idx = qlist.index(value, idx+1)\\n            indices.append(idx)\\n        except ValueError:\\n            break\\n    return indices\\n\\nall_indices(\"foo\", [\"foo\",\"bar\",\"baz\",\"foo\"])\\n</code>', '<code># if element is found it returns index of element else returns None\\n\\ndef find_element_in_list(element, list_element):\\n    try:\\n        index_element = list_element.index(element)\\n        return index_element\\n    except ValueError:\\n        return None\\n</code>', '<code>a = [\"foo\",\"bar\",\"baz\",\\'bar\\',\\'any\\',\\'much\\']\\n\\nindexes = [index for index in range(len(a)) if a[index] == \\'bar\\']\\n</code>', \"<code>if 'your_element' in mylist:\\n    print mylist.index('your_element')\\nelse:\\n    print None\\n</code>\", '<code>[i for i in range(len(mylist)) if mylist[i]==myterm]  # get the indices\\n\\n[each for each in mylist if each==myterm]             # get the items\\n\\nmylist.index(myterm) if myterm in mylist else None    # get the first index and fail quietly\\n</code>', '<code>import numpy as np\\n\\narray = [1,2,1,3,4,5,1]\\nitem = 1\\nnp_array = np.array(array)    \\nitem_index = np.where(np_array==item)\\nprint item_index\\n# Out: (array([0, 2, 6], dtype=int64),)\\n</code>', '<code>[\"foo\", \"bar\", \"baz\"]</code>', '<code>\"bar\"</code>', '<code>&gt;&gt;&gt; l = [\"foo\", \"bar\", \"baz\"]\\n&gt;&gt;&gt; l.index(\\'bar\\')\\n1\\n</code>', '<code>ValueError</code>', '<code>ValueError</code>', '<code>def index(a_list, value):\\n    try:\\n        return a_list.index(value)\\n    except ValueError:\\n        return None\\n</code>', \"<code>&gt;&gt;&gt; print(index(l, 'quux'))\\nNone\\n&gt;&gt;&gt; print(index(l, 'bar'))\\n1\\n</code>\", '<code>is</code>', '<code>is not</code>', '<code>result = index(a_list, value)\\nif result is not None:\\n    do_something(result)\\n</code>', '<code>list.index</code>', \"<code>&gt;&gt;&gt; l.append('bar')\\n&gt;&gt;&gt; l\\n['foo', 'bar', 'baz', 'bar']\\n&gt;&gt;&gt; l.index('bar')              # nothing at index 3?\\n1\\n</code>\", \"<code>&gt;&gt;&gt; [index for index, v in enumerate(l) if v == 'bar']\\n[1, 3]\\n&gt;&gt;&gt; [index for index, v in enumerate(l) if v == 'boink']\\n[]\\n</code>\", \"<code>indexes = [index for index, v in enumerate(l) if v == 'boink']\\nfor index in indexes:\\n    do_something(index)\\n</code>\", '<code>&gt;&gt;&gt; import pandas as pd\\n&gt;&gt;&gt; series = pd.Series(l)\\n&gt;&gt;&gt; series\\n0    foo\\n1    bar\\n2    baz\\n3    bar\\ndtype: object\\n</code>', \"<code>&gt;&gt;&gt; series == 'bar'\\n0    False\\n1     True\\n2    False\\n3     True\\ndtype: bool\\n</code>\", \"<code>&gt;&gt;&gt; series[series == 'bar']\\n1    bar\\n3    bar\\ndtype: object\\n</code>\", \"<code>&gt;&gt;&gt; series[series == 'bar'].index\\nInt64Index([1, 3], dtype='int64')\\n</code>\", \"<code>&gt;&gt;&gt; list(series[series == 'bar'].index)\\n[1, 3]\\n</code>\", \"<code>&gt;&gt;&gt; [i for i, value in enumerate(l) if value == 'bar']\\n[1, 3]\\n</code>\", '<code>ValueError</code>', '<code>list.index</code>', '<code>idlelib</code>', '<code>keyword</code>', '<code>key_list[key_list.index(old)] = new\\n</code>', '<code>del key_list[key_list.index(key)]\\n</code>', '<code>mon = MONTHS_LOWER.index(mon.lower())+1\\n</code>', '<code>members = members[:members.index(tarinfo)]\\n</code>', '<code>numtopop = before.index(markobject)\\n</code>', '<code>list.index</code>', \"<code>get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]\\n\\nprint get_indexes(2,[1,2,3,4,5,6,3,2,3,2])\\nprint get_indexes('f','xsfhhttytffsafweef')\\n</code>\", \"<code>a = [['hand', 'head'], ['phone', 'wallet'], ['lost', 'stock']]\\nb = ['phone', 'lost']\\n\\nres = [[x[0] for x in a].index(y) for y in b]\\n</code>\", \"<code>&gt;&gt;&gt; a = ['foo','bar','baz','bar','any', 'foo', 'much']\\n&gt;&gt;&gt; l = dict(zip(set(a), map(lambda y: [i for i,z in enumerate(a) if z is y ], set(a))))\\n&gt;&gt;&gt; l['foo']\\n[0, 5]\\n&gt;&gt;&gt; l ['much']\\n[6]\\n&gt;&gt;&gt; l\\n{'baz': [2], 'foo': [0, 5], 'bar': [1, 3], 'any': [4], 'much': [6]}\\n&gt;&gt;&gt; \\n</code>\", \"<code>&gt;&gt;&gt; a = ['red', 'blue', 'green', 'red']\\n&gt;&gt;&gt; b = 'red'\\n&gt;&gt;&gt; offset = 0;\\n&gt;&gt;&gt; indices = list()\\n&gt;&gt;&gt; for i in range(a.count(b)):\\n...     indices.append(a.index(b,offset))\\n...     offset = indices[-1]+1\\n... \\n&gt;&gt;&gt; indices\\n[0, 3]\\n&gt;&gt;&gt; \\n</code>\", '<code>def indices(l, val):\\n    \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\\n    retval = []\\n    last = 0\\n    while val in l[last:]:\\n            i = l[last:].index(val)\\n            retval.append(last + i)\\n            last += i + 1   \\n    return retval\\n\\nl = [\\'bar\\',\\'foo\\',\\'bar\\',\\'baz\\',\\'bar\\',\\'bar\\']\\nq = \\'bar\\'\\nprint indices(l,q)\\nprint indices(l,\\'bat\\')\\nprint indices(\\'abcdaababb\\',\\'a\\')\\n</code>', '<code>Python 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) \\n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n&gt;&gt;&gt; def indices(the_list, val):\\n...     \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\\n...     retval = []\\n...     last = 0\\n...     while val in the_list[last:]:\\n...             i = the_list[last:].index(val)\\n...             retval.append(last + i)\\n...             last += i + 1   \\n...     return retval\\n... \\n&gt;&gt;&gt; l = [\\'bar\\',\\'foo\\',\\'bar\\',\\'baz\\',\\'bar\\',\\'bar\\']\\n&gt;&gt;&gt; q = \\'bar\\'\\n&gt;&gt;&gt; print indices(l,q)\\n[0, 2, 4, 5]\\n&gt;&gt;&gt; print indices(l,\\'bat\\')\\n[]\\n&gt;&gt;&gt; print indices(\\'abcdaababb\\',\\'a\\')\\n[0, 4, 5, 7]\\n&gt;&gt;&gt; \\n</code>', '<code>for</code>', '<code>def find_element(p,t):\\n    i = 0\\n    for e in p:\\n        if e == t:\\n            return i\\n        else:\\n            i +=1\\n    return -1\\n</code>', \"<code>&gt;&gt;&gt; alist = ['foo','spam','egg','foo']\\n&gt;&gt;&gt; foo_indexes = [n for n,x in enumerate(alist) if x=='foo']\\n&gt;&gt;&gt; foo_indexes\\n[0, 3]\\n&gt;&gt;&gt;\\n</code>\", '<code>def findindex(item2find,listOrString):\\n  \"Search indexes of an item (arg.1) contained in a list or a string (arg.2)\"\\n  return [n for n,item in enumerate(listOrString) if item==item2find]\\n\\nindexes1 = findindex(\"1\",\"010101010\")\\nprint(indexes1)\\n</code>', '<code>[1, 3, 5, 7]\\n</code>', '<code>name =\"bar\"\\nlist = [[\"foo\", 1], [\"bar\", 2], [\"baz\", 3]]\\nnew_list=[]\\nfor item in list:\\n    new_list.append(item[0])\\nprint(new_list)\\ntry:\\n    location= new_list.index(name)\\nexcept:\\n    location=-1\\nprint (location)\\n</code>', \"<code>&gt;&gt;&gt; [i for i,j in zip(range(len(haystack)),haystack) if j == 'needle' ] \\n</code>\", '<code>mylist = [\"foo\", \"bar\", \"baz\", \"bar\"]\\nnewlist = enumerate(mylist)\\nfor index, item in newlist:\\n  if item == \"bar\":\\n    print(index, item)\\n</code>']",
         "title": "Finding the index of an item given a list containing it in Python",
         "_childDocuments_": [
            {
               "up_vote_count": 2956,
               "answer_id": 176921,
               "last_activity_date": 1446046826,
               "path": "3.stack.answer",
               "body_markdown": "    &gt;&gt;&gt; [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;].index(&quot;bar&quot;)\r\n    1\r\n\r\nReference: [Data Structures &gt; More on Lists][1]\r\n\r\n  [1]: http://docs.python.org/2/tutorial/datastructures.html#more-on-lists",
               "tags": [],
               "creation_date": 1223343649,
               "last_edit_date": 1446046826,
               "is_accepted": true,
               "id": "176921",
               "down_vote_count": 3,
               "score": 2953
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 742,
               "answer_id": 178399,
               "is_accepted": false,
               "last_activity_date": 1223385596,
               "body_markdown": "One thing that is really helpful in learning Python is to use the interactive help function:\r\n\r\n    &gt;&gt;&gt; help([&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;])\r\n    Help on list object:\r\n\r\n    class list(object)\r\n     ...\r\n\r\n     |\r\n     |  index(...)\r\n     |      L.index(value, [start, [stop]]) -&gt; integer -- return first index of value\r\n     |\r\n\r\nwhich will often lead you to the method you are looking for.",
               "id": "178399",
               "tags": [],
               "down_vote_count": 5,
               "creation_date": 1223385596,
               "score": 737
            },
            {
               "up_vote_count": 104,
               "answer_id": 7241298,
               "last_activity_date": 1331289489,
               "path": "3.stack.answer",
               "body_markdown": "`index()` returns the **first** index of value!\r\n&gt;  |  index(...)  \r\n&gt;  |      L.index(value, [start, [stop]]) -&gt; integer -- return first index of value\r\n\r\n    def all_indices(value, qlist):\r\n        indices = []\r\n        idx = -1\r\n        while True:\r\n            try:\r\n                idx = qlist.index(value, idx+1)\r\n                indices.append(idx)\r\n            except ValueError:\r\n                break\r\n        return indices\r\n\r\n    all_indices(&quot;foo&quot;, [&quot;foo&quot;,&quot;bar&quot;,&quot;baz&quot;,&quot;foo&quot;])\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1314697254,
               "last_edit_date": 1331289489,
               "is_accepted": false,
               "id": "7241298",
               "down_vote_count": 2,
               "score": 102
            },
            {
               "up_vote_count": 51,
               "answer_id": 12054409,
               "last_activity_date": 1509435946,
               "path": "3.stack.answer",
               "body_markdown": "    a = [&quot;foo&quot;,&quot;bar&quot;,&quot;baz&quot;,&#39;bar&#39;,&#39;any&#39;,&#39;much&#39;]\r\n    \r\n    indexes = [index for index in range(len(a)) if a[index] == &#39;bar&#39;]",
               "tags": [],
               "creation_date": 1345550514,
               "last_edit_date": 1509435946,
               "is_accepted": false,
               "id": "12054409",
               "down_vote_count": 0,
               "score": 51
            },
            {
               "up_vote_count": 62,
               "answer_id": 16034499,
               "last_activity_date": 1493028010,
               "path": "3.stack.answer",
               "body_markdown": "A problem will arise if the element is not in the list. This function handles the issue:\r\n\r\n\r\n    # if element is found it returns index of element else returns None\r\n\r\n    def find_element_in_list(element, list_element):\r\n        try:\r\n            index_element = list_element.index(element)\r\n            return index_element\r\n        except ValueError:\r\n            return None\r\n\r\n                    \r\n",
               "tags": [],
               "creation_date": 1366107576,
               "last_edit_date": 1493028010,
               "is_accepted": false,
               "id": "16034499",
               "down_vote_count": 2,
               "score": 60
            },
            {
               "up_vote_count": 36,
               "answer_id": 16593099,
               "last_activity_date": 1482589473,
               "path": "3.stack.answer",
               "body_markdown": "All of the proposed functions here reproduce inherent language behavior but obscure what&#39;s going on.\r\n\r\n    [i for i in range(len(mylist)) if mylist[i]==myterm]  # get the indices\r\n\r\n    [each for each in mylist if each==myterm]             # get the items\r\n\r\n    mylist.index(myterm) if myterm in mylist else None    # get the first index and fail quietly\r\n\r\nWhy write a function with exception handling if the language provides the methods to do what you want itself?",
               "tags": [],
               "creation_date": 1368722729,
               "last_edit_date": 1482589473,
               "is_accepted": false,
               "id": "16593099",
               "down_vote_count": 3,
               "score": 33
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 14,
               "answer_id": 16807733,
               "is_accepted": false,
               "last_activity_date": 1369811835,
               "body_markdown": "Simply you can go with\r\n\r\n    a = [[&#39;hand&#39;, &#39;head&#39;], [&#39;phone&#39;, &#39;wallet&#39;], [&#39;lost&#39;, &#39;stock&#39;]]\r\n    b = [&#39;phone&#39;, &#39;lost&#39;]\r\n    \r\n    res = [[x[0] for x in a].index(y) for y in b]\r\n\r\n",
               "id": "16807733",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1369811835,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 16822116,
               "is_accepted": false,
               "last_activity_date": 1369855041,
               "body_markdown": "Another option\r\n\r\n    &gt;&gt;&gt; a = [&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;red&#39;]\r\n    &gt;&gt;&gt; b = &#39;red&#39;\r\n    &gt;&gt;&gt; offset = 0;\r\n    &gt;&gt;&gt; indices = list()\r\n    &gt;&gt;&gt; for i in range(a.count(b)):\r\n    ...     indices.append(a.index(b,offset))\r\n    ...     offset = indices[-1]+1\r\n    ... \r\n    &gt;&gt;&gt; indices\r\n    [0, 3]\r\n    &gt;&gt;&gt; ",
               "id": "16822116",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1369855041,
               "score": 12
            },
            {
               "up_vote_count": 413,
               "answer_id": 17202481,
               "last_activity_date": 1509580558,
               "path": "3.stack.answer",
               "body_markdown": "The majority of answers explain how to find **a single index**, but their methods do not return multiple indexes if the item is in the list multiple times. Use [`enumerate()`](https://docs.python.org/3.6/library/functions.html#enumerate):\r\n\r\n    for i, j in enumerate([&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]):\r\n        if j == &#39;bar&#39;:\r\n            print(i)\r\n\r\nThe `index()` function only returns the first occurrence, while `enumerate()` returns all occurrences.\r\n\r\nAs a list comprehension:\r\n\r\n    [i for i, j in enumerate([&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]) if j == &#39;bar&#39;]\r\n\r\n---\r\n\r\nHere&#39;s also another small solution with [`itertools.count()`](http://docs.python.org/2/library/itertools.html#itertools.count) (which is pretty much the same approach as enumerate):\r\n\r\n    from itertools import izip as zip, count # izip for maximum efficiency\r\n    [i for i, j in zip(count(), [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]) if j == &#39;bar&#39;]\r\n\r\nThis is more efficient for larger lists than using `enumerate()`:\r\n\r\n    $ python -m timeit -s &quot;from itertools import izip as zip, count&quot; &quot;[i for i, j in zip(count(), [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]*500) if j == &#39;bar&#39;]&quot;\r\n    10000 loops, best of 3: 174 usec per loop\r\n    $ python -m timeit &quot;[i for i, j in enumerate([&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]*500) if j == &#39;bar&#39;]&quot;\r\n    10000 loops, best of 3: 196 usec per loop",
               "tags": [],
               "creation_date": 1371681112,
               "last_edit_date": 1509580558,
               "is_accepted": false,
               "id": "17202481",
               "down_vote_count": 3,
               "score": 410
            },
            {
               "up_vote_count": 108,
               "answer_id": 17300987,
               "last_activity_date": 1514047428,
               "path": "3.stack.answer",
               "body_markdown": "To get all indexes:\r\n\r\n     indexes = [i for i,x in enumerate(xs) if x == &#39;foo&#39;]",
               "tags": [],
               "creation_date": 1372172875,
               "last_edit_date": 1514047428,
               "is_accepted": false,
               "id": "17300987",
               "down_vote_count": 1,
               "score": 107
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 13,
               "answer_id": 22708420,
               "is_accepted": false,
               "last_activity_date": 1395997917,
               "body_markdown": "A variant on the answer from FMc and user7177 will give a dict that can return all indices for any entry:\r\n\r\n\r\n   \r\n    &gt;&gt;&gt; a = [&#39;foo&#39;,&#39;bar&#39;,&#39;baz&#39;,&#39;bar&#39;,&#39;any&#39;, &#39;foo&#39;, &#39;much&#39;]\r\n    &gt;&gt;&gt; l = dict(zip(set(a), map(lambda y: [i for i,z in enumerate(a) if z is y ], set(a))))\r\n    &gt;&gt;&gt; l[&#39;foo&#39;]\r\n    [0, 5]\r\n    &gt;&gt;&gt; l [&#39;much&#39;]\r\n    [6]\r\n    &gt;&gt;&gt; l\r\n    {&#39;baz&#39;: [2], &#39;foo&#39;: [0, 5], &#39;bar&#39;: [1, 3], &#39;any&#39;: [4], &#39;much&#39;: [6]}\r\n    &gt;&gt;&gt; \r\n\r\nYou could also use this as a one liner to get all indices for a single entry. There are no guarantees for efficiency, though I did use set(a) to reduce the number of times the lambda is called.\r\n\r\n",
               "id": "22708420",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1395997917,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 38,
               "answer_id": 23862698,
               "is_accepted": false,
               "last_activity_date": 1401078412,
               "body_markdown": "You have to set a condition to check if the element you&#39;re searching is in the list\r\n\r\n    if &#39;your_element&#39; in mylist:\r\n        print mylist.index(&#39;your_element&#39;)\r\n    else:\r\n        print None",
               "id": "23862698",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1401078412,
               "score": 38
            },
            {
               "up_vote_count": 10,
               "answer_id": 27712517,
               "last_activity_date": 1501277430,
               "path": "3.stack.answer",
               "body_markdown": "&lt;h1&gt;And now, for something completely different...  &lt;/h1&gt;\r\n... like confirming the existence of the item before getting the index.  The nice thing about this approach is the function always returns a list of indices -- even if it is an empty list.  It works with strings as well.\r\n\r\n    def indices(l, val):\r\n\t    &quot;&quot;&quot;Always returns a list containing the indices of val in the_list&quot;&quot;&quot;\r\n\t    retval = []\r\n\t    last = 0\r\n\t    while val in l[last:]:\r\n\t            i = l[last:].index(val)\r\n\t            retval.append(last + i)\r\n\t            last += i + 1   \r\n\t    return retval\r\n\r\n\tl = [&#39;bar&#39;,&#39;foo&#39;,&#39;bar&#39;,&#39;baz&#39;,&#39;bar&#39;,&#39;bar&#39;]\r\n\tq = &#39;bar&#39;\r\n\tprint indices(l,q)\r\n\tprint indices(l,&#39;bat&#39;)\r\n\tprint indices(&#39;abcdaababb&#39;,&#39;a&#39;)\r\n\r\nWhen pasted into an interactive python window:\r\n\r\n\tPython 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) \r\n\t[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\r\n\tType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\r\n\t&gt;&gt;&gt; def indices(the_list, val):\r\n\t...     &quot;&quot;&quot;Always returns a list containing the indices of val in the_list&quot;&quot;&quot;\r\n\t...     retval = []\r\n\t...     last = 0\r\n\t...     while val in the_list[last:]:\r\n\t...             i = the_list[last:].index(val)\r\n\t...             retval.append(last + i)\r\n\t...             last += i + 1   \r\n\t...     return retval\r\n\t... \r\n\t&gt;&gt;&gt; l = [&#39;bar&#39;,&#39;foo&#39;,&#39;bar&#39;,&#39;baz&#39;,&#39;bar&#39;,&#39;bar&#39;]\r\n\t&gt;&gt;&gt; q = &#39;bar&#39;\r\n\t&gt;&gt;&gt; print indices(l,q)\r\n\t[0, 2, 4, 5]\r\n\t&gt;&gt;&gt; print indices(l,&#39;bat&#39;)\r\n\t[]\r\n\t&gt;&gt;&gt; print indices(&#39;abcdaababb&#39;,&#39;a&#39;)\r\n\t[0, 4, 5, 7]\r\n\t&gt;&gt;&gt; \r\n",
               "tags": [],
               "creation_date": 1419973390,
               "last_edit_date": 1501277430,
               "is_accepted": false,
               "id": "27712517",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 10,
               "answer_id": 30283031,
               "last_activity_date": 1510466872,
               "path": "3.stack.answer",
               "body_markdown": "This solution is not as powerful as others, but if you&#39;re a beginner and only know about `for`loops it&#39;s still possible to find the first index of an item while avoiding the ValueError:\r\n\r\n    def find_element(p,t):\r\n        i = 0\r\n        for e in p:\r\n            if e == t:\r\n                return i\r\n            else:\r\n                i +=1\r\n        return -1\r\n",
               "tags": [],
               "creation_date": 1431832860,
               "last_edit_date": 1510466872,
               "is_accepted": false,
               "id": "30283031",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 31230699,
               "is_accepted": false,
               "last_activity_date": 1436101939,
               "body_markdown": "    name =&quot;bar&quot;\r\n    list = [[&quot;foo&quot;, 1], [&quot;bar&quot;, 2], [&quot;baz&quot;, 3]]\r\n    new_list=[]\r\n    for item in list:\r\n        new_list.append(item[0])\r\n    print(new_list)\r\n    try:\r\n        location= new_list.index(name)\r\n    except:\r\n        location=-1\r\n    print (location)\r\n\r\nThis accounts for if the string is not in the list too, if it isn&#39;t in the list then location = -1",
               "id": "31230699",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1436101939,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 33644671,
               "is_accepted": false,
               "last_activity_date": 1447218998,
               "body_markdown": "all indexes with zip function\r\n\r\n    get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]\r\n\r\n    print get_indexes(2,[1,2,3,4,5,6,3,2,3,2])\r\n    print get_indexes(&#39;f&#39;,&#39;xsfhhttytffsafweef&#39;)\r\n",
               "id": "33644671",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1447218998,
               "score": 16
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 28,
               "answer_id": 33765024,
               "is_accepted": false,
               "last_activity_date": 1447787123,
               "body_markdown": "If you want all indexes, then you can use numpy:\r\n\r\n    import numpy as np\r\n    \r\n    array = [1,2,1,3,4,5,1]\r\n    item = 1\r\n    np_array = np.array(array)    \r\n    item_index = np.where(np_array==item)\r\n    print item_index\r\n    # Out: (array([0, 2, 6], dtype=int64),)\r\n\r\nIt is clear, readable solution.",
               "id": "33765024",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1447787123,
               "score": 27
            },
            {
               "up_vote_count": 8,
               "answer_id": 45559614,
               "last_activity_date": 1519061381,
               "path": "3.stack.answer",
               "body_markdown": "&lt;h1&gt;Getting all the occurrences and the position of one or more (identical) items in a list&lt;/h1&gt;\r\n\r\nWith enumerate(alist) you can store the first element (n) that is the index of the list when the element x is equal to what you look for.\r\n\r\n    &gt;&gt;&gt; alist = [&#39;foo&#39;,&#39;spam&#39;,&#39;egg&#39;,&#39;foo&#39;]\r\n    &gt;&gt;&gt; foo_indexes = [n for n,x in enumerate(alist) if x==&#39;foo&#39;]\r\n    &gt;&gt;&gt; foo_indexes\r\n    [0, 3]\r\n    &gt;&gt;&gt;\r\n\r\n\r\n&lt;h1&gt;Let&#39;s make our Function findindex&lt;/h1&gt;\r\nThis function takes the item and the list as arg and return the position of the item in the liste, like we saw before.\r\n\r\n    def findindex(item2find,listOrString):\r\n      &quot;Search indexes of an item (arg.1) contained in a list or a string (arg.2)&quot;\r\n      return [n for n,item in enumerate(listOrString) if item==item2find]\r\n    \r\n    indexes1 = findindex(&quot;1&quot;,&quot;010101010&quot;)\r\n    print(indexes1)\r\n\r\n----------------------\r\n&gt;output\r\n\r\n----------------------\r\n\r\n    [1, 3, 5, 7]\r\n\r\n",
               "tags": [],
               "creation_date": 1502168461,
               "last_edit_date": 1519061381,
               "is_accepted": false,
               "id": "45559614",
               "down_vote_count": 0,
               "score": 8
            },
            {
               "up_vote_count": 4,
               "answer_id": 45654421,
               "last_activity_date": 1502569166,
               "path": "3.stack.answer",
               "body_markdown": "Since Python lists are zero-based,we can use the zip built-in function as follows :\r\n\r\n    &gt;&gt;&gt; [i for i,j in zip(range(len(haystack)),haystack) if j == &#39;needle&#39; ] \r\nwhere &quot;haystack&quot; is the list in question and &quot;needle&quot; is the item to look for.\r\n\r\n(Note : Here we are iterating using i to get the indexes but if we need rather to focus on the items we can switch to j)",
               "tags": [],
               "creation_date": 1502568068,
               "last_edit_date": 1502569166,
               "is_accepted": false,
               "id": "45654421",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 19,
               "answer_id": 45808300,
               "last_activity_date": 1504396580,
               "path": "3.stack.answer",
               "body_markdown": "&gt;##Finding the index of an item given a list containing it in Python\r\n\r\n&gt; For a list `[&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;]` and an item in the list `&quot;bar&quot;`, what&#39;s the cleanest way to get its index (1) in Python?\r\n\r\nWell, sure, there&#39;s the index method, which returns the index of the first occurrence:\r\n\r\n    &gt;&gt;&gt; l = [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;]\r\n    &gt;&gt;&gt; l.index(&#39;bar&#39;)\r\n    1\r\n\r\nThere are a couple of issues with this method:\r\n\r\n- if the value isn&#39;t in the list, you&#39;ll get a `ValueError`\r\n- if more than one of the value is in the list, you only get the index for the first one\r\n\r\n### No values\r\n\r\nIf the value could be missing, you need to catch the `ValueError`. \r\n\r\nYou can do so with a reusable definition like this:\r\n\r\n    def index(a_list, value):\r\n        try:\r\n            return a_list.index(value)\r\n        except ValueError:\r\n            return None\r\n\r\nAnd use it like this:\r\n\r\n    &gt;&gt;&gt; print(index(l, &#39;quux&#39;))\r\n    None\r\n    &gt;&gt;&gt; print(index(l, &#39;bar&#39;))\r\n    1\r\n\r\nAnd the downside of this is that you will probably have a check for if the returned value `is` or `is not` None:\r\n\r\n    result = index(a_list, value)\r\n    if result is not None:\r\n        do_something(result)\r\n\r\n\r\n### More than one value in the list\r\n\r\n\r\nIf you could have more occurrences, you&#39;ll **not** get complete information with `list.index`:\r\n\r\n    &gt;&gt;&gt; l.append(&#39;bar&#39;)\r\n    &gt;&gt;&gt; l\r\n    [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;bar&#39;]\r\n    &gt;&gt;&gt; l.index(&#39;bar&#39;)              # nothing at index 3?\r\n    1\r\n\r\nYou might enumerate into a list comprehension the indexes:\r\n\r\n    &gt;&gt;&gt; [index for index, v in enumerate(l) if v == &#39;bar&#39;]\r\n    [1, 3]\r\n    &gt;&gt;&gt; [index for index, v in enumerate(l) if v == &#39;boink&#39;]\r\n    []\r\n\r\nIf you have no occurrences, you can check for that with boolean check of the result, or just do nothing if you loop over the results:\r\n\r\n    indexes = [index for index, v in enumerate(l) if v == &#39;boink&#39;]\r\n    for index in indexes:\r\n        do_something(index)\r\n\r\n### Better data munging with pandas\r\n\r\nIf you have pandas, you can easily get this information with a Series object:\r\n\r\n    &gt;&gt;&gt; import pandas as pd\r\n    &gt;&gt;&gt; series = pd.Series(l)\r\n    &gt;&gt;&gt; series\r\n    0    foo\r\n    1    bar\r\n    2    baz\r\n    3    bar\r\n    dtype: object\r\n\r\nA comparison check will return a series of booleans:\r\n\r\n    &gt;&gt;&gt; series == &#39;bar&#39;\r\n    0    False\r\n    1     True\r\n    2    False\r\n    3     True\r\n    dtype: bool\r\n\r\nPass that series of booleans to the series via subscript notation, and you get just the matching members:\r\n\r\n    &gt;&gt;&gt; series[series == &#39;bar&#39;]\r\n    1    bar\r\n    3    bar\r\n    dtype: object\r\n\r\nIf you want just the indexes, the index attribute returns a series of integers:\r\n\r\n    &gt;&gt;&gt; series[series == &#39;bar&#39;].index\r\n    Int64Index([1, 3], dtype=&#39;int64&#39;)\r\n\r\nAnd if you want them in a list or tuple, just pass them to the constructor:\r\n\r\n    &gt;&gt;&gt; list(series[series == &#39;bar&#39;].index)\r\n    [1, 3]\r\n\r\nYes, you could use a list comprehension with enumerate too, but that&#39;s just not as elegant, in my opinion - you&#39;re doing tests for equality in Python, instead of letting builtin code written in C handle it:\r\n\r\n    &gt;&gt;&gt; [i for i, value in enumerate(l) if value == &#39;bar&#39;]\r\n    [1, 3]\r\n\r\n\r\n## Is this an [XY problem][1]?\r\n\r\n&gt; The XY problem is asking about your attempted solution rather than your actual problem.\r\n\r\nWhy do you think you need the index given an element in a list? \r\n\r\nIf you already know the value, why do you care where it is in a list?\r\n\r\nIf the value isn&#39;t there, catching the `ValueError` is rather verbose - and I prefer to avoid that.\r\n\r\nI&#39;m usually iterating over the list anyways, so I&#39;ll usually keep a pointer to any interesting information, getting the [index with enumerate.][2]\r\n\r\nIf you&#39;re munging data, you should probably be using pandas - which has far more elegant tools than the pure Python workarounds I&#39;ve shown.\r\n\r\nI do not recall needing `list.index`, myself. However, I have looked through the Python standard library, and I see some excellent uses for it. \r\n\r\nThere are many, many uses for it in `idlelib`, for GUI and text parsing.\r\n\r\nThe `keyword` module uses it to find comment markers in the module to automatically regenerate the list of keywords in it via metaprogramming.\r\n\r\nIn Lib/mailbox.py it seems to be using it like an ordered mapping:\r\n\r\n    key_list[key_list.index(old)] = new\r\n\r\nand\r\n\r\n    del key_list[key_list.index(key)]\r\n\r\nIn Lib/http/cookiejar.py, seems to be used to get the next month:\r\n\r\n    mon = MONTHS_LOWER.index(mon.lower())+1\r\n\r\nIn Lib/tarfile.py similar to distutils to get a slice up to an item:\r\n\r\n    members = members[:members.index(tarinfo)]\r\n\r\nIn Lib/pickletools.py:\r\n\r\n    numtopop = before.index(markobject)\r\n\r\nWhat these usages seem to have in common is that they seem to operate on lists of constrained sizes (important because of O(n) lookup time for `list.index`), and they&#39;re mostly used in parsing (and UI in the case of Idle). \r\n\r\nWhile there are use-cases for it, they are fairly uncommon. If you find yourself looking for this answer, ask yourself if what you&#39;re doing is the most direct usage of the tools provided by the language for your use-case.\r\n\r\n  [1]: https://meta.stackexchange.com/a/66378/239121\r\n  [2]: https://stackoverflow.com/q/522563/541136",
               "tags": [],
               "creation_date": 1503371339,
               "last_edit_date": 1504396580,
               "is_accepted": false,
               "id": "45808300",
               "down_vote_count": 1,
               "score": 18
            },
            {
               "up_vote_count": 1,
               "answer_id": 48530557,
               "last_activity_date": 1517347926,
               "path": "3.stack.answer",
               "body_markdown": "For those caming from another language like me, maybe with a simple loop it&#39;s easier to understand and use it:\r\n\r\n    mylist = [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;, &quot;bar&quot;]\r\n    newlist = enumerate(mylist)\r\n    for index, item in newlist:\r\n      if item == &quot;bar&quot;:\r\n        print(index, item)\r\n\r\nThankfull for https://www.codecademy.com/en/forum_questions/5087f2d786a27b02000041a9, that helped me to understand.",
               "tags": [],
               "creation_date": 1517346612,
               "last_edit_date": 1517347926,
               "is_accepted": false,
               "id": "48530557",
               "down_vote_count": 0,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/176918/finding-the-index-of-an-item-given-a-list-containing-it-in-python",
         "id": "858127-2287"
      },
      {
         "up_vote_count": "235",
         "path": "2.stack",
         "body_markdown": "##Background\r\nI just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:\r\n\r\n    E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\r\n    Try using .loc[row_index,col_indexer] = value instead\r\n      quote_df[&#39;TVol&#39;]   = quote_df[&#39;TVol&#39;]/TVOL_SCALE\r\n\r\nI want to know what exactly it means?  Do I need to change something?\r\n\r\nHow should I suspend the warning if I insist to use `quote_df[&#39;TVol&#39;]   = quote_df[&#39;TVol&#39;]/TVOL_SCALE`?\r\n\r\n##The function that gives errors\r\n\r\n    def _decode_stock_quote(list_of_150_stk_str):\r\n        &quot;&quot;&quot;decode the webpage and return dataframe&quot;&quot;&quot;\r\n    \r\n        from cStringIO import StringIO\r\n    \r\n        str_of_all = &quot;&quot;.join(list_of_150_stk_str)\r\n    \r\n        quote_df = pd.read_csv(StringIO(str_of_all), sep=&#39;,&#39;, names=list(&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg&#39;)) #dtype={&#39;A&#39;: object, &#39;B&#39;: object, &#39;C&#39;: np.float64}\r\n        quote_df.rename(columns={&#39;A&#39;:&#39;STK&#39;, &#39;B&#39;:&#39;TOpen&#39;, &#39;C&#39;:&#39;TPCLOSE&#39;, &#39;D&#39;:&#39;TPrice&#39;, &#39;E&#39;:&#39;THigh&#39;, &#39;F&#39;:&#39;TLow&#39;, &#39;I&#39;:&#39;TVol&#39;, &#39;J&#39;:&#39;TAmt&#39;, &#39;e&#39;:&#39;TDate&#39;, &#39;f&#39;:&#39;TTime&#39;}, inplace=True)\r\n        quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\r\n        quote_df[&#39;TClose&#39;] = quote_df[&#39;TPrice&#39;]\r\n        quote_df[&#39;RT&#39;]     = 100 * (quote_df[&#39;TPrice&#39;]/quote_df[&#39;TPCLOSE&#39;] - 1)\r\n        quote_df[&#39;TVol&#39;]   = quote_df[&#39;TVol&#39;]/TVOL_SCALE\r\n        quote_df[&#39;TAmt&#39;]   = quote_df[&#39;TAmt&#39;]/TAMT_SCALE\r\n        quote_df[&#39;STK_ID&#39;] = quote_df[&#39;STK&#39;].str.slice(13,19)\r\n        quote_df[&#39;STK_Name&#39;] = quote_df[&#39;STK&#39;].str.slice(21,30)#.decode(&#39;gb2312&#39;)\r\n        quote_df[&#39;TDate&#39;]  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\r\n        \r\n        return quote_df\r\n\r\n## More error messages\r\n\r\n    E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\r\n    Try using .loc[row_index,col_indexer] = value instead\r\n      quote_df[&#39;TVol&#39;]   = quote_df[&#39;TVol&#39;]/TVOL_SCALE\r\n    E:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\r\n    Try using .loc[row_index,col_indexer] = value instead\r\n      quote_df[&#39;TAmt&#39;]   = quote_df[&#39;TAmt&#39;]/TAMT_SCALE\r\n    E:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\r\n    Try using .loc[row_index,col_indexer] = value instead\r\n      quote_df[&#39;TDate&#39;]  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])",
         "view_count": "215233",
         "answer_count": "7",
         "tags": "['python', 'pandas', 'dataframe', 'chained-assignment']",
         "creation_date": "1387252082",
         "last_edit_date": "1512144957",
         "code_snippet": "[\"<code>E:\\\\FinReporter\\\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_index,col_indexer] = value instead\\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\\n</code>\", \"<code>quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE</code>\", '<code>def _decode_stock_quote(list_of_150_stk_str):\\n    \"\"\"decode the webpage and return dataframe\"\"\"\\n\\n    from cStringIO import StringIO\\n\\n    str_of_all = \"\".join(list_of_150_stk_str)\\n\\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=\\',\\', names=list(\\'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg\\')) #dtype={\\'A\\': object, \\'B\\': object, \\'C\\': np.float64}\\n    quote_df.rename(columns={\\'A\\':\\'STK\\', \\'B\\':\\'TOpen\\', \\'C\\':\\'TPCLOSE\\', \\'D\\':\\'TPrice\\', \\'E\\':\\'THigh\\', \\'F\\':\\'TLow\\', \\'I\\':\\'TVol\\', \\'J\\':\\'TAmt\\', \\'e\\':\\'TDate\\', \\'f\\':\\'TTime\\'}, inplace=True)\\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\\n    quote_df[\\'TClose\\'] = quote_df[\\'TPrice\\']\\n    quote_df[\\'RT\\']     = 100 * (quote_df[\\'TPrice\\']/quote_df[\\'TPCLOSE\\'] - 1)\\n    quote_df[\\'TVol\\']   = quote_df[\\'TVol\\']/TVOL_SCALE\\n    quote_df[\\'TAmt\\']   = quote_df[\\'TAmt\\']/TAMT_SCALE\\n    quote_df[\\'STK_ID\\'] = quote_df[\\'STK\\'].str.slice(13,19)\\n    quote_df[\\'STK_Name\\'] = quote_df[\\'STK\\'].str.slice(21,30)#.decode(\\'gb2312\\')\\n    quote_df[\\'TDate\\']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\\n\\n    return quote_df\\n</code>', \"<code>E:\\\\FinReporter\\\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_index,col_indexer] = value instead\\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\\nE:\\\\FinReporter\\\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_index,col_indexer] = value instead\\n  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE\\nE:\\\\FinReporter\\\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_index,col_indexer] = value instead\\n  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\\n</code>\", '<code>df.set_value</code>', '<code>SettingWithCopyWarning</code>', \"<code>df[df['A'] &gt; 2]['B'] = new_val  # new_val not set in df\\n</code>\", \"<code>df.loc[df['A'] &gt; 2, 'B'] = new_val\\n</code>\", \"<code>df = df[df['A'] &gt; 2]\\ndf['B'] = new_val\\n</code>\", \"<code>pd.options.mode.chained_assignment = None  # default='warn'\\n</code>\", '<code>.ix</code>', '<code>.iloc</code>', '<code>SettingWithCopyWarning</code>', \"<code>In [1]: df = DataFrame(np.random.randn(5,2),columns=list('AB'))\\n\\nIn [2]: dfa = df.ix[:,[1,0]]\\n\\nIn [3]: dfa.is_copy\\nOut[3]: True\\n\\nIn [4]: dfa['A'] /= 2\\n/usr/local/bin/ipython:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_index,col_indexer] = value instead\\n  #!/usr/local/bin/python\\n</code>\", '<code>is_copy</code>', '<code>False</code>', \"<code>In [5]: dfa.is_copy = False\\n\\nIn [6]: dfa['A'] /= 2\\n</code>\", \"<code>In [7]: dfa = df.ix[:,[1,0]].copy()\\n\\nIn [8]: dfa['A'] /= 2\\n</code>\", '<code>reindex</code>', \"<code>quote_df = quote_df.reindex(columns=['STK',.......])\\n</code>\", \"<code>quote_df = quote_df.reindex(['STK',.......], axis=1) # v.0.21\\n</code>\", '<code>0.16</code>', '<code>undefined</code>', '<code>C</code>', '<code>api</code>', \"<code>warnings.filterwarnings('error', r'SettingWithCopyWarning</code>\", '<code>.loc</code>', '<code>quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\\n</code>', '<code>pandas.ix</code>', '<code>.ix</code>', '<code>.ix</code>', '<code>df = pd.DataFrame({\"a\": [1,2,3,4], \"b\": [1,1,2,2]})\\n</code>', '<code>dfcopy = df.ix[:,[\"a\"]]\\ndfcopy.a.ix[0] = 2\\n</code>', '<code>dfcopy</code>', '<code>df</code>', '<code>df.ix[0, \"a\"] = 3\\n</code>', '<code>.loc</code>', '<code>.ix</code>', '<code>.iloc</code>', '<code>.loc</code>', '<code>.loc</code>', '<code>.loc</code>', '<code>pd.read_csv</code>', \"<code>quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}\\nquote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)\\nquote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\\n</code>\", \"<code>columns = ['STK', 'TPrice', 'TPCLOSE', 'TOpen', 'THigh', 'TLow', 'TVol', 'TAmt', 'TDate', 'TTime']\\ndf = pd.read_csv(StringIO(str_of_all), sep=',', usecols=[0,3,2,1,4,5,8,9,30,31])\\ndf.columns = columns\\n</code>\", '<code>.ix</code>', '<code>.iloc</code>', '<code>/opt/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:54:\\nSettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\\nSee the caveats in the documentation:\\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\\n</code>', \"<code>&gt;&gt; data1 = {'A': [111, 112, 113], 'B':[121, 122, 123]}\\n&gt;&gt; df1 = pd.DataFrame(data1)\\n&gt;&gt; df1\\n\\n    A   B\\n0   111 121\\n1   112 122\\n2   113 123\\n\\n\\n&gt;&gt; df2 = df1\\n&gt;&gt; df2\\n\\nA   B\\n0   111 121\\n1   112 122\\n2   113 123\\n\\n# Dropping a column on df1 affects df2\\n&gt;&gt; df1.drop('A', axis=1, inplace=True)\\n&gt;&gt; df2\\n    B\\n0   121\\n1   122\\n2   123\\n</code>\", \"<code>&gt;&gt; data1 = {'A': [111, 112, 113], 'B':[121, 122, 123]}\\n&gt;&gt; df1 = pd.DataFrame(data1)\\n&gt;&gt; df1\\n\\nA   B\\n0   111 121\\n1   112 122\\n2   113 123\\n\\n&gt;&gt; import copy\\n&gt;&gt; df2 = copy.deepcopy(df1)\\n&gt;&gt; df2\\nA   B\\n0   111 121\\n1   112 122\\n2   113 123\\n\\n# Dropping a column on df1 does not affect df2\\n&gt;&gt; df1.drop('A', axis=1, inplace=True)\\n&gt;&gt; df2\\n    A   B\\n0   111 121\\n1   112 122\\n2   113 123\\n</code>\", \"<code>&gt;&gt; data1 = {'A': [111, 112, 113], 'B':[121, 122, 123]}\\n&gt;&gt; df1 = pd.DataFrame(data1)\\n&gt;&gt; df1\\n\\n    A   B\\n0   111 121\\n1   112 122\\n2   113 123\\n\\n&gt;&gt; df2 = df1\\n&gt;&gt; df2\\n\\n    A   B\\n0   111 121\\n1   112 122\\n2   113 123\\n\\n# Dropping a column on df2 can affect df1\\n# No slice involved here, but I believe the principle remains the same?\\n# Let me know if not\\n&gt;&gt; df2.drop('A', axis=1, inplace=True)\\n&gt;&gt; df1\\n\\nB\\n0   121\\n1   122\\n2   123\\n</code>\", \"<code>&gt;&gt; data1 = {'A': [111, 112, 113], 'B':[121, 122, 123]}\\n&gt;&gt; df1 = pd.DataFrame(data1)\\n&gt;&gt; df1\\n\\n    A   B\\n0   111 121\\n1   112 122\\n2   113 123\\n\\n&gt;&gt; import copy\\n&gt;&gt; df2 = copy.deepcopy(df1)\\n&gt;&gt; df2\\n\\nA   B\\n0   111 121\\n1   112 122\\n2   113 123\\n\\n&gt;&gt; df2.drop('A', axis=1, inplace=True)\\n&gt;&gt; df1\\n\\nA   B\\n0   111 121\\n1   112 122\\n2   113 123\\n</code>\", \"<code>df2 = df[df['A'] &gt; 2]\\ndf2['B'] = value\\n</code>\", '<code>df2</code>', \"<code>df.loc[df2.index.tolist(), 'B'] = value\\n</code>\", '<code>df2.index.tolist()</code>', '<code>def update_old_dataframe(old_dataframe, new_dataframe):\\n    for new_index, new_row in new_dataframe.iterrorws():\\n        old_dataframe.loc[new_index] = update_row(old_dataframe.loc[new_index], new_row)\\n\\ndef update_row(old_row, new_row):\\n    for field in [list_of_columns]:\\n        # line with warning because of chain indexing old_dataframe[new_index][field]\\n        old_row[field] = new_row[field]  \\n    return old_row\\n</code>', '<code>old_row[field] = new_row[field]</code>', '<code>Series</code>', '<code>old_row.at[field] = new_row.at[field]\\n</code>', '<code>Series</code>', \"<code>return (\\n    pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}\\n    .rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)\\n    .ix[:,[0,3,2,1,4,5,8,9,30,31]]\\n    .assign(\\n        TClose=lambda df: df['TPrice'],\\n        RT=lambda df: 100 * (df['TPrice']/quote_df['TPCLOSE'] - 1),\\n        TVol=lambda df: df['TVol']/TVOL_SCALE,\\n        TAmt=lambda df: df['TAmt']/TAMT_SCALE,\\n        STK_ID=lambda df: df['STK'].str.slice(13,19),\\n        STK_Name=lambda df: df['STK'].str.slice(21,30)#.decode('gb2312'),\\n        TDate=lambda df: df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10]),\\n    )\\n)\\n</code>\"]",
         "title": "How to deal with SettingWithCopyWarning in Pandas?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 326,
               "answer_id": 20627316,
               "is_accepted": true,
               "last_activity_date": 1387261223,
               "body_markdown": "From what I gather, `SettingWithCopyWarning` was created to flag potentially confusing &quot;chained&quot; assignments, such as the following, which don&#39;t always work as expected, particularly when the first selection returns a *copy*.  [see [GH5390][1] and [GH5597][2] for background discussion.]\r\n\r\n    df[df[&#39;A&#39;] &gt; 2][&#39;B&#39;] = new_val  # new_val not set in df\r\n\r\nThe warning offers a suggestion to rewrite as follows:\r\n\r\n    df.loc[df[&#39;A&#39;] &gt; 2, &#39;B&#39;] = new_val\r\n\r\nHowever, this doesn&#39;t fit your usage, which is equivalent to:\r\n\r\n    df = df[df[&#39;A&#39;] &gt; 2]\r\n    df[&#39;B&#39;] = new_val\r\n\r\nWhile it&#39;s clear that you don&#39;t care about writes making it back to the original frame (since you overwrote the reference to it), unfortunately this pattern can not be differentiated from the first chained assignment example, hence the (false positive) warning.  The potential for false positives is addressed in the [docs on indexing][3], if you&#39;d like to read further.  You can safely disable this new warning with the following assignment.\r\n\r\n    pd.options.mode.chained_assignment = None  # default=&#39;warn&#39;\r\n\r\n\r\n  [1]: https://github.com/pydata/pandas/pull/5390\r\n  [2]: https://github.com/pydata/pandas/issues/5597\r\n  [3]: http://pandas.pydata.org/pandas-docs/dev/indexing.html#returning-a-view-versus-a-copy",
               "id": "20627316",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1387261223,
               "score": 326
            },
            {
               "up_vote_count": 88,
               "answer_id": 20644369,
               "last_activity_date": 1514369925,
               "path": "3.stack.answer",
               "body_markdown": "In general the point of the ``SettingWithCopyWarning`` is to show users (and esp new users) that they *may* be operating on a copy and not the original as they think. There *are* False positives (IOW you know what you are doing, so it *ok*). One possibility is simply to turn off the (by default *warn*) warning as @Garrett suggest.\r\n\r\nHere is a nother, per option.\r\n   \r\n    In [1]: df = DataFrame(np.random.randn(5,2),columns=list(&#39;AB&#39;))\r\n    \r\n    In [2]: dfa = df.ix[:,[1,0]]\r\n    \r\n    In [3]: dfa.is_copy\r\n    Out[3]: True\r\n    \r\n    In [4]: dfa[&#39;A&#39;] /= 2\r\n    /usr/local/bin/ipython:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\r\n    Try using .loc[row_index,col_indexer] = value instead\r\n      #!/usr/local/bin/python\r\n\r\nYou can set the ``is_copy`` flag to ``False``, which will effectively turn off the check, *for that object``\r\n    \r\n    In [5]: dfa.is_copy = False\r\n    \r\n    In [6]: dfa[&#39;A&#39;] /= 2\r\n    \r\nIf you explicity copy then you *know what you are doing*, so no further warning will happen.\r\n\r\n    In [7]: dfa = df.ix[:,[1,0]].copy()\r\n    \r\n    In [8]: dfa[&#39;A&#39;] /= 2\r\n\r\nThe code the OP is showing above, while legitimate, and probably something I do as well, is technically a case for this warning, and not a False positive. Another way to *not* have the warning would be to do the selection operation via ``reindex``, e.g.\r\n\r\n    quote_df = quote_df.reindex(columns=[&#39;STK&#39;,.......])\r\n\r\nOr, \r\n\r\n    quote_df = quote_df.reindex([&#39;STK&#39;,.......], axis=1) # v.0.21\r\n    ",
               "tags": [],
               "creation_date": 1387313350,
               "last_edit_date": 1514369925,
               "is_accepted": false,
               "id": "20644369",
               "down_vote_count": 0,
               "score": 88
            },
            {
               "up_vote_count": 25,
               "answer_id": 40214434,
               "last_activity_date": 1479113329,
               "path": "3.stack.answer",
               "body_markdown": "#Pandas dataframe copy warning\r\n\r\nWhen you go and do something like this:\r\n\r\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\r\n\r\n`pandas.ix` _in this case_ returns a new, stand alone dataframe.\r\n\r\nAny values you decide to change in this dataframe, will not change the original dataframe.\r\n\r\nThis is what pandas tries to warn you about.\r\n\r\n#Why `.ix` is a bad idea\r\nThe `.ix` object tries to do more than one thing, and for anyone who has read anything about clean code, this is a strong smell.\r\n\r\nGiven this dataframe:\r\n    \r\n    df = pd.DataFrame({&quot;a&quot;: [1,2,3,4], &quot;b&quot;: [1,1,2,2]})\r\n\r\nTwo behaviors:\r\n\r\n    dfcopy = df.ix[:,[&quot;a&quot;]]\r\n    dfcopy.a.ix[0] = 2\r\n\r\nBehavior one: `dfcopy` is now a stand alone dataframe. Changing it will not change `df`\r\n\r\n    df.ix[0, &quot;a&quot;] = 3\r\n\r\nBehavior two: This changes the original dataframe.\r\n\r\n#Use `.loc` instead\r\n\r\nThe pandas developers recognized that the `.ix` object was quite smelly[speculatively] and thus created two new objects which helps in the accession and assignment of data. (The other being `.iloc`)\r\n\r\n`.loc` is faster, because it does not try to create a copy of the data.\r\n\r\n`.loc` is meant to modify your existing dataframe inplace, which is more memory efficient.\r\n\r\n`.loc` is predictable, it has one behavior.\r\n\r\n#The solution\r\n\r\nWhat you are doing in your code example is loading a big file with lots of columns, then modifying it to be smaller.\r\n\r\nThe `pd.read_csv` function can help you out with a lot of this and also make the loading of the file a lot faster.\r\n\r\nSo instead of doing this\r\n\r\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=&#39;,&#39;, names=list(&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg&#39;)) #dtype={&#39;A&#39;: object, &#39;B&#39;: object, &#39;C&#39;: np.float64}\r\n    quote_df.rename(columns={&#39;A&#39;:&#39;STK&#39;, &#39;B&#39;:&#39;TOpen&#39;, &#39;C&#39;:&#39;TPCLOSE&#39;, &#39;D&#39;:&#39;TPrice&#39;, &#39;E&#39;:&#39;THigh&#39;, &#39;F&#39;:&#39;TLow&#39;, &#39;I&#39;:&#39;TVol&#39;, &#39;J&#39;:&#39;TAmt&#39;, &#39;e&#39;:&#39;TDate&#39;, &#39;f&#39;:&#39;TTime&#39;}, inplace=True)\r\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\r\n\r\nDo this\r\n\r\n    columns = [&#39;STK&#39;, &#39;TPrice&#39;, &#39;TPCLOSE&#39;, &#39;TOpen&#39;, &#39;THigh&#39;, &#39;TLow&#39;, &#39;TVol&#39;, &#39;TAmt&#39;, &#39;TDate&#39;, &#39;TTime&#39;]\r\n    df = pd.read_csv(StringIO(str_of_all), sep=&#39;,&#39;, usecols=[0,3,2,1,4,5,8,9,30,31])\r\n    df.columns = columns\r\n\r\nThis will only read the columns you are interested in, and name them properly. No need for using the evil `.ix` object to do magical stuff.\r\n",
               "tags": [],
               "creation_date": 1477299695,
               "last_edit_date": 1479113329,
               "is_accepted": false,
               "id": "40214434",
               "down_vote_count": 0,
               "score": 25
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 44731898,
               "is_accepted": false,
               "last_activity_date": 1498267848,
               "body_markdown": "If you have assigned the slice to a variable and want to set using the variable as in the following:\r\n\r\n    df2 = df[df[&#39;A&#39;] &gt; 2]\r\n    df2[&#39;B&#39;] = value\r\n\r\nAnd you do not want to use Jeffs solution because your condition computing `df2` is to long or for some other reason, then you can use the following:\r\n\r\n    df.loc[df2.index.tolist(), &#39;B&#39;] = value\r\n\r\n`df2.index.tolist()` returns the indices from all entries in df2, which will then be used to set column B in the original dataframe.",
               "id": "44731898",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1498267848,
               "score": 1
            },
            {
               "up_vote_count": 4,
               "answer_id": 45361923,
               "last_activity_date": 1501194313,
               "path": "3.stack.answer",
               "body_markdown": "To remove any doubt, my solution was to make a deep copy of the slice instead of a regular copy.\r\nThis may not be applicable depending on your context (Memory constraints / size of the slice, potential for performance degradation - especially if the copy occurs in a loop like it did for me, etc...)\r\n\r\nTo be clear, here is the warning I received:\r\n\r\n    /opt/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:54:\r\n    SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\r\n    See the caveats in the documentation:\r\n    http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\r\n\r\n\r\n## Illustration\r\nI had doubts that the warning was thrown because of a column I was dropping on a copy of the slice. While not technically trying to set a value in the copy of the slice, that was still a modification of the copy of the slice.\r\nBelow are the (simplified) steps I have taken to confirm the suspicion, I hope it will help those of us who are trying to understand the warning.\r\n\r\n\r\n### Example 1: dropping a column on the original affects the copy\r\nWe knew that already but this is a healthy reminder. This is *NOT* what the warning is about.\r\n\r\n    &gt;&gt; data1 = {&#39;A&#39;: [111, 112, 113], &#39;B&#39;:[121, 122, 123]}\r\n    &gt;&gt; df1 = pd.DataFrame(data1)\r\n    &gt;&gt; df1\r\n    \r\n    \tA\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n    \r\n    \r\n    &gt;&gt; df2 = df1\r\n    &gt;&gt; df2\r\n    \r\n    A\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n    \r\n    # Dropping a column on df1 affects df2\r\n    &gt;&gt; df1.drop(&#39;A&#39;, axis=1, inplace=True)\r\n    &gt;&gt; df2\r\n    \tB\r\n    0\t121\r\n    1\t122\r\n    2\t123\r\n\r\nIt is possible to avoid changes made on df1 to affect df2\r\n\r\n    &gt;&gt; data1 = {&#39;A&#39;: [111, 112, 113], &#39;B&#39;:[121, 122, 123]}\r\n    &gt;&gt; df1 = pd.DataFrame(data1)\r\n    &gt;&gt; df1\r\n    \r\n    A\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n    \r\n    &gt;&gt; import copy\r\n    &gt;&gt; df2 = copy.deepcopy(df1)\r\n    &gt;&gt; df2\r\n    A\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n    \r\n    # Dropping a column on df1 does not affect df2\r\n    &gt;&gt; df1.drop(&#39;A&#39;, axis=1, inplace=True)\r\n    &gt;&gt; df2\r\n    \tA\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n\r\n\r\n### Example 2: dropping a column on the copy may affect the original\r\nThis actually illustrates the warning.\r\n\r\n    &gt;&gt; data1 = {&#39;A&#39;: [111, 112, 113], &#39;B&#39;:[121, 122, 123]}\r\n    &gt;&gt; df1 = pd.DataFrame(data1)\r\n    &gt;&gt; df1\r\n    \r\n    \tA\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n    \r\n    &gt;&gt; df2 = df1\r\n    &gt;&gt; df2\r\n    \r\n    \tA\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n    \r\n    # Dropping a column on df2 can affect df1\r\n    # No slice involved here, but I believe the principle remains the same?\r\n    # Let me know if not\r\n    &gt;&gt; df2.drop(&#39;A&#39;, axis=1, inplace=True)\r\n    &gt;&gt; df1\r\n    \r\n    B\r\n    0\t121\r\n    1\t122\r\n    2\t123\r\n\r\n\r\nIt is possible to avoid changes made on df2 to affect df1\r\n\r\n    &gt;&gt; data1 = {&#39;A&#39;: [111, 112, 113], &#39;B&#39;:[121, 122, 123]}\r\n    &gt;&gt; df1 = pd.DataFrame(data1)\r\n    &gt;&gt; df1\r\n    \r\n    \tA\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n    \r\n    &gt;&gt; import copy\r\n    &gt;&gt; df2 = copy.deepcopy(df1)\r\n    &gt;&gt; df2\r\n    \r\n    A\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n    \r\n    &gt;&gt; df2.drop(&#39;A&#39;, axis=1, inplace=True)\r\n    &gt;&gt; df1\r\n    \r\n    A\tB\r\n    0\t111\t121\r\n    1\t112\t122\r\n    2\t113\t123\r\n\r\nCheers!",
               "tags": [],
               "creation_date": 1501193941,
               "last_edit_date": 1501194313,
               "is_accepted": false,
               "id": "45361923",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 0,
               "answer_id": 46732545,
               "last_activity_date": 1516138058,
               "path": "3.stack.answer",
               "body_markdown": "You could avoid the whole problem like this, I believe:\r\n\r\n    return (\r\n        pd.read_csv(StringIO(str_of_all), sep=&#39;,&#39;, names=list(&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg&#39;)) #dtype={&#39;A&#39;: object, &#39;B&#39;: object, &#39;C&#39;: np.float64}\r\n        .rename(columns={&#39;A&#39;:&#39;STK&#39;, &#39;B&#39;:&#39;TOpen&#39;, &#39;C&#39;:&#39;TPCLOSE&#39;, &#39;D&#39;:&#39;TPrice&#39;, &#39;E&#39;:&#39;THigh&#39;, &#39;F&#39;:&#39;TLow&#39;, &#39;I&#39;:&#39;TVol&#39;, &#39;J&#39;:&#39;TAmt&#39;, &#39;e&#39;:&#39;TDate&#39;, &#39;f&#39;:&#39;TTime&#39;}, inplace=True)\r\n        .ix[:,[0,3,2,1,4,5,8,9,30,31]]\r\n        .assign(\r\n            TClose=lambda df: df[&#39;TPrice&#39;],\r\n            RT=lambda df: 100 * (df[&#39;TPrice&#39;]/quote_df[&#39;TPCLOSE&#39;] - 1),\r\n            TVol=lambda df: df[&#39;TVol&#39;]/TVOL_SCALE,\r\n            TAmt=lambda df: df[&#39;TAmt&#39;]/TAMT_SCALE,\r\n            STK_ID=lambda df: df[&#39;STK&#39;].str.slice(13,19),\r\n            STK_Name=lambda df: df[&#39;STK&#39;].str.slice(21,30)#.decode(&#39;gb2312&#39;),\r\n            TDate=lambda df: df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10]),\r\n        )\r\n    )\r\n\r\nUsing Assign. From the [documentation][1]: Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones. \r\n\r\nSee Tom Augspurger&#39;s article on method chaining in pandas: https://tomaugspurger.github.io/method-chaining\r\n\r\n\r\n  [1]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html",
               "tags": [],
               "creation_date": 1507905916,
               "last_edit_date": 1516138058,
               "is_accepted": false,
               "id": "46732545",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47507827,
               "is_accepted": false,
               "last_activity_date": 1511775597,
               "body_markdown": "For me this issue occured in a following &gt;simplified&lt; example. And I was also able to solve it (hopefully with a correct solution):\r\n\r\nold code with warning:\r\n\r\n    def update_old_dataframe(old_dataframe, new_dataframe):\r\n        for new_index, new_row in new_dataframe.iterrorws():\r\n            old_dataframe.loc[new_index] = update_row(old_dataframe.loc[new_index], new_row)\r\n\r\n    def update_row(old_row, new_row):\r\n        for field in [list_of_columns]:\r\n            # line with warning because of chain indexing old_dataframe[new_index][field]\r\n            old_row[field] = new_row[field]  \r\n        return old_row\r\n\r\nThis printed the warning for the line `old_row[field] = new_row[field]`\r\n\r\nSince the rows in update_row method are actually type `Series`, I replaced the line with:\r\n\r\n    old_row.at[field] = new_row.at[field]\r\ni.e. [method][1] for accessing/lookups for a `Series`. Eventhough both works just fine and the result is same, this way I don&#39;t have to disable the warnings (=keep them for other chain indexing issues somewhere else).\r\n\r\nI hope this may help someone.\r\n\r\n\r\n  [1]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.at.html",
               "id": "47507827",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1511775597,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas",
         "id": "858127-2288"
      },
      {
         "up_vote_count": "215",
         "path": "2.stack",
         "body_markdown": "I have a date `&quot;10/10/11(m-d-y)&quot;` and I want to add 5 days to it using a Python script. Please consider a general solution that works on the month ends also.\r\n\r\nI am using following code:\r\n\r\n    import re\r\n    from datetime import datetime\r\n    \r\n    StartDate = &quot;10/10/11&quot;\r\n    \r\n    Date = datetime.strptime(StartDate, &quot;%m/%d/%y&quot;)\r\n\r\n`print Date` -&gt; is printing `&#39;2011-10-10 00:00:00&#39;`\r\n\r\nNow I want to add 5 days to this date. I used the following code:\r\n\r\n    EndDate = Date.today()+timedelta(days=10)\r\n\r\nWhich returned this error:\r\n\r\n    name &#39;timedelta&#39; is not defined",
         "view_count": "210471",
         "answer_count": "8",
         "tags": "['python', 'date', 'datetime']",
         "creation_date": "1311931039",
         "last_edit_date": "1468238993",
         "code_snippet": "['<code>\"10/10/11(m-d-y)\"</code>', '<code>import re\\nfrom datetime import datetime\\n\\nStartDate = \"10/10/11\"\\n\\nDate = datetime.strptime(StartDate, \"%m/%d/%y\")\\n</code>', '<code>print Date</code>', \"<code>'2011-10-10 00:00:00'</code>\", '<code>EndDate = Date.today()+timedelta(days=10)\\n</code>', \"<code>name 'timedelta' is not defined\\n</code>\", \"<code>name 'timedelta' is not defined</code>\", '<code>timedelta</code>', '<code>import datetime\\n</code>', '<code>datetime.timedelta</code>', '<code>date_1 = datetime.datetime.strptime(start_date, \"%m/%d/%y\")\\n\\nend_date = date_1 + datetime.timedelta(days=10)\\n</code>', '<code>strptime</code>', '<code>datetime</code>', '<code>datetime</code>', '<code>datetime.datetime</code>', '<code>from datetime import datetime, timedelta</code>', '<code>from datetime import DateTime</code>', '<code>timedelta</code>', '<code>from datetime import timedelta\\n</code>', '<code>Date.today()</code>', '<code>EndDate = Date + timedelta(days=10)\\n</code>', '<code>date</code>', '<code>timedelta</code>', '<code>from datetime import timedelta, date; date.today() + timedelta(days=10)</code>', '<code>from datetime import timedelta\\n</code>', '<code>import pandas as pd\\nstartdate = \"10/10/2011\"\\nenddate = pd.to_datetime(startdate) + pd.DateOffset(days=5)\\n</code>', \"<code>from datetime import datetime\\nfrom dateutil.relativedelta import relativedelta\\n\\nprint 'Today: ',datetime.now().strftime('%d/%m/%Y %H:%M:%S') \\ndate_after_month = datetime.now()+ relativedelta(days=5)\\nprint 'After 5 Days:', date_after_month.strftime('%d/%m/%Y %H:%M:%S')\\n</code>\", '<code>import datetime\\n\\ndef get_date(dateFormat=\"%d-%m-%Y\", addDays=0):\\n\\n    timeNow = datetime.datetime.now()\\n    if (addDays!=0):\\n        anotherTime = timeNow + datetime.timedelta(days=addDays)\\n    else:\\n        anotherTime = timeNow\\n\\n    return anotherTime.strftime(dateFormat)\\n</code>', \"<code>addDays = 3 #days\\noutput_format = '%d-%m-%Y'\\noutput = get_date(output_format, addDays)\\nprint output\\n</code>\", '<code>from datetime import datetime as DateTime, timedelta as TimeDelta\\n</code>', '<code>date_1 = DateTime.today() \\nend_date = date_1 + TimeDelta(days=10)\\n</code>', '<code>import datetime</code>', \"<code>from datetime import datetime\\nfrom datetime import timedelta\\n\\n\\ndate_now_more_5_days = (datetime.now() + timedelta(days=5) ).strftime('%Y-%m-%d')\\n</code>\"]",
         "title": "Adding 5 days to a date in Python",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 84,
               "answer_id": 6871054,
               "is_accepted": false,
               "last_activity_date": 1311931223,
               "body_markdown": "Import `timedelta` first.\r\n\r\n    from datetime import timedelta\r\n\r\n\r\nAnd `Date.today()` will return today&#39;s datetime, may be you want \r\n\r\n    EndDate = Date + timedelta(days=10)",
               "id": "6871054",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1311931223,
               "score": 84
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 6871056,
               "is_accepted": false,
               "last_activity_date": 1311931248,
               "body_markdown": "I guess you are missing something like that:\r\n\r\n&lt;!-- language: lang-python --&gt;\r\n\r\n    from datetime import timedelta",
               "id": "6871056",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1311931248,
               "score": 12
            },
            {
               "up_vote_count": 327,
               "answer_id": 6871482,
               "last_activity_date": 1443174201,
               "path": "3.stack.answer",
               "body_markdown": "The previous answers are correct but it&#39;s generally a better practice to do:\r\n\r\n    import datetime\r\n\r\nThen you&#39;ll have, using [`datetime.timedelta`](https://docs.python.org/2/library/datetime.html#datetime.timedelta):\r\n\r\n    date_1 = datetime.datetime.strptime(start_date, &quot;%m/%d/%y&quot;)\r\n    \r\n    end_date = date_1 + datetime.timedelta(days=10)",
               "tags": [],
               "creation_date": 1311933826,
               "last_edit_date": 1443174201,
               "is_accepted": true,
               "id": "6871482",
               "down_vote_count": 3,
               "score": 324
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 25570060,
               "is_accepted": false,
               "last_activity_date": 1409321419,
               "body_markdown": "If you happen to already be using pandas, you can save a little space by not specifying the format:\r\n\r\n    import pandas as pd\r\n    startdate = &quot;10/10/2011&quot;\r\n    enddate = pd.to_datetime(startdate) + pd.DateOffset(days=5)",
               "id": "25570060",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1409321419,
               "score": 8
            },
            {
               "up_vote_count": 7,
               "answer_id": 26925467,
               "last_activity_date": 1415953065,
               "path": "3.stack.answer",
               "body_markdown": "Here is a function of getting from now + specified days\r\n\r\n    import datetime\r\n    \r\n    def get_date(dateFormat=&quot;%d-%m-%Y&quot;, addDays=0):\r\n    \r\n        timeNow = datetime.datetime.now()\r\n        if (addDays!=0):\r\n            anotherTime = timeNow + datetime.timedelta(days=addDays)\r\n        else:\r\n            anotherTime = timeNow\r\n    \r\n        return anotherTime.strftime(dateFormat)\r\n\r\nUsage:\r\n\r\n    addDays = 3 #days\r\n    output_format = &#39;%d-%m-%Y&#39;\r\n    output = get_date(output_format, addDays)\r\n    print output",
               "tags": [],
               "creation_date": 1415952116,
               "last_edit_date": 1415953065,
               "is_accepted": false,
               "id": "26925467",
               "down_vote_count": 0,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 31051051,
               "is_accepted": false,
               "last_activity_date": 1435237013,
               "body_markdown": "\r\nHere is another method to add days on date using **dateutil&#39;s relativedelta**.\r\n\r\n\r\n    from datetime import datetime\r\n    from dateutil.relativedelta import relativedelta\r\n    \r\n    print &#39;Today: &#39;,datetime.now().strftime(&#39;%d/%m/%Y %H:%M:%S&#39;) \r\n    date_after_month = datetime.now()+ relativedelta(days=5)\r\n    print &#39;After 5 Days:&#39;, date_after_month.strftime(&#39;%d/%m/%Y %H:%M:%S&#39;)\r\n\r\nOutput:\r\n\r\n&gt; Today:  25/06/2015 15:56:09\r\n\r\n&gt; After 5 Days: 30/06/2015 15:56:09\r\n\r\n",
               "id": "31051051",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1435237013,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 46445823,
               "is_accepted": false,
               "last_activity_date": 1506508951,
               "body_markdown": "In order to have have a **less verbose code**, and avoid **name conflicts** *between datetime and datetime.datetime*, you should rename the classes with **CamelCase** names.\r\n\r\n    from datetime import datetime as DateTime, timedelta as TimeDelta\r\n\r\nSo you can do the following, which I think it is clear.\r\n\r\n    date_1 = DateTime.today() \r\n    end_date = date_1 + TimeDelta(days=10)\r\n\r\nAlso, there would be **no name conflict** if you want to `import datetime` later on.\r\n",
               "id": "46445823",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1506508951,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 48893159,
               "is_accepted": false,
               "last_activity_date": 1519156293,
               "body_markdown": "If you want add days to date now, you can use this code\r\n\r\n    from datetime import datetime\r\n    from datetime import timedelta\r\n    \r\n    \r\n    date_now_more_5_days = (datetime.now() + timedelta(days=5) ).strftime(&#39;%Y-%m-%d&#39;)",
               "id": "48893159",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519156293,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/6871016/adding-5-days-to-a-date-in-python",
         "id": "858127-2289"
      },
      {
         "up_vote_count": "183",
         "path": "2.stack",
         "body_markdown": "In python pandas, what&#39;s the best way to check whether a DataFrame has one (or more) NaN values?\r\n\r\nI know about the function `pd.isnan`, but this returns a DataFrame of booleans for each element. [This post][1] right here doesn&#39;t exactly answer my question either.\r\n\r\n  [1]: https://stackoverflow.com/questions/27754891/python-nan-value-in-pandas",
         "view_count": "223544",
         "answer_count": "10",
         "tags": "['python', 'pandas', 'nan']",
         "creation_date": "1428556179",
         "last_edit_date": "1495542866",
         "code_snippet": "['<code>pd.isnan</code>', '<code>df.isnull().values.any()\\n</code>', '<code>In [2]: df = pd.DataFrame(np.random.randn(1000,1000))\\n\\nIn [3]: df[df &gt; 0.9] = pd.np.nan\\n\\nIn [4]: %timeit df.isnull().any().any()\\n100 loops, best of 3: 14.7 ms per loop\\n\\nIn [5]: %timeit df.isnull().values.sum()\\n100 loops, best of 3: 2.15 ms per loop\\n\\nIn [6]: %timeit df.isnull().sum().sum()\\n100 loops, best of 3: 18 ms per loop\\n\\nIn [7]: %timeit df.isnull().values.any()\\n1000 loops, best of 3: 948 \u00b5s per loop\\n</code>', '<code>df.isnull().sum().sum()</code>', '<code>NaNs</code>', '<code>pandas</code>', '<code>df.describe()</code>', '<code>df.describe()</code>', '<code>NaN</code>', '<code>df.isnull().values.sum()</code>', '<code>df.isnull().values.flatten().sum()</code>', '<code>df.isnull().values.any()</code>', '<code>import pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame(np.random.randn(10,6))\\n# Make a few areas have NaN values\\ndf.iloc[1:3,1] = np.nan\\ndf.iloc[5,3] = np.nan\\ndf.iloc[7:9,5] = np.nan\\n</code>', '<code>          0         1         2         3         4         5\\n0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281\\n1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952\\n2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425\\n3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797\\n4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722\\n5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814\\n6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368\\n7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN\\n8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN\\n9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810\\n</code>', '<code>df.isnull().any().any()</code>', '<code>isnull()</code>', '<code>       0      1      2      3      4      5\\n0  False  False  False  False  False  False\\n1  False   True  False  False  False  False\\n2  False   True  False  False  False  False\\n3  False  False  False  False  False  False\\n4  False  False  False  False  False  False\\n5  False  False  False   True  False  False\\n6  False  False  False  False  False  False\\n7  False  False  False  False  False   True\\n8  False  False  False  False  False   True\\n9  False  False  False  False  False  False\\n</code>', '<code>df.isnull().any()</code>', '<code>NaN</code>', '<code>0    False\\n1     True\\n2    False\\n3     True\\n4    False\\n5     True\\ndtype: bool\\n</code>', '<code>.any()</code>', '<code>True</code>', '<code>&gt; df.isnull().any().any()\\nTrue\\n</code>', '<code>df.isnull().sum().sum()</code>', '<code>NaN</code>', '<code>.any().any()</code>', '<code>NaN</code>', '<code>df.isnull().sum()\\n0    0\\n1    2\\n2    0\\n3    1\\n4    0\\n5    2\\ndtype: int64\\n</code>', '<code>df.isnull().sum().sum()\\n5\\n</code>', '<code>df.isnull().T.any().T.sum()\\n</code>', '<code>nan_rows = df[df.isnull().T.any().T]\\n</code>', '<code>df.isnull().any().any()</code>', \"<code>nan_rows = df[df['name column'].isnull()]\\n</code>\", \"<code>non_nan_rows = df[df['name column'].notnull()]</code>\", '<code>hasnans</code>', '<code>df[i].hasnans</code>', '<code>True</code>', '<code>False</code>', \"<code>df = DataFrame([1,None], columns=['foo'])</code>\", '<code>df.hasnans</code>', '<code>AttributeError</code>', '<code>df.foo.hasnans</code>', '<code>True</code>', '<code>nan_rows = df[df.isnull().any(1)]\\n</code>', '<code>pandas</code>', '<code>DataFrame.dropna()</code>', '<code>DataFrame.count()</code>', '<code>DataFrame</code>', '<code>math.isnan(x)</code>', '<code>x</code>', '<code>for col in df:\\n   print df[col].value_counts(dropna=False)\\n</code>']",
         "title": "Python pandas: check if any value is NaN in DataFrame",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 13,
               "answer_id": 29530303,
               "is_accepted": false,
               "last_activity_date": 1428556616,
               "body_markdown": "`df.isnull().any().any()` should do it.",
               "id": "29530303",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1428556616,
               "score": 13
            },
            {
               "up_vote_count": 74,
               "answer_id": 29530559,
               "last_activity_date": 1517228782,
               "path": "3.stack.answer",
               "body_markdown": "You have a couple of options. \r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    \r\n    df = pd.DataFrame(np.random.randn(10,6))\r\n    # Make a few areas have NaN values\r\n    df.iloc[1:3,1] = np.nan\r\n    df.iloc[5,3] = np.nan\r\n    df.iloc[7:9,5] = np.nan\r\n\r\nNow the data frame looks something like this:\r\n\r\n              0         1         2         3         4         5\r\n    0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281\r\n    1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952\r\n    2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425\r\n    3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797\r\n    4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722\r\n    5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814\r\n    6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368\r\n    7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN\r\n    8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN\r\n    9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810\r\n\r\n* **Option 1**: `df.isnull().any().any()` - This returns a boolean value\r\n\r\nYou know of the `isnull()` which would return a dataframe like this:\r\n\r\n           0      1      2      3      4      5\r\n    0  False  False  False  False  False  False\r\n    1  False   True  False  False  False  False\r\n    2  False   True  False  False  False  False\r\n    3  False  False  False  False  False  False\r\n    4  False  False  False  False  False  False\r\n    5  False  False  False   True  False  False\r\n    6  False  False  False  False  False  False\r\n    7  False  False  False  False  False   True\r\n    8  False  False  False  False  False   True\r\n    9  False  False  False  False  False  False\r\n\r\nIf you make it `df.isnull().any()`, you can find just the columns that have `NaN` values:\r\n\r\n    0    False\r\n    1     True\r\n    2    False\r\n    3     True\r\n    4    False\r\n    5     True\r\n    dtype: bool\r\n\r\nOne more `.any()` will tell you if any of the above are `True`\r\n\r\n    &gt; df.isnull().any().any()\r\n    True\r\n\r\n* **Option 2**: `df.isnull().sum().sum()` - This returns an integer of the total number of `NaN` values:\r\n\r\nThis operates the same way as the `.any().any()` does, by first giving a summation of the number of `NaN` values in a column, then the summation of those values:\r\n\r\n    df.isnull().sum()\r\n    0    0\r\n    1    2\r\n    2    0\r\n    3    1\r\n    4    0\r\n    5    2\r\n    dtype: int64\r\n\r\nFinally, to get the total number of NaN values in the DataFrame:\r\n\r\n    df.isnull().sum().sum()\r\n    5\r\n\r\n",
               "tags": [],
               "creation_date": 1428557846,
               "last_edit_date": 1517228782,
               "is_accepted": false,
               "id": "29530559",
               "down_vote_count": 0,
               "score": 74
            },
            {
               "up_vote_count": 205,
               "answer_id": 29530601,
               "last_activity_date": 1441042509,
               "path": "3.stack.answer",
               "body_markdown": "[jwilner](https://stackoverflow.com/users/1567452/jwilner)&#39;s response is spot on. I was exploring to see if there&#39;s a faster option, since in my experience, summing flat arrays is (strangely) faster than counting. This code seems faster:\r\n\r\n    df.isnull().values.any()\r\n\r\nFor example:\r\n\r\n    In [2]: df = pd.DataFrame(np.random.randn(1000,1000))\r\n\r\n    In [3]: df[df &gt; 0.9] = pd.np.nan\r\n\r\n    In [4]: %timeit df.isnull().any().any()\r\n    100 loops, best of 3: 14.7 ms per loop\r\n\r\n    In [5]: %timeit df.isnull().values.sum()\r\n    100 loops, best of 3: 2.15 ms per loop\r\n\r\n    In [6]: %timeit df.isnull().sum().sum()\r\n    100 loops, best of 3: 18 ms per loop\r\n\r\n    In [7]: %timeit df.isnull().values.any()\r\n    1000 loops, best of 3: 948 &#181;s per loop\r\n\r\n`df.isnull().sum().sum()` is a bit slower, but of course, has additional information -- the number of `NaNs`.",
               "tags": [],
               "creation_date": 1428557994,
               "last_edit_date": 1495542866,
               "is_accepted": true,
               "id": "29530601",
               "down_vote_count": 1,
               "score": 204
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 36192391,
               "is_accepted": false,
               "last_activity_date": 1458787480,
               "body_markdown": "Depending on the type of data you&#39;re dealing with, you could also just get the value counts of each column while performing your EDA by setting dropna to False. \r\n\r\n    for col in df:\r\n       print df[col].value_counts(dropna=False)\r\n\r\nWorks well for categorical variables, not so much when you have many unique values.",
               "id": "36192391",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1458787480,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 18,
               "answer_id": 37442692,
               "is_accepted": false,
               "last_activity_date": 1464193065,
               "body_markdown": "If you need to know how many &quot;1 or more&quot; rows have NaNs:\r\n\r\n    df.isnull().T.any().T.sum()\r\n\r\nOr if you need to pull out these rows and examine them:\r\n\r\n    nan_rows = df[df.isnull().T.any().T]\r\n\r\n",
               "id": "37442692",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1464193065,
               "score": 18
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 37850278,
               "is_accepted": false,
               "last_activity_date": 1466053578,
               "body_markdown": "Since `pandas` has to find this out for `DataFrame.dropna()`, I took a look to see how they implement it and discovered that they made use of `DataFrame.count()`, which counts all non-null values in the `DataFrame`. Cf. [pandas source code](https://github.com/pydata/pandas/blob/master/pandas/core/frame.py#L3013). I haven&#39;t benchmarked this technique, but I figure the authors of the library are likely to have made a wise choice for how to do it.",
               "id": "37850278",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1466053578,
               "score": 4
            },
            {
               "up_vote_count": 9,
               "answer_id": 43807296,
               "last_activity_date": 1508329700,
               "path": "3.stack.answer",
               "body_markdown": "Since none have mentioned, there is just another variable called `hasnans`. \r\n\r\n`df[i].hasnans` will output to `True` if one or more of the values in the pandas Series is NaN, `False` if not. Note that its not a function.\r\n\r\npandas version &#39;0.19.2&#39; and &#39;0.20.2&#39;",
               "tags": [],
               "creation_date": 1493993844,
               "last_edit_date": 1508329700,
               "is_accepted": false,
               "id": "43807296",
               "down_vote_count": 0,
               "score": 9
            },
            {
               "up_vote_count": 5,
               "answer_id": 45829208,
               "last_activity_date": 1503452900,
               "path": "3.stack.answer",
               "body_markdown": "Adding to Hobs brilliant answer, I am very new to Python and Pandas so please point out if I am wrong.\r\n\r\nTo find out which rows have NaNs:\r\n\r\n\r\n    nan_rows = df[df.isnull().any(1)]\r\n\r\n\r\nwould perform the same operation without the need for transposing by specifying the axis of any() as 1 to check if &#39;True&#39; is present in rows. \r\n\r\n",
               "tags": [],
               "creation_date": 1503451352,
               "last_edit_date": 1503452900,
               "is_accepted": false,
               "id": "45829208",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 47066932,
               "is_accepted": false,
               "last_activity_date": 1509591962,
               "body_markdown": "Just using\r\n[math.isnan(x)](https://docs.python.org/3/library/math.html#math.isnan), Return True if x is a NaN (not a number), and False otherwise.",
               "id": "47066932",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1509591962,
               "score": 3
            },
            {
               "up_vote_count": 10,
               "answer_id": 47377251,
               "last_activity_date": 1511097797,
               "path": "3.stack.answer",
               "body_markdown": "To find out which rows have NaNs in a specific column:\r\n\r\n    nan_rows = df[df[&#39;name column&#39;].isnull()]",
               "tags": [],
               "creation_date": 1511097183,
               "last_edit_date": 1511097797,
               "is_accepted": false,
               "id": "47377251",
               "down_vote_count": 0,
               "score": 10
            }
         ],
         "link": "https://stackoverflow.com/questions/29530232/python-pandas-check-if-any-value-is-nan-in-dataframe",
         "id": "858127-2290"
      },
      {
         "up_vote_count": "311",
         "path": "2.stack",
         "body_markdown": "Python has an [ordered dictionary][1]. What about an ordered set?\r\n\r\n  [1]: http://www.python.org/dev/peps/pep-0372/",
         "view_count": "162146",
         "answer_count": "13",
         "tags": "['python', 'set']",
         "creation_date": "1256983927",
         "last_edit_date": "1489071686",
         "code_snippet": "['<code>collections.Counter</code>', '<code>OrderedSet([1, 2, 3])\\n</code>', '<code>.union</code>', '<code>__or__</code>', '<code>@staticmethod\\ndef union(*sets):\\n    union = OrderedSet()\\n    union.union(*sets)\\n    return union\\n\\ndef union(self, *sets):\\n    for set in sets:\\n        self |= set\\n</code>', '<code>update</code>', '<code>union</code>', '<code>intersection</code>', '<code>union</code>', '<code>OrderedSet.union</code>', '<code>None</code>', '<code>collections.OrderedDict</code>', '<code>collections.OrderedDict</code>', '<code>collections.MutableSet</code>', '<code>import collections\\n\\nclass OrderedSet(collections.OrderedDict, collections.MutableSet):\\n\\n    def update(self, *args, **kwargs):\\n        if kwargs:\\n            raise TypeError(\"update() takes no keyword arguments\")\\n\\n        for s in args:\\n            for e in s:\\n                 self.add(e)\\n\\n    def add(self, elem):\\n        self[elem] = None\\n\\n    def discard(self, elem):\\n        self.pop(elem, None)\\n\\n    def __le__(self, other):\\n        return all(e in other for e in self)\\n\\n    def __lt__(self, other):\\n        return self &lt;= other and self != other\\n\\n    def __ge__(self, other):\\n        return all(e in self for e in other)\\n\\n    def __gt__(self, other):\\n        return self &gt;= other and self != other\\n\\n    def __repr__(self):\\n        return \\'OrderedSet([%s])\\' % (\\', \\'.join(map(repr, self.keys())))\\n\\n    def __str__(self):\\n        return \\'{%s}\\' % (\\', \\'.join(map(repr, self.keys())))\\n\\n    difference = property(lambda self: self.__sub__)\\n    difference_update = property(lambda self: self.__isub__)\\n    intersection = property(lambda self: self.__and__)\\n    intersection_update = property(lambda self: self.__iand__)\\n    issubset = property(lambda self: self.__le__)\\n    issuperset = property(lambda self: self.__ge__)\\n    symmetric_difference = property(lambda self: self.__xor__)\\n    symmetric_difference_update = property(lambda self: self.__ixor__)\\n    union = property(lambda self: self.__or__)\\n</code>', '<code>OrderedSet</code>', '<code>OrderedDict</code>', '<code>abc.Set</code>', '<code>__len__</code>', '<code>__iter__</code>', '<code>__contains__</code>', '<code>collections</code>', '<code>my_set[5]</code>', '<code>remove(item)</code>', '<code>remove(item)</code>', '<code>add(item)</code>', '<code>__contains__(item)</code>', '<code>item in my_set</code>', '<code>set1.union(set2)</code>', '<code>set1 | set2</code>', '<code>remove(item)</code>', '<code>NotImplementedError</code>', '<code>set.union</code>', '<code>collections.abc.Set</code>', '<code>OrderedSet</code>', '<code>remove</code>', '<code>IndexedSet</code>', '<code>pip install boltons</code>', '<code>setutils.py</code>', '<code>IndexedSet</code>', \"<code>&gt;&gt;&gt; x = IndexedSet(list(range(4)) + list(range(8)))\\n&gt;&gt;&gt; x\\nIndexedSet([0, 1, 2, 3, 4, 5, 6, 7])\\n&gt;&gt;&gt; x - set(range(2))\\nIndexedSet([2, 3, 4, 5, 6, 7])\\n&gt;&gt;&gt; x[-1]\\n7\\n&gt;&gt;&gt; fcr = IndexedSet('freecreditreport.com')\\n&gt;&gt;&gt; ''.join(fcr[:fcr.index('.')])\\n'frecditpo'\\n</code>\", '<code>IndexedSet</code>', '<code>pip install sortedcontainers\\n</code>', '<code>pip install</code>', '<code>from sortedcontainers import SortedSet\\nhelp(SortedSet)\\n</code>', '<code>SortedSet</code>', '<code>set</code>', '<code>frozenset</code>', '<code>SortedSet</code>', '<code>setlist</code>', '<code>collections-extended</code>', '<code>Sequence</code>', '<code>Set</code>', \"<code>&gt;&gt;&gt; from collections_extended import setlist\\n&gt;&gt;&gt; sl = setlist('abracadabra')\\n&gt;&gt;&gt; sl\\nsetlist(('a', 'b', 'r', 'c', 'd'))\\n&gt;&gt;&gt; sl[3]\\n'c'\\n&gt;&gt;&gt; sl[-1]\\n'd'\\n&gt;&gt;&gt; 'r' in sl  # testing for inclusion is fast\\nTrue\\n&gt;&gt;&gt; sl.index('d')  # so is finding the index of an element\\n4\\n&gt;&gt;&gt; sl.insert(1, 'd')  # inserting an element already in raises a ValueError\\nValueError\\n&gt;&gt;&gt; sl.index('d')\\n4\\n</code>\", '<code>Index</code>', '<code>&gt;&gt;&gt; s = set([0, 1, 2, 99, 4, 40, 3, 20, 24, 100, 60])\\n&gt;&gt;&gt; sorted(s)\\n[0, 1, 2, 3, 4, 20, 24, 40, 60, 99, 100]\\n</code>', '<code>sorted()</code>', '<code>OrderedSet</code>', \"<code>DataStructure = {'Collections': {'Map': [('dict', 'OrderDict', 'defaultdict'),\\n                                         ('chainmap', 'types.MappingProxyType')],\\n                                 'Set': [('set', 'frozenset'), {'multiset':'collection.Counter'}]},\\n                                 'Sequence': {'Basic': ['list', 'tuple', 'iterator']},\\n                                              'Algorithm': {'Priority': ['heapq',\\n                                                                         'queue.PriorityQueue'],\\n                                                            'Queue': ['queue.Queue',\\n                                                                      'multiprocessing.Queue'],\\n                                                            'Stack': ['collection.deque',\\n                                                                      'queue.LifeQueue']},\\n                 'text_sequence': ['str', 'byte', 'bytearray']}\\n</code>\", '<code>reduce()</code>', '<code>&gt;&gt;&gt; mylist = [4, 1, 2, 1, 3, 2, 4, 1, 3, 2, 3, 1, 3, 2, 4]\\n&gt;&gt;&gt; reduce(lambda a, b: b[0] in a and a or a + b, [[i] for i in mylist])\\n[4, 1, 2, 3]\\n</code>', '<code>list(set(mylist))</code>', '<code>mylist</code>', '<code>list(set(mylist))</code>', '<code>set(mylist)</code>', \"<code>&gt;&gt;&gt; a = {3, 4, 2, 6, 1, 7}\\n&gt;&gt;&gt; type(a)\\n&lt;class 'set'&gt;\\n&gt;&gt;&gt; sorted(a, reverse=True)\\n[7, 6, 4, 3, 2, 1]\\n&gt;&gt;&gt; sorted(a)\\n[1, 2, 3, 4, 6, 7]\\n</code>\", '<code>OrderedSet</code>']",
         "title": "Does Python have an ordered set?",
         "_childDocuments_": [
            {
               "up_vote_count": 173,
               "answer_id": 1653974,
               "last_activity_date": 1507656582,
               "path": "3.stack.answer",
               "body_markdown": "There is an [ordered set][1] (possible [new link](https://github.com/ActiveState/code/blob/3b27230f418b714bc9a0f897cb8ea189c3515e99/recipes/Python/576696_OrderedSet_with_Weakrefs/README.md)) recipe for this which is referred to from the [Python 2 Documentation][2]. This runs on Py2.6 or later and 3.0 or later without any modifications. The interface is almost exactly the same as a normal set, except that initialisation should be done with a list.\r\n\r\n    OrderedSet([1, 2, 3])\r\n\r\nThis is a MutableSet, so the signature for `.union` doesn&#39;t match that of set, but since it includes `__or__` something similar can easily be added:\r\n\r\n    @staticmethod\r\n    def union(*sets):\r\n        union = OrderedSet()\r\n        union.union(*sets)\r\n        return union\r\n\r\n    def union(self, *sets):\r\n        for set in sets:\r\n            self |= set\r\n\r\n\r\n  [1]: http://code.activestate.com/recipes/576694/\r\n  [2]: https://docs.python.org/2/library/collections.html\r\n",
               "tags": [],
               "creation_date": 1256984106,
               "last_edit_date": 1507656582,
               "is_accepted": true,
               "id": "1653974",
               "down_vote_count": 7,
               "score": 166
            },
            {
               "up_vote_count": 106,
               "answer_id": 1653978,
               "last_activity_date": 1257091232,
               "path": "3.stack.answer",
               "body_markdown": "An ordered set is functionally a special case of an ordered dictionary.\r\n---\r\n\r\nThe keys of a dictionary are unique. Thus, if one disregards the values in an ordered dictionary (e.g. by assigning them `None`), then one has essentially an ordered set.\r\n\r\n[As of Python 3.1][1] there is [`collections.OrderedDict`][2]. The following is an example implementation of an OrderedSet. (Note that only few methods need to be defined or overridden: `collections.OrderedDict` and [`collections.MutableSet`][3] do the heavy lifting.)\r\n\r\n    import collections\r\n    \r\n    class OrderedSet(collections.OrderedDict, collections.MutableSet):\r\n    \r\n        def update(self, *args, **kwargs):\r\n            if kwargs:\r\n                raise TypeError(&quot;update() takes no keyword arguments&quot;)\r\n    \r\n            for s in args:\r\n                for e in s:\r\n                     self.add(e)\r\n    \r\n        def add(self, elem):\r\n            self[elem] = None\r\n    \r\n        def discard(self, elem):\r\n            self.pop(elem, None)\r\n    \r\n        def __le__(self, other):\r\n            return all(e in other for e in self)\r\n    \r\n        def __lt__(self, other):\r\n            return self &lt;= other and self != other\r\n    \r\n        def __ge__(self, other):\r\n            return all(e in self for e in other)\r\n    \r\n        def __gt__(self, other):\r\n            return self &gt;= other and self != other\r\n    \r\n        def __repr__(self):\r\n            return &#39;OrderedSet([%s])&#39; % (&#39;, &#39;.join(map(repr, self.keys())))\r\n    \r\n        def __str__(self):\r\n            return &#39;{%s}&#39; % (&#39;, &#39;.join(map(repr, self.keys())))\r\n    \r\n        difference = property(lambda self: self.__sub__)\r\n        difference_update = property(lambda self: self.__isub__)\r\n        intersection = property(lambda self: self.__and__)\r\n        intersection_update = property(lambda self: self.__iand__)\r\n        issubset = property(lambda self: self.__le__)\r\n        issuperset = property(lambda self: self.__ge__)\r\n        symmetric_difference = property(lambda self: self.__xor__)\r\n        symmetric_difference_update = property(lambda self: self.__ixor__)\r\n        union = property(lambda self: self.__or__)\r\n\r\n\r\n  [1]: http://docs.python.org/3.1/whatsnew/3.1.html\r\n  [2]: http://docs.python.org/dev/py3k/library/collections.html#collections.OrderedDict\r\n  [3]: http://docs.python.org/3.1/library/collections.html#abcs-abstract-base-classes",
               "tags": [],
               "creation_date": 1256984259,
               "last_edit_date": 1257091232,
               "is_accepted": false,
               "id": "1653978",
               "down_vote_count": 4,
               "score": 102
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 10178672,
               "is_accepted": false,
               "last_activity_date": 1334596847,
               "body_markdown": "\r\nThere are four kinds of ordering one might want, I believe:\r\n\r\n 1. Ordered by key\r\n 2. Ordered by value (I&#39;ve not heard of anyone ask for this one though)\r\n 3. Ordered by modification time\r\n 4. Ordered by addition time\r\n\r\nI believe collections.OrderedDict gets you #4.  Or you could remove a key and re-add it, for #3.\r\n\r\nFor #1, you probably should check into a red-black tree or treap:\r\n\r\n - http://pypi.python.org/pypi/bintrees/0.3.0\r\n - http://pypi.python.org/pypi/rbtree/\r\n - http://stromberg.dnsalias.org/~dstromberg/treap/\r\n\r\nRed-Black trees have low variability in operation times (so might be better for interactive applications), but aren&#39;t as fast as treaps on average (which might be better for batch processing - treaps don&#39;t reorganize themselves often making them fast on average, but when they do reorganize it might take a relatively long while).\r\n\r\nBoth of these are established data structures with implementations in many languages.\r\n\r\n",
               "id": "10178672",
               "tags": [],
               "down_vote_count": 7,
               "creation_date": 1334596847,
               "score": -4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 14991471,
               "is_accepted": false,
               "last_activity_date": 1361400764,
               "body_markdown": "For many purposes simply calling sorted will suffice.  For example\r\n\r\n    &gt;&gt;&gt; s = set([0, 1, 2, 99, 4, 40, 3, 20, 24, 100, 60])\r\n    &gt;&gt;&gt; sorted(s)\r\n    [0, 1, 2, 3, 4, 20, 24, 40, 60, 99, 100]\r\n\r\nIf you are going to use this repeatedly, there will be overhead incurred by calling the sorted function so you might want to save the resulting list, as long as you&#39;re done changing the set.  If you need to maintain unique elements and sorted, I agree with the suggestion of using OrderedDict from collections with an arbitrary value such as None.\r\n\r\n",
               "id": "14991471",
               "tags": [],
               "down_vote_count": 16,
               "creation_date": 1361400764,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 22766863,
               "is_accepted": false,
               "last_activity_date": 1396282829,
               "body_markdown": "    &gt;&gt;&gt; a = {3, 4, 2, 6, 1, 7}\r\n    &gt;&gt;&gt; type(a)\r\n    &lt;class &#39;set&#39;&gt;\r\n    &gt;&gt;&gt; sorted(a, reverse=True)\r\n    [7, 6, 4, 3, 2, 1]\r\n    &gt;&gt;&gt; sorted(a)\r\n    [1, 2, 3, 4, 6, 7]",
               "id": "22766863",
               "tags": [],
               "down_vote_count": 8,
               "creation_date": 1396282829,
               "score": -8
            },
            {
               "up_vote_count": 29,
               "answer_id": 23225031,
               "last_activity_date": 1442691871,
               "path": "3.stack.answer",
               "body_markdown": "Implementations on PyPI\r\n=======================\r\n\r\nWhile others have pointed out that there is no built-in implementation of an insertion-order preserving set in Python (yet), I am feeling that this question is missing an answer which states what there is to be found on [PyPI][1].\r\n\r\nTo the best of my knowledge there currently is:\r\n\r\n - [ordered-set][2]\r\n - [oset][3]\r\n\r\nBoth implementations are based on the [recipe posted by Raymond Hettinger to ActiveState][4] which is also mentioned in other answers here. I have checked out both and identified the following \r\n\r\ncritical differences:\r\n---------------------\r\n - ordered-set (version 1.1)\r\n  - advantage: O(1) for lookups by index (e.g. `my_set[5]`)\r\n  - disadvantage: `remove(item)` not implemented \r\n - oset (version 0.1.3)\r\n  - advantage: O(1) for `remove(item)`\r\n  - disadvantage: apparently O(n) for lookups by index\r\n\r\nBoth implementations have O(1) for `add(item)` and `__contains__(item)` (`item in my_set`).\r\n\r\nUnfortunately neither implementation has method-based set operations like `set1.union(set2)` -&gt; You have to use the operator-based form like `set1 | set2` instead. See the [Python documentation on Set Objects][5] for a full list of set operation methods and their operator-based equivalents.\r\n\r\nI first went with ordered-set until I used `remove(item)` for the first time which crashed my script with a `NotImplementedError`. As I have never used lookup by index so far, I meanwhile switched to oset. \r\n\r\n*If you know about other implementations on PyPI, let me know in the comments.*\r\n\r\n\r\n  [1]: https://pypi.python.org\r\n  [2]: https://pypi.python.org/pypi/ordered-set/\r\n  [3]: https://pypi.python.org/pypi/oset/\r\n  [4]: http://code.activestate.com/recipes/576694/\r\n  [5]: https://docs.python.org/2/library/sets.html#set-objects",
               "tags": [],
               "creation_date": 1398183773,
               "last_edit_date": 1442691871,
               "is_accepted": false,
               "id": "23225031",
               "down_vote_count": 0,
               "score": 29
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 25988702,
               "is_accepted": false,
               "last_activity_date": 1411455146,
               "body_markdown": "If you&#39;re using the ordered set to maintain a sorted order, consider using a sorted set implementation from PyPI. The [sortedcontainers][1] module provides a [SortedSet][2] for just this purpose. Some benefits: pure-Python, fast-as-C implementations, 100% unit test coverage, hours of stress testing.\r\n\r\nInstalling from PyPI is easy with pip:\r\n\r\n    pip install sortedcontainers\r\n\r\nNote that if you can&#39;t `pip install`, simply pull down the sortedlist.py and sortedset.py files from the [open-source repository][3].\r\n\r\nOnce installed you can simply:\r\n\r\n    from sortedcontainers import SortedSet\r\n    help(SortedSet)\r\n\r\nThe sortedcontainers module also maintains a [performance comparison][4] with several alternative implementations.\r\n\r\nFor the comment that asked about Python&#39;s bag data type, there&#39;s alternatively a [SortedList][5] data type which can be used to efficiently implement a bag.\r\n\r\n\r\n  [1]: http://www.grantjenks.com/docs/sortedcontainers/\r\n  [2]: http://www.grantjenks.com/docs/sortedcontainers/sortedset.html\r\n  [3]: https://github.com/grantjenks/sorted_containers\r\n  [4]: http://www.grantjenks.com/docs/sortedcontainers/performance.html\r\n  [5]: http://www.grantjenks.com/docs/sortedcontainers/sortedlist.html",
               "id": "25988702",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1411455146,
               "score": 16
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 26977777,
               "is_accepted": false,
               "last_activity_date": 1416243372,
               "body_markdown": "You can use `reduce()` to get a list of unique values in one line:\r\n        \r\n    &gt;&gt;&gt; mylist = [4, 1, 2, 1, 3, 2, 4, 1, 3, 2, 3, 1, 3, 2, 4]\r\n    &gt;&gt;&gt; reduce(lambda a, b: b[0] in a and a or a + b, [[i] for i in mylist])\r\n    [4, 1, 2, 3]\r\n",
               "id": "26977777",
               "tags": [],
               "down_vote_count": 6,
               "creation_date": 1416243372,
               "score": -6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 28052907,
               "is_accepted": false,
               "last_activity_date": 1421779571,
               "body_markdown": "A little late to the game, but I&#39;ve written a class `setlist` as part of `collections-extended` that fully implements both `Sequence` and `Set`\r\n\r\n    &gt;&gt;&gt; from collections_extended import setlist\r\n    &gt;&gt;&gt; sl = setlist(&#39;abracadabra&#39;)\r\n    &gt;&gt;&gt; sl\r\n    setlist((&#39;a&#39;, &#39;b&#39;, &#39;r&#39;, &#39;c&#39;, &#39;d&#39;))\r\n    &gt;&gt;&gt; sl[3]\r\n    &#39;c&#39;\r\n    &gt;&gt;&gt; sl[-1]\r\n    &#39;d&#39;\r\n    &gt;&gt;&gt; &#39;r&#39; in sl  # testing for inclusion is fast\r\n    True\r\n    &gt;&gt;&gt; sl.index(&#39;d&#39;)  # so is finding the index of an element\r\n    4\r\n    &gt;&gt;&gt; sl.insert(1, &#39;d&#39;)  # inserting an element already in raises a ValueError\r\n    ValueError\r\n    &gt;&gt;&gt; sl.index(&#39;d&#39;)\r\n    4\r\n\r\nGitHub: https://github.com/mlenzen/collections-extended\r\n\r\nDocumentation: http://collections-extended.lenzm.net/en/latest/\r\n\r\nPyPI: https://pypi.python.org/pypi/collections-extended\r\n\r\n",
               "id": "28052907",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1421779571,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 32784166,
               "is_accepted": false,
               "last_activity_date": 1443190388,
               "body_markdown": "In case you&#39;re already using pandas in your code, its `Index` object behaves pretty like an ordered set, as shown in [this article][1].\r\n\r\n\r\n  [1]: https://www.oreilly.com/learning/introducing-pandas-objects",
               "id": "32784166",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1443190388,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 19,
               "answer_id": 35258869,
               "is_accepted": false,
               "last_activity_date": 1454877705,
               "body_markdown": "I can do you one better than an OrderedSet: boltons has [a pure-Python, 2/3-compatible `IndexedSet` type](http://boltons.readthedocs.org/en/latest/setutils.html) that is not only an ordered set, but also supports indexing (as with lists).\r\n\r\nSimply `pip install boltons` (or copy `setutils.py` into your codebase), import the `IndexedSet` and:\r\n\r\n&lt;!-- lang: python --&gt;\r\n\r\n    &gt;&gt;&gt; x = IndexedSet(list(range(4)) + list(range(8)))\r\n    &gt;&gt;&gt; x\r\n    IndexedSet([0, 1, 2, 3, 4, 5, 6, 7])\r\n    &gt;&gt;&gt; x - set(range(2))\r\n    IndexedSet([2, 3, 4, 5, 6, 7])\r\n    &gt;&gt;&gt; x[-1]\r\n    7\r\n    &gt;&gt;&gt; fcr = IndexedSet(&#39;freecreditreport.com&#39;)\r\n    &gt;&gt;&gt; &#39;&#39;.join(fcr[:fcr.index(&#39;.&#39;)])\r\n    &#39;frecditpo&#39;\r\n\r\nEverything is unique and retained in order. Full disclosure: I wrote the `IndexedSet`, but that also means [you can bug me if there are any issues](https://github.com/mahmoud/boltons/issues). :)",
               "id": "35258869",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1454877705,
               "score": 19
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 41785358,
               "is_accepted": false,
               "last_activity_date": 1485038701,
               "body_markdown": "The [ParallelRegression](https://github.com/rcbellamy/ParallelRegression) package provides a [setList( )](https://parallel-regression.readthedocs.io/en/latest/ParallelRegression.html#setlist) ordered set class that is more method-complete than the options based on the ActiveState recipe.  It supports all methods available for lists and most if not all methods available for sets.",
               "id": "41785358",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1485038701,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47672481,
               "is_accepted": false,
               "last_activity_date": 1512557434,
               "body_markdown": "There&#39;s no `OrderedSet` in official library.\r\nI make an exhaustive cheatsheet of all the data structure for your reference.\r\n\r\n    DataStructure = {&#39;Collections&#39;: {&#39;Map&#39;: [(&#39;dict&#39;, &#39;OrderDict&#39;, &#39;defaultdict&#39;),\r\n                                             (&#39;chainmap&#39;, &#39;types.MappingProxyType&#39;)],\r\n                                     &#39;Set&#39;: [(&#39;set&#39;, &#39;frozenset&#39;), {&#39;multiset&#39;:&#39;collection.Counter&#39;}]},\r\n                                     &#39;Sequence&#39;: {&#39;Basic&#39;: [&#39;list&#39;, &#39;tuple&#39;, &#39;iterator&#39;]},\r\n                                                  &#39;Algorithm&#39;: {&#39;Priority&#39;: [&#39;heapq&#39;,\r\n                                                                             &#39;queue.PriorityQueue&#39;],\r\n                                                                &#39;Queue&#39;: [&#39;queue.Queue&#39;,\r\n                                                                          &#39;multiprocessing.Queue&#39;],\r\n                                                                &#39;Stack&#39;: [&#39;collection.deque&#39;,\r\n                                                                          &#39;queue.LifeQueue&#39;]},\r\n                     &#39;text_sequence&#39;: [&#39;str&#39;, &#39;byte&#39;, &#39;bytearray&#39;]}",
               "id": "47672481",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512557434,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/1653970/does-python-have-an-ordered-set",
         "id": "858127-2291"
      },
      {
         "up_vote_count": "347",
         "path": "2.stack",
         "body_markdown": "I have the following `DataFrame` (`df`):\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    \r\n    df = pd.DataFrame(np.random.rand(10, 5))\r\n\r\nI add more column(s) by assignment:\r\n\r\n    df[&#39;mean&#39;] = df.mean(1)\r\n\r\nHow can I move the column `mean` to the front, i.e. set it as first column leaving the order of the other columns untouched?",
         "view_count": "270878",
         "answer_count": "24",
         "tags": "['python', 'pandas']",
         "creation_date": "1351635779",
         "last_edit_date": "1357279776",
         "code_snippet": "['<code>DataFrame</code>', '<code>df</code>', '<code>import numpy as np\\nimport pandas as pd\\n\\ndf = pd.DataFrame(np.random.rand(10, 5))\\n</code>', \"<code>df['mean'] = df.mean(1)\\n</code>\", '<code>mean</code>', \"<code>In [6]: df\\nOut[6]:\\n          0         1         2         3         4      mean\\n0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543\\n1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208\\n2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596\\n3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653\\n4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371\\n5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165\\n6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529\\n7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149\\n8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195\\n9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593\\n\\nIn [7]: cols = df.columns.tolist()\\n\\nIn [8]: cols\\nOut[8]: [0L, 1L, 2L, 3L, 4L, 'mean']\\n</code>\", '<code>cols</code>', \"<code>In [12]: cols = cols[-1:] + cols[:-1]\\n\\nIn [13]: cols\\nOut[13]: ['mean', 0L, 1L, 2L, 3L, 4L]\\n</code>\", '<code>In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]\\n\\nIn [17]: df\\nOut[17]:\\n       mean         0         1         2         3         4\\n0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616\\n1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551\\n2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694\\n3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019\\n4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485\\n5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447\\n6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473\\n7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914\\n8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561\\n9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399\\n</code>', '<code>cols</code>', '<code>list</code>', '<code>Index</code>', \"<code>df = df[['mean', '0', '1', '2', '3']]\\n</code>\", '<code>cols = list(df.columns.values)\\n</code>', \"<code>['0', '1', '2', '3', 'mean']\\n</code>\", '<code>df.columns.tolist()</code>', '<code>&lt;dataframe&gt;.columns</code>', \"<code>In [39]: df\\nOut[39]: \\n          0         1         2         3         4  mean\\n0  0.172742  0.915661  0.043387  0.712833  0.190717     1\\n1  0.128186  0.424771  0.590779  0.771080  0.617472     1\\n2  0.125709  0.085894  0.989798  0.829491  0.155563     1\\n3  0.742578  0.104061  0.299708  0.616751  0.951802     1\\n4  0.721118  0.528156  0.421360  0.105886  0.322311     1\\n5  0.900878  0.082047  0.224656  0.195162  0.736652     1\\n6  0.897832  0.558108  0.318016  0.586563  0.507564     1\\n7  0.027178  0.375183  0.930248  0.921786  0.337060     1\\n8  0.763028  0.182905  0.931756  0.110675  0.423398     1\\n9  0.848996  0.310562  0.140873  0.304561  0.417808     1\\n\\nIn [40]: df = df[['mean', 4,3,2,1]]\\n</code>\", '<code>In [41]: df\\nOut[41]: \\n   mean         4         3         2         1\\n0     1  0.190717  0.712833  0.043387  0.915661\\n1     1  0.617472  0.771080  0.590779  0.424771\\n2     1  0.155563  0.829491  0.989798  0.085894\\n3     1  0.951802  0.616751  0.299708  0.104061\\n4     1  0.322311  0.105886  0.421360  0.528156\\n5     1  0.736652  0.195162  0.224656  0.082047\\n6     1  0.507564  0.586563  0.318016  0.558108\\n7     1  0.337060  0.921786  0.930248  0.375183\\n8     1  0.423398  0.110675  0.931756  0.182905\\n9     1  0.417808  0.304561  0.140873  0.310562\\n</code>', '<code>&lt;df&gt;.columns</code>', \"<code>df.insert(0, 'mean', df.mean(1))\\n</code>\", '<code>pandas</code>', '<code>df.move(0,df.mean)</code>', \"<code>df = df.reindex_axis(['mean',0,1,2,3,4], axis=1)\\n</code>\", \"<code>df = df.reindex_axis(sorted(df.columns), axis=1)\\ndf = df.reindex_axis(['opened'] + list([a for a in df.columns if a != 'opened']), axis=1)\\n</code>\", '<code>reindex</code>', \"<code>df = df.reindex(columns=sorted(df.columns))\\ndf = df.reindex(columns=(['opened'] + list([a for a in df.columns if a != 'opened']) ))\\n</code>\", '<code>copy=False</code>', '<code>reindex_axis</code>', '<code>reindex_axis</code>', '<code>reindex</code>', '<code>df = df[cols]</code>', \"<code>cols = ['mean']  + [col for col in df if col != 'mean']\\ndf = df[cols]\\n</code>\", '<code>cols = [df.columns[-1]] + [col for col in df if col != df.columns[-1]]\\ndf = df[cols]\\n</code>', \"<code>inserted_cols = ['a', 'b', 'c']\\ncols = ([col for col in inserted_cols if col in df] \\n        + [col for col in df if col not in inserted cols])\\ndf = df[cols]\\n</code>\", \"<code>df = df[['mean'] + df.columns[:-1].tolist()]\\n</code>\", '<code>order = df.columns.tolist()</code>', \"<code>df['mean'] = df.mean(1)</code>\", \"<code>df.columns = ['mean'] + order</code>\", '<code>headers</code>', '<code>df.reindex(columns=headers)</code>', \"<code>df.set_index('some header name', inplace=True)</code>\", '<code>some header name</code>', \"<code>['mean'] + df.columns</code>\", \"<code>Index(u'meanAddress', u'meanCity', u'meanFirst Name'...</code>\", '<code>def order(frame,var):\\n    varlist =[w for w in frame.columns if w not in var]\\n    frame = frame[var+varlist]\\n    return frame \\n</code>', \"<code>frame = order(frame,['Total'])\\n</code>\", \"<code>frame = order(frame,['Total','Date'])\\n</code>\", '<code>frame = order(frame,[v for v in frame.columns if \"VAR\" in v])\\n</code>', \"<code>df = df.reindex_axis(['mean'] + list(df.columns[:-1]), axis=1)</code>\", '<code>Inplace = True</code>', \"<code>cols = df.columns.tolist()\\ncols.insert(0, cols.pop(-1))\\n\\ncols\\n&gt;&gt;&gt;['mean', 0L, 1L, 2L, 3L, 4L]\\n\\ndf = df[cols]\\n</code>\", '<code>df.reindex_axis(sorted(df.columns), axis=1)\\n</code>', '<code>def change_column_order(df, col_name, index):\\n    cols = df.columns.tolist()\\n    cols.remove(col_name)\\n    cols.insert(index, col_name)\\n    return df[cols]\\n</code>', \"<code>df = change_column_order(df, 'mean', 0)\\n</code>\", \"<code>df = df[['mean', Col1,Col2,Col3]]</code>\", '<code>set()</code>', '<code>set()</code>', \"<code>cols = list(set(df.columns.tolist()) - set(['mean']))\\ncols.insert(0, 'mean')\\ndf = df[cols]\\n</code>\", \"<code>df.T.reindex(['mean',0,1,2,3,4]).T\\n</code>\", '<code>insert()</code>', '<code>mean</code>', '<code>cols = cols[-1:] + cols[:-1]</code>', '<code>meanDf = pd.DataFrame(df.pop(\\'mean\\'))\\n# now df doesn\\'t contain \"mean\" anymore. Order of join will move it to left or right:\\nmeanDf.join(df) # has mean as first column\\ndf.join(meanDf) # has mean as last column\\n</code>', \"<code>df = df.reindex_axis(['Col1','Col2'] + list(df.columns.drop(['Col1','Col2'])), axis=1)\\n</code>\", '<code>DataFrame.sort_index(axis=1)</code>', '<code>concat</code>', \"<code>from boltons.setutils import IndexedSet\\ncols = list(IndexedSet(df.columns.tolist()) - set(['mean', 'std']))\\ncols[0:0] =['mean', 'std']\\ndf = df[cols]\\n</code>\", '<code>reindex</code>', \"<code>df\\n#           0         1         2         3         4      mean\\n# 0  0.943825  0.202490  0.071908  0.452985  0.678397  0.469921\\n# 1  0.745569  0.103029  0.268984  0.663710  0.037813  0.363821\\n# 2  0.693016  0.621525  0.031589  0.956703  0.118434  0.484254\\n# 3  0.284922  0.527293  0.791596  0.243768  0.629102  0.495336\\n# 4  0.354870  0.113014  0.326395  0.656415  0.172445  0.324628\\n# 5  0.815584  0.532382  0.195437  0.829670  0.019001  0.478415\\n# 6  0.944587  0.068690  0.811771  0.006846  0.698785  0.506136\\n# 7  0.595077  0.437571  0.023520  0.772187  0.862554  0.538182\\n# 8  0.700771  0.413958  0.097996  0.355228  0.656919  0.444974\\n# 9  0.263138  0.906283  0.121386  0.624336  0.859904  0.555009\\n\\ndf.reindex(['mean', *range(5)], axis=1)\\n\\n#        mean         0         1         2         3         4\\n# 0  0.469921  0.943825  0.202490  0.071908  0.452985  0.678397\\n# 1  0.363821  0.745569  0.103029  0.268984  0.663710  0.037813\\n# 2  0.484254  0.693016  0.621525  0.031589  0.956703  0.118434\\n# 3  0.495336  0.284922  0.527293  0.791596  0.243768  0.629102\\n# 4  0.324628  0.354870  0.113014  0.326395  0.656415  0.172445\\n# 5  0.478415  0.815584  0.532382  0.195437  0.829670  0.019001\\n# 6  0.506136  0.944587  0.068690  0.811771  0.006846  0.698785\\n# 7  0.538182  0.595077  0.437571  0.023520  0.772187  0.862554\\n# 8  0.444974  0.700771  0.413958  0.097996  0.355228  0.656919\\n# 9  0.555009  0.263138  0.906283  0.121386  0.624336  0.859904\\n</code>\", \"<code>my_column = df.pop('column name')\\ndf.insert(3, my_column.name, my_column)\\n</code>\", '<code>import pandas as pd\\ndf = pd.DataFrame({\"A\": [1,2,3], \\n                   \"B\": [2,4,8], \\n                   \"C\": [5,5,5]})\\n\\ncols = df.columns.tolist()\\ncolumn_to_move = \"C\"\\nnew_position = 1\\n\\ncols.insert(new_position, cols.pop(cols.index(column_to_move)))\\ndf = df[cols]\\n</code>', '<code>def mean_first(df):\\n    ncols = df.shape[1]        # Get the number of columns\\n    index = list(range(ncols)) # Create an index to reorder the columns\\n    index.insert(0,ncols)      # This puts the last column at the front\\n    return(df.assign(mean=df.mean(1)).iloc[:,index]) # new df with last column (mean) first\\n</code>']",
         "title": "How to change the order of DataFrame columns?",
         "_childDocuments_": [
            {
               "up_vote_count": 440,
               "answer_id": 13148611,
               "last_activity_date": 1351698682,
               "path": "3.stack.answer",
               "body_markdown": "One easy way would be to reassign the dataframe with a list of the columns, rearranged as needed. \r\n\r\nThis is what you have now: \r\n\r\n    In [6]: df\r\n    Out[6]:\r\n              0         1         2         3         4      mean\r\n    0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543\r\n    1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208\r\n    2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596\r\n    3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653\r\n    4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371\r\n    5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165\r\n    6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529\r\n    7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149\r\n    8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195\r\n    9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593\r\n\r\n    In [7]: cols = df.columns.tolist()\r\n    \r\n    In [8]: cols\r\n    Out[8]: [0L, 1L, 2L, 3L, 4L, &#39;mean&#39;]\r\n\r\nRearrange `cols` in any way you want. This is how I moved the last element to the first position: \r\n\r\n    In [12]: cols = cols[-1:] + cols[:-1]\r\n    \r\n    In [13]: cols\r\n    Out[13]: [&#39;mean&#39;, 0L, 1L, 2L, 3L, 4L]\r\n\r\nThen reorder the dataframe like this: \r\n\r\n    In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]\r\n\r\n    In [17]: df\r\n    Out[17]:\r\n           mean         0         1         2         3         4\r\n    0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616\r\n    1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551\r\n    2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694\r\n    3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019\r\n    4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485\r\n    5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447\r\n    6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473\r\n    7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914\r\n    8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561\r\n    9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399",
               "tags": [],
               "creation_date": 1351636729,
               "last_edit_date": 1351698682,
               "is_accepted": false,
               "id": "13148611",
               "down_vote_count": 1,
               "score": 439
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 66,
               "answer_id": 13316001,
               "is_accepted": false,
               "last_activity_date": 1352495043,
               "body_markdown": "How about:\r\n\r\n    df.insert(0, &#39;mean&#39;, df.mean(1))\r\n\r\nhttp://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion",
               "id": "13316001",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1352495043,
               "score": 65
            },
            {
               "up_vote_count": 7,
               "answer_id": 14151942,
               "last_activity_date": 1357279486,
               "path": "3.stack.answer",
               "body_markdown": "This question has been answered [before][1]:\r\n\r\n    df.reindex_axis(sorted(df.columns), axis=1)\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/11067027/python-pandas-re-ordering-columns-in-a-dataframe-based-on-column-name",
               "tags": [],
               "creation_date": 1357279486,
               "last_edit_date": 1495535498,
               "is_accepted": false,
               "id": "14151942",
               "down_vote_count": 3,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 138,
               "answer_id": 23741480,
               "is_accepted": false,
               "last_activity_date": 1400512833,
               "body_markdown": "You could also do something like this:\r\n\r\n    df = df[[&#39;mean&#39;, &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;]]\r\n\r\nYou can get the list of columns with:\r\n\r\n    cols = list(df.columns.values)\r\n\r\nThe output will produce:\r\n\r\n    [&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;mean&#39;]\r\n\r\n...which is then easy to rearrange manually before dropping it into the first function",
               "id": "23741480",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1400512833,
               "score": 138
            },
            {
               "up_vote_count": 9,
               "answer_id": 25023460,
               "last_activity_date": 1471032483,
               "path": "3.stack.answer",
               "body_markdown": "    def order(frame,var):\r\n        varlist =[w for w in frame.columns if w not in var]\r\n        frame = frame[var+varlist]\r\n        return frame \r\n\r\nThis function takes two arguments, the first is the dataset, the second are the columns in the data set that you want to bring to the front. \r\n\r\nSo in my case I have a data set called Frame with variables A1, A2, B1, B2, Total and Date. If I want to bring Total to the front then all I have to do is: \r\n\r\n    frame = order(frame,[&#39;Total&#39;])\r\n\r\nIf I want to bring Total and Date to the front then I do:\r\n\r\n    frame = order(frame,[&#39;Total&#39;,&#39;Date&#39;])\r\n\r\nEDIT:\r\n\r\nAnother useful way to use this is, if you have an unfamiliar table and you&#39;re looking with variables with a particular term in them, like VAR1, VAR2,... you may execute something like: \r\n\r\n    frame = order(frame,[v for v in frame.columns if &quot;VAR&quot; in v])\r\n",
               "tags": [],
               "creation_date": 1406662218,
               "last_edit_date": 1471032483,
               "is_accepted": false,
               "id": "25023460",
               "down_vote_count": 1,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 25535803,
               "is_accepted": false,
               "last_activity_date": 1409168971,
               "body_markdown": "I ran into a similar question myself, and just wanted to add what I settled on. I liked the reindex_axis( ) method for changing column order. This worked:\r\n\r\n&lt;code&gt;df = df.reindex_axis([&#39;mean&#39;] + list(df.columns[:-1]), axis=1)&lt;/code&gt;\r\n",
               "id": "25535803",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1409168971,
               "score": 6
            },
            {
               "up_vote_count": 1,
               "answer_id": 29195706,
               "last_activity_date": 1427037791,
               "path": "3.stack.answer",
               "body_markdown": "I believe [@Aman&#39;s answer][1] is the best if you know the location of the other column.\r\n\r\nIf you don&#39;t know the location of `mean`, but only have its name, you  cannot resort directly to `cols = cols[-1:] + cols[:-1]`. Following is the next-best thing I could come up with:\r\n\r\n    meanDf = pd.DataFrame(df.pop(&#39;mean&#39;))\r\n    # now df doesn&#39;t contain &quot;mean&quot; anymore. Order of join will move it to left or right:\r\n    meanDf.join(df) # has mean as first column\r\n    df.join(meanDf) # has mean as last column\r\n\r\n   \r\n\r\n\r\n  [1]: https://stackoverflow.com/a/13148611/1082349",
               "tags": [],
               "creation_date": 1427035383,
               "last_edit_date": 1495541448,
               "is_accepted": false,
               "id": "29195706",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 13,
               "answer_id": 29916004,
               "last_activity_date": 1499847790,
               "path": "3.stack.answer",
               "body_markdown": "Simply do,\r\n\r\n    df = df[[&#39;mean&#39;] + df.columns[:-1].tolist()]\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1430214619,
               "last_edit_date": 1499847790,
               "is_accepted": false,
               "id": "29916004",
               "down_vote_count": 1,
               "score": 12
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 96,
               "answer_id": 29922207,
               "is_accepted": false,
               "last_activity_date": 1430230789,
               "body_markdown": "Just assign the column names in the order you want them, to `&lt;dataframe&gt;.columns` like below:\r\n\r\n    In [39]: df\r\n    Out[39]: \r\n              0         1         2         3         4  mean\r\n    0  0.172742  0.915661  0.043387  0.712833  0.190717     1\r\n    1  0.128186  0.424771  0.590779  0.771080  0.617472     1\r\n    2  0.125709  0.085894  0.989798  0.829491  0.155563     1\r\n    3  0.742578  0.104061  0.299708  0.616751  0.951802     1\r\n    4  0.721118  0.528156  0.421360  0.105886  0.322311     1\r\n    5  0.900878  0.082047  0.224656  0.195162  0.736652     1\r\n    6  0.897832  0.558108  0.318016  0.586563  0.507564     1\r\n    7  0.027178  0.375183  0.930248  0.921786  0.337060     1\r\n    8  0.763028  0.182905  0.931756  0.110675  0.423398     1\r\n    9  0.848996  0.310562  0.140873  0.304561  0.417808     1\r\n    \r\n    In [40]: df = df[[&#39;mean&#39;, 4,3,2,1]]\r\n    \r\nNow, &#39;mean&#39; column comes out in the front:\r\n\r\n    In [41]: df\r\n    Out[41]: \r\n       mean         4         3         2         1\r\n    0     1  0.190717  0.712833  0.043387  0.915661\r\n    1     1  0.617472  0.771080  0.590779  0.424771\r\n    2     1  0.155563  0.829491  0.989798  0.085894\r\n    3     1  0.951802  0.616751  0.299708  0.104061\r\n    4     1  0.322311  0.105886  0.421360  0.528156\r\n    5     1  0.736652  0.195162  0.224656  0.082047\r\n    6     1  0.507564  0.586563  0.318016  0.558108\r\n    7     1  0.337060  0.921786  0.930248  0.375183\r\n    8     1  0.423398  0.110675  0.931756  0.182905\r\n    9     1  0.417808  0.304561  0.140873  0.310562\r\n\r\n",
               "id": "29922207",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1430230789,
               "score": 96
            },
            {
               "up_vote_count": 18,
               "answer_id": 32131398,
               "last_activity_date": 1460492052,
               "path": "3.stack.answer",
               "body_markdown": "You need to create a new list of your columns in the desired order, then use `df = df[cols]` to rearrange the columns in this new order.\r\n\r\n    cols = [&#39;mean&#39;]  + [col for col in df if col != &#39;mean&#39;]\r\n    df = df[cols]\r\n\r\nYou can also use a more general approach.  In this example, the last column (indicated by -1) is inserted as the first column.\r\n\r\n    cols = [df.columns[-1]] + [col for col in df if col != df.columns[-1]]\r\n    df = df[cols]\r\n\r\nYou can also use this approach for reordering columns in a desired order if they are present in the DataFrame.\r\n\r\n    inserted_cols = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]\r\n    cols = ([col for col in inserted_cols if col in df] \r\n            + [col for col in df if col not in inserted cols])\r\n    df = df[cols]",
               "tags": [],
               "creation_date": 1440123532,
               "last_edit_date": 1460492052,
               "is_accepted": false,
               "id": "32131398",
               "down_vote_count": 1,
               "score": 17
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 37071454,
               "is_accepted": false,
               "last_activity_date": 1462534773,
               "body_markdown": "Just type the column name you want to change, and set the index for the new location.\r\n\r\n    def change_column_order(df, col_name, index):\r\n        cols = df.columns.tolist()\r\n        cols.remove(col_name)\r\n        cols.insert(index, col_name)\r\n        return df[cols]\r\n\r\nFor your case, this would be like:\r\n\r\n    df = change_column_order(df, &#39;mean&#39;, 0)\r\n\r\n\r\n\r\n",
               "id": "37071454",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1462534773,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 38044211,
               "is_accepted": false,
               "last_activity_date": 1466984788,
               "body_markdown": "How about using &quot;T&quot;?\r\n\r\n    df.T.reindex([&#39;mean&#39;,0,1,2,3,4]).T",
               "id": "38044211",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1466984788,
               "score": 3
            },
            {
               "up_vote_count": 35,
               "answer_id": 39237712,
               "last_activity_date": 1516608269,
               "path": "3.stack.answer",
               "body_markdown": "\r\n\r\n\r\nIn your case,\r\n\r\n    df = df.reindex_axis([&#39;mean&#39;,0,1,2,3,4], axis=1)\r\n\r\nwill do exactly what you want.\r\n\r\n**In my case (general form):**\r\n\r\n    df = df.reindex_axis(sorted(df.columns), axis=1)\r\n    df = df.reindex_axis([&#39;opened&#39;] + list([a for a in df.columns if a != &#39;opened&#39;]), axis=1)\r\n\n### update Jan 2018\n**If you want to use `reindex`:** \n\n    df = df.reindex(columns=sorted(df.columns))\n    df = df.reindex(columns=([&#39;opened&#39;] + list([a for a in df.columns if a != &#39;opened&#39;]) ))\n\n\n",
               "tags": [],
               "creation_date": 1472594256,
               "last_edit_date": 1516608269,
               "is_accepted": false,
               "id": "39237712",
               "down_vote_count": 1,
               "score": 34
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 41042896,
               "is_accepted": false,
               "last_activity_date": 1481210559,
               "body_markdown": "You could do the following (borrowing parts from Aman&#39;s answer):\r\n\r\n    cols = df.columns.tolist()\r\n    cols.insert(0, cols.pop(-1))\r\n    \r\n    cols\r\n    &gt;&gt;&gt;[&#39;mean&#39;, 0L, 1L, 2L, 3L, 4L]\r\n\r\n    df = df[cols]",
               "id": "41042896",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1481210559,
               "score": 5
            },
            {
               "up_vote_count": 3,
               "answer_id": 44280676,
               "last_activity_date": 1496223253,
               "path": "3.stack.answer",
               "body_markdown": "I tried the `insert()` function as suggested by Wes McKinney.\r\n\r\n&gt; df.insert(0, &#39;mean&#39;, df.mean(1))\r\n\r\nThis got the result that Timmie wanted, in one line, without the need to move that last column.",
               "tags": [],
               "creation_date": 1496222443,
               "last_edit_date": 1496223253,
               "is_accepted": false,
               "id": "44280676",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 45346504,
               "is_accepted": false,
               "last_activity_date": 1501147299,
               "body_markdown": "@clocker: Your solution was very helpful for me, as I wanted to bring two columns in front from a dataframe where I do not know exactly the names of all columns, because they are generated from a pivot statement before.\r\nSo, if you are in the same situation: To bring columns in front that you know the name of and then let them follow by &quot;all the other columns&quot;, I came up with the following general solution;\r\n\r\n    df = df.reindex_axis([&#39;Col1&#39;,&#39;Col2&#39;] + list(df.columns.drop([&#39;Col1&#39;,&#39;Col2&#39;])), axis=1)",
               "id": "45346504",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1501147299,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 45778665,
               "is_accepted": false,
               "last_activity_date": 1503205598,
               "body_markdown": "The simplest way would be to change the order of the column names like this\r\n\r\n`df = df[[&#39;mean&#39;, Col1,Col2,Col3]]`",
               "id": "45778665",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1503205598,
               "score": 4
            },
            {
               "up_vote_count": 1,
               "answer_id": 45839408,
               "last_activity_date": 1504516081,
               "path": "3.stack.answer",
               "body_markdown": "`DataFrame.sort_index(axis=1)` is quite clean.[Check doc here][1].\r\nAnd then `concat` \r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_index.html",
               "tags": [],
               "creation_date": 1503490377,
               "last_edit_date": 1504516081,
               "is_accepted": false,
               "id": "45839408",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 46166838,
               "is_accepted": false,
               "last_activity_date": 1505181984,
               "body_markdown": "**`set()`:**\r\n\r\nA simple approach is using **`set()`**, in particular when you have a long list of columns and do not want to handle them manually:\r\n\r\n    cols = list(set(df.columns.tolist()) - set([&#39;mean&#39;]))\r\n    cols.insert(0, &#39;mean&#39;)\r\n    df = df[cols]",
               "id": "46166838",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1505181984,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47725257,
               "is_accepted": false,
               "last_activity_date": 1512792885,
               "body_markdown": "I liked [Shoresh&#39;s answer](https://stackoverflow.com/a/46166838/9075180) to use set functionality to remove columns when you don&#39;t know the location, however this didn&#39;t work for my purpose as I need to keep the original column order (which has arbitrary column labels). \r\n\r\nI got this to work though by using [IndexedSet](http://boltons.readthedocs.io/en/latest/setutils.html#) from the boltons package.\r\n\r\nI also needed to re-add multiple column labels, so for a more general case I used the following code:\r\n\r\n    from boltons.setutils import IndexedSet\r\n    cols = list(IndexedSet(df.columns.tolist()) - set([&#39;mean&#39;, &#39;std&#39;]))\r\n    cols[0:0] =[&#39;mean&#39;, &#39;std&#39;]\r\n    df = df[cols]\r\n\r\nHope this is useful to anyone searching this thread for a general solution.",
               "id": "47725257",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512792885,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47871469,
               "is_accepted": false,
               "last_activity_date": 1513610647,
               "body_markdown": "You can use `reindex` which can be used for both axis:\r\n\r\n    df\r\n    #           0         1         2         3         4      mean\r\n    # 0  0.943825  0.202490  0.071908  0.452985  0.678397  0.469921\r\n    # 1  0.745569  0.103029  0.268984  0.663710  0.037813  0.363821\r\n    # 2  0.693016  0.621525  0.031589  0.956703  0.118434  0.484254\r\n    # 3  0.284922  0.527293  0.791596  0.243768  0.629102  0.495336\r\n    # 4  0.354870  0.113014  0.326395  0.656415  0.172445  0.324628\r\n    # 5  0.815584  0.532382  0.195437  0.829670  0.019001  0.478415\r\n    # 6  0.944587  0.068690  0.811771  0.006846  0.698785  0.506136\r\n    # 7  0.595077  0.437571  0.023520  0.772187  0.862554  0.538182\r\n    # 8  0.700771  0.413958  0.097996  0.355228  0.656919  0.444974\r\n    # 9  0.263138  0.906283  0.121386  0.624336  0.859904  0.555009\r\n    \r\n    df.reindex([&#39;mean&#39;, *range(5)], axis=1)\r\n    \r\n    #        mean         0         1         2         3         4\r\n    # 0  0.469921  0.943825  0.202490  0.071908  0.452985  0.678397\r\n    # 1  0.363821  0.745569  0.103029  0.268984  0.663710  0.037813\r\n    # 2  0.484254  0.693016  0.621525  0.031589  0.956703  0.118434\r\n    # 3  0.495336  0.284922  0.527293  0.791596  0.243768  0.629102\r\n    # 4  0.324628  0.354870  0.113014  0.326395  0.656415  0.172445\r\n    # 5  0.478415  0.815584  0.532382  0.195437  0.829670  0.019001\r\n    # 6  0.506136  0.944587  0.068690  0.811771  0.006846  0.698785\r\n    # 7  0.538182  0.595077  0.437571  0.023520  0.772187  0.862554\r\n    # 8  0.444974  0.700771  0.413958  0.097996  0.355228  0.656919\r\n    # 9  0.555009  0.263138  0.906283  0.121386  0.624336  0.859904",
               "id": "47871469",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1513610647,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48096340,
               "is_accepted": false,
               "last_activity_date": 1515072354,
               "body_markdown": "Here&#39;s a way to move one existing column that will modify the existing data frame in place.\r\n\r\n    my_column = df.pop(&#39;column name&#39;)\r\n    df.insert(3, my_column.name, my_column)",
               "id": "48096340",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1515072354,
               "score": 0
            },
            {
               "up_vote_count": 0,
               "answer_id": 48508148,
               "last_activity_date": 1519818541,
               "path": "3.stack.answer",
               "body_markdown": "Here is a function to do this for any number of columns. \r\n\r\n    def mean_first(df):\r\n        ncols = df.shape[1]        # Get the number of columns\r\n        index = list(range(ncols)) # Create an index to reorder the columns\r\n        index.insert(0,ncols)      # This puts the last column at the front\r\n        return(df.assign(mean=df.mean(1)).iloc[:,index]) # new df with last column (mean) first\r\n\r\n    \r\n\r\n    ",
               "tags": [],
               "creation_date": 1517252238,
               "last_edit_date": 1519818541,
               "is_accepted": false,
               "id": "48508148",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 49010588,
               "is_accepted": false,
               "last_activity_date": 1519740301,
               "body_markdown": "Moving any column to any position: \r\n\r\n    import pandas as pd\r\n    df = pd.DataFrame({&quot;A&quot;: [1,2,3], \r\n                       &quot;B&quot;: [2,4,8], \r\n                       &quot;C&quot;: [5,5,5]})\r\n    \r\n    cols = df.columns.tolist()\r\n    column_to_move = &quot;C&quot;\r\n    new_position = 1\r\n    \r\n    cols.insert(new_position, cols.pop(cols.index(column_to_move)))\r\n    df = df[cols]",
               "id": "49010588",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519740301,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns",
         "id": "858127-2292"
      },
      {
         "up_vote_count": "298",
         "path": "2.stack",
         "body_markdown": "I&#39;m looking at some Python code which used the `@` symbol, but I have no idea what it does. I also do not know what to search for as searching Python docs or Google does not return relevant results when the `@` symbol is included.",
         "view_count": "129218",
         "answer_count": "9",
         "tags": "['python', 'syntax', 'decorator']",
         "creation_date": "1308352770",
         "last_edit_date": "1494666927",
         "code_snippet": "['<code>@</code>', '<code>@</code>', '<code>@</code>', '<code>@</code>', \"<code>class Pizza(object):\\n    def __init__(self):\\n        self.toppings = []\\n    def __call__(self, topping):\\n        # when using '@instance_of_pizza' before a function def\\n        # the function gets passed onto 'topping'\\n        self.toppings.append(topping())\\n    def __repr__(self):\\n        return str(self.toppings)\\n\\npizza = Pizza()\\n\\n@pizza\\ndef cheese():\\n    return 'cheese'\\n@pizza\\ndef sauce():\\n    return 'sauce'\\n\\nprint pizza\\n# ['cheese', 'sauce']\\n</code>\", '<code>function</code>', '<code>method</code>', '<code>class</code>', '<code>argument</code>', '<code>function</code>', '<code>method</code>', '<code>@</code>', '<code>from flask import Flask\\napp = Flask(__name__)\\n\\n@app.route(\"/\")\\ndef hello():\\n    return \"Hello World!\"\\n</code>', '<code>rule      = \"/\"\\nview_func = hello\\n# they go as arguments here in \\'flask/app.py\\'\\ndef add_url_rule(self, rule, endpoint=None, view_func=None, **options):\\n    pass\\n</code>', '<code>def decorator(func):\\n   return func\\n\\n@decorator\\ndef some_func():\\n    pass\\n</code>', '<code>def decorator(func):\\n    return func\\n\\ndef some_func():\\n    pass\\n\\nsome_func = decorator(some_func)\\n</code>', '<code>@</code>', '<code>__matmul__</code>', '<code>class Mat(list) :\\n    def __matmul__(self, B) :\\n        A = self\\n        return Mat([[sum(A[i][k]*B[k][j] for k in range(len(B)))\\n                    for j in range(len(B[0])) ] for i in range(len(A))])\\n\\nA = Mat([[1,3],[7,5]])\\nB = Mat([[6,8],[4,2]])\\n\\nprint(A @ B)\\n</code>', '<code>[[18, 14], [62, 66]]\\n</code>', '<code>@=</code>', '<code>__imatmul__</code>', '<code>@decorator\\ndef decorated_function():\\n    \"\"\"this function is decorated\"\"\"\\n</code>', '<code>def decorated_function():\\n    \"\"\"this function is decorated\"\"\"\\n\\ndecorated_function = decorator(decorated_function)\\n</code>', '<code>a @ b</code>', '<code>a.__matmul__(b)</code>', '<code>a @ b\\n</code>', '<code>dot(a, b)\\n</code>', '<code>a @= b\\n</code>', '<code>a = dot(a, b)\\n</code>', '<code>dot</code>', '<code>a</code>', '<code>b</code>', '<code>~$ grep -C 1 \"@\" cpython/Grammar/Grammar \\n\\ndecorator: \\'@\\' dotted_name [ \\'(\\' [arglist] \\')\\' ] NEWLINE\\ndecorators: decorator+\\n--\\ntestlist_star_expr: (test|star_expr) (\\',\\' (test|star_expr))* [\\',\\']\\naugassign: (\\'+=\\' | \\'-=\\' | \\'*=\\' | \\'@=\\' | \\'/=\\' | \\'%=\\' | \\'&amp;=\\' | \\'|=\\' | \\'^=\\' |\\n            \\'&lt;&lt;=\\' | \\'&gt;&gt;=\\' | \\'**=\\' | \\'//=\\')\\n--\\narith_expr: term ((\\'+\\'|\\'-\\') term)*\\nterm: factor ((\\'*\\'|\\'@\\'|\\'/\\'|\\'%\\'|\\'//\\') factor)*\\nfactor: (\\'+\\'|\\'-\\'|\\'~\\') factor | power\\n</code>', '<code>@</code>', '<code>@wrapper</code>', '<code>classmethod()</code>', '<code>staticmethod()</code>', '<code>def f(...):\\n    ...\\nf = staticmethod(f)\\n\\n@staticmethod\\ndef f(...):\\n    ...\\n</code>', '<code>@foo\\ndef bar():\\n    pass\\n</code>', '<code>def bar():\\n    pass\\n\\nbar = foo(bar)\\n</code>', '<code>@</code>', '<code>@f1(arg)\\n@f2\\ndef func(): pass\\n</code>', '<code>def func(): pass\\nfunc = f1(arg)(f2(func))\\n</code>', '<code>@</code>', '<code>@</code>', '<code>@</code>', '<code>+       -       *       **      /       //      %      @\\n&lt;&lt;      &gt;&gt;      &amp;       |       ^       ~\\n&lt;       &gt;       &lt;=      &gt;=      ==      !=\\n</code>', '<code>object.__add__(self, other)\\nobject.__sub__(self, other) \\nobject.__mul__(self, other) \\nobject.__matmul__(self, other) \\nobject.__truediv__(self, other) \\nobject.__floordiv__(self, other)\\n</code>', '<code>+</code>', '<code>-</code>', '<code>*</code>', '<code>@</code>', '<code>/</code>', '<code>//</code>', '<code>__matmul__</code>', '<code>@</code>', '<code>__matmul__()</code>', '<code>__rmatmul__()</code>', '<code>__imatmul__()</code>', '<code>@=</code>', '<code>S = (H @ beta - r).T @ inv(H @ V @ H.T) @ (H @ beta - r)\\n</code>', '<code>S = dot((dot(H, beta) - r).T,\\n        dot(inv(dot(dot(H, V), H.T)), dot(H, beta) - r))\\n</code>', '<code>numpy</code>', '<code>&gt;&gt;&gt; from numpy import array, matrix\\n&gt;&gt;&gt; array([[1,2,3]]).T @ array([[1,2,3]])\\narray([[1, 2, 3],\\n       [2, 4, 6],\\n       [3, 6, 9]])\\n&gt;&gt;&gt; array([[1,2,3]]) @ array([[1,2,3]]).T\\narray([[14]])\\n&gt;&gt;&gt; matrix([1,2,3]).T @ matrix([1,2,3])\\nmatrix([[1, 2, 3],\\n        [2, 4, 6],\\n        [3, 6, 9]])\\n&gt;&gt;&gt; matrix([1,2,3]) @ matrix([1,2,3]).T\\nmatrix([[14]])\\n</code>', '<code>@=</code>', '<code>&gt;&gt;&gt; m = matrix([1,2,3])\\n&gt;&gt;&gt; m @= m.T\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nTypeError: In-place matrix multiplication is not (yet) supported. Use \\'a = a @ b\\' instead of \\'a @= b\\'.\\n</code>', '<code>&gt;&gt;&gt; m = matrix([1,2,3])\\n&gt;&gt;&gt; m @= m.T\\n&gt;&gt;&gt; m\\nmatrix([[14]])\\n</code>', '<code>pandas.DataFrame.query</code>', \"<code>df = pandas.DataFrame({'foo': [1,2,15,17]})\\ny = 10\\ndf &gt;&gt; query('foo &gt; @y') # plydata\\ndf.query('foo &gt; @y') # pandas\\n</code>\"]",
         "title": "What does the &quot;at&quot; (@) symbol do in Python?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 6392764,
               "is_accepted": false,
               "last_activity_date": 1308352971,
               "body_markdown": "It indicates that you are using a decorator. Here is [Bruce Eckel&#39;s example][1] from 2008.\r\n\r\n\r\n  [1]: http://www.artima.com/weblogs/viewpost.jsp?thread=240808",
               "id": "6392764",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1308352971,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 161,
               "answer_id": 6392768,
               "is_accepted": true,
               "last_activity_date": 1308353024,
               "body_markdown": "The `@` symbol is used for class, function and method *decorators*.\r\n\r\nRead more here:\r\n\r\n[PEP 318: Decorators][1]\r\n\r\n[Python Decorators][2]\r\n\r\nThe most common Python decorators you&#39;ll run into are:\r\n\r\n[@property][3]\r\n\r\n[@classmethod][4]\r\n\r\n[@staticmethod][5]\r\n\r\n\r\n  [1]: http://www.python.org/dev/peps/pep-0318/\r\n  [2]: http://wiki.python.org/moin/PythonDecorators\r\n  [3]: http://docs.python.org/library/functions.html#property\r\n  [4]: http://docs.python.org/library/functions.html#classmethod\r\n  [5]: http://docs.python.org/library/functions.html#staticmethod",
               "id": "6392768",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1308353024,
               "score": 159
            },
            {
               "up_vote_count": 122,
               "answer_id": 14799490,
               "last_activity_date": 1459851849,
               "path": "3.stack.answer",
               "body_markdown": "This code snippet:\r\n\r\n    def decorator(func):\r\n       return func\r\n\r\n    @decorator\r\n    def some_func():\r\n        pass\r\n\r\nIs equivalent to this code:\r\n\r\n    def decorator(func):\r\n        return func\r\n\r\n    def some_func():\r\n        pass\r\n    \r\n    some_func = decorator(some_func)\r\n\r\nIn the definition of decorator you can add some modified things that wouldn&#39;t be returned by function normally.\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1360511251,
               "last_edit_date": 1459851849,
               "is_accepted": false,
               "id": "14799490",
               "down_vote_count": 0,
               "score": 122
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 214,
               "answer_id": 15839702,
               "is_accepted": false,
               "last_activity_date": 1365182118,
               "body_markdown": "## Preamble\r\nI admit it took more than a few moments to fully grasp this concept for me, so I&#39;ll share what I&#39;ve learned to save others the trouble.\r\n\r\nThe name **decorator** - the thing we define using the `@` syntax before a function definition - was probably the main culprit here.\r\n\r\n## Example\r\n\r\n    class Pizza(object):\r\n        def __init__(self):\r\n            self.toppings = []\r\n        def __call__(self, topping):\r\n            # when using &#39;@instance_of_pizza&#39; before a function def\r\n            # the function gets passed onto &#39;topping&#39;\r\n            self.toppings.append(topping())\r\n        def __repr__(self):\r\n            return str(self.toppings)\r\n    \r\n    pizza = Pizza()\r\n    \r\n    @pizza\r\n    def cheese():\r\n        return &#39;cheese&#39;\r\n    @pizza\r\n    def sauce():\r\n        return &#39;sauce&#39;\r\n    \r\n    print pizza\r\n    # [&#39;cheese&#39;, &#39;sauce&#39;]\r\n\r\nWhat this shows is that the `function`/`method`/`class` you&#39;re defining after a **decorator** is just basically passed on as an `argument` to the `function`/`method` immediatelly after the `@` sign.\r\n\r\n## First sighting\r\n\r\nThe microframework **Flask** introduces **decorators** from the very beginning in the following format:\r\n\r\n    from flask import Flask\r\n    app = Flask(__name__)\r\n    \r\n    @app.route(&quot;/&quot;)\r\n    def hello():\r\n        return &quot;Hello World!&quot;\r\n\r\nThis in turn translates to:\r\n\r\n    rule      = &quot;/&quot;\r\n    view_func = hello\r\n    # they go as arguments here in &#39;flask/app.py&#39;\r\n    def add_url_rule(self, rule, endpoint=None, view_func=None, **options):\r\n        pass\r\n\r\nRealizing this finally allowed me to feel at peace with flask.",
               "id": "15839702",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1365182118,
               "score": 213
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 21637034,
               "is_accepted": false,
               "last_activity_date": 1391804426,
               "body_markdown": "To say what others have in a different way:  yes, it is a decorator.\r\n\r\nIn Python, it&#39;s like:\r\n\r\n   1. Creating a function (follows under the @ call)\r\n   2. Calling another function to operate on your created function.  This returns a new function.  The function that you call is the argument of the @.\r\n   3. Replacing the function defined with the new function returned.\r\n\r\nThis can be used for all kinds of useful things, made possible because functions are objects and just necessary just instructions.",
               "id": "21637034",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1391804426,
               "score": 1
            },
            {
               "up_vote_count": 63,
               "answer_id": 28997112,
               "last_activity_date": 1467304963,
               "path": "3.stack.answer",
               "body_markdown": "In python3.5 you can overload `@` as an operator. It is named as `__matmul__` because It is designed to do matrix multiplication, but It can be anything you want. see [PEP465](http://www.python.org/dev/peps/pep-0465/) for details.\r\n\r\nThis is a simple implementation of matrix multiplication.  \r\n\r\n    class Mat(list) :\r\n        def __matmul__(self, B) :\r\n            A = self\r\n            return Mat([[sum(A[i][k]*B[k][j] for k in range(len(B)))\r\n                        for j in range(len(B[0])) ] for i in range(len(A))])\r\n\r\n    A = Mat([[1,3],[7,5]])\r\n    B = Mat([[6,8],[4,2]])\r\n\r\n    print(A @ B)\r\n\r\nThis code yields \r\n\r\n    [[18, 14], [62, 66]]",
               "tags": [],
               "creation_date": 1426107098,
               "last_edit_date": 1467304963,
               "is_accepted": false,
               "id": "28997112",
               "down_vote_count": 0,
               "score": 63
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 35856075,
               "is_accepted": false,
               "last_activity_date": 1457392721,
               "body_markdown": "Starting with Python 3.5, the &#39;@&#39; is used as a dedicated infix symbol for MATRIX MULTIPLICATION (PEP 0465 -- see https://www.python.org/dev/peps/pep-0465/)\r\n",
               "id": "35856075",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1457392721,
               "score": 5
            },
            {
               "up_vote_count": 20,
               "answer_id": 46027116,
               "last_activity_date": 1507223942,
               "path": "3.stack.answer",
               "body_markdown": "# What does the \u201cat\u201d (@) symbol do in Python?\r\n\r\nIn short, it is used in decorator syntax and for matrix multiplication.\r\n\r\nIn the context of decorators, this syntax:\r\n\r\n    @decorator\r\n    def decorated_function():\r\n        &quot;&quot;&quot;this function is decorated&quot;&quot;&quot;\r\n\r\nis equivalent to this:\r\n\r\n    def decorated_function():\r\n        &quot;&quot;&quot;this function is decorated&quot;&quot;&quot;\r\n\r\n    decorated_function = decorator(decorated_function)\r\n\r\nIn the context of matrix multiplication, `a @ b` invokes `a.__matmul__(b)` - making this syntax:\r\n\r\n    a @ b\r\n\r\nequivalent to \r\n\r\n    dot(a, b)\r\n\r\nand \r\n\r\n    a @= b\r\n\r\nequivalent to \r\n\r\n    a = dot(a, b)\r\n\r\nwhere `dot` is, for example, the numpy matrix multiplication function and `a` and `b` are matrices.\r\n\r\n## How could you discover this on your own?\r\n\r\n&gt; I also do not know what to search for as searching Python docs or Google does not return relevant results when the @ symbol is included.\r\n\r\nIf you want to have a rather complete view of what a particular piece of python syntax does, look directly at the grammar file. For the Python 3 branch:\r\n\r\n    ~$ grep -C 1 &quot;@&quot; cpython/Grammar/Grammar \r\n    \r\n    decorator: &#39;@&#39; dotted_name [ &#39;(&#39; [arglist] &#39;)&#39; ] NEWLINE\r\n    decorators: decorator+\r\n    --\r\n    testlist_star_expr: (test|star_expr) (&#39;,&#39; (test|star_expr))* [&#39;,&#39;]\r\n    augassign: (&#39;+=&#39; | &#39;-=&#39; | &#39;*=&#39; | &#39;@=&#39; | &#39;/=&#39; | &#39;%=&#39; | &#39;&amp;=&#39; | &#39;|=&#39; | &#39;^=&#39; |\r\n                &#39;&lt;&lt;=&#39; | &#39;&gt;&gt;=&#39; | &#39;**=&#39; | &#39;//=&#39;)\r\n    --\r\n    arith_expr: term ((&#39;+&#39;|&#39;-&#39;) term)*\r\n    term: factor ((&#39;*&#39;|&#39;@&#39;|&#39;/&#39;|&#39;%&#39;|&#39;//&#39;) factor)*\r\n    factor: (&#39;+&#39;|&#39;-&#39;|&#39;~&#39;) factor | power\r\n\r\n\r\nWe can see here that `@` is used in three contexts:\r\n\r\n- decorators\r\n- an operator between factors\r\n- an augmented assignment operator\r\n\r\n## Decorator Syntax:\r\n\r\nA google search for &quot;decorator python docs&quot; gives as one of the top results, the &quot;Compound Statements&quot; section of the &quot;Python Language Reference.&quot; Scrolling down to the [section on function definitions][1], which we can find by searching for the word, &quot;decorator&quot;, we see that... there&#39;s a lot to read. But the word, [&quot;decorator&quot; is a link to the glossary][2], which tells us:\r\n\r\n\r\n&gt; ###decorator\r\n&gt;\r\n&gt; A function returning another function, usually applied as a function transformation using the `@wrapper` syntax. Common\r\n&gt; examples for decorators are `classmethod()` and `staticmethod()`.\r\n&gt; \r\n&gt; The decorator syntax is merely syntactic sugar, the following two\r\n&gt; function definitions are semantically equivalent:\r\n&gt; \r\n&gt;     def f(...):\r\n&gt;         ...\r\n&gt;     f = staticmethod(f)\r\n&gt;     \r\n&gt;     @staticmethod\r\n&gt;     def f(...):\r\n&gt;         ...\r\n&gt; \r\n&gt; The same concept exists for classes, but is less commonly used there.\r\n&gt; See the documentation for function definitions and class definitions\r\n&gt; for more about decorators.\r\n\r\nSo, we see that\r\n\r\n\r\n    @foo\r\n    def bar():\r\n        pass\r\n\r\nis semantically the same as:\r\n\r\n    def bar():\r\n        pass\r\n\r\n    bar = foo(bar)\r\n\r\nThey are not exactly the same because Python evaluates the foo expression (which could be a dotted lookup and a function call) before bar with the decorator (`@`) syntax, but evaluates the foo expression *after* bar in the other case.\r\n\r\n&lt;sub&gt;(If this difference makes a difference in the meaning of your code, you should reconsider what you&#39;re doing with your life, because that would be pathological.)&lt;/sub&gt;\r\n\r\n### Stacked Decorators\r\n\r\nIf we go back to the function definition syntax documentation, we see:\r\n\r\n&gt;     @f1(arg)\r\n&gt;     @f2\r\n&gt;     def func(): pass\r\n&gt; \r\n&gt; is roughly equivalent to\r\n&gt; \r\n&gt;     def func(): pass\r\n&gt;     func = f1(arg)(f2(func))\r\n\r\nThis is a demonstration that we can call a function that&#39;s a decorator first, as well as stack decorators. Functions, in Python, are first class objects - which means you can pass a function as an argument to another function, and return functions. Decorators do both of these things.\r\n\r\nIf we stack decorators, the function, as defined, gets passed first to the decorator immediately above it, then the next, and so on.\r\n\r\nThat about sums up the usage for `@` in the context of decorators.\r\n\r\n## The Operator, `@`\r\n\r\nIn the lexical analysis section of the language reference, we have a [section on operators][3], which includes `@`, which makes it also an operator:\r\n\r\n&gt; The following tokens are operators:\r\n&gt; \r\n&gt;     +       -       *       **      /       //      %      @\r\n&gt;     &lt;&lt;      &gt;&gt;      &amp;       |       ^       ~\r\n&gt;     &lt;       &gt;       &lt;=      &gt;=      ==      !=\r\n\r\nand in the next page, the Data Model, we have the [section, emulating numeric types][4], \r\n\r\n&gt;     object.__add__(self, other)\r\n&gt;     object.__sub__(self, other) \r\n&gt;     object.__mul__(self, other) \r\n&gt;     object.__matmul__(self, other) \r\n&gt;     object.__truediv__(self, other) \r\n&gt;     object.__floordiv__(self, other)\r\n&gt; [...]\r\n&gt; These methods are called to implement the binary arithmetic operations (`+`, `-`, `*`, `@`, `/`, `//`, [...]\r\n\r\nAnd we see that `__matmul__` corresponds to `@`. If we search the documentation for &quot;matmul&quot; we get a link to [What&#39;s new in Python 3.5][5] with &quot;matmul&quot; under a heading &quot;PEP 465 - A dedicated infix operator for matrix multiplication&quot;.\r\n\r\n&gt; it can be implemented by defining `__matmul__()`, `__rmatmul__()`, and\r\n&gt; `__imatmul__()` for regular, reflected, and in-place matrix multiplication.\r\n\r\n(So now we learn that `@=` is the in-place version). It further explains:\r\n\r\n&gt; Matrix multiplication is a notably common operation in many fields of\r\n&gt; mathematics, science, engineering, and the addition of @ allows\r\n&gt; writing cleaner code:\r\n&gt; \r\n&gt;     S = (H @ beta - r).T @ inv(H @ V @ H.T) @ (H @ beta - r)\r\n&gt; \r\n&gt; instead of:\r\n&gt; \r\n&gt;     S = dot((dot(H, beta) - r).T,\r\n&gt;             dot(inv(dot(dot(H, V), H.T)), dot(H, beta) - r))\r\n\r\nWhile this operator can be overloaded to do almost anything, in `numpy`, for example, we would use this syntax to calculate the inner and outer product of arrays and matrices:\r\n\r\n    &gt;&gt;&gt; from numpy import array, matrix\r\n    &gt;&gt;&gt; array([[1,2,3]]).T @ array([[1,2,3]])\r\n    array([[1, 2, 3],\r\n           [2, 4, 6],\r\n           [3, 6, 9]])\r\n    &gt;&gt;&gt; array([[1,2,3]]) @ array([[1,2,3]]).T\r\n    array([[14]])\r\n    &gt;&gt;&gt; matrix([1,2,3]).T @ matrix([1,2,3])\r\n    matrix([[1, 2, 3],\r\n            [2, 4, 6],\r\n            [3, 6, 9]])\r\n    &gt;&gt;&gt; matrix([1,2,3]) @ matrix([1,2,3]).T\r\n    matrix([[14]])\r\n\r\n### Inplace matrix multiplication: `@=`\r\n\r\nWhile researching the prior usage, we learn that there is also the inplace matrix multiplication. If we attempt to use it, we may find it is not yet implemented for numpy:\r\n\r\n    &gt;&gt;&gt; m = matrix([1,2,3])\r\n    &gt;&gt;&gt; m @= m.T\r\n    Traceback (most recent call last):\r\n      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\n    TypeError: In-place matrix multiplication is not (yet) supported. Use &#39;a = a @ b&#39; instead of &#39;a @= b&#39;.\r\n\r\nWhen it is implemented, I would expect the result to look like this:\r\n\r\n\r\n    &gt;&gt;&gt; m = matrix([1,2,3])\r\n    &gt;&gt;&gt; m @= m.T\r\n    &gt;&gt;&gt; m\r\n    matrix([[14]])\r\n\r\n\r\n  [1]: https://docs.python.org/3/reference/compound_stmts.html#class-definitions\r\n  [2]: https://docs.python.org/3/glossary.html#term-decorator\r\n  [3]: https://docs.python.org/3/reference/lexical_analysis.html#operators\r\n  [4]: https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types\r\n  [5]: https://docs.python.org/3/whatsnew/3.5.html?highlight=matmul#pep-465-a-dedicated-infix-operator-for-matrix-multiplication\r\n\r\n",
               "tags": [],
               "creation_date": 1504466904,
               "last_edit_date": 1507223942,
               "is_accepted": false,
               "id": "46027116",
               "down_vote_count": 0,
               "score": 20
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47317286,
               "is_accepted": false,
               "last_activity_date": 1510780131,
               "body_markdown": "@ symbol is also used to access variables inside a plydata / pandas dataframe query, `pandas.DataFrame.query`.\r\nExample:\r\n\r\n    df = pandas.DataFrame({&#39;foo&#39;: [1,2,15,17]})\r\n    y = 10\r\n    df &gt;&gt; query(&#39;foo &gt; @y&#39;) # plydata\r\n    df.query(&#39;foo &gt; @y&#39;) # pandas",
               "id": "47317286",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1510780131,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/6392739/what-does-the-at-symbol-do-in-python",
         "id": "858127-2293"
      },
      {
         "up_vote_count": "401",
         "path": "2.stack",
         "body_markdown": "I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.\r\n\r\nMy code so far:\r\n\r\n    for each_line in fileinput.input(input_file):\r\n        do_something(each_line)\r\n\r\n        for each_line_again in fileinput.input(input_file):\r\n            do_something(each_line_again)\r\n\r\nExecuting this code gives an error message: `device active`.\r\n\r\nAny suggestions?\r\n\r\n\r\nEDIT: The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line.",
         "view_count": "590842",
         "answer_count": "10",
         "tags": "['python', 'file-read']",
         "creation_date": "1320413189",
         "last_edit_date": "1512613530",
         "code_snippet": "['<code>for each_line in fileinput.input(input_file):\\n    do_something(each_line)\\n\\n    for each_line_again in fileinput.input(input_file):\\n        do_something(each_line_again)\\n</code>', '<code>device active</code>', '<code>for</code>', '<code>with open(...) as f:\\n    for line in f:\\n        &lt;do something with line&gt;\\n</code>', '<code>with</code>', '<code>for line in f</code>', '<code>f</code>', '<code>for line in f:</code>', '<code>__iter__</code>', '<code>with</code>', '<code>yield</code>', '<code>with</code>', '<code>with</code>', '<code>with</code>', '<code>with</code>', '<code>for</code>', '<code>f</code>', '<code>with open(\"x.txt\") as f:\\n    for line in f:\\n        do something with data\\n</code>', '<code>yield</code>', '<code>def readInChunks(fileObj, chunkSize=2048):\\n    \"\"\"\\n    Lazy function to read a file piece by piece.\\n    Default chunk size: 2kB.\\n    \"\"\"\\n    while True:\\n        data = fileObj.read(chunkSize)\\n        if not data:\\n            break\\n        yield data\\n\\nf = open(\\'bigFile\\')\\nfor chuck in readInChunks(f):\\n    do_something(chunk)\\nf.close()\\n</code>', \"<code>for line in open('myfile','r').readlines():\\n    do_something(line)\\n</code>\", '<code>readlines()</code>', '<code>read()</code>', '<code>fileinput</code>', \"<code>import fileinput\\n\\nfor line in fileinput.input(['myfile']):\\n    do_something(line)\\n</code>\", '<code>fileinput.input()</code>', '<code>file</code>', '<code>for line in open(...).readlines(): &lt;do stuff&gt;</code>', '<code>readlines</code>', '<code>fileinput</code>', \"<code>with open(file_path, 'rU') as f:\\n    for line_terminated in f:\\n        line = line_terminated.rstrip('\\\\n')\\n        ...\\n</code>\", \"<code>'\\\\n'</code>\", \"<code>'\\\\r'</code>\", \"<code>'\\\\n'</code>\", \"<code>'\\\\r\\\\n'</code>\", \"<code>open(file_path, mode='rU')</code>\", \"<code>open(file_path, mode='rU')</code>\", '<code>open(file_path, newline=None)</code>', '<code>newline</code>', '<code>None</code>', '<code>mode</code>', \"<code>'r'</code>\", '<code>U</code>', '<code>\\\\r\\\\n</code>', '<code>\\\\n</code>', \"<code>with open(file_path, 'rb') as f:\\n    with line_native_terminated in f:\\n        ...\\n</code>\", '<code>in</code>', \"<code>open(file_path, 'rU')</code>\", '<code>f = open(input_file)\\nfor line in f:\\n    do_stuff(line)\\nf.close()\\n</code>', '<code>with open(input_file) as f:</code>', '<code>f.close()</code>', \"<code>with f_outer as open(input_file, 'r'):\\n    for line_outer in f_outer:\\n        with f_inner as open(input_file, 'r'):\\n            for line_inner in f_inner:\\n                compute_distance(line_outer, line_inner)\\n</code>\", '<code>HP-Z820:/mnt/fastssd/fast_file_reader$ ls -l /mnt/fastssd/nzv/HIGGS.csv\\n-rw-rw-r-- 1 8035497980 Jan 24 16:00 /mnt/fastssd/nzv/HIGGS.csv\\n\\nHP-Z820:/mnt/fastssd$ ls -l all_bin.csv\\n-rw-rw-r-- 1 40412077758 Feb  2 09:00 all_bin.csv\\n\\nga@ga-HP-Z820:/mnt/fastssd$ time python fastread.py --fileName=\"all_bin.csv\" --numProcesses=32 --balanceFactor=2\\n2367496\\n\\nreal    0m8.920s\\nuser    1m30.056s\\nsys 2m38.744s\\n\\nIn [1]: 40412077758. / 8.92\\nOut[1]: 4530501990.807175\\n</code>', '<code>HP-Z820:/mnt/fastssd$ time wc -l all_bin.csv\\n2367496 all_bin.csv\\n\\nreal    0m8.807s\\nuser    0m1.168s\\nsys 0m7.636s\\n\\n\\nHP-Z820:/mnt/fastssd/fast_file_reader$ time python fastread.py --fileName=\"HIGGS.csv\" --numProcesses=16 --balanceFactor=2\\n11000000\\n\\nreal    0m2.257s\\nuser    0m12.088s\\nsys 0m20.512s\\n\\nHP-Z820:/mnt/fastssd/fast_file_reader$ time wc -l HIGGS.csv\\n11000000 HIGGS.csv\\n\\nreal    0m1.820s\\nuser    0m0.364s\\nsys 0m1.456s\\n</code>', '<code>HP-Z820:/mnt/fastssd/fast_file_reader$ time python fastread.py --fileName=\"HIGGS.csv\" --numProcesses=16 --balanceFactor=2\\n11000000\\n\\nreal    0m2.256s\\nuser    0m10.696s\\nsys 0m19.952s\\n\\nHP-Z820:/mnt/fastssd/fast_file_reader$ time python fastread.py --fileName=\"HIGGS.csv\" --numProcesses=1 --balanceFactor=1\\n11000000\\n\\nreal    0m17.380s\\nuser    0m11.124s\\nsys 0m6.272s\\n</code>', '<code>HP-Z820:/mnt/fastssd/fast_file_reader$ time python fastread.py --fileName=\"HIGGS.csv\" --numProcesses=1 --balanceFactor=2\\n11000000\\n\\nreal    1m37.077s\\nuser    0m12.432s\\nsys 1m24.700s\\n</code>', \"<code>fileBytes = stat(fileName).st_size  # Read quickly from OS how many bytes are in a text file\\nstartByte, endByte = PartitionDataToWorkers(workers=numProcesses, items=fileBytes, balanceFactor=balanceFactor)\\np = Pool(numProcesses)\\npartialSum = p.starmap(ReadFileSegment, zip(startByte, endByte, repeat(fileName))) # startByte is already a list. fileName is made into a same-length list of duplicates values.\\nglobalSum = sum(partialSum)\\nprint(globalSum)\\n\\n\\ndef ReadFileSegment(startByte, endByte, fileName, searchChar='\\\\n'):  # counts number of searchChar appearing in the byte range\\n    with open(fileName, 'r') as f:\\n        f.seek(startByte-1)  # seek is initially at byte 0 and then moves forward the specified amount, so seek(5) points at the 6th byte.\\n        bytes = f.read(endByte - startByte + 1)\\n        cnt = len(re.findall(searchChar, bytes)) # findall with implicit compiling runs just as fast here as re.compile once + re.finditer many times.\\n    return cnt\\n</code>\", '<code>#Using a text file for the example\\nwith open(\"yourFile.txt\",\"r\") as f:\\n    text = f.readlines()\\nfor line in text:\\n    print line\\n</code>', '<code>for line in text:\\n    if len(line) &gt; 10:\\n        print line\\n</code>', '<code>sys.argv[1:]</code>', '<code>sys.stdin</code>', '<code>fileinput.FileInput([files[, inplace[, backup[, mode[, openhook]]]]])\\n</code>', '<code>files</code>', '<code>for each_line in fileinput.input([input_file, input_file]):\\n  do_something(each_line)\\n</code>', '<code>import numpy as np\\nimport math\\n\\nlines_total = n    \\nsimilarity = np.zeros(n,n)\\nlines_per_chunk = m\\nn_chunks = math.ceil(float(n)/m)\\nfor i in xrange(n_chunks):\\n    for j in xrange(n_chunks):\\n        chunk_i = (function of your choice to read lines i*lines_per_chunk to (i+1)*lines_per_chunk)\\n        chunk_j = (function of your choice to read lines j*lines_per_chunk to (j+1)*lines_per_chunk)\\n        similarity[i*lines_per_chunk:(i+1)*lines_per_chunk,\\n                   j*lines_per_chunk:(j+1)*lines_per_chunk] = fast_operation(chunk_i, chunk_j) \\n</code>', '<code>with open(file_name, \"rU\") as read_file:\\n    for i, row in enumerate(read_file, 1):\\n        #do something\\n        #i in line of that line\\n        #row containts all data of that line\\n</code>']",
         "title": "How to read large file, line by line in python",
         "_childDocuments_": [
            {
               "up_vote_count": 90,
               "answer_id": 8009942,
               "last_activity_date": 1497593155,
               "path": "3.stack.answer",
               "body_markdown": "Two memory efficient ways in ranked order (first is best) -\r\n\r\n 1. use of `with` - supported from python 2.5 and above\r\n 2. use of `yield` if you really want to have control over how much to read\r\n\r\n## 1. use of `with` ##\r\n`with` is the nice and efficient pythonic way to read large files. advantages - 1) file object is automatically closed after exiting from `with` execution block. 2) exception handling inside the `with` block. 3) memory `for` loop iterates through the `f` file object line by line. internally it does buffered IO (to optimized on costly IO operations) and memory management. \r\n\r\n    with open(&quot;x.txt&quot;) as f:\r\n        for line in f:\r\n            do something with data\r\n\r\n## 2. use of `yield` ##\r\nSometimes one might want more fine-grained control over how much to read in each iteration. In that case use [iter][1] &amp; [yield][2]. Note with this method one explicitly needs close the file at the end.\r\n\r\n    def readInChunks(fileObj, chunkSize=2048):\r\n        &quot;&quot;&quot;\r\n        Lazy function to read a file piece by piece.\r\n        Default chunk size: 2kB.\r\n        &quot;&quot;&quot;\r\n        while True:\r\n            data = fileObj.read(chunkSize)\r\n            if not data:\r\n                break\r\n            yield data\r\n\r\n    f = open(&#39;bigFile&#39;)\r\n    for chuck in readInChunks(f):\r\n        do_something(chunk)\r\n    f.close()\r\n\r\n----\r\n**Pitfalls and for the sake of completeness** - below methods are not as good or not as elegant for reading large files but please read to get rounded understanding.\r\n\r\nIn Python, the most common way to read lines from a file is to do the following: \r\n\r\n    for line in open(&#39;myfile&#39;,&#39;r&#39;).readlines():\r\n        do_something(line)\r\n\r\nWhen this is done, however, the `readlines()` function (same applies for `read()` function) loads the entire file into memory, then iterates over it. A slightly better approach (the first mentioned two methods are the best) for large files is to use the `fileinput` module, as follows:\r\n\r\n    import fileinput\r\n\r\n    for line in fileinput.input([&#39;myfile&#39;]):\r\n        do_something(line)\r\n\r\nthe `fileinput.input()` call reads lines sequentially, but doesn&#39;t keep them in memory after they&#39;ve been read or even simply so this, since `file` in python is iterable. \r\n\r\n## References ##\r\n\r\n 1. [Python with statement][3]\r\n\r\n\r\n  [1]: http://docs.python.org/library/functions.html#iter\r\n  [2]: http://docs.python.org/tutorial/classes.html#generators\r\n  [3]: http://effbot.org/zone/python-with-statement.htm",
               "tags": [],
               "creation_date": 1320413502,
               "last_edit_date": 1497593155,
               "is_accepted": false,
               "id": "8009942",
               "down_vote_count": 8,
               "score": 82
            },
            {
               "up_vote_count": 1,
               "answer_id": 8009952,
               "last_activity_date": 1403810263,
               "path": "3.stack.answer",
               "body_markdown": "From the python documentation for [fileinput][1].input():\r\n\r\n&gt; This iterates over the lines of all files listed in `sys.argv[1:]`, defaulting to `sys.stdin` if the list is empty\r\n\r\nfurther, the definition of the function is:\r\n\r\n    fileinput.FileInput([files[, inplace[, backup[, mode[, openhook]]]]])\r\n\r\nreading between the lines, this tells me that `files` can be a list so you could have something like:\r\n\r\n    for each_line in fileinput.input([input_file, input_file]):\r\n      do_something(each_line)\r\n\r\nSee [here][1] for more information\r\n\r\n\r\n  [1]: http://docs.python.org/library/fileinput.html",
               "tags": [],
               "creation_date": 1320413525,
               "last_edit_date": 1403810263,
               "is_accepted": false,
               "id": "8009952",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 15,
               "answer_id": 8009974,
               "last_activity_date": 1483546432,
               "path": "3.stack.answer",
               "body_markdown": "this is a possible way of reading a file in python:\r\n\r\n    f = open(input_file)\r\n    for line in f:\r\n        do_stuff(line)\r\n    f.close()\r\n\r\nit does not allocate a full list. It iterates over the lines.",
               "tags": [],
               "creation_date": 1320413617,
               "last_edit_date": 1483546432,
               "is_accepted": false,
               "id": "8009974",
               "down_vote_count": 1,
               "score": 14
            },
            {
               "up_vote_count": 1028,
               "answer_id": 8010133,
               "last_activity_date": 1321392933,
               "path": "3.stack.answer",
               "body_markdown": "Nobody has given the correct, fully Pythonic way to read a file. It&#39;s the following:\r\n\r\n    with open(...) as f:\r\n        for line in f:\r\n            &lt;do something with line&gt;\r\n\r\nThe `with` statement handles opening and closing the file, including if an exception is raised in the inner block. The `for line in f` treats the file object `f` as an iterable, which automatically uses buffered IO and memory management so you don&#39;t have to worry about large files.\r\n\r\n&gt; There should be one -- and preferably only one -- obvious way to do it.",
               "tags": [],
               "creation_date": 1320414404,
               "last_edit_date": 1321392933,
               "is_accepted": true,
               "id": "8010133",
               "down_vote_count": 2,
               "score": 1026
            },
            {
               "up_vote_count": 4,
               "answer_id": 8010456,
               "last_activity_date": 1321464785,
               "path": "3.stack.answer",
               "body_markdown": "Katrielalex provided the way to open &amp; read one file.\r\n\r\nHowever the way your algorithm goes it reads the whole file for each line of the file. That means the overall amount of reading a file - and computing the [Levenshtein distance](https://secure.wikimedia.org/wikipedia/en/wiki/Levenshtein_distance) - will be done N*N if N is the amount of lines in the file. Since you&#39;re concerned about file size and don&#39;t want to keep it in memory, I am concerned about the resulting [quadratic runtime](https://secure.wikimedia.org/wikipedia/en/wiki/Big_O_notation#Orders_of_common_functions). Your algorithm is in the O(n^2) class of algorithms which often can be improved with specialization.\r\n\r\nI suspect that you already know the tradeoff of memory versus runtime here, but maybe you would want to investigate if there&#39;s an efficient way to compute multiple Levenshtein distances in parallel. If so it would be interesting to share your solution here.\r\n\r\nHow many lines do your files have, and on what kind of machine (mem &amp; cpu power) does your algorithm have to run, and what&#39;s the tolerated runtime?\r\n\r\nCode would look like:\r\n\r\n    with f_outer as open(input_file, &#39;r&#39;):\r\n        for line_outer in f_outer:\r\n            with f_inner as open(input_file, &#39;r&#39;):\r\n                for line_inner in f_inner:\r\n                    compute_distance(line_outer, line_inner)\r\n\r\nBut the questions are how do you store the distances (matrix?) and can you gain an advantage of preparing e.g. the outer_line for processing, or caching some intermediate results for reuse.\r\n\r\n",
               "tags": [],
               "creation_date": 1320415754,
               "last_edit_date": 1321464785,
               "is_accepted": false,
               "id": "8010456",
               "down_vote_count": 1,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 26432277,
               "is_accepted": false,
               "last_activity_date": 1413574751,
               "body_markdown": "I would strongly recommend not using the default file loading as it is horrendously slow. You should look into the numpy functions and the IOpro functions (e.g. numpy.loadtxt()).\r\n\r\nhttp://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html\r\n\r\nhttps://store.continuum.io/cshop/iopro/\r\n\r\nThen you can break your pairwise operation into chunks:\r\n\r\n    import numpy as np\r\n    import math\r\n\r\n    lines_total = n    \r\n    similarity = np.zeros(n,n)\r\n    lines_per_chunk = m\r\n    n_chunks = math.ceil(float(n)/m)\r\n    for i in xrange(n_chunks):\r\n        for j in xrange(n_chunks):\r\n            chunk_i = (function of your choice to read lines i*lines_per_chunk to (i+1)*lines_per_chunk)\r\n            chunk_j = (function of your choice to read lines j*lines_per_chunk to (j+1)*lines_per_chunk)\r\n            similarity[i*lines_per_chunk:(i+1)*lines_per_chunk,\r\n                       j*lines_per_chunk:(j+1)*lines_per_chunk] = fast_operation(chunk_i, chunk_j) \r\n\r\nIt&#39;s almost always much faster to load data in chunks and then do matrix operations on it than to do it element by element!!",
               "id": "26432277",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1413574751,
               "score": 1
            },
            {
               "up_vote_count": 22,
               "answer_id": 32589529,
               "last_activity_date": 1512485308,
               "path": "3.stack.answer",
               "body_markdown": "To strip newlines:\r\n------------------\r\n\r\n    with open(file_path, &#39;rU&#39;) as f:\r\n        for line_terminated in f:\r\n            line = line_terminated.rstrip(&#39;\\n&#39;)\r\n            ...\r\n\r\nWith [universal newline support](https://docs.python.org/2/library/functions.html#open) all text file lines will seem to be terminated with `&#39;\\n&#39;`, whatever the terminators in the file, `&#39;\\r&#39;`, `&#39;\\n&#39;`, or `&#39;\\r\\n&#39;`. \r\n\r\n**EDIT -** To specify universal newline support:\r\n\r\n * Python 2 on Unix - `open(file_path, mode=&#39;rU&#39;)` - required &lt;sup&gt;[thanks [@Dave](https://stackoverflow.com/a/32589529/673991#comment82243835_32589529)]&lt;/sup&gt;\r\n * Python 2 on Windows - `open(file_path, mode=&#39;rU&#39;)` - optional\r\n * Python 3 - `open(file_path, newline=None)` - optional\r\n\r\nThe `newline` parameter is only supported in Python 3 and defaults to `None`. The `mode` parameter defaults to `&#39;r&#39;` in all cases. The `U` is deprecated in Python 3. In Python 2 on Windows some other mechanism appears to translate `\\r\\n` to `\\n`.\r\n\r\nDocs:\r\n\r\n * [open() for Python 2](https://docs.python.org/2/library/functions.html#open)\r\n * [open() for Python 3](https://docs.python.org/3/library/functions.html#open)\r\n\r\nTo preserve native line terminators:\r\n------------------------------------\r\n\r\n    with open(file_path, &#39;rb&#39;) as f:\r\n        with line_native_terminated in f:\r\n            ...\r\n\r\nBinary mode can still parse the file into lines with `in`.  Each line will have whatever terminators it has in the file.\r\n\r\n*Thanks to [@katrielalex][1]&#39;s [answer][2], Python&#39;s [open()](https://docs.python.org/2/library/functions.html#open) doc, and [iPython](http://ipython.org/) experiments.*\r\n\r\n\r\n  [1]: https://stackoverflow.com/users/398968/katrielalex\r\n  [2]: https://stackoverflow.com/a/8010133/673991",
               "tags": [],
               "creation_date": 1442329672,
               "last_edit_date": 1512485308,
               "is_accepted": false,
               "id": "32589529",
               "down_vote_count": 0,
               "score": 22
            },
            {
               "up_vote_count": 2,
               "answer_id": 38669685,
               "last_activity_date": 1479992050,
               "path": "3.stack.answer",
               "body_markdown": "    #Using a text file for the example\r\n    with open(&quot;yourFile.txt&quot;,&quot;r&quot;) as f:\r\n        text = f.readlines()\r\n    for line in text:\r\n        print line\r\n\r\n- Open your file for reading (r)\r\n- Read the whole file and save each line into a **list** (text)\r\n- Loop through the list printing each line.\r\n\r\n*If you want, for example, to check a specific line for a length greater than 10, work with what you already have available.*\r\n\r\n    for line in text:\r\n    \tif len(line) &gt; 10:\r\n    \t\tprint line\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1469844061,
               "last_edit_date": 1479992050,
               "is_accepted": false,
               "id": "38669685",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 42007811,
               "is_accepted": false,
               "last_activity_date": 1486054112,
               "body_markdown": "Some context up front as to where I am coming from. Code snippets are at the end.\r\n\r\nWhen I can, I prefer to use an open source tool like H2O to do super high performance parallel CSV file reads, but this tool is limited in feature set. I end up writing a lot of code to create data science pipelines before feeding to H2O cluster for the supervised learning proper.\r\n\r\nI have been reading files like 8GB HIGGS dataset from UCI repo and even 40GB CSV files for data science purposes significantly faster by adding lots of parallelism with the multiprocessing library&#39;s pool object and map function. For example clustering with nearest neighbor searches and also DBSCAN and Markov clustering algorithms requires some parallel programming finesse to bypass some seriously challenging memory and wall clock time problems. \r\n\r\nI usually like to break the file row-wise into parts using gnu tools first and then glob-filemask them all to find and read them in parallel in the python program. I use something like 1000+ partial files commonly. Doing these tricks helps immensely with processing speed and memory limits.  \r\n\r\nThe pandas dataframe.read_csv is single threaded so you can do these tricks to make pandas quite faster by running a map() for parallel execution.  You can use htop to see that with plain old sequential pandas dataframe.read_csv, 100% cpu on just one core is the actual bottleneck in pd.read_csv, not the disk at all.\r\n\r\nI should add I&#39;m using an SSD on fast video card bus, not a spinning HD on SATA6 bus, plus 16 CPU cores.\r\n\r\nAlso, another technique that I discovered works great in some applications is parallel CSV file reads all within one giant file, starting each worker at different offset into the file, rather than pre-splitting one big file into many part files. Use python&#39;s file seek() and tell() in each parallel worker to read the big text file in strips, at different byte offset start-byte and end-byte locations in the big file, all at the same time concurrently. You can do a regex findall on the bytes, and return the count of linefeeds. This is a partial sum.  Finally sum up the partial sums to get the global sum when the map function returns after the workers finished.\r\n\r\nFollowing is some example benchmarks using the parallel byte offset trick:\r\n\r\nI use 2 files: HIGGS.csv is 8 GB. It is from the UCI machine learning repository.  all_bin .csv is 40.4 GB and is from my current project.\r\nI use 2 programs: GNU wc program which comes with Linux, and the pure python fastread.py program which I developed.\r\n\r\n    HP-Z820:/mnt/fastssd/fast_file_reader$ ls -l /mnt/fastssd/nzv/HIGGS.csv\r\n    -rw-rw-r-- 1 8035497980 Jan 24 16:00 /mnt/fastssd/nzv/HIGGS.csv\r\n    \r\n    HP-Z820:/mnt/fastssd$ ls -l all_bin.csv\r\n    -rw-rw-r-- 1 40412077758 Feb  2 09:00 all_bin.csv\r\n    \r\n    ga@ga-HP-Z820:/mnt/fastssd$ time python fastread.py --fileName=&quot;all_bin.csv&quot; --numProcesses=32 --balanceFactor=2\r\n    2367496\r\n    \r\n    real\t0m8.920s\r\n    user\t1m30.056s\r\n    sys\t2m38.744s\r\n    \r\n    In [1]: 40412077758. / 8.92\r\n    Out[1]: 4530501990.807175\r\n\r\nThat\u2019s some 4.5 GB/s, or 45 Gb/s, file slurping speed.  That ain\u2019t no spinning hard disk, my friend. That\u2019s actually a Samsung Pro 950 SSD. \r\n\r\nBelow is the speed benchmark for the same file being line-counted by gnu wc, a pure C compiled program.\r\n\r\nWhat is cool is you can see my pure python program essentially matched the speed of the gnu wc compiled C program in this case.  Python is interpreted but C is compiled, so this is a pretty interesting feat of speed, I think you would agree.  Of course, wc really needs to be changed to a parallel program, and then it would really beat the socks off my python program. But as it stands today, gnu wc is just a sequential program.  You do what you can, and python can do parallel today. Cython compiling might be able to help me (for some other time). Also memory mapped files was not explored yet.\r\n\r\n\r\n    HP-Z820:/mnt/fastssd$ time wc -l all_bin.csv\r\n    2367496 all_bin.csv\r\n    \r\n    real\t0m8.807s\r\n    user\t0m1.168s\r\n    sys\t0m7.636s\r\n    \r\n    \r\n    HP-Z820:/mnt/fastssd/fast_file_reader$ time python fastread.py --fileName=&quot;HIGGS.csv&quot; --numProcesses=16 --balanceFactor=2\r\n    11000000\r\n    \r\n    real\t0m2.257s\r\n    user\t0m12.088s\r\n    sys\t0m20.512s\r\n    \r\n    HP-Z820:/mnt/fastssd/fast_file_reader$ time wc -l HIGGS.csv\r\n    11000000 HIGGS.csv\r\n    \r\n    real\t0m1.820s\r\n    user\t0m0.364s\r\n    sys\t0m1.456s\r\n\r\nConclusion: The speed is good for a pure python program compared to a C program. However, it\u2019s not good enough to use the pure python program over the C program, at least for linecounting purpose. Generally the technique can be used for other file processing, so this python code is still good.\r\n\r\nQuestion: Does compiling the regex just one time and passing it to all workers will improve speed? Answer: Regex pre-compiling does NOT help in this application. I suppose the reason is that the overhead of process serialization and creation for all the workers is dominating.\r\n\r\nOne more thing. \r\nDoes parallel CSV file reading even help?  Is the disk the bottleneck, or is it the CPU?  Many so-called top-rated answers on stackoverflow contain the common dev wisdom that you only need one thread to read a file, best you can do, they say. Are they sure, though?\r\n\r\nLet\u2019s find out:\r\n\r\n    HP-Z820:/mnt/fastssd/fast_file_reader$ time python fastread.py --fileName=&quot;HIGGS.csv&quot; --numProcesses=16 --balanceFactor=2\r\n    11000000\r\n    \r\n    real\t0m2.256s\r\n    user\t0m10.696s\r\n    sys\t0m19.952s\r\n    \r\n    HP-Z820:/mnt/fastssd/fast_file_reader$ time python fastread.py --fileName=&quot;HIGGS.csv&quot; --numProcesses=1 --balanceFactor=1\r\n    11000000\r\n    \r\n    real\t0m17.380s\r\n    user\t0m11.124s\r\n    sys\t0m6.272s\r\n\r\nOh yes, yes it does. Parallel file reading works quite well.  Well there you go!\r\n\r\nPs. In case some of you wanted to know, what if the balanceFactor was 2 when using a single worker process? Well, it\u2019s horrible:\r\n\r\n    HP-Z820:/mnt/fastssd/fast_file_reader$ time python fastread.py --fileName=&quot;HIGGS.csv&quot; --numProcesses=1 --balanceFactor=2\r\n    11000000\r\n    \r\n    real\t1m37.077s\r\n    user\t0m12.432s\r\n    sys\t1m24.700s\r\n\r\n\r\nKey parts of the fastread.py python program:\r\n\r\n\tfileBytes = stat(fileName).st_size  # Read quickly from OS how many bytes are in a text file\r\n\tstartByte, endByte = PartitionDataToWorkers(workers=numProcesses, items=fileBytes, balanceFactor=balanceFactor)\r\n\tp = Pool(numProcesses)\r\n\tpartialSum = p.starmap(ReadFileSegment, zip(startByte, endByte, repeat(fileName))) # startByte is already a list. fileName is made into a same-length list of duplicates values.\r\n\tglobalSum = sum(partialSum)\r\n\tprint(globalSum)\r\n\r\n\r\n\tdef ReadFileSegment(startByte, endByte, fileName, searchChar=&#39;\\n&#39;):  # counts number of searchChar appearing in the byte range\r\n\t\twith open(fileName, &#39;r&#39;) as f:\r\n\t\t\tf.seek(startByte-1)  # seek is initially at byte 0 and then moves forward the specified amount, so seek(5) points at the 6th byte.\r\n\t\t\tbytes = f.read(endByte - startByte + 1)\r\n\t\t\tcnt = len(re.findall(searchChar, bytes)) # findall with implicit compiling runs just as fast here as re.compile once + re.finditer many times.\r\n\t\treturn cnt\r\n\r\nThe def for PartitionDataToWorkers is just ordinary sequential code. I left it out in case someone else wants to get some practice on what parallel programming is like. I gave away for free the harder parts: the tested and working parallel code, for your learning benefit.\r\n\r\nThanks to:  The open-source H2O project, by Arno and Cliff and the H2O staff for their great software and instructional videos, which have provided me the inspiration for this pure python high performance parallel byte offset reader as shown above.  H2O does parallel file reading using java, is callable by python and R programs, and is crazy fast, faster than anything on the planet at reading big CSV files.",
               "id": "42007811",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1486054112,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 45855005,
               "is_accepted": false,
               "last_activity_date": 1503558133,
               "body_markdown": "Best way to read large file, line by line is to use python **enumerate** function\r\n \r\n\r\n    with open(file_name, &quot;rU&quot;) as read_file:\r\n        for i, row in enumerate(read_file, 1):\r\n            #do something\r\n            #i in line of that line\r\n            #row containts all data of that line",
               "id": "45855005",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1503558133,
               "score": -2
            }
         ],
         "link": "https://stackoverflow.com/questions/8009882/how-to-read-large-file-line-by-line-in-python",
         "id": "858127-2294"
      },
      {
         "up_vote_count": "845",
         "path": "2.stack",
         "body_markdown": "Given two data frames:\r\n\r\n    df1 = data.frame(CustomerId = c(1:6), Product = c(rep(&quot;Toaster&quot;, 3), rep(&quot;Radio&quot;, 3)))\r\n    df2 = data.frame(CustomerId = c(2, 4, 6), State = c(rep(&quot;Alabama&quot;, 2), rep(&quot;Ohio&quot;, 1)))\r\n\r\n    df1\r\n    #  CustomerId Product\r\n    #           1 Toaster\r\n    #           2 Toaster\r\n    #           3 Toaster\r\n    #           4   Radio\r\n    #           5   Radio\r\n    #           6   Radio\r\n    \r\n    df2\r\n    #  CustomerId   State\r\n    #           2 Alabama\r\n    #           4 Alabama\r\n    #           6    Ohio\r\n\r\nHow can I do database style, i.e., [sql style, joins](http://en.wikipedia.org/wiki/Join_%28SQL%29)? That is, how do I get:\r\n\r\n - An [inner join](http://en.wikipedia.org/wiki/Join_%28SQL%29#Inner_join) of `df1` and `df2`:  \r\nReturn only the rows in which the left table have matching keys in the right table.\r\n - An [outer join](http://en.wikipedia.org/wiki/Join_%28SQL%29#Outer_join) of `df1` and `df2`:    \r\nReturns all rows from both tables, join records from the left which have matching keys in the right table.\r\n - A [left outer join (or simply left join)](http://en.wikipedia.org/wiki/Join_%28SQL%29#Left_outer_join) of `df1` and `df2`   \r\nReturn all rows from the left table, and any rows with matching keys from the right table.\r\n - A [right outer join](http://en.wikipedia.org/wiki/Join_%28SQL%29#Right_outer_join) of `df1` and `df2`    \r\nReturn all rows from the right table, and any rows with matching keys from the left table.\r\n\r\nExtra credit:\r\n\r\nHow can I do a SQL style select statement?",
         "view_count": "657291",
         "answer_count": "12",
         "tags": "['r', 'join', 'merge', 'dataframe', 'r-faq']",
         "creation_date": "1250687891",
         "last_edit_date": "1490199290",
         "code_snippet": "['<code>df1 = data.frame(CustomerId = c(1:6), Product = c(rep(\"Toaster\", 3), rep(\"Radio\", 3)))\\ndf2 = data.frame(CustomerId = c(2, 4, 6), State = c(rep(\"Alabama\", 2), rep(\"Ohio\", 1)))\\n\\ndf1\\n#  CustomerId Product\\n#           1 Toaster\\n#           2 Toaster\\n#           3 Toaster\\n#           4   Radio\\n#           5   Radio\\n#           6   Radio\\n\\ndf2\\n#  CustomerId   State\\n#           2 Alabama\\n#           4 Alabama\\n#           6    Ohio\\n</code>', '<code>df1</code>', '<code>df2</code>', '<code>df1</code>', '<code>df2</code>', '<code>df1</code>', '<code>df2</code>', '<code>df1</code>', '<code>df2</code>', '<code>merge</code>', '<code>merge(df1, df2)</code>', '<code>merge(df1, df2, by = \"CustomerId\")</code>', '<code>by.x</code>', '<code>by.y</code>', '<code>merge(x = df1, y = df2, by = \"CustomerId\", all = TRUE)</code>', '<code>merge(x = df1, y = df2, by = \"CustomerId\", all.x = TRUE)</code>', '<code>merge(x = df1, y = df2, by = \"CustomerId\", all.y = TRUE)</code>', '<code>merge(x = df1, y = df2, by = NULL)</code>', '<code>data.table</code>', '<code>merge(x=df1,y=df2, by.x=c(\"x_col1\",\"x_col2\"), by.y=c(\"y_col1\",\"y_col2\"))</code>', '<code>data.table</code>', '<code>library(sqldf)\\n\\n## inner join\\ndf3 &lt;- sqldf(\"SELECT CustomerId, Product, State \\n              FROM df1\\n              JOIN df2 USING(CustomerID)\")\\n\\n## left join (substitute \\'right\\' for right join)\\ndf4 &lt;- sqldf(\"SELECT CustomerId, Product, State \\n              FROM df1\\n              LEFT JOIN df2 USING(CustomerID)\")\\n</code>', '<code>library(data.table)\\n\\ndt1 &lt;- data.table(df1, key = \"CustomerId\") \\ndt2 &lt;- data.table(df2, key = \"CustomerId\")\\n\\njoined.dt1.dt.2 &lt;- dt1[dt2]\\n</code>', '<code>merge</code>', '<code>merge.data.table</code>', '<code>merge(dt1, dt2)\\n</code>', '<code>join</code>', '<code>library(plyr)\\n\\njoin(df1, df2,\\n     type = \"inner\")\\n\\n#   CustomerId Product   State\\n# 1          2 Toaster Alabama\\n# 2          4   Radio Alabama\\n# 3          6   Radio    Ohio\\n</code>', '<code>type</code>', '<code>inner</code>', '<code>left</code>', '<code>right</code>', '<code>full</code>', '<code>?join</code>', '<code>merge</code>', '<code>join</code>', '<code>plyr::join</code>', '<code>merge</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>data.table</code>', '<code>nomatch = 0L</code>', '<code>library(dplyr)\\n\\n#make sure that CustomerId cols are both type numeric\\n#they ARE not using the provided code in question and dplyr will complain\\ndf1$CustomerId &lt;- as.numeric(df1$CustomerId)\\ndf2$CustomerId &lt;- as.numeric(df2$CustomerId)\\n</code>', '<code>#inner\\ninner_join(df1, df2)\\n\\n#left outer\\nleft_join(df1, df2)\\n\\n#right outer\\nright_join(df1, df2)\\n\\n#alternate right outer\\nleft_join(df2, df1)\\n\\n#full join\\nfull_join(df1, df2)\\n</code>', '<code>semi_join(df1, df2) #keep only observations in df1 that match in df2.\\nanti_join(df1, df2) #drops all observations in df1 that match in df2.\\n</code>', '<code>CustomerId</code>', '<code>plyr</code>', '<code>dplyr</code>', '<code>character</code>', '<code>plyr</code>', '<code>merge(df1,df2)\\n</code>', '<code>merge(df1,df2, all=TRUE)\\n</code>', '<code>merge(df1,df2, all.x=TRUE)\\n</code>', '<code>merge(df1,df2, all.y=TRUE)\\n</code>', '<code>df1[,\"State\"]&lt;-df2[df1[ ,\"Product\"], \"State\"]\\n</code>', '<code>dplyr</code>', '<code>inner_join(x, y, by = NULL, copy = FALSE, ...)</code>', '<code>left_join(x, y, by = NULL, copy = FALSE, ...)</code>', '<code>semi_join(x, y, by = NULL, copy = FALSE, ...)</code>', '<code>anti_join(x, y, by = NULL, copy = FALSE, ...)</code>', '<code>select(df,\"column\")</code>', '<code>sql()</code>', '<code>sql(\"SELECT * FROM hflights\")</code>', '<code>[.data.table</code>', '<code>merge</code>', '<code>df1 = data.frame(CustomerId = c(1:6), Product = c(rep(\"Toaster\", 3), rep(\"Radio\", 3)))\\ndf2 = data.frame(CustomerId = c(2L, 4L, 7L), State = c(rep(\"Alabama\", 2), rep(\"Ohio\", 1))) # one value changed to show full outer join\\n\\nlibrary(data.table)\\n\\ndt1 = as.data.table(df1)\\ndt2 = as.data.table(df2)\\nsetkey(dt1, CustomerId)\\nsetkey(dt2, CustomerId)\\n# right outer join keyed data.tables\\ndt1[dt2]\\n\\nsetkey(dt1, NULL)\\nsetkey(dt2, NULL)\\n# right outer join unkeyed data.tables - use `on` argument\\ndt1[dt2, on = \"CustomerId\"]\\n\\n# left outer join - swap dt1 with dt2\\ndt2[dt1, on = \"CustomerId\"]\\n\\n# inner join - use `nomatch` argument\\ndt1[dt2, nomatch=0L, on = \"CustomerId\"]\\n\\n# anti join - use `!` operator\\ndt1[!dt2, on = \"CustomerId\"]\\n\\n# inner join\\nmerge(dt1, dt2, by = \"CustomerId\")\\n\\n# full outer join\\nmerge(dt1, dt2, by = \"CustomerId\", all = TRUE)\\n\\n# see ?merge.data.table arguments for other cases\\n</code>', '<code>library(microbenchmark)\\nlibrary(sqldf)\\nlibrary(dplyr)\\nlibrary(data.table)\\n\\nn = 5e6\\nset.seed(123)\\ndf1 = data.frame(x=sample(n,n-1L), y1=rnorm(n-1L))\\ndf2 = data.frame(x=sample(n,n-1L), y2=rnorm(n-1L))\\ndt1 = as.data.table(df1)\\ndt2 = as.data.table(df2)\\n\\n# inner join\\nmicrobenchmark(times = 10L,\\n               base = merge(df1, df2, by = \"x\"),\\n               sqldf = sqldf(\"SELECT * FROM df1 INNER JOIN df2 ON df1.x = df2.x\"),\\n               dplyr = inner_join(df1, df2, by = \"x\"),\\n               data.table = dt1[dt2, nomatch = 0L, on = \"x\"])\\n#Unit: milliseconds\\n#       expr        min         lq      mean     median        uq       max neval\\n#       base 15546.0097 16083.4915 16687.117 16539.0148 17388.290 18513.216    10\\n#      sqldf 44392.6685 44709.7128 45096.401 45067.7461 45504.376 45563.472    10\\n#      dplyr  4124.0068  4248.7758  4281.122  4272.3619  4342.829  4411.388    10\\n# data.table   937.2461   946.0227  1053.411   973.0805  1214.300  1281.958    10\\n\\n# left outer join\\nmicrobenchmark(times = 10L,\\n               base = merge(df1, df2, by = \"x\", all.x = TRUE),\\n               sqldf = sqldf(\"SELECT * FROM df1 LEFT OUTER JOIN df2 ON df1.x = df2.x\"),\\n               dplyr = left_join(df1, df2, by = c(\"x\"=\"x\")),\\n               data.table = dt2[dt1, on = \"x\"])\\n#Unit: milliseconds\\n#       expr       min         lq       mean     median         uq       max neval\\n#       base 16140.791 17107.7366 17441.9538 17414.6263 17821.9035 19453.034    10\\n#      sqldf 43656.633 44141.9186 44777.1872 44498.7191 45288.7406 47108.900    10\\n#      dplyr  4062.153  4352.8021  4780.3221  4409.1186  4450.9301  8385.050    10\\n# data.table   823.218   823.5557   901.0383   837.9206   883.3292  1277.239    10\\n\\n# right outer join\\nmicrobenchmark(times = 10L,\\n               base = merge(df1, df2, by = \"x\", all.y = TRUE),\\n               sqldf = sqldf(\"SELECT * FROM df2 LEFT OUTER JOIN df1 ON df2.x = df1.x\"),\\n               dplyr = right_join(df1, df2, by = \"x\"),\\n               data.table = dt1[dt2, on = \"x\"])\\n#Unit: milliseconds\\n#       expr        min         lq       mean     median        uq       max neval\\n#       base 15821.3351 15954.9927 16347.3093 16044.3500 16621.887 17604.794    10\\n#      sqldf 43635.5308 43761.3532 43984.3682 43969.0081 44044.461 44499.891    10\\n#      dplyr  3936.0329  4028.1239  4102.4167  4045.0854  4219.958  4307.350    10\\n# data.table   820.8535   835.9101   918.5243   887.0207  1005.721  1068.919    10\\n\\n# full outer join\\nmicrobenchmark(times = 10L,\\n               base = merge(df1, df2, by = \"x\", all = TRUE),\\n               #sqldf = sqldf(\"SELECT * FROM df1 FULL OUTER JOIN df2 ON df1.x = df2.x\"), # not supported\\n               dplyr = full_join(df1, df2, by = \"x\"),\\n               data.table = merge(dt1, dt2, by = \"x\", all = TRUE))\\n#Unit: seconds\\n#       expr       min        lq      mean    median        uq       max neval\\n#       base 16.176423 16.908908 17.485457 17.364857 18.271790 18.626762    10\\n#      dplyr  7.610498  7.666426  7.745850  7.710638  7.832125  7.951426    10\\n# data.table  2.052590  2.130317  2.352626  2.208913  2.470721  2.951948    10\\n</code>', '<code>on = </code>', '<code>on</code>', '<code>merge.data.table</code>', '<code>sort = TRUE</code>', '<code>data.table</code>', '<code>full_join</code>', '<code>right_join</code>', '<code>dplyr</code>', '<code>dplyr</code>', '<code>merge(..., all.x = TRUE, all.y = TRUE)</code>', '<code>dplyr::full_join()</code>', '<code>0..*:0..1</code>', '<code>0..1:0..*</code>', '<code>0..1</code>', '<code>0..*</code>', '<code>match()</code>', '<code>df2</code>', '<code>df1</code>', '<code>df2</code>', \"<code>df1 &lt;- data.frame(CustomerId=1:6,Product=c(rep('Toaster',3L),rep('Radio',3L)));\\ndf2 &lt;- data.frame(CustomerId=c(2L,4L,6L,7L),State=c(rep('Alabama',2L),'Ohio','Texas'));\\ndf1[names(df2)[-1L]] &lt;- df2[match(df1[,1L],df2[,1L]),-1L];\\ndf1;\\n##   CustomerId Product   State\\n## 1          1 Toaster    &lt;NA&gt;\\n## 2          2 Toaster Alabama\\n## 3          3 Toaster    &lt;NA&gt;\\n## 4          4   Radio Alabama\\n## 5          5   Radio    &lt;NA&gt;\\n## 6          6   Radio    Ohio\\n</code>\", '<code>match(interaction(df1$a,df1$b),interaction(df2$a,df2$b))</code>', '<code>paste()</code>', '<code>outer(df1$a,df2$a,`==`) &amp; outer(df1$b,df2$b,`==`)</code>', '<code>merge()</code>', \"<code>library(microbenchmark);\\nlibrary(data.table);\\nlibrary(sqldf);\\nlibrary(plyr);\\nlibrary(dplyr);\\n\\nsolSpecs &lt;- list(\\n    merge=list(testFuncs=list(\\n        inner=function(df1,df2,key) merge(df1,df2,key),\\n        left =function(df1,df2,key) merge(df1,df2,key,all.x=T),\\n        right=function(df1,df2,key) merge(df1,df2,key,all.y=T),\\n        full =function(df1,df2,key) merge(df1,df2,key,all=T)\\n    )),\\n    data.table.unkeyed=list(argSpec='data.table.unkeyed',testFuncs=list(\\n        inner=function(dt1,dt2,key) dt1[dt2,on=key,nomatch=0L,allow.cartesian=T],\\n        left =function(dt1,dt2,key) dt2[dt1,on=key,allow.cartesian=T],\\n        right=function(dt1,dt2,key) dt1[dt2,on=key,allow.cartesian=T],\\n        full =function(dt1,dt2,key) merge(dt1,dt2,key,all=T,allow.cartesian=T) ## calls merge.data.table()\\n    )),\\n    data.table.keyed=list(argSpec='data.table.keyed',testFuncs=list(\\n        inner=function(dt1,dt2) dt1[dt2,nomatch=0L,allow.cartesian=T],\\n        left =function(dt1,dt2) dt2[dt1,allow.cartesian=T],\\n        right=function(dt1,dt2) dt1[dt2,allow.cartesian=T],\\n        full =function(dt1,dt2) merge(dt1,dt2,all=T,allow.cartesian=T) ## calls merge.data.table()\\n    )),\\n    sqldf.unindexed=list(testFuncs=list( ## note: must pass connection=NULL to avoid running against the live DB connection, which would result in collisions with the residual tables from the last query upload\\n        inner=function(df1,df2,key) sqldf(paste0('select * from df1 inner join df2 using(',paste(collapse=',',key),')'),connection=NULL),\\n        left =function(df1,df2,key) sqldf(paste0('select * from df1 left join df2 using(',paste(collapse=',',key),')'),connection=NULL),\\n        right=function(df1,df2,key) sqldf(paste0('select * from df2 left join df1 using(',paste(collapse=',',key),')'),connection=NULL) ## can't do right join proper, not yet supported; inverted left join is equivalent\\n        ##full =function(df1,df2,key) sqldf(paste0('select * from df1 full join df2 using(',paste(collapse=',',key),')'),connection=NULL) ## can't do full join proper, not yet supported; possible to hack it with a union of left joins, but too unreasonable to include in testing\\n    )),\\n    sqldf.indexed=list(testFuncs=list( ## important: requires an active DB connection with preindexed main.df1 and main.df2 ready to go; arguments are actually ignored\\n        inner=function(df1,df2,key) sqldf(paste0('select * from main.df1 inner join main.df2 using(',paste(collapse=',',key),')')),\\n        left =function(df1,df2,key) sqldf(paste0('select * from main.df1 left join main.df2 using(',paste(collapse=',',key),')')),\\n        right=function(df1,df2,key) sqldf(paste0('select * from main.df2 left join main.df1 using(',paste(collapse=',',key),')')) ## can't do right join proper, not yet supported; inverted left join is equivalent\\n        ##full =function(df1,df2,key) sqldf(paste0('select * from main.df1 full join main.df2 using(',paste(collapse=',',key),')')) ## can't do full join proper, not yet supported; possible to hack it with a union of left joins, but too unreasonable to include in testing\\n    )),\\n    plyr=list(testFuncs=list(\\n        inner=function(df1,df2,key) join(df1,df2,key,'inner'),\\n        left =function(df1,df2,key) join(df1,df2,key,'left'),\\n        right=function(df1,df2,key) join(df1,df2,key,'right'),\\n        full =function(df1,df2,key) join(df1,df2,key,'full')\\n    )),\\n    dplyr=list(testFuncs=list(\\n        inner=function(df1,df2,key) inner_join(df1,df2,key),\\n        left =function(df1,df2,key) left_join(df1,df2,key),\\n        right=function(df1,df2,key) right_join(df1,df2,key),\\n        full =function(df1,df2,key) full_join(df1,df2,key)\\n    )),\\n    in.place=list(testFuncs=list(\\n        left =function(df1,df2,key) { cns &lt;- setdiff(names(df2),key); df1[cns] &lt;- df2[match(df1[,key],df2[,key]),cns]; df1; },\\n        right=function(df1,df2,key) { cns &lt;- setdiff(names(df1),key); df2[cns] &lt;- df1[match(df2[,key],df1[,key]),cns]; df2; }\\n    ))\\n);\\n\\ngetSolTypes &lt;- function() names(solSpecs);\\ngetJoinTypes &lt;- function() unique(unlist(lapply(solSpecs,function(x) names(x$testFuncs))));\\ngetArgSpec &lt;- function(argSpecs,key=NULL) if (is.null(key)) argSpecs$default else argSpecs[[key]];\\n\\ninitSqldf &lt;- function() {\\n    sqldf(); ## creates sqlite connection on first run, cleans up and closes existing connection otherwise\\n    if (exists('sqldfInitFlag',envir=globalenv(),inherits=F) &amp;&amp; sqldfInitFlag) { ## false only on first run\\n        sqldf(); ## creates a new connection\\n    } else {\\n        assign('sqldfInitFlag',T,envir=globalenv()); ## set to true for the one and only time\\n    }; ## end if\\n    invisible();\\n}; ## end initSqldf()\\n\\nsetUpBenchmarkCall &lt;- function(argSpecs,joinType,solTypes=getSolTypes(),env=parent.frame()) {\\n    ## builds and returns a list of expressions suitable for passing to the list argument of microbenchmark(), and assigns variables to resolve symbol references in those expressions\\n    callExpressions &lt;- list();\\n    nms &lt;- character();\\n    for (solType in solTypes) {\\n        testFunc &lt;- solSpecs[[solType]]$testFuncs[[joinType]];\\n        if (is.null(testFunc)) next; ## this join type is not defined for this solution type\\n        testFuncName &lt;- paste0('tf.',solType);\\n        assign(testFuncName,testFunc,envir=env);\\n        argSpecKey &lt;- solSpecs[[solType]]$argSpec;\\n        argSpec &lt;- getArgSpec(argSpecs,argSpecKey);\\n        argList &lt;- setNames(nm=names(argSpec$args),vector('list',length(argSpec$args)));\\n        for (i in seq_along(argSpec$args)) {\\n            argName &lt;- paste0('tfa.',argSpecKey,i);\\n            assign(argName,argSpec$args[[i]],envir=env);\\n            argList[[i]] &lt;- if (i%in%argSpec$copySpec) call('copy',as.symbol(argName)) else as.symbol(argName);\\n        }; ## end for\\n        callExpressions[[length(callExpressions)+1L]] &lt;- do.call(call,c(list(testFuncName),argList),quote=T);\\n        nms[length(nms)+1L] &lt;- solType;\\n    }; ## end for\\n    names(callExpressions) &lt;- nms;\\n    callExpressions;\\n}; ## end setUpBenchmarkCall()\\n\\nharmonize &lt;- function(res) {\\n    res &lt;- as.data.frame(res); ## coerce to data.frame\\n    for (ci in which(sapply(res,is.factor))) res[[ci]] &lt;- as.character(res[[ci]]); ## coerce factor columns to character\\n    for (ci in which(sapply(res,is.logical))) res[[ci]] &lt;- as.integer(res[[ci]]); ## coerce logical columns to integer (works around sqldf quirk of munging logicals to integers)\\n    ##for (ci in which(sapply(res,inherits,'POSIXct'))) res[[ci]] &lt;- as.double(res[[ci]]); ## coerce POSIXct columns to double (works around sqldf quirk of losing POSIXct class) ----- POSIXct doesn't work at all in sqldf.indexed\\n    res &lt;- res[order(names(res))]; ## order columns\\n    res &lt;- res[do.call(order,res),]; ## order rows\\n    res;\\n}; ## end harmonize()\\n\\ncheckIdentical &lt;- function(argSpecs,solTypes=getSolTypes()) {\\n    for (joinType in getJoinTypes()) {\\n        callExpressions &lt;- setUpBenchmarkCall(argSpecs,joinType,solTypes);\\n        if (length(callExpressions)&lt;2L) next;\\n        ex &lt;- harmonize(eval(callExpressions[[1L]]));\\n        for (i in seq(2L,len=length(callExpressions)-1L)) {\\n            y &lt;- harmonize(eval(callExpressions[[i]]));\\n            if (!isTRUE(all.equal(ex,y,check.attributes=F))) {\\n                ex &lt;&lt;- ex;\\n                y &lt;&lt;- y;\\n                solType &lt;- names(callExpressions)[i];\\n                stop(paste0('non-identical: ',solType,' ',joinType,'.'));\\n            }; ## end if\\n        }; ## end for\\n    }; ## end for\\n    invisible();\\n}; ## end checkIdentical()\\n\\ntestJoinType &lt;- function(argSpecs,joinType,solTypes=getSolTypes(),metric=NULL,times=100L) {\\n    callExpressions &lt;- setUpBenchmarkCall(argSpecs,joinType,solTypes);\\n    bm &lt;- microbenchmark(list=callExpressions,times=times);\\n    if (is.null(metric)) return(bm);\\n    bm &lt;- summary(bm);\\n    res &lt;- setNames(nm=names(callExpressions),bm[[metric]]);\\n    attr(res,'unit') &lt;- attr(bm,'unit');\\n    res;\\n}; ## end testJoinType()\\n\\ntestAllJoinTypes &lt;- function(argSpecs,solTypes=getSolTypes(),metric=NULL,times=100L) {\\n    joinTypes &lt;- getJoinTypes();\\n    resList &lt;- setNames(nm=joinTypes,lapply(joinTypes,function(joinType) testJoinType(argSpecs,joinType,solTypes,metric,times)));\\n    if (is.null(metric)) return(resList);\\n    units &lt;- unname(unlist(lapply(resList,attr,'unit')));\\n    res &lt;- do.call(data.frame,c(list(join=joinTypes),setNames(nm=solTypes,rep(list(rep(NA_real_,length(joinTypes))),length(solTypes))),list(unit=units,stringsAsFactors=F)));\\n    for (i in seq_along(resList)) res[i,match(names(resList[[i]]),names(res))] &lt;- resList[[i]];\\n    res;\\n}; ## end testAllJoinTypes()\\n\\ntestGrid &lt;- function(makeArgSpecsFunc,sizes,overlaps,solTypes=getSolTypes(),joinTypes=getJoinTypes(),metric='median',times=100L) {\\n\\n    res &lt;- expand.grid(size=sizes,overlap=overlaps,joinType=joinTypes,stringsAsFactors=F);\\n    res[solTypes] &lt;- NA_real_;\\n    res$unit &lt;- NA_character_;\\n    for (ri in seq_len(nrow(res))) {\\n\\n        size &lt;- res$size[ri];\\n        overlap &lt;- res$overlap[ri];\\n        joinType &lt;- res$joinType[ri];\\n\\n        argSpecs &lt;- makeArgSpecsFunc(size,overlap);\\n\\n        checkIdentical(argSpecs,solTypes);\\n\\n        cur &lt;- testJoinType(argSpecs,joinType,solTypes,metric,times);\\n        res[ri,match(names(cur),names(res))] &lt;- cur;\\n        res$unit[ri] &lt;- attr(cur,'unit');\\n\\n    }; ## end for\\n\\n    res;\\n\\n}; ## end testGrid()\\n</code>\", \"<code>## OP's example, supplemented with a non-matching row in df2\\nargSpecs &lt;- list(\\n    default=list(copySpec=1:2,args=list(\\n        df1 &lt;- data.frame(CustomerId=1:6,Product=c(rep('Toaster',3L),rep('Radio',3L))),\\n        df2 &lt;- data.frame(CustomerId=c(2L,4L,6L,7L),State=c(rep('Alabama',2L),'Ohio','Texas')),\\n        'CustomerId'\\n    )),\\n    data.table.unkeyed=list(copySpec=1:2,args=list(\\n        as.data.table(df1),\\n        as.data.table(df2),\\n        'CustomerId'\\n    )),\\n    data.table.keyed=list(copySpec=1:2,args=list(\\n        setkey(as.data.table(df1),CustomerId),\\n        setkey(as.data.table(df2),CustomerId)\\n    ))\\n);\\n## prepare sqldf\\ninitSqldf();\\nsqldf('create index df1_key on df1(CustomerId);'); ## upload and create an sqlite index on df1\\nsqldf('create index df2_key on df2(CustomerId);'); ## upload and create an sqlite index on df2\\n\\ncheckIdentical(argSpecs);\\n\\ntestAllJoinTypes(argSpecs,metric='median');\\n##    join    merge data.table.unkeyed data.table.keyed sqldf.unindexed sqldf.indexed      plyr    dplyr in.place         unit\\n## 1 inner  644.259           861.9345          923.516        9157.752      1580.390  959.2250 270.9190       NA microseconds\\n## 2  left  713.539           888.0205          910.045        8820.334      1529.714  968.4195 270.9185 224.3045 microseconds\\n## 3 right 1221.804           909.1900          923.944        8930.668      1533.135 1063.7860 269.8495 218.1035 microseconds\\n## 4  full 1302.203          3107.5380         3184.729              NA            NA 1593.6475 270.7055       NA microseconds\\n</code>\", '<code>0..1:0..1</code>', \"<code>makeArgSpecs.singleIntegerKey.optionalOneToOne &lt;- function(size,overlap) {\\n\\n    com &lt;- as.integer(size*overlap);\\n\\n    argSpecs &lt;- list(\\n        default=list(copySpec=1:2,args=list(\\n            df1 &lt;- data.frame(id=sample(size),y1=rnorm(size),y2=rnorm(size)),\\n            df2 &lt;- data.frame(id=sample(c(if (com&gt;0L) sample(df1$id,com) else integer(),seq(size+1L,len=size-com))),y3=rnorm(size),y4=rnorm(size)),\\n            'id'\\n        )),\\n        data.table.unkeyed=list(copySpec=1:2,args=list(\\n            as.data.table(df1),\\n            as.data.table(df2),\\n            'id'\\n        )),\\n        data.table.keyed=list(copySpec=1:2,args=list(\\n            setkey(as.data.table(df1),id),\\n            setkey(as.data.table(df2),id)\\n        ))\\n    );\\n    ## prepare sqldf\\n    initSqldf();\\n    sqldf('create index df1_key on df1(id);'); ## upload and create an sqlite index on df1\\n    sqldf('create index df2_key on df2(id);'); ## upload and create an sqlite index on df2\\n\\n    argSpecs;\\n\\n}; ## end makeArgSpecs.singleIntegerKey.optionalOneToOne()\\n\\n## cross of various input sizes and key overlaps\\nsizes &lt;- c(1e1L,1e3L,1e6L);\\noverlaps &lt;- c(0.99,0.5,0.01);\\nsystem.time({ res &lt;- testGrid(makeArgSpecs.singleIntegerKey.optionalOneToOne,sizes,overlaps); });\\n##     user   system  elapsed\\n## 22024.65 12308.63 34493.19\\n</code>\", \"<code>plotRes &lt;- function(res,titleFunc,useFloor=F) {\\n    solTypes &lt;- setdiff(names(res),c('size','overlap','joinType','unit')); ## derive from res\\n    normMult &lt;- c(microseconds=1e-3,milliseconds=1); ## normalize to milliseconds\\n    joinTypes &lt;- getJoinTypes();\\n    cols &lt;- c(merge='purple',data.table.unkeyed='blue',data.table.keyed='#00DDDD',sqldf.unindexed='brown',sqldf.indexed='orange',plyr='red',dplyr='#00BB00',in.place='magenta');\\n    pchs &lt;- list(inner=20L,left='&lt;',right='&gt;',full=23L);\\n    cexs &lt;- c(inner=0.7,left=1,right=1,full=0.7);\\n    NP &lt;- 60L;\\n    ord &lt;- order(decreasing=T,colMeans(res[res$size==max(res$size),solTypes],na.rm=T));\\n    ymajors &lt;- data.frame(y=c(1,1e3),label=c('1ms','1s'),stringsAsFactors=F);\\n    for (overlap in unique(res$overlap)) {\\n        x1 &lt;- res[res$overlap==overlap,];\\n        x1[solTypes] &lt;- x1[solTypes]*normMult[x1$unit]; x1$unit &lt;- NULL;\\n        xlim &lt;- c(1e1,max(x1$size));\\n        xticks &lt;- 10^seq(log10(xlim[1L]),log10(xlim[2L]));\\n        ylim &lt;- c(1e-1,10^((if (useFloor) floor else ceiling)(log10(max(x1[solTypes],na.rm=T))))); ## use floor() to zoom in a little more, only sqldf.unindexed will break above, but xpd=NA will keep it visible\\n        yticks &lt;- 10^seq(log10(ylim[1L]),log10(ylim[2L]));\\n        yticks.minor &lt;- rep(yticks[-length(yticks)],each=9L)*1:9;\\n        plot(NA,xlim=xlim,ylim=ylim,xaxs='i',yaxs='i',axes=F,xlab='size (rows)',ylab='time (ms)',log='xy');\\n        abline(v=xticks,col='lightgrey');\\n        abline(h=yticks.minor,col='lightgrey',lty=3L);\\n        abline(h=yticks,col='lightgrey');\\n        axis(1L,xticks,parse(text=sprintf('10^%d',as.integer(log10(xticks)))));\\n        axis(2L,yticks,parse(text=sprintf('10^%d',as.integer(log10(yticks)))),las=1L);\\n        axis(4L,ymajors$y,ymajors$label,las=1L,tick=F,cex.axis=0.7,hadj=0.5);\\n        for (joinType in rev(joinTypes)) { ## reverse to draw full first, since it's larger and would be more obtrusive if drawn last\\n            x2 &lt;- x1[x1$joinType==joinType,];\\n            for (solType in solTypes) {\\n                if (any(!is.na(x2[[solType]]))) {\\n                    xy &lt;- spline(x2$size,x2[[solType]],xout=10^(seq(log10(x2$size[1L]),log10(x2$size[nrow(x2)]),len=NP)));\\n                    points(xy$x,xy$y,pch=pchs[[joinType]],col=cols[solType],cex=cexs[joinType],xpd=NA);\\n                }; ## end if\\n            }; ## end for\\n        }; ## end for\\n        ## custom legend\\n        ## due to logarithmic skew, must do all distance calcs in inches, and convert to user coords afterward\\n        ## the bottom-left corner of the legend will be defined in normalized figure coords, although we can convert to inches immediately\\n        leg.cex &lt;- 0.7;\\n        leg.x.in &lt;- grconvertX(0.275,'nfc','in');\\n        leg.y.in &lt;- grconvertY(0.6,'nfc','in');\\n        leg.x.user &lt;- grconvertX(leg.x.in,'in');\\n        leg.y.user &lt;- grconvertY(leg.y.in,'in');\\n        leg.outpad.w.in &lt;- 0.1;\\n        leg.outpad.h.in &lt;- 0.1;\\n        leg.midpad.w.in &lt;- 0.1;\\n        leg.midpad.h.in &lt;- 0.1;\\n        leg.sol.w.in &lt;- max(strwidth(solTypes,'in',leg.cex));\\n        leg.sol.h.in &lt;- max(strheight(solTypes,'in',leg.cex))*1.5; ## multiplication factor for greater line height\\n        leg.join.w.in &lt;- max(strheight(joinTypes,'in',leg.cex))*1.5; ## ditto\\n        leg.join.h.in &lt;- max(strwidth(joinTypes,'in',leg.cex));\\n        leg.main.w.in &lt;- leg.join.w.in*length(joinTypes);\\n        leg.main.h.in &lt;- leg.sol.h.in*length(solTypes);\\n        leg.x2.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in*2+leg.main.w.in+leg.midpad.w.in+leg.sol.w.in,'in');\\n        leg.y2.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in*2+leg.main.h.in+leg.midpad.h.in+leg.join.h.in,'in');\\n        leg.cols.x.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in+leg.join.w.in*(0.5+seq(0L,length(joinTypes)-1L)),'in');\\n        leg.lines.y.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in+leg.main.h.in-leg.sol.h.in*(0.5+seq(0L,length(solTypes)-1L)),'in');\\n        leg.sol.x.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in+leg.main.w.in+leg.midpad.w.in,'in');\\n        leg.join.y.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in+leg.main.h.in+leg.midpad.h.in,'in');\\n        rect(leg.x.user,leg.y.user,leg.x2.user,leg.y2.user,col='white');\\n        text(leg.sol.x.user,leg.lines.y.user,solTypes[ord],cex=leg.cex,pos=4L,offset=0);\\n        text(leg.cols.x.user,leg.join.y.user,joinTypes,cex=leg.cex,pos=4L,offset=0,srt=90); ## srt rotation applies *after* pos/offset positioning\\n        for (i in seq_along(joinTypes)) {\\n            joinType &lt;- joinTypes[i];\\n            points(rep(leg.cols.x.user[i],length(solTypes)),ifelse(colSums(!is.na(x1[x1$joinType==joinType,solTypes[ord]]))==0L,NA,leg.lines.y.user),pch=pchs[[joinType]],col=cols[solTypes[ord]]);\\n        }; ## end for\\n        title(titleFunc(overlap));\\n        readline(sprintf('overlap %.02f',overlap));\\n    }; ## end for\\n}; ## end plotRes()\\n\\ntitleFunc &lt;- function(overlap) sprintf('R merge solutions: single-column integer key, 0..1:0..1 cardinality, %d%% overlap',as.integer(overlap*100));\\nplotRes(res,titleFunc,T);\\n</code>\", '<code>0..*:0..*</code>', '<code>sqldf.indexed</code>', \"<code>makeArgSpecs.assortedKey.optionalManyToMany &lt;- function(size,overlap,uniquePct=75) {\\n\\n    ## number of unique keys in df1\\n    u1Size &lt;- as.integer(size*uniquePct/100);\\n\\n    ## (roughly) divide u1Size into bases, so we can use expand.grid() to produce the required number of unique key values with repetitions within individual key columns\\n    ## use ceiling() to ensure we cover u1Size; will truncate afterward\\n    u1SizePerKeyColumn &lt;- as.integer(ceiling(u1Size^(1/3)));\\n\\n    ## generate the unique key values for df1\\n    keys1 &lt;- expand.grid(stringsAsFactors=F,\\n        idCharacter=replicate(u1SizePerKeyColumn,paste(collapse='',sample(letters,sample(4:12,1L),T))),\\n        idInteger=sample(u1SizePerKeyColumn),\\n        idLogical=sample(c(F,T),u1SizePerKeyColumn,T)\\n        ##idPOSIXct=as.POSIXct('2016-01-01 00:00:00','UTC')+sample(u1SizePerKeyColumn)\\n    )[seq_len(u1Size),];\\n\\n    ## rbind some repetitions of the unique keys; this will prepare one side of the many-to-many relationship\\n    ## also scramble the order afterward\\n    keys1 &lt;- rbind(keys1,keys1[sample(nrow(keys1),size-u1Size,T),])[sample(size),];\\n\\n    ## common and unilateral key counts\\n    com &lt;- as.integer(size*overlap);\\n    uni &lt;- size-com;\\n\\n    ## generate some unilateral keys for df2 by synthesizing outside of the idInteger range of df1\\n    keys2 &lt;- data.frame(stringsAsFactors=F,\\n        idCharacter=replicate(uni,paste(collapse='',sample(letters,sample(4:12,1L),T))),\\n        idInteger=u1SizePerKeyColumn+sample(uni),\\n        idLogical=sample(c(F,T),uni,T)\\n        ##idPOSIXct=as.POSIXct('2016-01-01 00:00:00','UTC')+u1SizePerKeyColumn+sample(uni)\\n    );\\n\\n    ## rbind random keys from df1; this will complete the many-to-many relationship\\n    ## also scramble the order afterward\\n    keys2 &lt;- rbind(keys2,keys1[sample(nrow(keys1),com,T),])[sample(size),];\\n\\n    ##keyNames &lt;- c('idCharacter','idInteger','idLogical','idPOSIXct');\\n    keyNames &lt;- c('idCharacter','idInteger','idLogical');\\n    ## note: was going to use raw and complex type for two of the non-key columns, but data.table doesn't seem to fully support them\\n    argSpecs &lt;- list(\\n        default=list(copySpec=1:2,args=list(\\n            df1 &lt;- cbind(stringsAsFactors=F,keys1,y1=sample(c(F,T),size,T),y2=sample(size),y3=rnorm(size),y4=replicate(size,paste(collapse='',sample(letters,sample(4:12,1L),T)))),\\n            df2 &lt;- cbind(stringsAsFactors=F,keys2,y5=sample(c(F,T),size,T),y6=sample(size),y7=rnorm(size),y8=replicate(size,paste(collapse='',sample(letters,sample(4:12,1L),T)))),\\n            keyNames\\n        )),\\n        data.table.unkeyed=list(copySpec=1:2,args=list(\\n            as.data.table(df1),\\n            as.data.table(df2),\\n            keyNames\\n        )),\\n        data.table.keyed=list(copySpec=1:2,args=list(\\n            setkeyv(as.data.table(df1),keyNames),\\n            setkeyv(as.data.table(df2),keyNames)\\n        ))\\n    );\\n    ## prepare sqldf\\n    initSqldf();\\n    sqldf(paste0('create index df1_key on df1(',paste(collapse=',',keyNames),');')); ## upload and create an sqlite index on df1\\n    sqldf(paste0('create index df2_key on df2(',paste(collapse=',',keyNames),');')); ## upload and create an sqlite index on df2\\n\\n    argSpecs;\\n\\n}; ## end makeArgSpecs.assortedKey.optionalManyToMany()\\n\\nsizes &lt;- c(1e1L,1e3L,1e5L); ## 1e5L instead of 1e6L to respect more heavy-duty inputs\\noverlaps &lt;- c(0.99,0.5,0.01);\\nsolTypes &lt;- setdiff(getSolTypes(),'in.place');\\nsystem.time({ res &lt;- testGrid(makeArgSpecs.assortedKey.optionalManyToMany,sizes,overlaps,solTypes); });\\n##     user   system  elapsed\\n## 38895.50   784.19 39745.53\\n</code>\", \"<code>titleFunc &lt;- function(overlap) sprintf('R merge solutions: character/integer/logical key, 0..*:0..* cardinality, %d%% overlap',as.integer(overlap*100));\\nplotRes(res,titleFunc,F);\\n</code>\", '<code>fintersect</code>', '<code>intersect</code>', '<code>merge</code>', '<code>by</code>', '<code>merge(df1, df2)\\n#   V1 V2\\n# 1  B  2\\n# 2  C  3\\ndplyr::intersect(df1, df2)\\n#   V1 V2\\n# 1  B  2\\n# 2  C  3\\ndata.table::fintersect(setDT(df1), setDT(df2))\\n#    V1 V2\\n# 1:  B  2\\n# 2:  C  3\\n</code>', '<code>df1 &lt;- data.frame(V1 = LETTERS[1:4], V2 = 1:4)\\ndf2 &lt;- data.frame(V1 = LETTERS[2:3], V2 = 2:3)\\n</code>', '<code>merge</code>', '<code>select a.* from df1 a inner join df2 b on a.CustomerId=b.CustomerId</code>', '<code>merge(df1, df2, by.x = \"CustomerId\", by.y = \"CustomerId\")[,names(df1)]</code>', '<code>select b.* from df1 a inner join df2 b on a.CustomerId=b.CustomerId</code>', '<code>merge(df1, df2, by.x = \"CustomerId\", by.y =\\n\"CustomerId\")[,names(df2)]</code>']",
         "title": "How to join (merge) data frames (inner, outer, left, right)?",
         "_childDocuments_": [
            {
               "up_vote_count": 67,
               "answer_id": 1300615,
               "last_activity_date": 1250704261,
               "path": "3.stack.answer",
               "body_markdown": "There are some good examples of doing this over at the [R Wiki][1]. I&#39;ll steal a couple here:\r\n\r\n\r\n**Merge Method**\r\n\r\nSince your keys are named the same the short way to do an inner join is merge():\r\n\r\n    merge(df1,df2)\r\n\r\na full inner join (all records from both tables) can be created with the &quot;all&quot; keyword:\r\n\r\n    merge(df1,df2, all=TRUE)\r\n\r\na left outer join of df1 and df2:\r\n\r\n    merge(df1,df2, all.x=TRUE)\r\n\r\na right outer join of df1 and df2:\r\n\r\n    merge(df1,df2, all.y=TRUE)\r\n\r\nyou can flip &#39;em, slap &#39;em and rub &#39;em down to get the other two outer joins you asked about :)\r\n\r\n**Subscript Method**\r\n\r\nA left outer join with df1 on the left using a subscript method would be:\r\n\r\n    df1[,&quot;State&quot;]&lt;-df2[df1[ ,&quot;Product&quot;], &quot;State&quot;]\r\n\r\nThe other combination of outer joins can be created by mungling the left outer join subscript example. (yeah, I know that&#39;s the equivalent of saying &quot;I&#39;ll leave it as an exercise for the reader...&quot;)\r\n\r\n  [1]: http://wiki.r-project.org/rwiki/doku.php?id=tips:data-frames:merge",
               "tags": [],
               "creation_date": 1250694910,
               "last_edit_date": 1250704261,
               "is_accepted": false,
               "id": "1300615",
               "down_vote_count": 0,
               "score": 67
            },
            {
               "up_vote_count": 936,
               "answer_id": 1300618,
               "last_activity_date": 1434091365,
               "path": "3.stack.answer",
               "body_markdown": "By using the `merge` function and its optional parameters:\r\n\r\n***Inner join:*** `merge(df1, df2)` will work for these examples because R automatically joins the frames by common variable names, but you would most likely want to specify `merge(df1, df2, by = &quot;CustomerId&quot;)` to make sure that you were matching on only the fields you desired.  You can also use the `by.x` and `by.y` parameters if the matching variables have different names in the different data frames.\r\n\r\n***Outer join:*** `merge(x = df1, y = df2, by = &quot;CustomerId&quot;, all = TRUE)`\r\n\r\n***Left outer:*** `merge(x = df1, y = df2, by = &quot;CustomerId&quot;, all.x = TRUE)`\r\n\r\n***Right outer:*** `merge(x = df1, y = df2, by = &quot;CustomerId&quot;, all.y = TRUE)`\r\n\r\n***Cross join:*** `merge(x = df1, y = df2, by = NULL)`\r\n\r\n&lt;strike&gt;Just as with the inner join, you would probably want to explicitly pass &quot;CustomerId&quot; to R as the matching variable.&lt;/strike&gt;  I think it&#39;s almost always best to explicitly state the identifiers on which you want to merge; it&#39;s safer if the input data.frames change unexpectedly and easier to read later on.",
               "tags": [],
               "creation_date": 1250694941,
               "last_edit_date": 1434091365,
               "is_accepted": true,
               "id": "1300618",
               "down_vote_count": 0,
               "score": 936
            },
            {
               "up_vote_count": 169,
               "answer_id": 1307824,
               "last_activity_date": 1434093909,
               "path": "3.stack.answer",
               "body_markdown": "I would recommend checking out [Gabor Grothendieck&#39;s sqldf package][1], which allows you to express these operations in SQL.\r\n\r\n    library(sqldf)\r\n    \r\n    ## inner join\r\n    df3 &lt;- sqldf(&quot;SELECT CustomerId, Product, State \r\n                  FROM df1\r\n                  JOIN df2 USING(CustomerID)&quot;)\r\n    \r\n    ## left join (substitute &#39;right&#39; for right join)\r\n    df4 &lt;- sqldf(&quot;SELECT CustomerId, Product, State \r\n                  FROM df1\r\n                  LEFT JOIN df2 USING(CustomerID)&quot;)\r\n\r\nI find the SQL syntax to be simpler and more natural than its R equivalent (but this may just reflect my RDBMS bias).\r\n\r\nSee [Gabor&#39;s sqldf GitHub][2] for more information on joins. \r\n\r\n\r\n  [1]: http://cran.r-project.org/web/packages/sqldf/index.html\r\n  [2]: https://github.com/ggrothendieck/sqldf#example-4-join\r\n",
               "tags": [],
               "creation_date": 1250790889,
               "last_edit_date": 1434093909,
               "is_accepted": false,
               "id": "1307824",
               "down_vote_count": 1,
               "score": 168
            },
            {
               "up_vote_count": 154,
               "answer_id": 9652931,
               "last_activity_date": 1497167294,
               "path": "3.stack.answer",
               "body_markdown": "There is the **data.table** approach for an inner join, which is very time and memory efficient (and necessary for some larger data.frames):\r\n\r\n\r\n    library(data.table)\r\n      \r\n    dt1 &lt;- data.table(df1, key = &quot;CustomerId&quot;) \r\n    dt2 &lt;- data.table(df2, key = &quot;CustomerId&quot;)\r\n    \r\n    joined.dt1.dt.2 &lt;- dt1[dt2]\r\n\r\n`merge` also works on data.tables (as it is generic and calls `merge.data.table`)\r\n\r\n    merge(dt1, dt2)\r\n\r\n\r\n\r\ndata.table documented on stackoverflow:   \r\nhttps://stackoverflow.com/questions/2232699/r-how-to-do-a-data-table-merge-operation  \r\nhttps://stackoverflow.com/questions/9914734/translating-sql-joins-on-foreign-keys-to-r-data-table-syntax  \r\nhttps://stackoverflow.com/questions/11146967/efficient-alternatives-to-merge-for-larger-data-frames-r  \r\nhttps://stackoverflow.com/questions/7090621/how-to-do-a-basic-left-outer-join-with-data-table-in-r\r\n\r\n\r\nYet another option is the `join` function found in the [**plyr**](http://cran.r-project.org/web/packages/plyr/index.html) package\r\n\r\n \r\n    library(plyr)\r\n    \r\n    join(df1, df2,\r\n         type = &quot;inner&quot;)\r\n    \r\n    #   CustomerId Product   State\r\n    # 1          2 Toaster Alabama\r\n    # 2          4   Radio Alabama\r\n    # 3          6   Radio    Ohio\r\n\r\nOptions for `type`: `inner`, `left`, `right`, `full`.\r\n\r\nFrom `?join`: Unlike `merge`, [`join`] preserves the order of x no matter what join type is used.",
               "tags": [],
               "creation_date": 1331447055,
               "last_edit_date": 1497167294,
               "is_accepted": false,
               "id": "9652931",
               "down_vote_count": 0,
               "score": 154
            },
            {
               "up_vote_count": 60,
               "answer_id": 21438584,
               "last_activity_date": 1440534127,
               "path": "3.stack.answer",
               "body_markdown": "New in 2014: \r\n\r\nEspecially if you&#39;re also interested in data manipulation in general (including sorting, filtering, subsetting, summarizing etc.), you should definitely take a look at `dplyr`, which comes with a variety of functions all designed to facilitate your work specifically with data frames and certain other database types. It even offers quite an elaborate SQL interface, and even a function to convert (most) SQL code directly into R.\r\n\r\nThe four joining-related functions in the dplyr package are (to quote):\r\n\r\n - `inner_join(x, y, by = NULL, copy = FALSE, ...)`: return all rows from\r\n   x where there are matching values in y, and all columns from x and y \r\n - `left_join(x, y, by = NULL, copy = FALSE, ...)`: return all rows from x, and all columns from x and y \r\n - `semi_join(x, y, by = NULL, copy = FALSE, ...)`: return all rows from x where there are matching values in\r\n   y, keeping just columns from x.  \r\n - `anti_join(x, y, by = NULL, copy = FALSE, ...)`: return all rows from x\r\n   where there are not matching values in y, keeping just columns from x\r\n\r\nIt&#39;s all [here][2] in great detail.\r\n\r\nSelecting columns can be done by `select(df,&quot;column&quot;)`. If that&#39;s not SQL-ish enough for you, then there&#39;s the `sql()` function, into which you can enter SQL code as-is, and it will do the operation you specified just like you were writing in R all along (for more information, please refer to the [dplyr/databases vignette][1]). For example, if applied correctly, `sql(&quot;SELECT * FROM hflights&quot;)` will select all the columns from the &quot;hflights&quot; dplyr table (a &quot;tbl&quot;).\r\n\r\n\r\n  [1]: http://cran.r-project.org/web/packages/dplyr/vignettes/databases.html\r\n  [2]: http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html",
               "tags": [],
               "creation_date": 1391017406,
               "last_edit_date": 1440534127,
               "is_accepted": false,
               "id": "21438584",
               "down_vote_count": 1,
               "score": 59
            },
            {
               "up_vote_count": 128,
               "answer_id": 21614548,
               "last_activity_date": 1475163359,
               "path": "3.stack.answer",
               "body_markdown": "You can do joins as well using Hadley Wickham&#39;s awesome [dplyr](http://blog.rstudio.org/2014/01/17/introducing-dplyr/) package.  \r\n\r\n\r\n    library(dplyr)\r\n\r\n    #make sure that CustomerId cols are both type numeric\r\n    #they ARE not using the provided code in question and dplyr will complain\r\n    df1$CustomerId &lt;- as.numeric(df1$CustomerId)\r\n    df2$CustomerId &lt;- as.numeric(df2$CustomerId)\r\n\r\n## Mutating joins: add columns to df1 using matches in df2\r\n    #inner\r\n    inner_join(df1, df2)\r\n\r\n    #left outer\r\n    left_join(df1, df2)\r\n\r\n    #right outer\r\n    right_join(df1, df2)\r\n\r\n    #alternate right outer\r\n    left_join(df2, df1)\r\n\r\n    #full join\r\n    full_join(df1, df2)\r\n\r\n## Filtering joins: filter out rows in df1, don&#39;t modify columns\r\n\r\n    semi_join(df1, df2) #keep only observations in df1 that match in df2.\r\n    anti_join(df1, df2) #drops all observations in df1 that match in df2.\r\n\r\n    \r\n",
               "tags": [],
               "creation_date": 1391722506,
               "last_edit_date": 1475163359,
               "is_accepted": false,
               "id": "21614548",
               "down_vote_count": 1,
               "score": 127
            },
            {
               "up_vote_count": 28,
               "answer_id": 23041682,
               "last_activity_date": 1421981815,
               "path": "3.stack.answer",
               "body_markdown": "dplyr is very good and performant. In addition to the other answers on it, here was/is its status as of \r\n\r\n***v0.1.3*** (4/2014)\r\n\r\n - has **inner_join, left_join, semi_join, anti_join**\r\n - **outer_join** not implemented yet, fallback is use base::merge() (or plyr::join())\r\n - [Hadley mentioning other advantages here](https://groups.google.com/forum/#!topic/manipulatr/OuAPC4VyfIc)\r\n - one minor feature merge currently has that dplyr doesn&#39;t is [the ability to have separate by.x,by.y columns](https://github.com/hadley/dplyr/issues/177) as e.g. Python pandas does.\r\n - [Implement right_join and **outer_join**](https://github.com/hadley/dplyr/issues/96) is tagged for v0.3 (presumably at least 2015 or beyond)\r\n\r\nPer hadley&#39;s comments in that issue:\r\n\r\n - **right_join**(x,y) is the same as left_join(y,x) in terms of the rows, just the columns will be different orders. Easily worked around with select(new_column_order)\r\n -  **outer_join** is basically union(left_join(x, y), right_join(x, y)) - i.e. preserve all rows in both data frames.",
               "tags": [],
               "creation_date": 1397385543,
               "last_edit_date": 1421981815,
               "is_accepted": false,
               "id": "23041682",
               "down_vote_count": 3,
               "score": 25
            },
            {
               "up_vote_count": 18,
               "answer_id": 28749730,
               "last_activity_date": 1427822808,
               "path": "3.stack.answer",
               "body_markdown": "In joining two data frames with ~1 million rows each, one with 2 columns and the other with ~20, I&#39;ve surprisingly found `merge(..., all.x = TRUE, all.y = TRUE)` to be faster then `dplyr::full_join()`. This is with dplyr v0.4 \r\n\r\nMerge takes ~17 seconds, full_join takes ~65 seconds.  \r\n\r\nSome food for though, since I generally default to dplyr for manipulation tasks.",
               "tags": [],
               "creation_date": 1424974273,
               "last_edit_date": 1427822808,
               "is_accepted": false,
               "id": "28749730",
               "down_vote_count": 0,
               "score": 18
            },
            {
               "up_vote_count": 4,
               "answer_id": 32223455,
               "last_activity_date": 1516342330,
               "path": "3.stack.answer",
               "body_markdown": "1.  Using `merge` function we can select the variable of left table or right table, same way like we all familiar with select statement in SQL (EX : Select a.* ...or Select b.* from .....)\r\n2.  We have to add extra code which will subset from the newly joined table .\r\n\r\n \r\n\r\n - SQL :-  `select a.* from df1 a inner join df2 b on a.CustomerId=b.CustomerId`\r\n\r\n \r\n\r\n - R :- `merge(df1, df2, by.x = &quot;CustomerId&quot;, by.y = &quot;CustomerId&quot;)[,names(df1)]`\r\n\r\nSame way \r\n\r\n \r\n\r\n - SQL :- `select b.* from df1 a inner join df2 b on a.CustomerId=b.CustomerId`\r\n\r\n \r\n\r\n - R :- `merge(df1, df2, by.x = &quot;CustomerId&quot;, by.y =\r\n   &quot;CustomerId&quot;)[,names(df2)]`\r\n\r\n",
               "tags": [],
               "creation_date": 1440583066,
               "last_edit_date": 1516342330,
               "is_accepted": false,
               "id": "32223455",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 52,
               "answer_id": 34219998,
               "last_activity_date": 1459515475,
               "path": "3.stack.answer",
               "body_markdown": "Update on data.table methods for joining datasets. See below examples for each type of join. There are two methods, one from `[.data.table` when passing second data.table as the first argument to subset, another way is to use `merge` function which dispatched to fast data.table method.  \r\n\r\n**Update on 2016-04-01 - and it isn&#39;t April Fools joke!**  \r\nIn 1.9.7 version of data.table joins are now capable to use existing index which tremendously reduce the timing of a join. **Below code and benchmark does NOT use data.table indices on join**. If you are looking for near real-time join you should use data.table indices.\r\n\r\n&lt;!-- language: r --!&gt;\r\n\r\n    df1 = data.frame(CustomerId = c(1:6), Product = c(rep(&quot;Toaster&quot;, 3), rep(&quot;Radio&quot;, 3)))\r\n    df2 = data.frame(CustomerId = c(2L, 4L, 7L), State = c(rep(&quot;Alabama&quot;, 2), rep(&quot;Ohio&quot;, 1))) # one value changed to show full outer join\r\n    \r\n    library(data.table)\r\n    \r\n    dt1 = as.data.table(df1)\r\n    dt2 = as.data.table(df2)\r\n    setkey(dt1, CustomerId)\r\n    setkey(dt2, CustomerId)\r\n    # right outer join keyed data.tables\r\n    dt1[dt2]\r\n    \r\n    setkey(dt1, NULL)\r\n    setkey(dt2, NULL)\r\n    # right outer join unkeyed data.tables - use `on` argument\r\n    dt1[dt2, on = &quot;CustomerId&quot;]\r\n    \r\n    # left outer join - swap dt1 with dt2\r\n    dt2[dt1, on = &quot;CustomerId&quot;]\r\n    \r\n    # inner join - use `nomatch` argument\r\n    dt1[dt2, nomatch=0L, on = &quot;CustomerId&quot;]\r\n    \r\n    # anti join - use `!` operator\r\n    dt1[!dt2, on = &quot;CustomerId&quot;]\r\n    \r\n    # inner join\r\n    merge(dt1, dt2, by = &quot;CustomerId&quot;)\r\n    \r\n    # full outer join\r\n    merge(dt1, dt2, by = &quot;CustomerId&quot;, all = TRUE)\r\n    \r\n    # see ?merge.data.table arguments for other cases\r\n\r\nBelow benchmark tests base R, sqldf, dplyr and data.table.  \r\nBenchmark tests unkeyed/unindexed datasets. You can get even better performance if you are using keys on your data.tables or indexes with sqldf. Base R and dplyr does not have indexes or keys so I did not include that scenario in benchmark.  \r\nBenchmark is performed on 5M-1 rows datasets, there are 5M-2 common values on join column so each scenario (left, right, full, inner) can be tested and join is still not trivial to perform.  \r\n\r\n&lt;!-- language: r --!&gt;\r\n        \r\n    library(microbenchmark)\r\n    library(sqldf)\r\n    library(dplyr)\r\n    library(data.table)\r\n    \r\n    n = 5e6\r\n    set.seed(123)\r\n    df1 = data.frame(x=sample(n,n-1L), y1=rnorm(n-1L))\r\n    df2 = data.frame(x=sample(n,n-1L), y2=rnorm(n-1L))\r\n    dt1 = as.data.table(df1)\r\n    dt2 = as.data.table(df2)\r\n    \r\n    # inner join\r\n    microbenchmark(times = 10L,\r\n                   base = merge(df1, df2, by = &quot;x&quot;),\r\n                   sqldf = sqldf(&quot;SELECT * FROM df1 INNER JOIN df2 ON df1.x = df2.x&quot;),\r\n                   dplyr = inner_join(df1, df2, by = &quot;x&quot;),\r\n                   data.table = dt1[dt2, nomatch = 0L, on = &quot;x&quot;])\r\n    #Unit: milliseconds\r\n    #       expr        min         lq      mean     median        uq       max neval\r\n    #       base 15546.0097 16083.4915 16687.117 16539.0148 17388.290 18513.216    10\r\n    #      sqldf 44392.6685 44709.7128 45096.401 45067.7461 45504.376 45563.472    10\r\n    #      dplyr  4124.0068  4248.7758  4281.122  4272.3619  4342.829  4411.388    10\r\n    # data.table   937.2461   946.0227  1053.411   973.0805  1214.300  1281.958    10\r\n    \r\n    # left outer join\r\n    microbenchmark(times = 10L,\r\n                   base = merge(df1, df2, by = &quot;x&quot;, all.x = TRUE),\r\n                   sqldf = sqldf(&quot;SELECT * FROM df1 LEFT OUTER JOIN df2 ON df1.x = df2.x&quot;),\r\n                   dplyr = left_join(df1, df2, by = c(&quot;x&quot;=&quot;x&quot;)),\r\n                   data.table = dt2[dt1, on = &quot;x&quot;])\r\n    #Unit: milliseconds\r\n    #       expr       min         lq       mean     median         uq       max neval\r\n    #       base 16140.791 17107.7366 17441.9538 17414.6263 17821.9035 19453.034    10\r\n    #      sqldf 43656.633 44141.9186 44777.1872 44498.7191 45288.7406 47108.900    10\r\n    #      dplyr  4062.153  4352.8021  4780.3221  4409.1186  4450.9301  8385.050    10\r\n    # data.table   823.218   823.5557   901.0383   837.9206   883.3292  1277.239    10\r\n    \r\n    # right outer join\r\n    microbenchmark(times = 10L,\r\n                   base = merge(df1, df2, by = &quot;x&quot;, all.y = TRUE),\r\n                   sqldf = sqldf(&quot;SELECT * FROM df2 LEFT OUTER JOIN df1 ON df2.x = df1.x&quot;),\r\n                   dplyr = right_join(df1, df2, by = &quot;x&quot;),\r\n                   data.table = dt1[dt2, on = &quot;x&quot;])\r\n    #Unit: milliseconds\r\n    #       expr        min         lq       mean     median        uq       max neval\r\n    #       base 15821.3351 15954.9927 16347.3093 16044.3500 16621.887 17604.794    10\r\n    #      sqldf 43635.5308 43761.3532 43984.3682 43969.0081 44044.461 44499.891    10\r\n    #      dplyr  3936.0329  4028.1239  4102.4167  4045.0854  4219.958  4307.350    10\r\n    # data.table   820.8535   835.9101   918.5243   887.0207  1005.721  1068.919    10\r\n    \r\n    # full outer join\r\n    microbenchmark(times = 10L,\r\n                   base = merge(df1, df2, by = &quot;x&quot;, all = TRUE),\r\n                   #sqldf = sqldf(&quot;SELECT * FROM df1 FULL OUTER JOIN df2 ON df1.x = df2.x&quot;), # not supported\r\n                   dplyr = full_join(df1, df2, by = &quot;x&quot;),\r\n                   data.table = merge(dt1, dt2, by = &quot;x&quot;, all = TRUE))\r\n    #Unit: seconds\r\n    #       expr       min        lq      mean    median        uq       max neval\r\n    #       base 16.176423 16.908908 17.485457 17.364857 18.271790 18.626762    10\r\n    #      dplyr  7.610498  7.666426  7.745850  7.710638  7.832125  7.951426    10\r\n    # data.table  2.052590  2.130317  2.352626  2.208913  2.470721  2.951948    10\r\n\r\n",
               "tags": [],
               "creation_date": 1449825789,
               "last_edit_date": 1459515475,
               "is_accepted": false,
               "id": "34219998",
               "down_vote_count": 0,
               "score": 52
            },
            {
               "up_vote_count": 16,
               "answer_id": 38130460,
               "last_activity_date": 1473336253,
               "path": "3.stack.answer",
               "body_markdown": "For the case of a left join with a `0..*:0..1` cardinality or a right join with a `0..1:0..*` cardinality it is possible to assign in-place the unilateral columns from the joiner (the `0..1` table) directly onto the joinee (the `0..*` table), and thereby avoid the creation of an entirely new table of data. This requires matching the key columns from the joinee into the joiner and indexing+ordering the joiner&#39;s rows accordingly for the assignment.\r\n\r\nIf the key is a single column, then we can use a single call to [`match()`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/match.html) to do the matching. This is the case I&#39;ll cover in this answer.\r\n\r\nHere&#39;s an example based on the OP, except I&#39;ve added an extra row to `df2` with an id of 7 to test the case of a non-matching key in the joiner. This is effectively `df1` left join `df2`:\r\n\r\n    df1 &lt;- data.frame(CustomerId=1:6,Product=c(rep(&#39;Toaster&#39;,3L),rep(&#39;Radio&#39;,3L)));\r\n    df2 &lt;- data.frame(CustomerId=c(2L,4L,6L,7L),State=c(rep(&#39;Alabama&#39;,2L),&#39;Ohio&#39;,&#39;Texas&#39;));\r\n    df1[names(df2)[-1L]] &lt;- df2[match(df1[,1L],df2[,1L]),-1L];\r\n    df1;\r\n    ##   CustomerId Product   State\r\n    ## 1          1 Toaster    &lt;NA&gt;\r\n    ## 2          2 Toaster Alabama\r\n    ## 3          3 Toaster    &lt;NA&gt;\r\n    ## 4          4   Radio Alabama\r\n    ## 5          5   Radio    &lt;NA&gt;\r\n    ## 6          6   Radio    Ohio\r\n\r\nIn the above I hard-coded an assumption that the key column is the first column of both input tables. I would argue that, in general, this is not an unreasonable assumption, since, if you have a data.frame with a key column, it would be strange if it had not been set up as the first column of the data.frame from the outset. And you can always reorder the columns to make it so. An advantageous consequence of this assumption is that the name of the key column does not have to be hard-coded, although I suppose it&#39;s just replacing one assumption with another. Concision is another advantage of integer indexing, as well as speed. In the benchmarks below I&#39;ll change the implementation to use string name indexing to match the competing implementations.\r\n\r\nI think this is a particularly appropriate solution if you have several tables that you want to left join against a single large table. Repeatedly rebuilding the entire table for each merge would be unnecessary and inefficient.\r\n\r\nOn the other hand, if you need the joinee to remain unaltered through this operation for whatever reason, then this solution cannot be used, since it modifies the joinee directly. Although in that case you could simply make a copy and perform the in-place assignment(s) on the copy.\r\n\r\n---\r\n\r\nAs a side note, I briefly looked into possible matching solutions for multicolumn keys. Unfortunately, the only matching solutions I found were:\r\n\r\n- inefficient concatenations. e.g. `match(interaction(df1$a,df1$b),interaction(df2$a,df2$b))`, or the same idea with `paste()`.\r\n- inefficient cartesian conjunctions, e.g. `` outer(df1$a,df2$a,`==`) &amp; outer(df1$b,df2$b,`==`) ``.\r\n- base R `merge()` and equivalent package-based merge functions, which always allocate a new table to return the merged result, and thus are not suitable for an in-place assignment-based solution.\r\n\r\nFor example, see https://stackoverflow.com/questions/13286881/matching-multiple-columns-on-different-data-frames-and-getting-other-column-as-r, https://stackoverflow.com/questions/6880450/match-two-columns-with-two-other-columns, [Matching on multiple columns](https://stat.ethz.ch/pipermail/r-help/2007-January/123488.html), and the dupe of this question where I originally came up with the in-place solution, https://stackoverflow.com/questions/38066077/combine-two-data-frames-with-different-number-of-rows-in-r/38066281#38066281.\r\n\r\n---\r\n\r\nBenchmarking\r\n===\r\n\r\nI decided to do my own benchmarking to see how the in-place assignment approach compares to the other solutions that have been offered in this question.\r\n\r\nTesting code:\r\n\r\n    library(microbenchmark);\r\n    library(data.table);\r\n    library(sqldf);\r\n    library(plyr);\r\n    library(dplyr);\r\n\r\n    solSpecs &lt;- list(\r\n        merge=list(testFuncs=list(\r\n            inner=function(df1,df2,key) merge(df1,df2,key),\r\n            left =function(df1,df2,key) merge(df1,df2,key,all.x=T),\r\n            right=function(df1,df2,key) merge(df1,df2,key,all.y=T),\r\n            full =function(df1,df2,key) merge(df1,df2,key,all=T)\r\n        )),\r\n        data.table.unkeyed=list(argSpec=&#39;data.table.unkeyed&#39;,testFuncs=list(\r\n            inner=function(dt1,dt2,key) dt1[dt2,on=key,nomatch=0L,allow.cartesian=T],\r\n            left =function(dt1,dt2,key) dt2[dt1,on=key,allow.cartesian=T],\r\n            right=function(dt1,dt2,key) dt1[dt2,on=key,allow.cartesian=T],\r\n            full =function(dt1,dt2,key) merge(dt1,dt2,key,all=T,allow.cartesian=T) ## calls merge.data.table()\r\n        )),\r\n        data.table.keyed=list(argSpec=&#39;data.table.keyed&#39;,testFuncs=list(\r\n            inner=function(dt1,dt2) dt1[dt2,nomatch=0L,allow.cartesian=T],\r\n            left =function(dt1,dt2) dt2[dt1,allow.cartesian=T],\r\n            right=function(dt1,dt2) dt1[dt2,allow.cartesian=T],\r\n            full =function(dt1,dt2) merge(dt1,dt2,all=T,allow.cartesian=T) ## calls merge.data.table()\r\n        )),\r\n        sqldf.unindexed=list(testFuncs=list( ## note: must pass connection=NULL to avoid running against the live DB connection, which would result in collisions with the residual tables from the last query upload\r\n            inner=function(df1,df2,key) sqldf(paste0(&#39;select * from df1 inner join df2 using(&#39;,paste(collapse=&#39;,&#39;,key),&#39;)&#39;),connection=NULL),\r\n            left =function(df1,df2,key) sqldf(paste0(&#39;select * from df1 left join df2 using(&#39;,paste(collapse=&#39;,&#39;,key),&#39;)&#39;),connection=NULL),\r\n            right=function(df1,df2,key) sqldf(paste0(&#39;select * from df2 left join df1 using(&#39;,paste(collapse=&#39;,&#39;,key),&#39;)&#39;),connection=NULL) ## can&#39;t do right join proper, not yet supported; inverted left join is equivalent\r\n            ##full =function(df1,df2,key) sqldf(paste0(&#39;select * from df1 full join df2 using(&#39;,paste(collapse=&#39;,&#39;,key),&#39;)&#39;),connection=NULL) ## can&#39;t do full join proper, not yet supported; possible to hack it with a union of left joins, but too unreasonable to include in testing\r\n        )),\r\n        sqldf.indexed=list(testFuncs=list( ## important: requires an active DB connection with preindexed main.df1 and main.df2 ready to go; arguments are actually ignored\r\n            inner=function(df1,df2,key) sqldf(paste0(&#39;select * from main.df1 inner join main.df2 using(&#39;,paste(collapse=&#39;,&#39;,key),&#39;)&#39;)),\r\n            left =function(df1,df2,key) sqldf(paste0(&#39;select * from main.df1 left join main.df2 using(&#39;,paste(collapse=&#39;,&#39;,key),&#39;)&#39;)),\r\n            right=function(df1,df2,key) sqldf(paste0(&#39;select * from main.df2 left join main.df1 using(&#39;,paste(collapse=&#39;,&#39;,key),&#39;)&#39;)) ## can&#39;t do right join proper, not yet supported; inverted left join is equivalent\r\n            ##full =function(df1,df2,key) sqldf(paste0(&#39;select * from main.df1 full join main.df2 using(&#39;,paste(collapse=&#39;,&#39;,key),&#39;)&#39;)) ## can&#39;t do full join proper, not yet supported; possible to hack it with a union of left joins, but too unreasonable to include in testing\r\n        )),\r\n        plyr=list(testFuncs=list(\r\n            inner=function(df1,df2,key) join(df1,df2,key,&#39;inner&#39;),\r\n            left =function(df1,df2,key) join(df1,df2,key,&#39;left&#39;),\r\n            right=function(df1,df2,key) join(df1,df2,key,&#39;right&#39;),\r\n            full =function(df1,df2,key) join(df1,df2,key,&#39;full&#39;)\r\n        )),\r\n        dplyr=list(testFuncs=list(\r\n            inner=function(df1,df2,key) inner_join(df1,df2,key),\r\n            left =function(df1,df2,key) left_join(df1,df2,key),\r\n            right=function(df1,df2,key) right_join(df1,df2,key),\r\n            full =function(df1,df2,key) full_join(df1,df2,key)\r\n        )),\r\n        in.place=list(testFuncs=list(\r\n            left =function(df1,df2,key) { cns &lt;- setdiff(names(df2),key); df1[cns] &lt;- df2[match(df1[,key],df2[,key]),cns]; df1; },\r\n            right=function(df1,df2,key) { cns &lt;- setdiff(names(df1),key); df2[cns] &lt;- df1[match(df2[,key],df1[,key]),cns]; df2; }\r\n        ))\r\n    );\r\n\r\n    getSolTypes &lt;- function() names(solSpecs);\r\n    getJoinTypes &lt;- function() unique(unlist(lapply(solSpecs,function(x) names(x$testFuncs))));\r\n    getArgSpec &lt;- function(argSpecs,key=NULL) if (is.null(key)) argSpecs$default else argSpecs[[key]];\r\n\r\n    initSqldf &lt;- function() {\r\n        sqldf(); ## creates sqlite connection on first run, cleans up and closes existing connection otherwise\r\n        if (exists(&#39;sqldfInitFlag&#39;,envir=globalenv(),inherits=F) &amp;&amp; sqldfInitFlag) { ## false only on first run\r\n            sqldf(); ## creates a new connection\r\n        } else {\r\n            assign(&#39;sqldfInitFlag&#39;,T,envir=globalenv()); ## set to true for the one and only time\r\n        }; ## end if\r\n        invisible();\r\n    }; ## end initSqldf()\r\n\r\n    setUpBenchmarkCall &lt;- function(argSpecs,joinType,solTypes=getSolTypes(),env=parent.frame()) {\r\n        ## builds and returns a list of expressions suitable for passing to the list argument of microbenchmark(), and assigns variables to resolve symbol references in those expressions\r\n        callExpressions &lt;- list();\r\n        nms &lt;- character();\r\n        for (solType in solTypes) {\r\n            testFunc &lt;- solSpecs[[solType]]$testFuncs[[joinType]];\r\n            if (is.null(testFunc)) next; ## this join type is not defined for this solution type\r\n            testFuncName &lt;- paste0(&#39;tf.&#39;,solType);\r\n            assign(testFuncName,testFunc,envir=env);\r\n            argSpecKey &lt;- solSpecs[[solType]]$argSpec;\r\n            argSpec &lt;- getArgSpec(argSpecs,argSpecKey);\r\n            argList &lt;- setNames(nm=names(argSpec$args),vector(&#39;list&#39;,length(argSpec$args)));\r\n            for (i in seq_along(argSpec$args)) {\r\n                argName &lt;- paste0(&#39;tfa.&#39;,argSpecKey,i);\r\n                assign(argName,argSpec$args[[i]],envir=env);\r\n                argList[[i]] &lt;- if (i%in%argSpec$copySpec) call(&#39;copy&#39;,as.symbol(argName)) else as.symbol(argName);\r\n            }; ## end for\r\n            callExpressions[[length(callExpressions)+1L]] &lt;- do.call(call,c(list(testFuncName),argList),quote=T);\r\n            nms[length(nms)+1L] &lt;- solType;\r\n        }; ## end for\r\n        names(callExpressions) &lt;- nms;\r\n        callExpressions;\r\n    }; ## end setUpBenchmarkCall()\r\n\r\n    harmonize &lt;- function(res) {\r\n        res &lt;- as.data.frame(res); ## coerce to data.frame\r\n        for (ci in which(sapply(res,is.factor))) res[[ci]] &lt;- as.character(res[[ci]]); ## coerce factor columns to character\r\n        for (ci in which(sapply(res,is.logical))) res[[ci]] &lt;- as.integer(res[[ci]]); ## coerce logical columns to integer (works around sqldf quirk of munging logicals to integers)\r\n        ##for (ci in which(sapply(res,inherits,&#39;POSIXct&#39;))) res[[ci]] &lt;- as.double(res[[ci]]); ## coerce POSIXct columns to double (works around sqldf quirk of losing POSIXct class) ----- POSIXct doesn&#39;t work at all in sqldf.indexed\r\n        res &lt;- res[order(names(res))]; ## order columns\r\n        res &lt;- res[do.call(order,res),]; ## order rows\r\n        res;\r\n    }; ## end harmonize()\r\n\r\n    checkIdentical &lt;- function(argSpecs,solTypes=getSolTypes()) {\r\n        for (joinType in getJoinTypes()) {\r\n            callExpressions &lt;- setUpBenchmarkCall(argSpecs,joinType,solTypes);\r\n            if (length(callExpressions)&lt;2L) next;\r\n            ex &lt;- harmonize(eval(callExpressions[[1L]]));\r\n            for (i in seq(2L,len=length(callExpressions)-1L)) {\r\n                y &lt;- harmonize(eval(callExpressions[[i]]));\r\n                if (!isTRUE(all.equal(ex,y,check.attributes=F))) {\r\n                    ex &lt;&lt;- ex;\r\n                    y &lt;&lt;- y;\r\n                    solType &lt;- names(callExpressions)[i];\r\n                    stop(paste0(&#39;non-identical: &#39;,solType,&#39; &#39;,joinType,&#39;.&#39;));\r\n                }; ## end if\r\n            }; ## end for\r\n        }; ## end for\r\n        invisible();\r\n    }; ## end checkIdentical()\r\n\r\n    testJoinType &lt;- function(argSpecs,joinType,solTypes=getSolTypes(),metric=NULL,times=100L) {\r\n        callExpressions &lt;- setUpBenchmarkCall(argSpecs,joinType,solTypes);\r\n        bm &lt;- microbenchmark(list=callExpressions,times=times);\r\n        if (is.null(metric)) return(bm);\r\n        bm &lt;- summary(bm);\r\n        res &lt;- setNames(nm=names(callExpressions),bm[[metric]]);\r\n        attr(res,&#39;unit&#39;) &lt;- attr(bm,&#39;unit&#39;);\r\n        res;\r\n    }; ## end testJoinType()\r\n\r\n    testAllJoinTypes &lt;- function(argSpecs,solTypes=getSolTypes(),metric=NULL,times=100L) {\r\n        joinTypes &lt;- getJoinTypes();\r\n        resList &lt;- setNames(nm=joinTypes,lapply(joinTypes,function(joinType) testJoinType(argSpecs,joinType,solTypes,metric,times)));\r\n        if (is.null(metric)) return(resList);\r\n        units &lt;- unname(unlist(lapply(resList,attr,&#39;unit&#39;)));\r\n        res &lt;- do.call(data.frame,c(list(join=joinTypes),setNames(nm=solTypes,rep(list(rep(NA_real_,length(joinTypes))),length(solTypes))),list(unit=units,stringsAsFactors=F)));\r\n        for (i in seq_along(resList)) res[i,match(names(resList[[i]]),names(res))] &lt;- resList[[i]];\r\n        res;\r\n    }; ## end testAllJoinTypes()\r\n\r\n    testGrid &lt;- function(makeArgSpecsFunc,sizes,overlaps,solTypes=getSolTypes(),joinTypes=getJoinTypes(),metric=&#39;median&#39;,times=100L) {\r\n\r\n        res &lt;- expand.grid(size=sizes,overlap=overlaps,joinType=joinTypes,stringsAsFactors=F);\r\n        res[solTypes] &lt;- NA_real_;\r\n        res$unit &lt;- NA_character_;\r\n        for (ri in seq_len(nrow(res))) {\r\n\r\n            size &lt;- res$size[ri];\r\n            overlap &lt;- res$overlap[ri];\r\n            joinType &lt;- res$joinType[ri];\r\n\r\n            argSpecs &lt;- makeArgSpecsFunc(size,overlap);\r\n\r\n            checkIdentical(argSpecs,solTypes);\r\n\r\n            cur &lt;- testJoinType(argSpecs,joinType,solTypes,metric,times);\r\n            res[ri,match(names(cur),names(res))] &lt;- cur;\r\n            res$unit[ri] &lt;- attr(cur,&#39;unit&#39;);\r\n\r\n        }; ## end for\r\n\r\n        res;\r\n\r\n    }; ## end testGrid()\r\n\r\n---\r\n\r\nHere&#39;s a benchmark of the example based on the OP that I demonstrated earlier:\r\n\r\n    ## OP&#39;s example, supplemented with a non-matching row in df2\r\n    argSpecs &lt;- list(\r\n        default=list(copySpec=1:2,args=list(\r\n            df1 &lt;- data.frame(CustomerId=1:6,Product=c(rep(&#39;Toaster&#39;,3L),rep(&#39;Radio&#39;,3L))),\r\n            df2 &lt;- data.frame(CustomerId=c(2L,4L,6L,7L),State=c(rep(&#39;Alabama&#39;,2L),&#39;Ohio&#39;,&#39;Texas&#39;)),\r\n            &#39;CustomerId&#39;\r\n        )),\r\n        data.table.unkeyed=list(copySpec=1:2,args=list(\r\n            as.data.table(df1),\r\n            as.data.table(df2),\r\n            &#39;CustomerId&#39;\r\n        )),\r\n        data.table.keyed=list(copySpec=1:2,args=list(\r\n            setkey(as.data.table(df1),CustomerId),\r\n            setkey(as.data.table(df2),CustomerId)\r\n        ))\r\n    );\r\n    ## prepare sqldf\r\n    initSqldf();\r\n    sqldf(&#39;create index df1_key on df1(CustomerId);&#39;); ## upload and create an sqlite index on df1\r\n    sqldf(&#39;create index df2_key on df2(CustomerId);&#39;); ## upload and create an sqlite index on df2\r\n\r\n    checkIdentical(argSpecs);\r\n\r\n    testAllJoinTypes(argSpecs,metric=&#39;median&#39;);\r\n    ##    join    merge data.table.unkeyed data.table.keyed sqldf.unindexed sqldf.indexed      plyr    dplyr in.place         unit\r\n    ## 1 inner  644.259           861.9345          923.516        9157.752      1580.390  959.2250 270.9190       NA microseconds\r\n    ## 2  left  713.539           888.0205          910.045        8820.334      1529.714  968.4195 270.9185 224.3045 microseconds\r\n    ## 3 right 1221.804           909.1900          923.944        8930.668      1533.135 1063.7860 269.8495 218.1035 microseconds\r\n    ## 4  full 1302.203          3107.5380         3184.729              NA            NA 1593.6475 270.7055       NA microseconds\r\n\r\n---\r\n\r\nHere I benchmark on random input data, trying different scales and different patterns of key overlap between the two input tables. This benchmark is still restricted to the case of a single-column integer key. As well, to ensure that the in-place solution would work for both left and right joins of the same tables, all random test data uses `0..1:0..1` cardinality. This is implemented by sampling without replacement the key column of the first data.frame when generating the key column of the second data.frame.\r\n\r\n    makeArgSpecs.singleIntegerKey.optionalOneToOne &lt;- function(size,overlap) {\r\n\r\n        com &lt;- as.integer(size*overlap);\r\n\r\n        argSpecs &lt;- list(\r\n            default=list(copySpec=1:2,args=list(\r\n                df1 &lt;- data.frame(id=sample(size),y1=rnorm(size),y2=rnorm(size)),\r\n                df2 &lt;- data.frame(id=sample(c(if (com&gt;0L) sample(df1$id,com) else integer(),seq(size+1L,len=size-com))),y3=rnorm(size),y4=rnorm(size)),\r\n                &#39;id&#39;\r\n            )),\r\n            data.table.unkeyed=list(copySpec=1:2,args=list(\r\n                as.data.table(df1),\r\n                as.data.table(df2),\r\n                &#39;id&#39;\r\n            )),\r\n            data.table.keyed=list(copySpec=1:2,args=list(\r\n                setkey(as.data.table(df1),id),\r\n                setkey(as.data.table(df2),id)\r\n            ))\r\n        );\r\n        ## prepare sqldf\r\n        initSqldf();\r\n        sqldf(&#39;create index df1_key on df1(id);&#39;); ## upload and create an sqlite index on df1\r\n        sqldf(&#39;create index df2_key on df2(id);&#39;); ## upload and create an sqlite index on df2\r\n\r\n        argSpecs;\r\n\r\n    }; ## end makeArgSpecs.singleIntegerKey.optionalOneToOne()\r\n\r\n    ## cross of various input sizes and key overlaps\r\n    sizes &lt;- c(1e1L,1e3L,1e6L);\r\n    overlaps &lt;- c(0.99,0.5,0.01);\r\n    system.time({ res &lt;- testGrid(makeArgSpecs.singleIntegerKey.optionalOneToOne,sizes,overlaps); });\r\n    ##     user   system  elapsed\r\n    ## 22024.65 12308.63 34493.19\r\n\r\nI wrote some code to create log-log plots of the above results. I generated a separate plot for each overlap percentage. It&#39;s a little bit cluttered, but I like having all the solution types and join types represented in the same plot.\r\n\r\nI used spline interpolation to show a smooth curve for each solution/join type combination, drawn with individual pch symbols. The join type is captured by the pch symbol, using a dot for inner, left and right angle brackets for left and right, and a diamond for full. The solution type is captured by the color as shown in the legend.\r\n\r\n    plotRes &lt;- function(res,titleFunc,useFloor=F) {\r\n        solTypes &lt;- setdiff(names(res),c(&#39;size&#39;,&#39;overlap&#39;,&#39;joinType&#39;,&#39;unit&#39;)); ## derive from res\r\n        normMult &lt;- c(microseconds=1e-3,milliseconds=1); ## normalize to milliseconds\r\n        joinTypes &lt;- getJoinTypes();\r\n        cols &lt;- c(merge=&#39;purple&#39;,data.table.unkeyed=&#39;blue&#39;,data.table.keyed=&#39;#00DDDD&#39;,sqldf.unindexed=&#39;brown&#39;,sqldf.indexed=&#39;orange&#39;,plyr=&#39;red&#39;,dplyr=&#39;#00BB00&#39;,in.place=&#39;magenta&#39;);\r\n        pchs &lt;- list(inner=20L,left=&#39;&lt;&#39;,right=&#39;&gt;&#39;,full=23L);\r\n        cexs &lt;- c(inner=0.7,left=1,right=1,full=0.7);\r\n        NP &lt;- 60L;\r\n        ord &lt;- order(decreasing=T,colMeans(res[res$size==max(res$size),solTypes],na.rm=T));\r\n        ymajors &lt;- data.frame(y=c(1,1e3),label=c(&#39;1ms&#39;,&#39;1s&#39;),stringsAsFactors=F);\r\n        for (overlap in unique(res$overlap)) {\r\n            x1 &lt;- res[res$overlap==overlap,];\r\n            x1[solTypes] &lt;- x1[solTypes]*normMult[x1$unit]; x1$unit &lt;- NULL;\r\n            xlim &lt;- c(1e1,max(x1$size));\r\n            xticks &lt;- 10^seq(log10(xlim[1L]),log10(xlim[2L]));\r\n            ylim &lt;- c(1e-1,10^((if (useFloor) floor else ceiling)(log10(max(x1[solTypes],na.rm=T))))); ## use floor() to zoom in a little more, only sqldf.unindexed will break above, but xpd=NA will keep it visible\r\n            yticks &lt;- 10^seq(log10(ylim[1L]),log10(ylim[2L]));\r\n            yticks.minor &lt;- rep(yticks[-length(yticks)],each=9L)*1:9;\r\n            plot(NA,xlim=xlim,ylim=ylim,xaxs=&#39;i&#39;,yaxs=&#39;i&#39;,axes=F,xlab=&#39;size (rows)&#39;,ylab=&#39;time (ms)&#39;,log=&#39;xy&#39;);\r\n            abline(v=xticks,col=&#39;lightgrey&#39;);\r\n            abline(h=yticks.minor,col=&#39;lightgrey&#39;,lty=3L);\r\n            abline(h=yticks,col=&#39;lightgrey&#39;);\r\n            axis(1L,xticks,parse(text=sprintf(&#39;10^%d&#39;,as.integer(log10(xticks)))));\r\n            axis(2L,yticks,parse(text=sprintf(&#39;10^%d&#39;,as.integer(log10(yticks)))),las=1L);\r\n            axis(4L,ymajors$y,ymajors$label,las=1L,tick=F,cex.axis=0.7,hadj=0.5);\r\n            for (joinType in rev(joinTypes)) { ## reverse to draw full first, since it&#39;s larger and would be more obtrusive if drawn last\r\n                x2 &lt;- x1[x1$joinType==joinType,];\r\n                for (solType in solTypes) {\r\n                    if (any(!is.na(x2[[solType]]))) {\r\n                        xy &lt;- spline(x2$size,x2[[solType]],xout=10^(seq(log10(x2$size[1L]),log10(x2$size[nrow(x2)]),len=NP)));\r\n                        points(xy$x,xy$y,pch=pchs[[joinType]],col=cols[solType],cex=cexs[joinType],xpd=NA);\r\n                    }; ## end if\r\n                }; ## end for\r\n            }; ## end for\r\n            ## custom legend\r\n            ## due to logarithmic skew, must do all distance calcs in inches, and convert to user coords afterward\r\n            ## the bottom-left corner of the legend will be defined in normalized figure coords, although we can convert to inches immediately\r\n            leg.cex &lt;- 0.7;\r\n            leg.x.in &lt;- grconvertX(0.275,&#39;nfc&#39;,&#39;in&#39;);\r\n            leg.y.in &lt;- grconvertY(0.6,&#39;nfc&#39;,&#39;in&#39;);\r\n            leg.x.user &lt;- grconvertX(leg.x.in,&#39;in&#39;);\r\n            leg.y.user &lt;- grconvertY(leg.y.in,&#39;in&#39;);\r\n            leg.outpad.w.in &lt;- 0.1;\r\n            leg.outpad.h.in &lt;- 0.1;\r\n            leg.midpad.w.in &lt;- 0.1;\r\n            leg.midpad.h.in &lt;- 0.1;\r\n            leg.sol.w.in &lt;- max(strwidth(solTypes,&#39;in&#39;,leg.cex));\r\n            leg.sol.h.in &lt;- max(strheight(solTypes,&#39;in&#39;,leg.cex))*1.5; ## multiplication factor for greater line height\r\n            leg.join.w.in &lt;- max(strheight(joinTypes,&#39;in&#39;,leg.cex))*1.5; ## ditto\r\n            leg.join.h.in &lt;- max(strwidth(joinTypes,&#39;in&#39;,leg.cex));\r\n            leg.main.w.in &lt;- leg.join.w.in*length(joinTypes);\r\n            leg.main.h.in &lt;- leg.sol.h.in*length(solTypes);\r\n            leg.x2.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in*2+leg.main.w.in+leg.midpad.w.in+leg.sol.w.in,&#39;in&#39;);\r\n            leg.y2.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in*2+leg.main.h.in+leg.midpad.h.in+leg.join.h.in,&#39;in&#39;);\r\n            leg.cols.x.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in+leg.join.w.in*(0.5+seq(0L,length(joinTypes)-1L)),&#39;in&#39;);\r\n            leg.lines.y.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in+leg.main.h.in-leg.sol.h.in*(0.5+seq(0L,length(solTypes)-1L)),&#39;in&#39;);\r\n            leg.sol.x.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in+leg.main.w.in+leg.midpad.w.in,&#39;in&#39;);\r\n            leg.join.y.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in+leg.main.h.in+leg.midpad.h.in,&#39;in&#39;);\r\n            rect(leg.x.user,leg.y.user,leg.x2.user,leg.y2.user,col=&#39;white&#39;);\r\n            text(leg.sol.x.user,leg.lines.y.user,solTypes[ord],cex=leg.cex,pos=4L,offset=0);\r\n            text(leg.cols.x.user,leg.join.y.user,joinTypes,cex=leg.cex,pos=4L,offset=0,srt=90); ## srt rotation applies *after* pos/offset positioning\r\n            for (i in seq_along(joinTypes)) {\r\n                joinType &lt;- joinTypes[i];\r\n                points(rep(leg.cols.x.user[i],length(solTypes)),ifelse(colSums(!is.na(x1[x1$joinType==joinType,solTypes[ord]]))==0L,NA,leg.lines.y.user),pch=pchs[[joinType]],col=cols[solTypes[ord]]);\r\n            }; ## end for\r\n            title(titleFunc(overlap));\r\n            readline(sprintf(&#39;overlap %.02f&#39;,overlap));\r\n        }; ## end for\r\n    }; ## end plotRes()\r\n\r\n    titleFunc &lt;- function(overlap) sprintf(&#39;R merge solutions: single-column integer key, 0..1:0..1 cardinality, %d%% overlap&#39;,as.integer(overlap*100));\r\n    plotRes(res,titleFunc,T);\r\n\r\n[![R-merge-benchmark-single-column-integer-key-optional-one-to-one-99][1]][1]\r\n\r\n[![R-merge-benchmark-single-column-integer-key-optional-one-to-one-50][2]][2]\r\n\r\n[![R-merge-benchmark-single-column-integer-key-optional-one-to-one-1][3]][3]\r\n\r\n---\r\n\r\nHere&#39;s a second large-scale benchmark that&#39;s more heavy-duty, with respect to the number and types of key columns, as well as cardinality. For this benchmark I use three key columns: one character, one integer, and one logical, with no restrictions on cardinality (that is, `0..*:0..*`). (In general it&#39;s not advisable to define key columns with double or complex values due to floating-point comparison complications, and basically no one ever uses the raw type, much less for key columns, so I haven&#39;t included those types in the key columns. Also, for information&#39;s sake, I initially tried to use four key columns by including a POSIXct key column, but the POSIXct type didn&#39;t play well with the `sqldf.indexed` solution for some reason, possibly due to floating-point comparison anomalies, so I removed it.)\r\n\r\n    makeArgSpecs.assortedKey.optionalManyToMany &lt;- function(size,overlap,uniquePct=75) {\r\n\r\n        ## number of unique keys in df1\r\n        u1Size &lt;- as.integer(size*uniquePct/100);\r\n\r\n        ## (roughly) divide u1Size into bases, so we can use expand.grid() to produce the required number of unique key values with repetitions within individual key columns\r\n        ## use ceiling() to ensure we cover u1Size; will truncate afterward\r\n        u1SizePerKeyColumn &lt;- as.integer(ceiling(u1Size^(1/3)));\r\n\r\n        ## generate the unique key values for df1\r\n        keys1 &lt;- expand.grid(stringsAsFactors=F,\r\n            idCharacter=replicate(u1SizePerKeyColumn,paste(collapse=&#39;&#39;,sample(letters,sample(4:12,1L),T))),\r\n            idInteger=sample(u1SizePerKeyColumn),\r\n            idLogical=sample(c(F,T),u1SizePerKeyColumn,T)\r\n            ##idPOSIXct=as.POSIXct(&#39;2016-01-01 00:00:00&#39;,&#39;UTC&#39;)+sample(u1SizePerKeyColumn)\r\n        )[seq_len(u1Size),];\r\n\r\n        ## rbind some repetitions of the unique keys; this will prepare one side of the many-to-many relationship\r\n        ## also scramble the order afterward\r\n        keys1 &lt;- rbind(keys1,keys1[sample(nrow(keys1),size-u1Size,T),])[sample(size),];\r\n\r\n        ## common and unilateral key counts\r\n        com &lt;- as.integer(size*overlap);\r\n        uni &lt;- size-com;\r\n\r\n        ## generate some unilateral keys for df2 by synthesizing outside of the idInteger range of df1\r\n        keys2 &lt;- data.frame(stringsAsFactors=F,\r\n            idCharacter=replicate(uni,paste(collapse=&#39;&#39;,sample(letters,sample(4:12,1L),T))),\r\n            idInteger=u1SizePerKeyColumn+sample(uni),\r\n            idLogical=sample(c(F,T),uni,T)\r\n            ##idPOSIXct=as.POSIXct(&#39;2016-01-01 00:00:00&#39;,&#39;UTC&#39;)+u1SizePerKeyColumn+sample(uni)\r\n        );\r\n\r\n        ## rbind random keys from df1; this will complete the many-to-many relationship\r\n        ## also scramble the order afterward\r\n        keys2 &lt;- rbind(keys2,keys1[sample(nrow(keys1),com,T),])[sample(size),];\r\n\r\n        ##keyNames &lt;- c(&#39;idCharacter&#39;,&#39;idInteger&#39;,&#39;idLogical&#39;,&#39;idPOSIXct&#39;);\r\n        keyNames &lt;- c(&#39;idCharacter&#39;,&#39;idInteger&#39;,&#39;idLogical&#39;);\r\n        ## note: was going to use raw and complex type for two of the non-key columns, but data.table doesn&#39;t seem to fully support them\r\n        argSpecs &lt;- list(\r\n            default=list(copySpec=1:2,args=list(\r\n                df1 &lt;- cbind(stringsAsFactors=F,keys1,y1=sample(c(F,T),size,T),y2=sample(size),y3=rnorm(size),y4=replicate(size,paste(collapse=&#39;&#39;,sample(letters,sample(4:12,1L),T)))),\r\n                df2 &lt;- cbind(stringsAsFactors=F,keys2,y5=sample(c(F,T),size,T),y6=sample(size),y7=rnorm(size),y8=replicate(size,paste(collapse=&#39;&#39;,sample(letters,sample(4:12,1L),T)))),\r\n                keyNames\r\n            )),\r\n            data.table.unkeyed=list(copySpec=1:2,args=list(\r\n                as.data.table(df1),\r\n                as.data.table(df2),\r\n                keyNames\r\n            )),\r\n            data.table.keyed=list(copySpec=1:2,args=list(\r\n                setkeyv(as.data.table(df1),keyNames),\r\n                setkeyv(as.data.table(df2),keyNames)\r\n            ))\r\n        );\r\n        ## prepare sqldf\r\n        initSqldf();\r\n        sqldf(paste0(&#39;create index df1_key on df1(&#39;,paste(collapse=&#39;,&#39;,keyNames),&#39;);&#39;)); ## upload and create an sqlite index on df1\r\n        sqldf(paste0(&#39;create index df2_key on df2(&#39;,paste(collapse=&#39;,&#39;,keyNames),&#39;);&#39;)); ## upload and create an sqlite index on df2\r\n\r\n        argSpecs;\r\n\r\n    }; ## end makeArgSpecs.assortedKey.optionalManyToMany()\r\n\r\n    sizes &lt;- c(1e1L,1e3L,1e5L); ## 1e5L instead of 1e6L to respect more heavy-duty inputs\r\n    overlaps &lt;- c(0.99,0.5,0.01);\r\n    solTypes &lt;- setdiff(getSolTypes(),&#39;in.place&#39;);\r\n    system.time({ res &lt;- testGrid(makeArgSpecs.assortedKey.optionalManyToMany,sizes,overlaps,solTypes); });\r\n    ##     user   system  elapsed\r\n    ## 38895.50   784.19 39745.53\r\n\r\nThe resulting plots, using the same plotting code given above:\r\n\r\n    titleFunc &lt;- function(overlap) sprintf(&#39;R merge solutions: character/integer/logical key, 0..*:0..* cardinality, %d%% overlap&#39;,as.integer(overlap*100));\r\n    plotRes(res,titleFunc,F);\r\n\r\n[![R-merge-benchmark-assorted-key-optional-many-to-many-99][4]][4]\r\n\r\n[![R-merge-benchmark-assorted-key-optional-many-to-many-50][5]][5]\r\n\r\n[![R-merge-benchmark-assorted-key-optional-many-to-many-1][6]][6]\r\n\r\n  [1]: http://i.stack.imgur.com/yvTPK.png\r\n  [2]: http://i.stack.imgur.com/kUyPB.png\r\n  [3]: http://i.stack.imgur.com/vAuxQ.png\r\n  [4]: http://i.stack.imgur.com/7gShO.png\r\n  [5]: http://i.stack.imgur.com/yTeRO.png\r\n  [6]: http://i.stack.imgur.com/buxZ2.png",
               "tags": [],
               "creation_date": 1467310286,
               "last_edit_date": 1495540503,
               "is_accepted": false,
               "id": "38130460",
               "down_vote_count": 0,
               "score": 16
            },
            {
               "up_vote_count": 5,
               "answer_id": 46154770,
               "last_activity_date": 1509594514,
               "path": "3.stack.answer",
               "body_markdown": "For an inner join on all columns, you could also use `fintersect` from the *data.table*-package or `intersect` from the *dplyr*-package as an alternative to `merge` without specifying the `by`-columns. this will give the rows that are equal between two dataframes:\r\n\r\n    merge(df1, df2)\r\n    #   V1 V2\r\n    # 1  B  2\r\n    # 2  C  3\r\n    dplyr::intersect(df1, df2)\r\n    #   V1 V2\r\n    # 1  B  2\r\n    # 2  C  3\r\n    data.table::fintersect(setDT(df1), setDT(df2))\r\n    #    V1 V2\r\n    # 1:  B  2\r\n    # 2:  C  3\r\n\r\n----------\r\nExample data:\r\n\r\n    df1 &lt;- data.frame(V1 = LETTERS[1:4], V2 = 1:4)\r\n    df2 &lt;- data.frame(V1 = LETTERS[2:3], V2 = 2:3)\r\n",
               "tags": [],
               "creation_date": 1505129731,
               "last_edit_date": 1509594514,
               "is_accepted": false,
               "id": "46154770",
               "down_vote_count": 0,
               "score": 5
            }
         ],
         "link": "https://stackoverflow.com/questions/1299871/how-to-join-merge-data-frames-inner-outer-left-right",
         "id": "858127-2295"
      },
      {
         "up_vote_count": "371",
         "path": "2.stack",
         "body_markdown": "I understand that pandas is designed to load fully populated `DataFrame` but I need to **create an empty DataFrame then add rows, one by one**.\r\nWhat is the best way to do this ?\r\n\r\nI successfully created an empty DataFrame with :\r\n\r\n    res = DataFrame(columns=(&#39;lib&#39;, &#39;qty1&#39;, &#39;qty2&#39;))\r\n\r\nThen I can add a new row and fill a field with :\r\n\r\n    res = res.set_value(len(res), &#39;qty1&#39;, 10.0)\r\n\r\nIt works but seems very odd :-/ (it fails for adding string value)\r\n\r\nHow can I add a new row to my DataFrame (with different columns type) ?\r\n",
         "view_count": "464606",
         "answer_count": "16",
         "tags": "['python', 'pandas']",
         "creation_date": "1337760751",
         "last_edit_date": "1504166957",
         "code_snippet": "['<code>DataFrame</code>', \"<code>res = DataFrame(columns=('lib', 'qty1', 'qty2'))\\n</code>\", \"<code>res = res.set_value(len(res), 'qty1', 10.0)\\n</code>\", \"<code>&gt;&gt;&gt; import pandas as pd\\n&gt;&gt;&gt; df = pd.DataFrame(columns=['lib', 'qty1', 'qty2'])\\n&gt;&gt;&gt; for i in range(5):\\n&gt;&gt;&gt;     df.loc[i] = [randint(-1,1) for n in range(3)]\\n&gt;&gt;&gt;\\n&gt;&gt;&gt; print(df)\\n    lib  qty1  qty2\\n0    0     0    -1\\n1   -1    -1     1\\n2    1    -1     1\\n3    0     0     0\\n4    1    -1    -1\\n\\n[5 rows x 3 columns]\\n</code>\", '<code>.loc</code>', '<code>.loc</code>', '<code>df.loc[df.index.max() + 1] = [randint(...</code>', '<code>pandas.concat()</code>', '<code>DataFrame.append()</code>', '<code>rows_list = []\\nfor row in input_rows:\\n\\n        dict1 = {}\\n        # get input row in dictionary format\\n        # key = col_name\\n        dict1.update(blah..) \\n\\n        rows_list.append(dict1)\\n\\ndf = pd.DataFrame(rows_list)               \\n</code>', '<code>It is worth noting however, that concat (and therefore append) makes a full copy of the data, and that constantly reusing this function can create a significant performance hit. If you need to use the operation over several datasets, use a list comprehension.</code>', \"<code>import pandas as pd\\nimport numpy as np\\n# we know we're gonna have 5 rows of data\\nnumberOfRows = 5\\n# create dataframe\\ndf = pd.DataFrame(index=np.arange(0, numberOfRows), columns=('lib', 'qty1', 'qty2') )\\n\\n# now fill it up row by row\\nfor x in np.arange(0, numberOfRows):\\n    #loc or iloc both work here since the index is natural numbers\\n    df.loc[x] = [np.random.randint(-1,1) for n in range(3)]\\nIn[23]: df\\nOut[23]: \\n   lib  qty1  qty2\\n0   -1    -1    -1\\n1    0     0     0\\n2   -1     0    -1\\n3    0    -1     0\\n4   -1     0     0\\n</code>\", '<code>In[30]: %timeit tryThis() # function wrapper for this answer\\nIn[31]: %timeit tryOther() # function wrapper without index (see, for example, @fred)\\n1000 loops, best of 3: 1.23 ms per loop\\n100 loops, best of 3: 2.31 ms per loop\\n</code>', '<code>loc/ix</code>', '<code>In [1]: se = pd.Series([1,2,3])\\n\\nIn [2]: se\\nOut[2]: \\n0    1\\n1    2\\n2    3\\ndtype: int64\\n\\nIn [3]: se[5] = 5.\\n\\nIn [4]: se\\nOut[4]: \\n0    1.0\\n1    2.0\\n2    3.0\\n5    5.0\\ndtype: float64\\n</code>', \"<code>In [1]: dfi = pd.DataFrame(np.arange(6).reshape(3,2),\\n   .....:                 columns=['A','B'])\\n   .....: \\n\\nIn [2]: dfi\\nOut[2]: \\n   A  B\\n0  0  1\\n1  2  3\\n2  4  5\\n\\nIn [3]: dfi.loc[:,'C'] = dfi.loc[:,'A']\\n\\nIn [4]: dfi\\nOut[4]: \\n   A  B  C\\n0  0  1  0\\n1  2  3  2\\n2  4  5  4\\nIn [5]: dfi.loc[3] = 5\\n\\nIn [6]: dfi\\nOut[6]: \\n   A  B  C\\n0  0  1  0\\n1  2  3  2\\n2  4  5  4\\n3  5  5  5\\n</code>\", \"<code>mycolumns = ['A', 'B']\\ndf = pd.DataFrame(columns=mycolumns)\\nrows = [[1,2],[3,4],[5,6]]\\nfor row in rows:\\n    df.loc[len(df)] = row\\n</code>\", '<code>ignore_index</code>', \"<code>&gt;&gt;&gt; f = pandas.DataFrame(data = {'Animal':['cow','horse'], 'Color':['blue', 'red']})\\n&gt;&gt;&gt; f\\n  Animal Color\\n0    cow  blue\\n1  horse   red\\n&gt;&gt;&gt; f.append({'Animal':'mouse', 'Color':'black'}, ignore_index=True)\\n  Animal  Color\\n0    cow   blue\\n1  horse    red\\n2  mouse  black\\n</code>\", '<code>f.append(&lt;stuff&gt;)</code>', '<code>f = f.append(&lt;stuff&gt;)</code>', \"<code>res = pd.DataFrame(columns=('lib', 'qty1', 'qty2'))\\nres = res.append([{'qty1':10.0}], ignore_index=True)\\nprint(res.head())\\n\\n   lib  qty1  qty2\\n0  NaN  10.0   NaN\\n</code>\", \"<code>import pandas as pd \\n\\nBaseData = pd.DataFrame({ 'Customer' : ['Acme','Mega','Acme','Acme','Mega','Acme'],\\n                          'Territory'  : ['West','East','South','West','East','South'],\\n                          'Product'  : ['Econ','Luxe','Econ','Std','Std','Econ']})\\nBaseData\\n\\ncolumns = ['Customer','Num Unique Products', 'List Unique Products']\\n\\nrows_list=[]\\nfor name, group in BaseData.groupby('Customer'):\\n    RecordtoAdd={} #initialise an empty dict \\n    RecordtoAdd.update({'Customer' : name}) #\\n    RecordtoAdd.update({'Num Unique Products' : len(pd.unique(group['Product']))})      \\n    RecordtoAdd.update({'List Unique Products' : pd.unique(group['Product'])})                   \\n\\n    rows_list.append(RecordtoAdd)\\n\\nAnalysedData = pd.DataFrame(rows_list)\\n\\nprint('Base Data : \\\\n',BaseData,'\\\\n\\\\n Analysed Data : \\\\n',AnalysedData)\\n</code>\", \"<code>new_record = pd.DataFrame([[0,'abcd',0,1,123]],columns=['a','b','c','d','e'])\\n\\nold_data_frame = pd.concat([old_data_frame,new_record])\\n</code>\", '<code># add a row\\ndef add_row(df, row):\\n    colnames = list(df.columns)\\n    ncol = len(colnames)\\n    assert ncol == len(row), \"Length of row must be the same as width of DataFrame: %s\" % row\\n    return df.append(pd.DataFrame([row], columns=colnames))\\n</code>', '<code>import pandas as pd\\ndef add_row(self, row):\\n    self.loc[len(self.index)] = row\\npd.DataFrame.add_row = add_row\\n</code>', \"<code>import pandas as pd\\n\\nrows = []\\ncolumns = ['i','double','square']\\n\\nfor i in range(6):\\n    row = [i, i*2, i*i]\\n    rows.append(row)\\n\\ndf = pd.DataFrame(rows, columns=columns)\\n</code>\", '<code>Adding    1000 rows  5000 rows   10000 rows\\n.append   1.04       4.84        9.56\\n.loc      1.16       5.59        11.50\\ndict      0.23       0.26        0.34\\n</code>', \"<code>import pandas\\nimport numpy\\nimport time\\n\\nnumOfRows = 10000\\nstartTime = time.perf_counter()\\ndf1 = pandas.DataFrame(numpy.random.randint(100, size=(5,5)), columns=['A', 'B', 'C', 'D', 'E'])\\nfor i in range( 1,numOfRows):\\n    df1 = df1.append( dict( (a,numpy.random.randint(100)) for a in ['A','B','C','D','E']), ignore_index=True)\\nprint('Elapsed time: {:6.3f} seconds for {:d} rows'.format(time.perf_counter() - startTime, numOfRows))\\n\\nstartTime = time.perf_counter()\\ndf2 = pandas.DataFrame(numpy.random.randint(100, size=(5,5)), columns=['A', 'B', 'C', 'D', 'E'])\\nfor i in range( 1,numOfRows):\\n    df2.loc[df2.index.max()+1]  = numpy.random.randint(100, size=(1,5))[0]\\nprint('Elapsed time: {:6.3f} seconds for {:d} rows'.format(time.perf_counter() - startTime, numOfRows))\\n\\nstartTime = time.perf_counter()\\nrow_list = []\\nfor i in range (0,5):\\n    row_list.append(dict( (a,numpy.random.randint(100)) for a in ['A','B','C','D','E']))\\nfor i in range( 1,numOfRows):\\n    dict1 = dict( (a,numpy.random.randint(100)) for a in ['A','B','C','D','E'])\\n    row_list.append(dict1)\\n\\ndf3 = pandas.DataFrame(row_list, columns=['A','B','C','D','E'])\\nprint('Elapsed time: {:6.3f} seconds for {:d} rows'.format(time.perf_counter() - startTime, numOfRows))\\n</code>\", '<code>import pandas as pd \\nt1=pd.DataFrame()\\nfor i in range(len(the number of rows)):\\n    #add rows as columns\\n    t1[i]=list(rows)\\nt1=t1.transpose()\\nt1.columns=list(columns)\\n</code>', \"<code>import pandas as pd  \\nres = pd.DataFrame(columns=('lib', 'qty1', 'qty2'))  \\nfor i in range(5):  \\n    res_list = list(map(int, input().split()))  \\n    res = res.append(pd.Series(res_list,index=['lib','qty1','qty2']), ignore_index=True)\\n</code>\", \"<code>df = pd.DataFrame(columns=['timeMS', 'accelX', 'accelY', 'accelZ', 'gyroX', 'gyroY', 'gyroZ'])\\n\\ndf.loc[0 if math.isnan(df.index.max()) else df.index.max() + 1] = [x for x in range(7)]\\n</code>\"]",
         "title": "add one row in a pandas.DataFrame",
         "_childDocuments_": [
            {
               "up_vote_count": 210,
               "answer_id": 10716007,
               "last_activity_date": 1350502693,
               "path": "3.stack.answer",
               "body_markdown": "You could use `pandas.concat()` or `DataFrame.append()`. For details and examples, see [Merge, join, and concatenate][1].\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/merging.html",
               "tags": [],
               "creation_date": 1337760883,
               "last_edit_date": 1350502693,
               "is_accepted": false,
               "id": "10716007",
               "down_vote_count": 4,
               "score": 206
            },
            {
               "up_vote_count": 165,
               "answer_id": 17496530,
               "last_activity_date": 1494151843,
               "path": "3.stack.answer",
               "body_markdown": "You could create a list of dictionaries, where each dictionary corresponds to an input data row. Once the list is complete, then create a data frame. This is a much faster approach. \r\n\r\nI had a similar problem where if I created a data frame for each row and appended it to the main data frame it took 30 mins. On the other hand, if I used the below methodology, it was successful within seconds.\r\n\r\n    rows_list = []\r\n    for row in input_rows:\r\n           \r\n            dict1 = {}\r\n            # get input row in dictionary format\r\n            # key = col_name\r\n            dict1.update(blah..) \r\n\r\n            rows_list.append(dict1)\r\n\r\n    df = pd.DataFrame(rows_list)               \r\n\r\n",
               "tags": [],
               "creation_date": 1373056693,
               "last_edit_date": 1494151843,
               "is_accepted": false,
               "id": "17496530",
               "down_vote_count": 1,
               "score": 164
            },
            {
               "up_vote_count": 44,
               "answer_id": 23394497,
               "last_activity_date": 1488160027,
               "path": "3.stack.answer",
               "body_markdown": "For efficient appending see https://stackoverflow.com/questions/19365513/how-to-add-an-extra-row-to-a-pandas-dataframe/19368360#19368360 and [*Setting With Enlargement*][1].\r\n\r\nAdd rows through `loc/ix` on **non existing** key index data. e.g. :\r\n\r\n    In [1]: se = pd.Series([1,2,3])\r\n    \r\n    In [2]: se\r\n    Out[2]: \r\n    0    1\r\n    1    2\r\n    2    3\r\n    dtype: int64\r\n    \r\n    In [3]: se[5] = 5.\r\n    \r\n    In [4]: se\r\n    Out[4]: \r\n    0    1.0\r\n    1    2.0\r\n    2    3.0\r\n    5    5.0\r\n    dtype: float64\r\n\r\nOr:\r\n\r\n    In [1]: dfi = pd.DataFrame(np.arange(6).reshape(3,2),\r\n       .....:                 columns=[&#39;A&#39;,&#39;B&#39;])\r\n       .....: \r\n    \r\n    In [2]: dfi\r\n    Out[2]: \r\n       A  B\r\n    0  0  1\r\n    1  2  3\r\n    2  4  5\r\n    \r\n    In [3]: dfi.loc[:,&#39;C&#39;] = dfi.loc[:,&#39;A&#39;]\r\n    \r\n    In [4]: dfi\r\n    Out[4]: \r\n       A  B  C\r\n    0  0  1  0\r\n    1  2  3  2\r\n    2  4  5  4\r\n    In [5]: dfi.loc[3] = 5\r\n    \r\n    In [6]: dfi\r\n    Out[6]: \r\n       A  B  C\r\n    0  0  1  0\r\n    1  2  3  2\r\n    2  4  5  4\r\n    3  5  5  5\r\n\r\n  [1]: http://pandas-docs.github.io/pandas-docs-travis/indexing.html#setting-with-enlargement",
               "tags": [],
               "creation_date": 1398879064,
               "last_edit_date": 1495542393,
               "is_accepted": false,
               "id": "23394497",
               "down_vote_count": 0,
               "score": 44
            },
            {
               "up_vote_count": 228,
               "answer_id": 24888331,
               "last_activity_date": 1508774545,
               "path": "3.stack.answer",
               "body_markdown": "Example at @Nasser&#39;s answer:\r\n    \r\n    &gt;&gt;&gt; import pandas as pd\r\n    &gt;&gt;&gt; df = pd.DataFrame(columns=[&#39;lib&#39;, &#39;qty1&#39;, &#39;qty2&#39;])\r\n    &gt;&gt;&gt; for i in range(5):\r\n    &gt;&gt;&gt;     df.loc[i] = [randint(-1,1) for n in range(3)]\r\n    &gt;&gt;&gt;\r\n    &gt;&gt;&gt; print(df)\r\n        lib  qty1  qty2\r\n    0    0     0    -1\r\n    1   -1    -1     1\r\n    2    1    -1     1\r\n    3    0     0     0\r\n    4    1    -1    -1\r\n\r\n    [5 rows x 3 columns]\r\n\r\n \r\n",
               "tags": [],
               "creation_date": 1406034625,
               "last_edit_date": 1508774545,
               "is_accepted": true,
               "id": "24888331",
               "down_vote_count": 3,
               "score": 225
            },
            {
               "up_vote_count": 55,
               "answer_id": 24913075,
               "last_activity_date": 1427976228,
               "path": "3.stack.answer",
               "body_markdown": "If you know the number of entries ex ante, you should preallocate the space by also providing the index (taking the data example from a different answer):\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    # we know we&#39;re gonna have 5 rows of data\r\n    numberOfRows = 5\r\n    # create dataframe\r\n    df = pd.DataFrame(index=np.arange(0, numberOfRows), columns=(&#39;lib&#39;, &#39;qty1&#39;, &#39;qty2&#39;) )\r\n    \r\n    # now fill it up row by row\r\n    for x in np.arange(0, numberOfRows):\r\n        #loc or iloc both work here since the index is natural numbers\r\n        df.loc[x] = [np.random.randint(-1,1) for n in range(3)]\r\n    In[23]: df\r\n    Out[23]: \r\n       lib  qty1  qty2\r\n    0   -1    -1    -1\r\n    1    0     0     0\r\n    2   -1     0    -1\r\n    3    0    -1     0\r\n    4   -1     0     0\r\n\r\n**Speed comparison**\r\n\r\n    In[30]: %timeit tryThis() # function wrapper for this answer\r\n    In[31]: %timeit tryOther() # function wrapper without index (see, for example, @fred)\r\n    1000 loops, best of 3: 1.23 ms per loop\r\n    100 loops, best of 3: 2.31 ms per loop\r\n\r\nAnd - as from the comments - with a size of 6000, the speed difference becomes even larger:\r\n\r\n&gt; Increasing the size of the array (12) and the number of rows (500) makes\r\n&gt; the speed difference more striking: 313ms vs 2.29s",
               "tags": [],
               "creation_date": 1406125305,
               "last_edit_date": 1427976228,
               "is_accepted": false,
               "id": "24913075",
               "down_vote_count": 1,
               "score": 54
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 41,
               "answer_id": 31037040,
               "is_accepted": false,
               "last_activity_date": 1435179991,
               "body_markdown": "    mycolumns = [&#39;A&#39;, &#39;B&#39;]\r\n    df = pd.DataFrame(columns=mycolumns)\r\n    rows = [[1,2],[3,4],[5,6]]\r\n    for row in rows:\r\n        df.loc[len(df)] = row",
               "id": "31037040",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1435179991,
               "score": 41
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 24,
               "answer_id": 35583219,
               "is_accepted": false,
               "last_activity_date": 1456245787,
               "body_markdown": "You can append a single row as a dictionary using the `ignore_index` option.\r\n\r\n    &gt;&gt;&gt; f = pandas.DataFrame(data = {&#39;Animal&#39;:[&#39;cow&#39;,&#39;horse&#39;], &#39;Color&#39;:[&#39;blue&#39;, &#39;red&#39;]})\r\n    &gt;&gt;&gt; f\r\n      Animal Color\r\n    0    cow  blue\r\n    1  horse   red\r\n    &gt;&gt;&gt; f.append({&#39;Animal&#39;:&#39;mouse&#39;, &#39;Color&#39;:&#39;black&#39;}, ignore_index=True)\r\n      Animal  Color\r\n    0    cow   blue\r\n    1  horse    red\r\n    2  mouse  black\r\n\r\n",
               "id": "35583219",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1456245787,
               "score": 24
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 11,
               "answer_id": 38348167,
               "is_accepted": false,
               "last_activity_date": 1468403358,
               "body_markdown": "This is not an answer to the OP question but a toy example to illustrate the answer of @ShikharDua above which I found very useful. \r\n\r\nWhile this fragment is trivial, in the actual data I had 1,000s of rows, and many columns, and I wished to be able to group by different columns and then perform the stats below for more than one taget column. So having a reliable method for building the data frame one row at a time was a great convenience. Thank you @ShikharDua ! \r\n\r\n    import pandas as pd \r\n    \r\n    BaseData = pd.DataFrame({ &#39;Customer&#39; : [&#39;Acme&#39;,&#39;Mega&#39;,&#39;Acme&#39;,&#39;Acme&#39;,&#39;Mega&#39;,&#39;Acme&#39;],\r\n                              &#39;Territory&#39;  : [&#39;West&#39;,&#39;East&#39;,&#39;South&#39;,&#39;West&#39;,&#39;East&#39;,&#39;South&#39;],\r\n                              &#39;Product&#39;  : [&#39;Econ&#39;,&#39;Luxe&#39;,&#39;Econ&#39;,&#39;Std&#39;,&#39;Std&#39;,&#39;Econ&#39;]})\r\n    BaseData\r\n    \r\n    columns = [&#39;Customer&#39;,&#39;Num Unique Products&#39;, &#39;List Unique Products&#39;]\r\n        \r\n    rows_list=[]\r\n    for name, group in BaseData.groupby(&#39;Customer&#39;):\r\n        RecordtoAdd={} #initialise an empty dict \r\n        RecordtoAdd.update({&#39;Customer&#39; : name}) #\r\n        RecordtoAdd.update({&#39;Num Unique Products&#39; : len(pd.unique(group[&#39;Product&#39;]))})      \r\n        RecordtoAdd.update({&#39;List Unique Products&#39; : pd.unique(group[&#39;Product&#39;])})                   \r\n            \r\n        rows_list.append(RecordtoAdd)\r\n        \r\n    AnalysedData = pd.DataFrame(rows_list)\r\n    \r\n    print(&#39;Base Data : \\n&#39;,BaseData,&#39;\\n\\n Analysed Data : \\n&#39;,AnalysedData)",
               "id": "38348167",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1468403358,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 38433682,
               "is_accepted": false,
               "last_activity_date": 1468835643,
               "body_markdown": "Create a **new record(data frame)** and add to **old_data_frame**.    \r\npass list of **values** and corresponding **column** names to create a **new_record** (data_frame)\r\n\r\n    new_record = pd.DataFrame([[0,&#39;abcd&#39;,0,1,123]],columns=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;])\r\n\r\n    old_data_frame = pd.concat([old_data_frame,new_record])",
               "id": "38433682",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1468835643,
               "score": 4
            },
            {
               "up_vote_count": 1,
               "answer_id": 40553910,
               "last_activity_date": 1478888700,
               "path": "3.stack.answer",
               "body_markdown": "Another way to do it (probably not very performant):\r\n\r\n    # add a row\r\n    def add_row(df, row):\r\n        colnames = list(df.columns)\r\n        ncol = len(colnames)\r\n        assert ncol == len(row), &quot;Length of row must be the same as width of DataFrame: %s&quot; % row\r\n        return df.append(pd.DataFrame([row], columns=colnames))\r\n\r\n\r\nYou can also enhance the DataFrame class like this:\r\n\r\n    import pandas as pd\r\n    def add_row(self, row):\r\n        self.loc[len(self.index)] = row\r\n    pd.DataFrame.add_row = add_row\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1478888289,
               "last_edit_date": 1478888700,
               "is_accepted": false,
               "id": "40553910",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 15,
               "answer_id": 45528721,
               "last_activity_date": 1507686274,
               "path": "3.stack.answer",
               "body_markdown": "For the sake of Pythonic way, here add my answer:\r\n\r\n    res = pd.DataFrame(columns=(&#39;lib&#39;, &#39;qty1&#39;, &#39;qty2&#39;))\r\n    res = res.append([{&#39;qty1&#39;:10.0}], ignore_index=True)\r\n    print(res.head())\r\n\r\n       lib  qty1  qty2\r\n    0  NaN  10.0   NaN",
               "tags": [],
               "creation_date": 1501995998,
               "last_edit_date": 1507686274,
               "is_accepted": false,
               "id": "45528721",
               "down_vote_count": 0,
               "score": 15
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 45850399,
               "is_accepted": false,
               "last_activity_date": 1503528366,
               "body_markdown": "    import pandas as pd \r\n    t1=pd.DataFrame()\r\n    for i in range(len(the number of rows)):\r\n        #add rows as columns\r\n        t1[i]=list(rows)\r\n    t1=t1.transpose()\r\n    t1.columns=list(columns)",
               "id": "45850399",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1503528366,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 45885112,
               "is_accepted": false,
               "last_activity_date": 1503676024,
               "body_markdown": "Make it simple. By taking list as input which will be appended as row in data-frame:-  \r\n\r\n    import pandas as pd  \r\n    res = pd.DataFrame(columns=(&#39;lib&#39;, &#39;qty1&#39;, &#39;qty2&#39;))  \r\n    for i in range(5):  \r\n        res_list = list(map(int, input().split()))  \r\n        res = res.append(pd.Series(res_list,index=[&#39;lib&#39;,&#39;qty1&#39;,&#39;qty2&#39;]), ignore_index=True)\r\n\r\n",
               "id": "45885112",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1503676024,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 46729742,
               "is_accepted": false,
               "last_activity_date": 1507896983,
               "body_markdown": "You can also build up a list of lists and convert it to a dataframe - \r\n\r\n    import pandas as pd\r\n\r\n    rows = []\r\n    columns = [&#39;i&#39;,&#39;double&#39;,&#39;square&#39;]\r\n\r\n    for i in range(6):\r\n        row = [i, i*2, i*i]\r\n        rows.append(row)\r\n\r\n    df = pd.DataFrame(rows, columns=columns)\r\n\r\ngiving\r\n\r\n&lt;pre&gt;\r\n \ti \tdouble \tsquare\r\n0 \t0 \t0 \t0\r\n1 \t1 \t2 \t1\r\n2 \t2 \t4 \t4\r\n3 \t3 \t6 \t9\r\n4 \t4 \t8 \t16\r\n5 \t5 \t10 \t25\r\n&lt;/pre&gt;",
               "id": "46729742",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1507896983,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 46735444,
               "is_accepted": false,
               "last_activity_date": 1507916932,
               "body_markdown": "This will take care of adding an item to an empty DataFrame. The issue is that df.index.max() == nan for the first index:\r\n\r\n    df = pd.DataFrame(columns=[&#39;timeMS&#39;, &#39;accelX&#39;, &#39;accelY&#39;, &#39;accelZ&#39;, &#39;gyroX&#39;, &#39;gyroY&#39;, &#39;gyroZ&#39;])\r\n\r\n    df.loc[0 if math.isnan(df.index.max()) else df.index.max() + 1] = [x for x in range(7)]\r\n",
               "id": "46735444",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1507916932,
               "score": -1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47979665,
               "is_accepted": false,
               "last_activity_date": 1514296957,
               "body_markdown": "It&#39;s been a long time, but I faced the same problem too. And found here a lot of interesting answers. So I was confused what method to use.\r\n\r\nIn the case of adding a lot of rows to dataframe I interested **in speed performance**. So I tried 3 most popular methods and checked their speed:\r\n\r\n1. Using .append ([NPE&#39;s answer][1])\r\n2. Using .loc ([fred&#39;s answer][2] and [FooBar&#39;s answer][3])\r\n3. Using dict and create DataFrame in the end ([ShikharDua&#39;s answer][4])\r\n\r\n**Results (in secs):**\r\n\r\n    Adding    1000 rows  5000 rows   10000 rows\r\n    .append   1.04       4.84        9.56\r\n    .loc      1.16       5.59        11.50\r\n    dict      0.23       0.26        0.34\r\n\r\nSo I use addition through the dictionary for myself.\r\n\r\n\r\n----------\r\n**Code:**\r\n\r\n    import pandas\r\n    import numpy\r\n    import time\r\n    \r\n    numOfRows = 10000\r\n    startTime = time.perf_counter()\r\n    df1 = pandas.DataFrame(numpy.random.randint(100, size=(5,5)), columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;])\r\n    for i in range( 1,numOfRows):\r\n        df1 = df1.append( dict( (a,numpy.random.randint(100)) for a in [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;]), ignore_index=True)\r\n    print(&#39;Elapsed time: {:6.3f} seconds for {:d} rows&#39;.format(time.perf_counter() - startTime, numOfRows))\r\n    \r\n    startTime = time.perf_counter()\r\n    df2 = pandas.DataFrame(numpy.random.randint(100, size=(5,5)), columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;])\r\n    for i in range( 1,numOfRows):\r\n        df2.loc[df2.index.max()+1]  = numpy.random.randint(100, size=(1,5))[0]\r\n    print(&#39;Elapsed time: {:6.3f} seconds for {:d} rows&#39;.format(time.perf_counter() - startTime, numOfRows))\r\n    \r\n    startTime = time.perf_counter()\r\n    row_list = []\r\n    for i in range (0,5):\r\n        row_list.append(dict( (a,numpy.random.randint(100)) for a in [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;]))\r\n    for i in range( 1,numOfRows):\r\n        dict1 = dict( (a,numpy.random.randint(100)) for a in [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;])\r\n        row_list.append(dict1)\r\n    \r\n    df3 = pandas.DataFrame(row_list, columns=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;])\r\n    print(&#39;Elapsed time: {:6.3f} seconds for {:d} rows&#39;.format(time.perf_counter() - startTime, numOfRows))\r\n\r\n\r\nP.S. I believe, my realization isn&#39;t perfect, and maybe there is some optimization. \r\n\r\n  [1]: https://stackoverflow.com/a/10716007/4960953\r\n  [2]: https://stackoverflow.com/a/24888331/4960953\r\n  [3]: https://stackoverflow.com/a/24913075/4960953\r\n  [4]: https://stackoverflow.com/a/17496530/4960953",
               "id": "47979665",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1514296957,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/10715965/add-one-row-in-a-pandas-dataframe",
         "id": "858127-2296"
      },
      {
         "up_vote_count": "465",
         "path": "2.stack",
         "body_markdown": "I am finding it hard to understand the process of Naive Bayes, and I was wondering if someone could explain it with a simple step by step process in English. I understand it takes comparisons by times occurred as a probability, but I have no idea how the training data is related to the actual dataset.\r\n\r\nPlease give me an explanation of what role the training set plays. I am giving a very simple example for fruits here, like banana for example\r\n\r\n    training set---\r\n    round-red\r\n    round-orange\r\n    oblong-yellow\r\n    round-red\r\n    \r\n    dataset----\r\n    round-red\r\n    round-orange\r\n    round-red\r\n    round-orange\r\n    oblong-yellow\r\n    round-red\r\n    round-orange\r\n    oblong-yellow\r\n    oblong-yellow\r\n    round-red",
         "view_count": "319906",
         "answer_count": "5",
         "tags": "['algorithm', 'machine-learning', 'dataset', 'classification', 'naivebayes']",
         "creation_date": "1333846579",
         "last_edit_date": "1482711701",
         "code_snippet": "['<code>training set---\\nround-red\\nround-orange\\noblong-yellow\\nround-red\\n\\ndataset----\\nround-red\\nround-orange\\nround-red\\nround-orange\\noblong-yellow\\nround-red\\nround-orange\\noblong-yellow\\noblong-yellow\\nround-red\\n</code>', '<code>GREEN</code>', '<code>RED</code>', '<code>GREEN</code>', '<code>RED</code>', '<code>GREEN</code>', '<code>RED</code>', '<code>GREEN</code>', '<code>RED</code>', '<code>GREEN</code>', '<code>number of GREEN objects / total number of objects</code>', '<code>RED</code>', '<code>number of RED objects / total number of objects</code>', '<code>60</code>', '<code>40</code>', '<code>GREEN</code>', '<code>RED</code>', '<code>GREEN</code>', '<code>40 / 60</code>', '<code>RED</code>', '<code>20 / 60</code>', '<code>WHITE</code>', '<code>GREEN</code>', '<code>RED</code>', '<code>X</code>', '<code>GREEN</code>', '<code>X</code>', '<code>RED</code>', '<code>1</code>', '<code>GREEN</code>', '<code>3</code>', '<code>RED</code>', '<code>X</code>', '<code>GREEN</code>', '<code>GREEN</code>', '<code>RED</code>', '<code>X</code>', '<code>RED</code>', '<code>RED</code>', '<code>X</code>', '<code>GREEN</code>', '<code>RED</code>', '<code>  P(Democrat &amp; Female) = P(Democrat) * P(Female | Democrat) \\n</code>', '<code>  P(Democrat &amp; Female) = P(Female) * P(Democrat | Female) \\n</code>', '<code>Probability of Disease D given Test-positive = \\n\\n               Prob(Test is positive|Disease) * P(Disease)\\n     _______________________________________________________________\\n     (scaled by) Prob(Testing Positive, with or without the disease)\\n</code>', '<code>P(Outcome|Multiple Evidence) = \\nP(Evidence1|Outcome) * P(Evidence2|outcome) * ... * P(EvidenceN|outcome) * P(Outcome)\\nscaled by P(Multiple Evidence)\\n</code>', '<code>                      P(Likelihood of Evidence) * Prior prob of outcome\\nP(outcome|evidence) = _________________________________________________\\n                                         P(Evidence)\\n</code>', '<code>base rates</code>', '<code>class</code>', '<code>class label.</code>', '<code>Type           Long | Not Long || Sweet | Not Sweet || Yellow |Not Yellow|Total\\n             ___________________________________________________________________\\nBanana      |  400  |    100   || 350   |    150    ||  450   |  50      |  500\\nOrange      |    0  |    300   || 150   |    150    ||  300   |   0      |  300\\nOther Fruit |  100  |    100   || 150   |     50    ||   50   | 150      |  200\\n            ____________________________________________________________________\\nTotal       |  500  |    500   || 650   |    350    ||  800   | 200      | 1000\\n             ___________________________________________________________________\\n</code>', '<code>base rates.</code>', '<code> P(Banana)      = 0.5 (500/1000)\\n P(Orange)      = 0.3\\n P(Other Fruit) = 0.2\\n</code>', '<code>p(Long)   = 0.5\\nP(Sweet)  = 0.65\\nP(Yellow) = 0.8\\n</code>', '<code>P(Long|Banana) = 0.8\\nP(Long|Orange) = 0  [Oranges are never long in all the fruit we have seen.]\\n ....\\n\\nP(Yellow|Other Fruit)     =  50/200 = 0.25\\nP(Not Yellow|Other Fruit) = 0.75\\n</code>', '<code>P(Banana|Long, Sweet and Yellow) \\n      P(Long|Banana) * P(Sweet|Banana) * P(Yellow|Banana) * P(banana)\\n    = _______________________________________________________________\\n                      P(Long) * P(Sweet) * P(Yellow)\\n\\n    = 0.8 * 0.7 * 0.9 * 0.5 / P(evidence)\\n\\n    = 0.252 / P(evidence)\\n\\n\\nP(Orange|Long, Sweet and Yellow) = 0\\n\\n\\nP(Other Fruit|Long, Sweet and Yellow)\\n      P(Long|Other fruit) * P(Sweet|Other fruit) * P(Yellow|Other fruit) * P(Other Fruit)\\n    = ____________________________________________________________________________________\\n                                          P(evidence)\\n\\n    = (100/200 * 150/200 * 50/200 * 200/1000) / P(evidence)\\n\\n    = 0.01875 / P(evidence)\\n</code>', '<code>0.252 &gt;&gt; 0.01875</code>', '<code>Let z = 1 / P(evidence).</code>', '<code>P(Banana|evidence) = z * Prob(Banana) * Prob(Evidence1|Banana) * Prob(Evidence2|Banana) ...\\nP(Orange|Evidence) = z * Prob(Orange) * Prob(Evidence1|Orange) * Prob(Evidence2|Orange) ...\\nP(Other|Evidence)  = z * Prob(Other)  * Prob(Evidence1|Other)  * Prob(Evidence2|Other)  ...\\n</code>', '<code>{\"Age\":\\'&lt;=30\\', \"Income\":\"medium\", \"Student\":\\'yes\\' , \"Creadit_Rating\":\\'fair\\'}</code>', '<code>new_dataset.csv</code>', '<code>Age,Income,Student,Creadit_Rating,Buys_Computer\\n&lt;=30,high,no,fair,no\\n&lt;=30,high,no,excellent,no\\n31-40,high,no,fair,yes\\n&gt;40,medium,no,fair,yes\\n&gt;40,low,yes,fair,yes\\n&gt;40,low,yes,excellent,no\\n31-40,low,yes,excellent,yes\\n&lt;=30,medium,no,fair,no\\n&lt;=30,low,yes,fair,yes\\n&gt;40,medium,yes,fair,yes\\n&lt;=30,medium,yes,excellent,yes\\n31-40,medium,no,excellent,yes\\n31-40,high,yes,fair,yes\\n&gt;40,medium,no,excellent,no\\n</code>', '<code>import pandas as pd \\nimport pprint \\n\\nclass Classifier():\\n    data = None\\n    class_attr = None\\n    priori = {}\\n    cp = {}\\n    hypothesis = None\\n\\n\\n    def __init__(self,filename=None, class_attr=None ):\\n        self.data = pd.read_csv(filename, sep=\\',\\', header =(0))\\n        self.class_attr = class_attr\\n\\n    \\'\\'\\'\\n        probability(class) =    How many  times it appears in cloumn\\n                             __________________________________________\\n                                  count of all class attribute\\n    \\'\\'\\'\\n    def calculate_priori(self):\\n        class_values = list(set(self.data[self.class_attr]))\\n        class_data =  list(self.data[self.class_attr])\\n        for i in class_values:\\n            self.priori[i]  = class_data.count(i)/float(len(class_data))\\n        print \"Priori Values: \", self.priori\\n\\n    \\'\\'\\'\\n        Here we calculate the individual probabilites \\n        P(outcome|evidence) =   P(Likelihood of Evidence) x Prior prob of outcome\\n                               ___________________________________________\\n                                                    P(Evidence)\\n    \\'\\'\\'\\n    def get_cp(self, attr, attr_type, class_value):\\n        data_attr = list(self.data[attr])\\n        class_data = list(self.data[self.class_attr])\\n        total =1\\n        for i in range(0, len(data_attr)):\\n            if class_data[i] == class_value and data_attr[i] == attr_type:\\n                total+=1\\n        return total/float(class_data.count(class_value))\\n\\n    \\'\\'\\'\\n        Here we calculate Likelihood of Evidence and multiple all individual probabilities with priori\\n        (Outcome|Multiple Evidence) = P(Evidence1|Outcome) x P(Evidence2|outcome) x ... x P(EvidenceN|outcome) x P(Outcome)\\n        scaled by P(Multiple Evidence)\\n    \\'\\'\\'\\n    def calculate_conditional_probabilities(self, hypothesis):\\n        for i in self.priori:\\n            self.cp[i] = {}\\n            for j in hypothesis:\\n                self.cp[i].update({ hypothesis[j]: self.get_cp(j, hypothesis[j], i)})\\n        print \"\\\\nCalculated Conditional Probabilities: \\\\n\"\\n        pprint.pprint(self.cp)\\n\\n    def classify(self):\\n        print \"Result: \"\\n        for i in self.cp:\\n            print i, \" ==&gt; \", reduce(lambda x, y: x*y, self.cp[i].values())*self.priori[i]\\n\\nif __name__ == \"__main__\":\\n    c = Classifier(filename=\"new_dataset.csv\", class_attr=\"Buys_Computer\" )\\n    c.calculate_priori()\\n    c.hypothesis = {\"Age\":\\'&lt;=30\\', \"Income\":\"medium\", \"Student\":\\'yes\\' , \"Creadit_Rating\":\\'fair\\'}\\n\\n    c.calculate_conditional_probabilities(c.hypothesis)\\n    c.classify()\\n</code>', \"<code>Priori Values:  {'yes': 0.6428571428571429, 'no': 0.35714285714285715}\\n\\nCalculated Conditional Probabilities: \\n\\n{\\n 'no': {\\n        '&lt;=30': 0.8,\\n        'fair': 0.6, \\n        'medium': 0.6, \\n        'yes': 0.4\\n        },\\n'yes': {\\n        '&lt;=30': 0.3333333333333333,\\n        'fair': 0.7777777777777778,\\n        'medium': 0.5555555555555556,\\n        'yes': 0.7777777777777778\\n      }\\n}\\n\\nResult: \\nyes  ==&gt;  0.0720164609053\\nno  ==&gt;  0.0411428571429\\n</code>\", '<code>Problem: Find out the possibility of whether the player plays in Rainy condition?\\n\\nP(Yes|Rainy) = P(Rainy|Yes) * P(Yes) / P(Rainy)\\n\\nP(Rainy|Yes) = 2/9 = 0.222\\nP(Yes) = 9/14 = 0.64\\nP(Rainy) = 5/14 = 0.36\\n\\nNow, P(Yes|Rainy) = 0.222*0.64/0.36 = 0.39 which is lower probability which means chances of the match played is low.\\n</code>', '<code> X = smoker | he is a man and under 20\\n</code>', '<code>P(f) vs. P(f | x)</code>', '<code>(10% * (90% / 90%) = 10%)</code>', '<code>(10% * (90% / 40%) = 22.5% )</code>', '<code>(10% * (90% / 95%) = 9.5%)</code>', '<code>P(X) = \\nP(smoker)* \\n(P(being a man | smoker)/P(being a man))*\\n(P(under 20 | smoker)/ P(under 20))\\n</code>']",
         "title": "A simple explanation of Naive Bayes Classification",
         "_childDocuments_": [
            {
               "up_vote_count": 675,
               "answer_id": 10062702,
               "last_activity_date": 1454350327,
               "path": "3.stack.answer",
               "body_markdown": "Your question as I understand is divided in two parts. One being you need more understanding for Naive Bayes classifier &amp; second being the confusion surrounding Training set. \r\n\r\nIn general all of Machine Learning Algorithms need to be trained for supervised learning tasks like classification, prediction etc. or for unsupervised learning tasks like clustering.\r\n\r\nBy training it means to train them on particular inputs so that later on we may test them for unknown inputs (which they have never seen before) for which they may classify or predict etc (in case of supervised learning) based on their learning. This is what most of the Machine Learning techniques like Neural Networks, SVM, Bayesian etc. are based upon.\r\n\r\nSo in a general Machine Learning project basically you have to divide your input set to a Development Set (Training Set + Dev-Test Set) &amp; a Test Set (or Evaluation set). Remember your basic objective would be that your system learns and classifies new inputs which they have never seen before in either Dev set or test set.\r\n\r\nThe test set typically has the same format as the training set. However, it is very important that the test set be distinct from the training corpus: if we simply\r\nreused the training set as the test set, then a model that simply memorized its input, without learning how to generalize to new examples, would receive misleadingly high scores.\r\n\r\nIn general, for an example, 70% can be training set cases. Also remember to partition the original set into the training and test sets *randomly*.\r\n\r\nNow I come to your other question about Naive Bayes.\r\n\r\n**Source for example below**: http://www.statsoft.com/textbook/naive-bayes-classifier\r\n\r\nTo demonstrate the concept of Na&#239;ve Bayes Classification, consider the example given below:\r\n\r\n![enter image description here][1]\r\n\r\nAs indicated, the objects can be classified as either `GREEN` or `RED`. Our task is to classify new cases as they arrive, i.e., decide to which class label they belong, based on the currently existing objects.\r\n \r\nSince there are twice as many `GREEN` objects as `RED`, it is reasonable to believe that a new case (which hasn&#39;t been observed yet) is twice as likely to have membership `GREEN` rather than `RED`. In the Bayesian analysis, this belief is known as the prior probability. Prior probabilities are based on previous experience, in this case the percentage of `GREEN` and `RED` objects, and often used to predict outcomes before they actually happen.\r\n\r\nThus, we can write:\r\n\r\n**Prior Probability of `GREEN`**: `number of GREEN objects / total number of objects`\r\n\r\n**Prior Probability of `RED`**: `number of RED objects / total number of objects`\r\n\r\nSince there is a total of `60` objects, `40` of which are `GREEN` and 20 `RED`, our prior probabilities for class membership are:\r\n\r\n**Prior Probability for `GREEN`**: `40 / 60`\r\n\r\n**Prior Probability for `RED`**: `20 / 60`\r\n\r\nHaving formulated our prior probability, we are now ready to classify a new object (`WHITE` circle in the diagram below). Since the objects are well clustered, it is reasonable to assume that the more `GREEN` (or `RED`) objects in the vicinity of X, the more likely that the new cases belong to that particular color. To measure this likelihood, we draw a circle around X which encompasses a number (to be chosen a priori) of points irrespective of their class labels. Then we calculate the number of points in the circle belonging to each class label. From this we calculate the likelihood:\r\n\r\n\r\n\r\n![enter image description here][2]\r\n\r\n![enter image description here][3]\r\n\r\nFrom the illustration above, it is clear that Likelihood of `X` given `GREEN` is smaller than Likelihood of `X` given `RED`, since the circle encompasses `1` `GREEN` object and `3` `RED` ones. Thus:\r\n\r\n![enter image description here][4]\r\n\r\n![enter image description here][5]\r\n\r\nAlthough the prior probabilities indicate that `X` may belong to `GREEN` (given that there are twice as many `GREEN` compared to `RED`) the likelihood indicates otherwise; that the class membership of `X` is `RED` (given that there are more `RED` objects in the vicinity of `X` than `GREEN`). In the Bayesian analysis, the final classification is produced by combining both sources of information, i.e., the prior and the likelihood, to form a posterior probability using the so-called Bayes&#39; rule (named after Rev. Thomas Bayes 1702-1761).\r\n\r\n![enter image description here][6]\r\n\r\nFinally, we classify X as `RED` since its class membership achieves the largest posterior probability.\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/Eh6HI.gif\r\n  [2]: http://i.stack.imgur.com/gVpJF.gif\r\n  [3]: http://i.stack.imgur.com/sh1zX.gif\r\n  [4]: http://i.stack.imgur.com/DLCqA.gif\r\n  [5]: http://i.stack.imgur.com/cJzst.gif\r\n  [6]: http://i.stack.imgur.com/N8MPA.gif",
               "tags": [],
               "creation_date": 1333887197,
               "last_edit_date": 1454350327,
               "is_accepted": true,
               "id": "10062702",
               "down_vote_count": 50,
               "score": 625
            },
            {
               "up_vote_count": 955,
               "answer_id": 20556654,
               "last_activity_date": 1482663222,
               "path": "3.stack.answer",
               "body_markdown": "I realize that this is an old question, with an established answer. The reason I&#39;m posting is that is the accepted answer has many elements of k-NN (*k*-nearest Neighbors), a different algorithm. \r\n\r\nBoth k-NN and NaiveBayes are classification algorithms. Conceptually, k-NN uses the idea of &quot;nearness&quot; to classify new entities. In k-NN &#39;nearness&#39; is modeled with ideas such as Euclidean Distance or Cosine Distance. By contrast, in NaiveBayes, the concept of &#39;probability&#39; is used to classify new entities. \r\n\r\nSince the question is about Naive Bayes, here&#39;s how I&#39;d describe the ideas and steps to someone. I&#39;ll try to do it with as few equations and in plain English as much as possible.\r\n\r\n###First, Conditional Probability &amp; Bayes&#39; Rule\r\nBefore someone can understand and appreciate the nuances of Naive Bayes&#39;, they need to know a couple of related concepts first, namely, the idea of Conditional Probability, and Bayes&#39; Rule. (If you are familiar with these concepts, skip to the section titled **Getting to Naive Bayes&#39;**)\r\n\r\n**Conditional Probability** in plain English: What is the probability that something will happen, *given that something else* has already happened.\r\n\r\nLet&#39;s say that there is some Outcome O. And some Evidence E. From the way these probabilities are defined: The Probability of having *both* the Outcome O and Evidence E is:\r\n   (Probability of O occurring) multiplied by the (Prob of E given that O happened)\r\n\r\n*One Example to understand Conditional Probability:*\r\n\r\n  Let say we have a collection of US Senators. Senators could be Democrats or Republicans. They are also either male or female.\r\n\r\n  If we select one senator completely randomly, what is the probability that this person is a female Democrat? Conditional Probability can help us answer that.\r\n\r\nProbability of (Democrat and Female Senator)= Prob(Senator is Democrat) multiplied by Conditional Probability of Being Female given that they are a Democrat.\r\n\r\n      P(Democrat &amp; Female) = P(Democrat) * P(Female | Democrat) \r\n\r\nWe could compute the exact same thing, the reverse way:\r\n\r\n      P(Democrat &amp; Female) = P(Female) * P(Democrat | Female) \r\n\r\n\r\n###Understanding Bayes Rule\r\nConceptually, this is a way to go from P(Evidence| Known Outcome) to P(Outcome|Known Evidence). Often, we know how frequently some particular evidence is observed, *given a known outcome*. We have to use this known fact to compute the reverse, to compute the chance of that *outcome happening*, given the evidence.\r\n\r\n  P(Outcome given that we know some Evidence) = P(Evidence given that we know the Outcome) times Prob(Outcome), scaled by the P(Evidence)\r\n\r\nThe classic example to understand Bayes&#39; Rule:\r\n\r\n    Probability of Disease D given Test-positive = \r\n\r\n                   Prob(Test is positive|Disease) * P(Disease)\r\n         _______________________________________________________________\r\n         (scaled by) Prob(Testing Positive, with or without the disease)\r\n\r\nNow, all this was just preamble, to get to Naive Bayes.\r\n\r\n##Getting to Naive Bayes&#39;\r\n\r\nSo far, we have talked only about one piece of evidence. In reality, we have to predict an outcome given **multiple evidence.** In that case, the math gets very complicated. To get around that complication, one approach is to &#39;uncouple&#39; multiple pieces of evidence, and to treat each of piece of evidence as independent. This approach is why this is called *naive* Bayes.\r\n\r\n    P(Outcome|Multiple Evidence) = \r\n    P(Evidence1|Outcome) * P(Evidence2|outcome) * ... * P(EvidenceN|outcome) * P(Outcome)\r\n    scaled by P(Multiple Evidence)\r\n\r\nMany people choose to remember this as:\r\n\r\n                          P(Likelihood of Evidence) * Prior prob of outcome\r\n    P(outcome|evidence) = _________________________________________________\r\n                                             P(Evidence)\r\n\r\nNotice a few things about this equation:\r\n\r\n - If the Prob(evidence|outcome) is 1, then we are just multiplying by 1. \r\n - If the Prob(some particular evidence|outcome) is 0, then the whole prob. becomes 0. If you see contradicting evidence, we can rule out that outcome. \r\n - Since we divide everything by P(Evidence), we can even get away without calculating it.\r\n - The intuition behind multiplying by the *prior* is so that we give high probability to more common outcomes, and low probabilities to unlikely outcomes. These are also called `base rates` and they are a way to scale our predicted probabilities. \r\n\r\n###How to Apply NaiveBayes to Predict an Outcome?\r\n\r\nJust run the formula above for each possible outcome. Since we are trying to *classify*, each outcome is called a `class` and it has a `class label.` Our job is to look at the evidence, to consider how likely it is to be this class or that class, and assign a label to each entity.\r\nAgain, we take a very simple approach: The class that has the highest probability is declared the &quot;winner&quot; and that class label gets assigned to that combination of evidences.\r\n\r\n### Fruit Example\r\nLet&#39;s try it out on an example to increase our understanding: The OP asked for a &#39;fruit&#39; identification example.\r\n\r\nLet&#39;s say that we have data on 1000 pieces of fruit. They happen to be **Banana**, **Orange** or some **Other Fruit**.\r\nWe know 3 characteristics about each fruit:\r\n\r\n 1. Whether it is Long\r\n 2. Whether it is Sweet and \r\n 3. If its color is Yellow.\r\n\r\nThis is our &#39;training set.&#39; We will use this to predict the type of any *new* fruit we encounter.\r\n\r\n    Type           Long | Not Long || Sweet | Not Sweet || Yellow |Not Yellow|Total\r\n                 ___________________________________________________________________\r\n    Banana      |  400  |    100   || 350   |    150    ||  450   |  50      |  500\r\n    Orange      |    0  |    300   || 150   |    150    ||  300   |   0      |  300\r\n    Other Fruit |  100  |    100   || 150   |     50    ||   50   | 150      |  200\r\n                ____________________________________________________________________\r\n    Total       |  500  |    500   || 650   |    350    ||  800   | 200      | 1000\r\n                 ___________________________________________________________________\r\n\r\nWe can pre-compute a lot of things about our fruit collection.\r\n\r\nThe so-called &quot;Prior&quot; probabilities. (If we didn&#39;t know any of the fruit attributes, this would be our guess.) These are our `base rates.`\r\n\r\n     P(Banana)      = 0.5 (500/1000)\r\n     P(Orange)      = 0.3\r\n     P(Other Fruit) = 0.2\r\n\r\nProbability of &quot;Evidence&quot;\r\n\r\n    p(Long)   = 0.5\r\n    P(Sweet)  = 0.65\r\n    P(Yellow) = 0.8\r\n\r\nProbability of &quot;Likelihood&quot;\r\n\r\n    P(Long|Banana) = 0.8\r\n    P(Long|Orange) = 0  [Oranges are never long in all the fruit we have seen.]\r\n     ....\r\n\r\n    P(Yellow|Other Fruit)     =  50/200 = 0.25\r\n    P(Not Yellow|Other Fruit) = 0.75\r\n\r\n### Given a Fruit, how to classify it?\r\n\r\nLet&#39;s say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit?\r\n\r\nWe can simply run the numbers for each of the 3 outcomes, one by one. Then we choose the highest probability and &#39;classify&#39; our unknown fruit as belonging to the class that had the highest probability based on our prior evidence (our 1000 fruit training set):\r\n\r\n    P(Banana|Long, Sweet and Yellow) \r\n          P(Long|Banana) * P(Sweet|Banana) * P(Yellow|Banana) * P(banana)\r\n        = _______________________________________________________________\r\n                          P(Long) * P(Sweet) * P(Yellow)\r\n                          \r\n        = 0.8 * 0.7 * 0.9 * 0.5 / P(evidence)\r\n    \r\n    \t= 0.252 / P(evidence)\r\n\r\n\r\n    P(Orange|Long, Sweet and Yellow) = 0\r\n\r\n\r\n    P(Other Fruit|Long, Sweet and Yellow)\r\n          P(Long|Other fruit) * P(Sweet|Other fruit) * P(Yellow|Other fruit) * P(Other Fruit)\r\n        = ____________________________________________________________________________________\r\n                                              P(evidence)\r\n\r\n        = (100/200 * 150/200 * 50/200 * 200/1000) / P(evidence)\r\n\r\n        = 0.01875 / P(evidence)\r\n\r\nBy an overwhelming margin (`0.252 &gt;&gt; 0.01875`), we classify this Sweet/Long/Yellow fruit as likely to be a Banana.\r\n\r\n###Why is Bayes Classifier so popular? \r\n\r\nLook at what it eventually comes down to. Just some counting and multiplication. We can pre-compute all these terms, and so classifying becomes easy, quick and efficient. \r\n\r\n`Let z = 1 / P(evidence).` Now we quickly compute the following three quantities.\r\n\r\n    P(Banana|evidence) = z * Prob(Banana) * Prob(Evidence1|Banana) * Prob(Evidence2|Banana) ...\r\n    P(Orange|Evidence) = z * Prob(Orange) * Prob(Evidence1|Orange) * Prob(Evidence2|Orange) ...\r\n    P(Other|Evidence)  = z * Prob(Other)  * Prob(Evidence1|Other)  * Prob(Evidence2|Other)  ...\r\n\r\nAssign the class label of whichever is the highest number, and you are done.\r\n\r\nDespite the name, Naive Bayes turns out to be excellent in certain applications. Text classification is one area where it really shines.\r\n\r\nHope that helps in understanding the concepts behind the Naive Bayes algorithm.",
               "tags": [],
               "creation_date": 1386892483,
               "last_edit_date": 1482663222,
               "is_accepted": false,
               "id": "20556654",
               "down_vote_count": 1,
               "score": 954
            },
            {
               "up_vote_count": 10,
               "answer_id": 35960103,
               "last_activity_date": 1519658327,
               "path": "3.stack.answer",
               "body_markdown": "I try to explain the Bayes rule with an example. \r\n\r\nSuppose that you know that **10%** of people are smokers.  You also know that **90%** of smokers are men and **80%** of them are above 20 years old.  \r\n\r\nNow you see someone who is a **man** and  **15** years old. You want to know the chance that he is a smoker:\r\n\r\n     X = smoker | he is a man and under 20\r\n\r\nSince you know that 10% of people are smokers your initial guess is 10% (*prior probability*, without knowing anything about the person) but the other **pieces of evidence ** (that he is a man and he is 15) can affect this guess. \r\n\r\nEach evidence may increase or decrease this chance. For example this fact that he is a man **may** increase the chance, providing that this percentage (being a man) among non-smokers is lower, for example, 40%. In the other words, being a man must be a good indicator of being a smoker rather than a non-smoker.\r\n\r\nWe can show this contribution in another way. For each feature, you need to compare the commonness (probability) of that feature (f) alone with its commonness under the given conditions. (`P(f) vs. P(f | x)`. For example, if we know that the probability of being a man is 90% in a society and 90% of smokers are also men, then knowing that someone is a man doesn&#39;t help us `(10% * (90% / 90%) = 10%)`. But if men contribute to 40% of the society, but 90% of the smokers, then knowing that someone is a man increases the chance of being an smoker `(10% * (90% / 40%) = 22.5% )`. In the same way, if the probability of being a man was 95% in the society, then regardless of the fact that the percentage of men among smokers is high (90%)!, the evidence that someone is a man decreases the chance of him being a smoker! `(10% * (90% / 95%) = 9.5%)`.\r\n\r\n\r\nSo we have:\r\n\r\n    P(X) = \r\n    P(smoker)* \r\n    (P(being a man | smoker)/P(being a man))*\r\n    (P(under 20 | smoker)/ P(under 20))\r\n\r\nNote that in this formula we assumed that **being a man** and **being under 20** are independent features so we multiplied them, it means that knowing that someone is under 20 has no effect on guessing that he is man or woman. But it may not be true, for example maybe most adolescence  in a society are men...\r\n\r\n**To use this formula in a classifier**\r\n\r\nThe classifier is given with some features (being a man and being under 20) and it must decide if he is an smoker or not. It uses the above formula to find that. To provide the required probabilities (90%, 10%, 80%...) it uses the training set. For example, it counts the people in the training set that are smokers and find they contribute 10% of the sample. Then for smokers checks how many of them are men or women .... how many are above 20 or under 20....",
               "tags": [],
               "creation_date": 1457799228,
               "last_edit_date": 1519658327,
               "is_accepted": false,
               "id": "35960103",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 15,
               "answer_id": 36282738,
               "last_activity_date": 1459252828,
               "path": "3.stack.answer",
               "body_markdown": "Ram Narasimhan explained the concept very nicely here below is an alternative explanation through the code example of Naive Bayes in action&lt;br&gt;\r\nIt uses an example problem from this [book on page 351](https://www.google.co.in/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=data%20mining%20concepts%20and%20techniques)&lt;br&gt;\r\nThis is the data set that we will be using&lt;br&gt; [![enter image description here][1]][1]&lt;br&gt;\r\nIn the above dataset if we give the hypothesis = `{&quot;Age&quot;:&#39;&lt;=30&#39;, &quot;Income&quot;:&quot;medium&quot;, &quot;Student&quot;:&#39;yes&#39; , &quot;Creadit_Rating&quot;:&#39;fair&#39;}` then what is the probability that he will buy or will not buy a computer.&lt;br&gt;\r\nThe code below exactly answers that question.&lt;br&gt;\r\nJust create a file called named `new_dataset.csv` and paste the following content.\r\n\r\n    Age,Income,Student,Creadit_Rating,Buys_Computer\r\n\t&lt;=30,high,no,fair,no\r\n\t&lt;=30,high,no,excellent,no\r\n\t31-40,high,no,fair,yes\r\n\t&gt;40,medium,no,fair,yes\r\n\t&gt;40,low,yes,fair,yes\r\n\t&gt;40,low,yes,excellent,no\r\n\t31-40,low,yes,excellent,yes\r\n\t&lt;=30,medium,no,fair,no\r\n\t&lt;=30,low,yes,fair,yes\r\n\t&gt;40,medium,yes,fair,yes\r\n\t&lt;=30,medium,yes,excellent,yes\r\n\t31-40,medium,no,excellent,yes\r\n\t31-40,high,yes,fair,yes\r\n\t&gt;40,medium,no,excellent,no\r\n\r\nHere is the code the comments explains everything we are doing here! [python]\r\n\r\n    import pandas as pd \r\n\timport pprint \r\n\r\n\tclass Classifier():\r\n\t\tdata = None\r\n\t\tclass_attr = None\r\n\t\tpriori = {}\r\n\t\tcp = {}\r\n\t\thypothesis = None\r\n\r\n\r\n\t\tdef __init__(self,filename=None, class_attr=None ):\r\n\t\t    self.data = pd.read_csv(filename, sep=&#39;,&#39;, header =(0))\r\n\t\t    self.class_attr = class_attr\r\n\r\n\t\t&#39;&#39;&#39;\r\n\t\t    probability(class) =    How many  times it appears in cloumn\r\n\t\t                         __________________________________________\r\n\t\t                              count of all class attribute\r\n\t\t&#39;&#39;&#39;\r\n\t\tdef calculate_priori(self):\r\n\t\t    class_values = list(set(self.data[self.class_attr]))\r\n\t\t    class_data =  list(self.data[self.class_attr])\r\n\t\t    for i in class_values:\r\n\t\t        self.priori[i]  = class_data.count(i)/float(len(class_data))\r\n\t\t    print &quot;Priori Values: &quot;, self.priori\r\n\r\n\t\t&#39;&#39;&#39;\r\n\t\t    Here we calculate the individual probabilites \r\n\t\t    P(outcome|evidence) =   P(Likelihood of Evidence) x Prior prob of outcome\r\n\t\t                           ___________________________________________\r\n\t\t                                                P(Evidence)\r\n\t\t&#39;&#39;&#39;\r\n\t\tdef get_cp(self, attr, attr_type, class_value):\r\n\t\t    data_attr = list(self.data[attr])\r\n\t\t    class_data = list(self.data[self.class_attr])\r\n\t\t    total =1\r\n\t\t    for i in range(0, len(data_attr)):\r\n\t\t        if class_data[i] == class_value and data_attr[i] == attr_type:\r\n\t\t            total+=1\r\n\t\t    return total/float(class_data.count(class_value))\r\n\r\n\t\t&#39;&#39;&#39;\r\n\t\t    Here we calculate Likelihood of Evidence and multiple all individual probabilities with priori\r\n\t\t    (Outcome|Multiple Evidence) = P(Evidence1|Outcome) x P(Evidence2|outcome) x ... x P(EvidenceN|outcome) x P(Outcome)\r\n\t\t    scaled by P(Multiple Evidence)\r\n\t\t&#39;&#39;&#39;\r\n\t\tdef calculate_conditional_probabilities(self, hypothesis):\r\n\t\t    for i in self.priori:\r\n\t\t        self.cp[i] = {}\r\n\t\t        for j in hypothesis:\r\n\t\t            self.cp[i].update({ hypothesis[j]: self.get_cp(j, hypothesis[j], i)})\r\n\t\t    print &quot;\\nCalculated Conditional Probabilities: \\n&quot;\r\n\t\t    pprint.pprint(self.cp)\r\n\r\n\t\tdef classify(self):\r\n\t\t    print &quot;Result: &quot;\r\n\t\t    for i in self.cp:\r\n\t\t        print i, &quot; ==&gt; &quot;, reduce(lambda x, y: x*y, self.cp[i].values())*self.priori[i]\r\n\r\n\tif __name__ == &quot;__main__&quot;:\r\n\t\tc = Classifier(filename=&quot;new_dataset.csv&quot;, class_attr=&quot;Buys_Computer&quot; )\r\n\t\tc.calculate_priori()\r\n\t\tc.hypothesis = {&quot;Age&quot;:&#39;&lt;=30&#39;, &quot;Income&quot;:&quot;medium&quot;, &quot;Student&quot;:&#39;yes&#39; , &quot;Creadit_Rating&quot;:&#39;fair&#39;}\r\n\r\n\t\tc.calculate_conditional_probabilities(c.hypothesis)\r\n\t\tc.classify()\r\n\r\noutput:\r\n\r\n    Priori Values:  {&#39;yes&#39;: 0.6428571428571429, &#39;no&#39;: 0.35714285714285715}\r\n\r\n    Calculated Conditional Probabilities: \r\n\r\n    {\r\n     &#39;no&#39;: {\r\n            &#39;&lt;=30&#39;: 0.8,\r\n            &#39;fair&#39;: 0.6, \r\n            &#39;medium&#39;: 0.6, \r\n            &#39;yes&#39;: 0.4\r\n            },\r\n    &#39;yes&#39;: {\r\n            &#39;&lt;=30&#39;: 0.3333333333333333,\r\n            &#39;fair&#39;: 0.7777777777777778,\r\n            &#39;medium&#39;: 0.5555555555555556,\r\n            &#39;yes&#39;: 0.7777777777777778\r\n          }\r\n    }\r\n\r\n    Result: \r\n    yes  ==&gt;  0.0720164609053\r\n    no  ==&gt;  0.0411428571429\r\n\r\n\r\nHope it helps in better understanding the problem \r\n\r\npeace\r\n  [1]: http://i.stack.imgur.com/lKPKB.png",
               "tags": [],
               "creation_date": 1459250123,
               "last_edit_date": 1459252828,
               "is_accepted": false,
               "id": "36282738",
               "down_vote_count": 0,
               "score": 15
            },
            {
               "up_vote_count": 13,
               "answer_id": 42289174,
               "last_activity_date": 1511441585,
               "path": "3.stack.answer",
               "body_markdown": "**Naive Bayes:**\r\nNaive Bayes comes under supervising machine learning which used to make classifications of data sets.\r\nIt is used to predict things based on its prior knowledge and independence assumptions.\r\n\r\nThey call it **naive** because it\u2019s assumptions (it assumes that all of the features in the dataset are equally important and independent) are really optimistic and rarely true in most real-world applications.\r\n\r\nIt is classification algorithm which makes the decision for the unknown data set. It is based on [Bayes Theorem][1] which describe the probability of an event based on its prior knowledge.\r\n\r\nBelow diagram shows how naive Bayes works\r\n\r\n[![enter image description here][2]][1]\r\n\r\n\r\n**Formula to predict NB:**\r\n\r\n[![enter image description here][3]][2]\r\n\r\n\r\n**How to use Naive Bayes Algorithm ?**\r\n\r\nLet&#39;s take an example of how N.B woks\r\n\r\nStep 1: First we find out Likelihood of table which shows the probability of yes or no in below diagram.\r\nStep 2: Find the posterior probability of each class.\r\n\r\n[![enter image description here][4]][3]\r\n\r\n\r\n    Problem: Find out the possibility of whether the player plays in Rainy condition?\r\n    \r\n    P(Yes|Rainy) = P(Rainy|Yes) * P(Yes) / P(Rainy)\r\n    \r\n    P(Rainy|Yes) = 2/9 = 0.222\r\n    P(Yes) = 9/14 = 0.64\r\n    P(Rainy) = 5/14 = 0.36\r\n    \r\n    Now, P(Yes|Rainy) = 0.222*0.64/0.36 = 0.39 which is lower probability which means chances of the match played is low.\r\n\r\nFor more reference refer these [blog.][5]\r\n\r\nRefer GitHub Repository [Naive-Bayes-Examples][6]\r\n\r\n\r\n  [1]: https://en.wikipedia.org/wiki/Bayes%27_theorem\r\n  [2]: https://i.stack.imgur.com/0QOII.png\r\n  [3]: https://i.stack.imgur.com/t5voX.png\r\n  [4]: https://i.stack.imgur.com/ZSv4b.png\r\n  [5]: http://naivebayesintro.blogspot.in/\r\n  [6]: https://github.com/jiteshmohite/Naive-Bayes-Examples",
               "tags": [],
               "creation_date": 1487304148,
               "last_edit_date": 1511441585,
               "is_accepted": false,
               "id": "42289174",
               "down_vote_count": 1,
               "score": 12
            }
         ],
         "link": "https://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification",
         "id": "858127-2297"
      },
      {
         "up_vote_count": "182",
         "path": "2.stack",
         "body_markdown": "How can I check which version of *NumPy* I&#39;m using? I&#39;m using Mac OS X 10.6.1 Snow Leopard.",
         "view_count": "173960",
         "answer_count": "12",
         "tags": "['python', 'macos', 'numpy']",
         "creation_date": "1254751010",
         "last_edit_date": "1516240560",
         "code_snippet": "['<code>import numpy\\nnumpy.version.version\\n</code>', '<code>__version__</code>', '<code>import numpy ; numpy.version.version</code>', '<code>import numpy</code>', '<code>__version__</code>', '<code>__version__</code>', '<code>version.version</code>', '<code>numpy.__version__</code>', '<code>&lt;package&gt;.__version__</code>', '<code>&gt;&gt; import numpy\\n&gt;&gt; print numpy.__version__\\n</code>', '<code>python -c \"import numpy; print(numpy.version.version)\"\\n</code>', '<code>python -c \"import numpy; print(numpy.__version__)\"\\n</code>', '<code>import numpy\\nnumpy.show_config()\\n</code>', \"<code>pip freeze | grep 'numpy'</code>\", '<code>pip list\\n</code>', '<code>...\\nnbpresent (3.0.2)\\nnetworkx (1.11)\\nnltk (3.2.2)\\nnose (1.3.7)\\nnotebook (5.0.0)\\nnumba (0.32.0+0.g139e4c6.dirty)\\nnumexpr (2.6.2)\\nnumpy (1.11.3) &lt;--\\nnumpydoc (0.6.0)\\nodo (0.5.0)\\nopenpyxl (2.4.1)\\npandas (0.20.1)\\npandocfilters (1.4.1)\\n....\\n</code>', '<code>pip freeze</code>', '<code>python -c \"import numpy; print (numpy.version.version)\"\\n</code>', '<code>python -c \"import numpy; print numpy.__version__\"\\n</code>', '<code>print(numpy.__version__)</code>', '<code>print numpy.__version__</code>', '<code>$ conda list | grep numpy\\nnumpy     1.11.3     py35_0\\n</code>', '<code>Python</code>', '<code>numexpr</code>', '<code>In [692]: import numexpr\\n\\nIn [693]: numexpr.print_versions()\\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\\nNumexpr version:   2.6.2\\nNumPy version:     1.13.3\\nPython version:    3.6.3 |Anaconda custom (64-bit)|\\n                   (default, Oct 13 2017, 12:02:49) \\n[GCC 7.2.0]\\nPlatform:          linux-x86_64\\nAMD/Intel CPU?     True\\nVML available?     False\\nNumber of threads used by default: 8 (out of 48 detected cores)\\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\\n</code>', '<code>conda list numpy</code>', '<code>import numpy\\nprint numpy.__version__\\n</code>', '<code>&gt;&gt;&gt; help()\\nhelp&gt; numpy\\n</code>', '<code>numpy.version.version\\n</code>', '<code>numpy.version.version</code>']",
         "title": "How to check which version of Numpy I&#39;m using?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 159,
               "answer_id": 1520264,
               "is_accepted": false,
               "last_activity_date": 1254751279,
               "body_markdown": "    &gt;&gt; import numpy\r\n    &gt;&gt; print numpy.__version__",
               "id": "1520264",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1254751279,
               "score": 159
            },
            {
               "up_vote_count": 265,
               "answer_id": 1520275,
               "last_activity_date": 1461334897,
               "path": "3.stack.answer",
               "body_markdown": "    import numpy\r\n    numpy.version.version",
               "tags": [],
               "creation_date": 1254751378,
               "last_edit_date": 1461334897,
               "is_accepted": true,
               "id": "1520275",
               "down_vote_count": 8,
               "score": 257
            },
            {
               "up_vote_count": 36,
               "answer_id": 1958122,
               "last_activity_date": 1468269647,
               "path": "3.stack.answer",
               "body_markdown": "from the command line, you can simply issue:\r\n\r\n    python -c &quot;import numpy; print(numpy.version.version)&quot;\r\n\r\nor\r\n\r\n    python -c &quot;import numpy; print(numpy.__version__)&quot;",
               "tags": [],
               "creation_date": 1261656638,
               "last_edit_date": 1468269647,
               "is_accepted": false,
               "id": "1958122",
               "down_vote_count": 0,
               "score": 36
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 15,
               "answer_id": 16904549,
               "is_accepted": false,
               "last_activity_date": 1370288400,
               "body_markdown": "You can also check if your version is using MKL with:\r\n\r\n    import numpy\r\n    numpy.show_config()",
               "id": "16904549",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1370288400,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 18930126,
               "is_accepted": false,
               "last_activity_date": 1379748934,
               "body_markdown": "    numpy.version.version\r\ntype it.",
               "id": "18930126",
               "tags": [],
               "down_vote_count": 8,
               "creation_date": 1379748934,
               "score": -6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 33469668,
               "is_accepted": false,
               "last_activity_date": 1446429969,
               "body_markdown": "We can use pip freeze to get any python package version without opening the python shell.\r\n\r\n`pip freeze | grep &#39;numpy&#39;`\r\n",
               "id": "33469668",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1446429969,
               "score": 5
            },
            {
               "up_vote_count": 2,
               "answer_id": 33490162,
               "last_activity_date": 1446519404,
               "path": "3.stack.answer",
               "body_markdown": "For python 3.X print syntax.\r\n\r\n    python -c &quot;import numpy; print (numpy.version.version)&quot;\r\nor\r\n\r\n    python -c &quot;import numpy; print numpy.__version__&quot;",
               "tags": [],
               "creation_date": 1446515490,
               "last_edit_date": 1446519404,
               "is_accepted": false,
               "id": "33490162",
               "down_vote_count": 1,
               "score": 1
            },
            {
               "up_vote_count": 1,
               "answer_id": 42354895,
               "last_activity_date": 1514273019,
               "path": "3.stack.answer",
               "body_markdown": "If you&#39;re using ***Numpy from the Anaconda distribution***, then you can just do:\r\n\r\n    $ conda list | grep numpy\r\n    numpy     1.11.3     py35_0\r\n\r\nThis gives the `Python` version as well.\r\n\r\n---------------\r\n\r\n###If you want something fancy, then use `numexpr`\r\nIt gives lot of information as you can see below:\r\n\r\n    In [692]: import numexpr\r\n    \r\n    In [693]: numexpr.print_versions()\r\n    -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\n    Numexpr version:   2.6.2\r\n    NumPy version:     1.13.3\r\n    Python version:    3.6.3 |Anaconda custom (64-bit)|\r\n                       (default, Oct 13 2017, 12:02:49) \r\n    [GCC 7.2.0]\r\n    Platform:          linux-x86_64\r\n    AMD/Intel CPU?     True\r\n    VML available?     False\r\n    Number of threads used by default: 8 (out of 48 detected cores)\r\n    -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\n\r\n",
               "tags": [],
               "creation_date": 1487627546,
               "last_edit_date": 1514273019,
               "is_accepted": false,
               "id": "42354895",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 43862161,
               "is_accepted": false,
               "last_activity_date": 1494309559,
               "body_markdown": "Run:\r\n\r\n    pip list\r\n\r\nShould generate a list of packages. Scroll through to numpy.\r\n\r\n    ...\r\n    nbpresent (3.0.2)\r\n    networkx (1.11)\r\n    nltk (3.2.2)\r\n    nose (1.3.7)\r\n    notebook (5.0.0)\r\n    numba (0.32.0+0.g139e4c6.dirty)\r\n    numexpr (2.6.2)\r\n    numpy (1.11.3) &lt;--\r\n    numpydoc (0.6.0)\r\n    odo (0.5.0)\r\n    openpyxl (2.4.1)\r\n    pandas (0.20.1)\r\n    pandocfilters (1.4.1)\r\n    ....\r\n",
               "id": "43862161",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1494309559,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 44995943,
               "is_accepted": false,
               "last_activity_date": 1499600047,
               "body_markdown": "    import numpy\r\n    print numpy.__version__",
               "id": "44995943",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1499600047,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 45322855,
               "is_accepted": false,
               "last_activity_date": 1501061326,
               "body_markdown": "In a python shell:\r\n\r\n    &gt;&gt;&gt; help()\r\n    help&gt; numpy\r\n\r\n",
               "id": "45322855",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1501061326,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 46330631,
               "is_accepted": false,
               "last_activity_date": 1505936831,
               "body_markdown": "You can try this:\r\n\r\n&gt; pip show numpy",
               "id": "46330631",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1505936831,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/1520234/how-to-check-which-version-of-numpy-im-using",
         "id": "858127-2298"
      },
      {
         "up_vote_count": "431",
         "path": "2.stack",
         "body_markdown": "Which method provides the best performance when removing the time portion from a datetime field in SQL Server?\r\n\r\n    a) select DATEADD(dd, DATEDIFF(dd, 0, getdate()), 0)\r\nor\r\n\r\n    b) select cast(convert(char(11), getdate(), 113) as datetime)\r\n\r\nThe second method does send a few more bytes either way but that might not be as important as the speed of the conversion.\r\n\r\nBoth also appear to be very fast, but there might be a difference in speed when dealing with hundreds-of-thousands or more rows?\r\n\r\nAlso, is it possible that there are even better methods to get rid of the time portion of a datetime in SQL?",
         "view_count": "572307",
         "answer_count": "23",
         "tags": "['sql', 'sql-server', 'tsql', 'datetime', 'date']",
         "creation_date": "1248439794",
         "last_edit_date": "1310453509",
         "code_snippet": "['<code>a) select DATEADD(dd, DATEDIFF(dd, 0, getdate()), 0)\\n</code>', '<code>b) select cast(convert(char(11), getdate(), 113) as datetime)\\n</code>', '<code>a</code>', '<code>a) select DATEADD(dd, DATEDIFF(dd, 0, getdate()), 0)\\n</code>', '<code>date</code>', '<code>date</code>', '<code>DATE</code>', '<code>DATEADD()</code>', '<code>CONVERT(DATE, getdate(), 101)\\n</code>', '<code>datetime</code>', '<code>date</code>', '<code>CONVERT(DATE,getdate())</code>', '<code>SELECT CONVERT(DATE,GETDATE())\\n</code>', '<code>SELECT CAST(FLOOR(CAST(getdate() AS FLOAT)) AS DATETIME)\\n</code>', '<code>CAST(GetDate() as DATE)\\n</code>', '<code>declare @Dt as DATE = GetDate()\\n</code>', '<code>SELECT CAST(CAST(getutcdate() - 0.50000004 AS int) AS datetime) \\n</code>', '<code>DATEADD   MAGIC FLOAT\\n500       453\\n453       360\\n375       375\\n406       360\\n</code>', \"<code>'12:00:00.003'</code>\", '<code>SELECT CAST(CAST(GETDATE() AS DATE) AS DATETIME)\\n</code>', '<code>select date_only(dd)\\n</code>', '<code>date_only</code>', '<code>WHERE DateAdd(DateDiff(Column)) = @DateValue</code>', '<code>WHERE Column &gt;= dbo.UDF(@DateValue) AND Column &lt; dbo.UDF(@DateValue + 1)</code>', '<code> CAST(\\nFLOOR( CAST( GETDATE() AS FLOAT ) )\\nAS DATETIME\\n)\\n</code>', '<code>CAST(round(cast(getdate()as real),0,1) AS datetime)\\n</code>', '<code>Date</code>', '<code>SELECT CONVERT(DATETIME, FLOOR(CONVERT(FLOAT,GETDATE())));\\n</code>', '<code>cast(floor(cast(getdate()as float))as datetime)</code>', '<code>cast(cast(getdate()+x-0.5 as int)as datetime)</code>', '<code>(about 0.49 microseconds CPU vs. 0.58)</code>', '<code>DATEADD(dd, DATEDIFF(dd, 0, getdate()), 0)</code>', '<code>create function dateonly (  @dt datetime )\\nreturns datetime\\nas\\nbegin\\nreturn cast(floor(cast(@dt as float))as int)\\nend\\n</code>', \"<code>select DATEADD(dd, DATEDIFF(dd, 0, '2013-12-31 23:59:59.999'), 0)\\n</code>\", '<code>2014-01-01 00:00:00.000</code>', \"<code>select cast(convert(char(11), '2013-12-31 23:59:59.999', 113) as datetime)\\n</code>\", '<code>2013-12-31 00:00:00.000</code>', '<code>DATETIME</code>', '<code>select cast(cast my_datetime_field as date) as datetime)</code>', '<code>TSQL</code>', '<code> select convert(datetime,convert(int,convert(float,[Modified])))\\n</code>', '<code>DateAdd</code>', '<code>select convert(datetime,ROUND(convert(float,[Modified]),0))\\n</code>', \"<code>create function dbo.uf_RoundDateTime(@dt as datetime, @part as char)\\n    returns datetime\\nas\\nbegin\\n    if CHARINDEX( @part, 'smhd',0) = 0 return @dt;\\n    return cast(\\n        Case @part\\n            when 's' then convert(varchar(19), @dt, 126)\\n            when 'm' then convert(varchar(17), @dt, 126) + '00'\\n            when 'h' then convert(varchar(14), @dt, 126) + '00:00'\\n            when 'd' then convert(varchar(14), @dt, 112)\\n       end as datetime )\\nend</code>\", '<code>CAST(CONVERT(DATE,GETDATE(),103) AS DATETIME)\\n</code>', '<code>cast</code>', '<code>datetime</code>', '<code>date</code>', '<code>[date] = CONVERT(VARCHAR(10), GETDATE(), 120)\\n</code>', '<code>120</code>', \"<code>'YYYY-MM-DD' or '2017-01-09'\\n</code>\", '<code>R</code>', '<code>Python</code>', '<code>[Microsoft.SqlServer.Server.SqlFunction]\\n    public static SqlDateTime DateOnly(SqlDateTime input)\\n    {\\n        if (!input.IsNull)\\n        {\\n            SqlDateTime dt = new SqlDateTime(input.Value.Year, input.Value.Month, input.Value.Day, 0, 0, 0);\\n\\n            return dt;\\n        }\\n        else\\n            return SqlDateTime.Null;\\n    }\\n</code>', '<code>WHERE DateColumn &gt;= {TimeTruncatingExpression}(@DateValue) AND DateColumn &lt; {TimeTruncatingExpression}(@DateValue + 1)</code>', \"<code>CAST\\n(\\nCAST(YEAR(DATEFIELD) as varchar(4)) + '/' CAST(MM(DATEFIELD) as varchar(2)) + '/' CAST(DD(DATEFIELD) as varchar(2)) as datetime\\n) \\n</code>\", '<code>datetime</code>', '<code>datetime</code>', '<code>datetime</code>', '<code>DATEADD</code>', '<code>DATEDIFF</code>', '<code>MM</code>', '<code>DD</code>']",
         "title": "Best approach to remove time part of datetime in SQL Server",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 1177467,
               "is_accepted": false,
               "last_activity_date": 1248440114,
               "body_markdown": "Strip time on inserts/updates in the first place. As for on-the-fly conversion, nothing can beat a user-defined function maintanability-wise:\r\n\r\n    select date_only(dd)\r\n\r\nThe implementation of `date_only` can be anything you like - now it&#39;s abstracted away and calling code is much much cleaner.",
               "id": "1177467",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1248440114,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 19,
               "answer_id": 1177479,
               "is_accepted": false,
               "last_activity_date": 1248440274,
               "body_markdown": "    SELECT CAST(FLOOR(CAST(getdate() AS FLOAT)) AS DATETIME)",
               "id": "1177479",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1248440274,
               "score": 19
            },
            {
               "up_vote_count": 1,
               "answer_id": 1177522,
               "last_activity_date": 1383144509,
               "path": "3.stack.answer",
               "body_markdown": "I, personally, almost always use [User Defined functions][1] for this if dealing with SQL Server 2005 (or lower version), however, it should be noted that there are specific drawbacks to using UDF&#39;s, especially if applying them to WHERE clauses (see below and the comments on this answer for further details).  If using SQL Server 2008 (or higher) - see below.\r\n\r\nIn fact, for most databases that I create, I add these UDF&#39;s in right near the start since I know there&#39;s a 99% chance I&#39;m going to need them sooner or later.\r\n\r\nI create one for &quot;date only&quot; &amp; &quot;time only&quot; (although the &quot;date only&quot; one is by far the most used of the two).\r\n\r\nHere&#39;s some links to a variety of date-related UDF&#39;s:\r\n\r\n[Essential SQL Server Date, Time and DateTime Functions][2]  \r\n[Get Date Only Function][3]  \r\n\r\nThat last link shows no less than 3 different ways to getting the date only part of a datetime field and mentions some pros and cons of each approach.\r\n\r\nIf using a UDF, it should be noted that you should try to avoid using the UDF as part of a WHERE clause in a query as this will greatly hinder performance of the query.  The main reason for this is that using a UDF in a WHERE clause renders that clause as [non-sargable][4], which means that SQL Server can no longer use an index with that clause in order to improve the speed of query execution.  With reference to my own usage of UDF&#39;s, I&#39;ll frequently use the &quot;raw&quot; date column within the WHERE clause, but apply the UDF to the SELECTed column.  In this way, the UDF is only applied to the filtered result-set and not every row of the table as part of the filter.\r\n\r\nOf course, the absolute *best* approach for this is to use SQL Server 2008 (or higher) and separate out your [dates and times][5], as the SQL Server database engine is then natively providing the individual date and time components, and can efficiently query these independently without the need for a UDF or other mechanism to extract either the date or time part from a composite datetime type.\r\n\r\n\r\n  [1]: http://www.sqlteam.com/article/user-defined-functions\r\n  [2]: http://weblogs.sqlteam.com/jeffs/archive/2007/01/02/56079.aspx\r\n  [3]: http://www.sql-server-helper.com/functions/get-date-only.aspx\r\n  [4]: http://en.wikipedia.org/wiki/Sargable\r\n  [5]: http://www.sql-server-performance.com/articles/dev/datetime_2008_p1.aspx",
               "tags": [],
               "creation_date": 1248440899,
               "last_edit_date": 1383144509,
               "is_accepted": false,
               "id": "1177522",
               "down_vote_count": 1,
               "score": 0
            },
            {
               "up_vote_count": 462,
               "answer_id": 1177529,
               "last_activity_date": 1398877836,
               "path": "3.stack.answer",
               "body_markdown": "Strictly, method `a` is the least resource intensive:\r\n\r\n    a) select DATEADD(dd, DATEDIFF(dd, 0, getdate()), 0)\r\n\r\nProven less CPU intensive for same total duration a million rows by some one with way too much time on their hands: [Most efficient way in SQL Server to get date from date+time?][1]\r\n\r\nI saw a similar test elsewhere with similar results too.\r\n\r\nI prefer the DATEADD/DATEDIFF because:\r\n\r\n - varchar is subject to language/dateformat issues&lt;br&gt;\r\n Example: https://stackoverflow.com/q/3596663/27535\r\n - float relies on internal storage\r\n - it extends to work out first day of month, tomorrow etc by changing &quot;0&quot; base\r\n\r\n**Edit, Oct 2011**\r\n\r\nFor SQL Server 2008+, you can CAST to `date`. Or just use `date` so no time to remove.\r\n\r\n**Edit, Jan 2012**\r\n\r\nA worked example of how flexible this is: https://stackoverflow.com/questions/8722022/need-to-calculate-by-rounded-time-or-date-figure-in-sql-server/8723311#8723311\r\n\r\n**Edit, May 2012**\r\n\r\nDo not use this in WHERE clauses and the like without thinking: adding a function or CAST to a column invalidates index usage. See number 2 here: http://www.simple-talk.com/sql/t-sql-programming/ten-common-sql-programming-mistakes/\r\n\r\nNow, this does have an example of later SQL Server optimiser versions managing CAST to date correctly, but *generally* it will be a bad idea ...\r\n  [1]:  https://stackoverflow.com/questions/133081/most-efficient-way-in-sql-server-to-get-date-from-datetime",
               "tags": [],
               "creation_date": 1248440978,
               "last_edit_date": 1495541904,
               "is_accepted": true,
               "id": "1177529",
               "down_vote_count": 0,
               "score": 462
            },
            {
               "up_vote_count": 4,
               "answer_id": 1177683,
               "last_activity_date": 1248442884,
               "path": "3.stack.answer",
               "body_markdown": "See this question:  \r\n&lt;https://stackoverflow.com/questions/923295/how-to-truncate-a-datetime-in-sql-server&gt;\r\n\r\nWhatever you do, **don&#39;t use the string method**.  That&#39;s about the worst way you could do it.",
               "tags": [],
               "creation_date": 1248442380,
               "last_edit_date": 1495542392,
               "is_accepted": false,
               "id": "1177683",
               "down_vote_count": 2,
               "score": 2
            },
            {
               "up_vote_count": 8,
               "answer_id": 1178009,
               "last_activity_date": 1248445217,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s yet another answer, from another [duplicate question:][1]\r\n    \r\n    SELECT CAST(CAST(getutcdate() - 0.50000004 AS int) AS datetime) \r\n\r\nThis magic number method performs slightly faster than the DATEADD method. (It looks like ~10%)\r\n\r\nThe CPU Time on several rounds of a million records:\r\n\r\n    DATEADD   MAGIC FLOAT\r\n    500       453\r\n    453       360\r\n    375       375\r\n    406       360\r\n\r\nBut note that these numbers are possibly irrelevant because they are already VERY fast.  Unless I had record sets of 100,000 or more, I couldn&#39;t even get the CPU Time to read above zero.\r\n\r\nConsidering the fact that DateAdd is meant for this purpose and is more robust, I&#39;d say use DateAdd.\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/2775/whats-the-best-way-to-remove-the-time-portion-of-a-datetime-value-sql-server/3154#3154",
               "tags": [],
               "creation_date": 1248445217,
               "last_edit_date": 1495542890,
               "is_accepted": false,
               "id": "1178009",
               "down_vote_count": 0,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 3214464,
               "is_accepted": false,
               "last_activity_date": 1278691605,
               "body_markdown": "Already answered but ill throw this out there too...\r\nthis suposedly also preforms well but it works by throwing away the decimal (which stores time) from the float and returning only whole part (which is date)\r\n\r\n     CAST(\r\n    FLOOR( CAST( GETDATE() AS FLOAT ) )\r\n    AS DATETIME\r\n    )\r\n\r\nsecond time I found this solution... [i grabbed this code off][1] \r\n\r\n\r\n  [1]: http://www.bennadel.com/blog/122-Getting-Only-the-Date-Part-of-a-Date-Time-Stamp-in-SQL-Server.htm",
               "id": "3214464",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1278691605,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 7755869,
               "is_accepted": false,
               "last_activity_date": 1318516819,
               "body_markdown": "If possible, for special things like this, I like to use CLR functions.\r\n\r\nIn this case:\r\n\r\n    [Microsoft.SqlServer.Server.SqlFunction]\r\n        public static SqlDateTime DateOnly(SqlDateTime input)\r\n        {\r\n            if (!input.IsNull)\r\n            {\r\n                SqlDateTime dt = new SqlDateTime(input.Value.Year, input.Value.Month, input.Value.Day, 0, 0, 0);\r\n\r\n                return dt;\r\n            }\r\n            else\r\n                return SqlDateTime.Null;\r\n        }",
               "id": "7755869",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1318516819,
               "score": 0
            },
            {
               "up_vote_count": 2,
               "answer_id": 8182791,
               "last_activity_date": 1321625425,
               "path": "3.stack.answer",
               "body_markdown": "    CAST(round(cast(getdate()as real),0,1) AS datetime)\r\n\r\nThis method does not use string function. `Date` is basically a real datatype with digits before decimal are fraction of a day. \r\n\r\nthis I guess will be faster than a lot.",
               "tags": [],
               "creation_date": 1321621046,
               "last_edit_date": 1321625425,
               "is_accepted": false,
               "id": "8182791",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 1,
               "answer_id": 9165708,
               "last_activity_date": 1351226256,
               "path": "3.stack.answer",
               "body_markdown": "I think you mean\r\n   ` cast(floor(cast(getdate()as float))as datetime)`\r\n\r\nreal is only 32-bits, and could lose some information\r\n\r\nThis is fastest\r\n`cast(cast(getdate()+x-0.5 as int)as datetime)`\r\n\r\n...though only about 10% faster` (about 0.49 microseconds CPU vs. 0.58)`\r\n\r\nThis was recommended, and takes the same time in my test just now:\r\n`DATEADD(dd, DATEDIFF(dd, 0, getdate()), 0)`\r\n\r\nIn SQL 2008, the SQL CLR function is about 5 times faster than using a SQL function would be, at 1.35 microseconds versus 6.5 microsections, indicating much lower function-call overhead for a SQL CLR function versus a simple SQL UDF.\r\n\r\nIn SQL 2005, the SQL CLR function is 16 times faster, per my testing, versus this slow function:\r\n\r\n    create function dateonly (  @dt datetime )\r\n\treturns datetime\r\n    as\r\n    begin\r\n    return cast(floor(cast(@dt as float))as int)\r\n    end",
               "tags": [],
               "creation_date": 1328555112,
               "last_edit_date": 1351226256,
               "is_accepted": false,
               "id": "9165708",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 28,
               "answer_id": 10451347,
               "last_activity_date": 1452085514,
               "path": "3.stack.answer",
               "body_markdown": "Of-course this is an old thread but to make it complete.\r\n\r\nFrom SQL 2008 you can use DATE datatype so you can simply do:\r\n\r\n    SELECT CONVERT(DATE,GETDATE())",
               "tags": [],
               "creation_date": 1336143640,
               "last_edit_date": 1452085514,
               "is_accepted": false,
               "id": "10451347",
               "down_vote_count": 0,
               "score": 28
            },
            {
               "up_vote_count": 2,
               "answer_id": 13980334,
               "last_activity_date": 1356038431,
               "path": "3.stack.answer",
               "body_markdown": "For me the code below is always a winner:\r\n\r\n    SELECT CONVERT(DATETIME, FLOOR(CONVERT(FLOAT,GETDATE())));\r\n\r\n\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1356036968,
               "last_edit_date": 1356038431,
               "is_accepted": false,
               "id": "13980334",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 4,
               "answer_id": 17449578,
               "last_activity_date": 1372859700,
               "path": "3.stack.answer",
               "body_markdown": "    SELECT CAST(CAST(GETDATE() AS DATE) AS DATETIME)",
               "tags": [],
               "creation_date": 1372858736,
               "last_edit_date": 1372859700,
               "is_accepted": false,
               "id": "17449578",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 1,
               "answer_id": 18521295,
               "last_activity_date": 1459011183,
               "path": "3.stack.answer",
               "body_markdown": "I think that if you stick strictly with `TSQL` that this is the fastest way to truncate the time:\r\n\r\n     select convert(datetime,convert(int,convert(float,[Modified])))\r\n\r\nI found this truncation method to be about 5% faster than the `DateAdd` method. And this can be easily modified to round to the nearest day like this:\r\n\r\n \r\n\r\n    select convert(datetime,ROUND(convert(float,[Modified]),0))\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1377811845,
               "last_edit_date": 1459011183,
               "is_accepted": false,
               "id": "18521295",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 49,
               "answer_id": 19541838,
               "last_activity_date": 1452322193,
               "path": "3.stack.answer",
               "body_markdown": "In SQL Server 2008, you can use:\r\n\r\n&lt;!-- language: lang-sql --&gt;\r\n\r\n    CONVERT(DATE, getdate(), 101)",
               "tags": [],
               "creation_date": 1382531634,
               "last_edit_date": 1452322193,
               "is_accepted": false,
               "id": "19541838",
               "down_vote_count": 2,
               "score": 47
            },
            {
               "up_vote_count": 1,
               "answer_id": 20681534,
               "last_activity_date": 1387456429,
               "path": "3.stack.answer",
               "body_markdown": "I would use:\r\n\r\n    CAST\r\n    (\r\n    CAST(YEAR(DATEFIELD) as varchar(4)) + &#39;/&#39; CAST(MM(DATEFIELD) as varchar(2)) + &#39;/&#39; CAST(DD(DATEFIELD) as varchar(2)) as datetime\r\n    ) \r\n\r\nThus effectively creating a new field from the date field you already have. \r\n",
               "tags": [],
               "creation_date": 1387455280,
               "last_edit_date": 1387456429,
               "is_accepted": false,
               "id": "20681534",
               "down_vote_count": 3,
               "score": -2
            },
            {
               "up_vote_count": 1,
               "answer_id": 21236499,
               "last_activity_date": 1390399306,
               "path": "3.stack.answer",
               "body_markdown": "BEWARE!\r\n\r\nMethod a) and b) does NOT always have the same output!\r\n\r\n    select DATEADD(dd, DATEDIFF(dd, 0, &#39;2013-12-31 23:59:59.999&#39;), 0)\r\n\r\nOutput: `2014-01-01 00:00:00.000`\r\n\r\n    select cast(convert(char(11), &#39;2013-12-31 23:59:59.999&#39;, 113) as datetime)\r\n\r\nOutput: `2013-12-31 00:00:00.000`\r\n\r\n(Tested on MS SQL Server 2005 and 2008 R2)\r\n\r\nEDIT: According to Adam&#39;s comment, this cannot happen if you read the date value from the table, but it can happen if you provide your date value as a literal (example: as a parameter of a stored procedure called via ADO.NET).",
               "tags": [],
               "creation_date": 1390228088,
               "last_edit_date": 1390399306,
               "is_accepted": false,
               "id": "21236499",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 21679457,
               "is_accepted": false,
               "last_activity_date": 1392041493,
               "body_markdown": "select CONVERT(char(10), GetDate(),126) ",
               "id": "21679457",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1392041493,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 22604729,
               "is_accepted": false,
               "last_activity_date": 1395651161,
               "body_markdown": "How about `select cast(cast my_datetime_field as date) as datetime)`?  This results in the same date, with the time set to 00:00, but avoids any conversion to text and also avoids any explicit numeric rounding.",
               "id": "22604729",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1395651161,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 11,
               "answer_id": 22850474,
               "is_accepted": false,
               "last_activity_date": 1396565653,
               "body_markdown": "In SQL Server 2008, there is a DATE datetype (also a TIME datatype).\r\n\r\n    CAST(GetDate() as DATE)\r\n\r\nor\r\n\r\n    declare @Dt as DATE = GetDate()\r\n\r\n\r\n",
               "id": "22850474",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1396565653,
               "score": 11
            },
            {
               "up_vote_count": 2,
               "answer_id": 26017207,
               "last_activity_date": 1459387958,
               "path": "3.stack.answer",
               "body_markdown": "Just in case anyone is looking in here for a Sybase version since several of the versions above didn&#39;t work \r\n\r\n    CAST(CONVERT(DATE,GETDATE(),103) AS DATETIME)\r\n\r\n- Tested in I SQL v11 running on Adaptive Server 15.7\r\n\r\n",
               "tags": [],
               "creation_date": 1411562540,
               "last_edit_date": 1459387958,
               "is_accepted": false,
               "id": "26017207",
               "down_vote_count": 1,
               "score": 1
            },
            {
               "up_vote_count": 1,
               "answer_id": 31011657,
               "last_activity_date": 1459012078,
               "path": "3.stack.answer",
               "body_markdown": "Here I made a function to remove some parts of a datetime for SQL Server. Usage:\r\n\r\n - First param is the datetime to be stripped off.\r\n - Second param is a char: \r\n   - s: rounds to seconds; removes milliseconds\r\n   - m: rounds to minutes; removes seconds and milliseconds\r\n   - h: rounds to hours; removes minutes, seconds and milliseconds.\r\n   - d: rounds to days; removes hours, minutes, seconds and milliseconds.\r\n - Returns the new datetime\r\n\r\n\r\n`create function dbo.uf_RoundDateTime(@dt as datetime, @part as char)\r\n    returns datetime\r\nas\r\nbegin\r\n    if CHARINDEX( @part, &#39;smhd&#39;,0) = 0 return @dt;\r\n    return cast(\r\n        Case @part\r\n            when &#39;s&#39; then convert(varchar(19), @dt, 126)\r\n            when &#39;m&#39; then convert(varchar(17), @dt, 126) + &#39;00&#39;\r\n            when &#39;h&#39; then convert(varchar(14), @dt, 126) + &#39;00:00&#39;\r\n            when &#39;d&#39; then convert(varchar(14), @dt, 112)\r\n       end as datetime )\r\nend`",
               "tags": [],
               "creation_date": 1435087243,
               "last_edit_date": 1459012078,
               "is_accepted": false,
               "id": "31011657",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 2,
               "answer_id": 41560027,
               "last_activity_date": 1484071089,
               "path": "3.stack.answer",
               "body_markdown": "I really like:\r\n\r\n    [date] = CONVERT(VARCHAR(10), GETDATE(), 120)\r\n\r\nThe `120` format code will coerce the date into the ISO 8601 standard:\r\n\r\n    &#39;YYYY-MM-DD&#39; or &#39;2017-01-09&#39;\r\n\r\nSuper easy to use in dplyr (`R`) and pandas (`Python`)!",
               "tags": [],
               "creation_date": 1484013963,
               "last_edit_date": 1484071089,
               "is_accepted": false,
               "id": "41560027",
               "down_vote_count": 1,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/1177449/best-approach-to-remove-time-part-of-datetime-in-sql-server",
         "id": "858127-2299"
      },
      {
         "up_vote_count": "1748",
         "path": "2.stack",
         "body_markdown": "What is the module/method used to get current time?",
         "view_count": "1835953",
         "answer_count": "28",
         "tags": "['python', 'datetime', 'time']",
         "creation_date": "1231217663",
         "last_edit_date": "1506797738",
         "code_snippet": "['<code>&gt;&gt;&gt; import datetime\\n&gt;&gt;&gt; datetime.datetime.now()\\ndatetime(2009, 1, 6, 15, 8, 24, 78915)\\n</code>', '<code>&gt;&gt;&gt; datetime.datetime.time(datetime.datetime.now())\\ndatetime.time(15, 8, 24, 78915)\\n</code>', '<code>&gt;&gt;&gt; datetime.datetime.now().time()\\n</code>', '<code>datetime</code>', '<code>datetime</code>', '<code>&gt;&gt;&gt; from datetime import datetime\\n</code>', '<code>datetime.</code>', \"<code>AttributeError: type object 'datetime.datetime' has no attribute 'datetime'</code>\", '<code>datetime</code>', '<code>datetime</code>', '<code>time.strftime()</code>', '<code>&gt;&gt;&gt; from time import gmtime, strftime\\n&gt;&gt;&gt; strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\\n\\'2009-01-05 22:14:39\\'\\n</code>', '<code>str()</code>', \"<code>&gt;&gt;&gt; from datetime import datetime\\n&gt;&gt;&gt; str(datetime.now())\\n'2011-05-03 17:45:35.177000'\\n</code>\", '<code>import datetime</code>', '<code>datetime.datetime.now()</code>', \"<code>&gt;&gt;&gt; from datetime import datetime\\n&gt;&gt;&gt; datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\n</code>\", \"<code>'2013-09-18 11:16:32'</code>\", '<code>time</code>', '<code>time</code>', '<code>import time\\n</code>', '<code>&gt;&gt;&gt; time.time()\\n1424233311.771502\\n</code>', '<code>time.ctime</code>', \"<code>&gt;&gt;&gt; time.ctime()\\n'Tue Feb 17 23:21:56 2015'\\n</code>\", '<code>ctime</code>', \"<code>&gt;&gt;&gt; time.ctime(1424233311.771502)\\n'Tue Feb 17 23:21:51 2015'\\n</code>\", '<code>datetime</code>', '<code>datetime</code>', '<code>&gt;&gt;&gt; import datetime\\n</code>', '<code>datetime.datetime.now</code>', '<code>datetime.now</code>', '<code>time.localtime</code>', '<code>str</code>', '<code>&gt;&gt;&gt; datetime.datetime.now()\\ndatetime.datetime(2015, 2, 17, 23, 43, 49, 94252)\\n&gt;&gt;&gt; print(datetime.datetime.now())\\n2015-02-17 23:43:51.782461\\n</code>', '<code>utcnow</code>', '<code>&gt;&gt;&gt; datetime.datetime.utcnow()\\ndatetime.datetime(2015, 2, 18, 4, 53, 28, 394163)\\n&gt;&gt;&gt; print(datetime.datetime.utcnow())\\n2015-02-18 04:53:31.783988\\n</code>', '<code>pytz</code>', '<code>&gt;&gt;&gt; import pytz\\n&gt;&gt;&gt; then = datetime.datetime.now(pytz.utc)\\n&gt;&gt;&gt; then\\ndatetime.datetime(2015, 2, 18, 4, 55, 58, 753949, tzinfo=&lt;UTC&gt;)\\n</code>', '<code>timezone</code>', '<code>timezone</code>', '<code>pytz</code>', '<code>&gt;&gt;&gt; datetime.datetime.now(datetime.timezone.utc)\\ndatetime.datetime(2015, 2, 18, 22, 31, 56, 564191, tzinfo=datetime.timezone.utc)\\n</code>', \"<code>&gt;&gt;&gt; print(then)\\n2015-02-18 04:55:58.753949+00:00\\n&gt;&gt;&gt; print(then.astimezone(pytz.timezone('US/Eastern')))\\n2015-02-17 23:55:58.753949-05:00\\n</code>\", '<code>pytz</code>', '<code>localize</code>', '<code>replace</code>', '<code>&gt;&gt;&gt; pytz.utc.localize(datetime.datetime.utcnow())\\ndatetime.datetime(2015, 2, 18, 6, 6, 29, 32285, tzinfo=&lt;UTC&gt;)\\n&gt;&gt;&gt; datetime.datetime.utcnow().replace(tzinfo=pytz.utc)\\ndatetime.datetime(2015, 2, 18, 6, 9, 30, 728550, tzinfo=&lt;UTC&gt;)\\n</code>', '<code>pytz</code>', '<code>datetime</code>', '<code>pytz</code>', '<code>ctime</code>', '<code>datetime</code>', '<code>from time import time\\n\\nt = time()\\n</code>', '<code>t</code>', '<code>&gt;&gt;&gt; from time import gmtime, strftime\\n&gt;&gt;&gt; strftime(\"%a, %d %b %Y %X +0000\", gmtime())\\n\\'Tue, 06 Jan 2009 04:54:56 +0000\\'\\n</code>', '<code>&gt;&gt;&gt; from datetime import datetime\\n&gt;&gt;&gt; datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\\n\\'2016-07-27 15:56:59\\'\\n&gt;&gt;&gt;\\n</code>', '<code>ctime()</code>', \"<code>In [2]: from time import ctime\\nIn [3]: ctime()\\nOut[3]: 'Thu Oct 31 11:40:53 2013'\\n</code>\", '<code>time</code>', '<code>&gt;&gt;&gt; import datetime\\n&gt;&gt;&gt; now = datetime.datetime.now()\\n&gt;&gt;&gt; datetime.time(now.hour, now.minute, now.second)\\ndatetime.time(11, 23, 44)\\n</code>', '<code>&gt;&gt;&gt; import time\\n&gt;&gt;&gt; time.strftime(\"%Y%m%d\")\\n\\'20130924\\'\\n</code>', \"<code>import requests\\nfrom lxml import html\\n\\npage = requests.get('http://tycho.usno.navy.mil/cgi-bin/timer.pl')\\ntree = html.fromstring(page.content)\\nprint(tree.xpath('//html//body//h3//pre/text()')[1])\\n</code>\", '<code>.isoformat()</code>', \"<code>&gt;&gt;&gt;import datetime\\n&gt;&gt;&gt; datetime.datetime.now().isoformat()\\n'2013-06-24T20:35:55.982000'\\n</code>\", '<code>datetime</code>', \"<code>import datetime\\nprint(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\\n</code>\", '<code>2017-10-17 23:48:55\\n</code>', '<code>time</code>', '<code>import time\\nprint(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()))\\n</code>', '<code>2017-10-17 18:22:26\\n</code>', '<code>&gt;&gt;&gt;from time import strftime\\n&gt;&gt;&gt;strftime(\"%m/%d/%Y %H:%M\")\\n01/09/2015 13:11\\n</code>', '<code>strftime(time_format)</code>', '<code>time_format</code>', '<code>time.strftime()</code>', '<code>datetime.strftime()</code>', '<code>%z</code>', '<code>time.strftime()</code>', '<code>time</code>', '<code>datetime</code>', '<code>datetime.now()</code>', '<code>from datetime import datetime\\n\\nutc_time = datetime.utcnow()\\nprint(utc_time) # -&gt; 2014-12-22 22:48:59.916417\\n</code>', '<code>from datetime import datetime, timezone\\n\\nnow = datetime.now(timezone.utc).astimezone()\\nprint(now) # -&gt; 2014-12-23 01:49:25.837541+03:00\\n</code>', '<code>import datetime\\ndate_time = datetime.datetime.now()\\n\\ndate = date_time.date()  # gives date\\ntime = date_time.time()  # gives time\\n\\nprint date.year, date.month, date.day\\nprint time.hour, time.minute, time.second, time.microsecond\\n</code>', '<code>dir(date)</code>', '<code>import datetime</code>', '<code>datetime.datetime.now()</code>', '<code>from datetime import datetime</code>', '<code>datetime.now()</code>', '<code>import time\\nprint time.strftime(\"%d/%m/%Y\")\\n\\n&gt;&gt;&gt; 06/02/2015\\n</code>', '<code>Y</code>', '<code>y</code>', '<code>06/02/15</code>', '<code>time.strftime(\"%a, %d %b %Y %H:%M:%S\")\\n&gt;&gt;&gt; \\'Fri, 06 Feb 2015 17:45:09\\'\\n</code>', '<code>&gt;&gt;&gt; import datetime, time\\n&gt;&gt;&gt; time = strftime(\"%H:%M:%S:%MS\", time.localtime())\\n&gt;&gt;&gt; print time\\n\\'00:20:58:20S\\'\\n</code>', '<code>import arrow\\narrow.now()\\n</code>', '<code>arrow.utcnow()\\n</code>', \"<code>arrow.utcnow().format('YYYY-MM-DD HH:mm:ss ZZ')\\n</code>\", \"<code>arrow.now('US/Pacific')\\n</code>\", '<code>arrow.utcnow().replace(hours=-1)\\n</code>', \"<code>arrow.get('2013-05-11T21:23:58.970460+00:00').humanize()\\n&gt;&gt;&gt; '2 years ago'\\n</code>\", \"<code>arrow.now('Time/Zone')</code>\", '<code>arrow</code>', '<code>dateutil</code>', '<code>arrow.now()</code>', '<code>pytz</code>', '<code>import pandas as pd\\nprint (pd.datetime.now())\\nprint (pd.datetime.now().date())\\nprint (pd.datetime.now().year)\\nprint (pd.datetime.now().month)\\nprint (pd.datetime.now().day)\\nprint (pd.datetime.now().hour)\\nprint (pd.datetime.now().minute)\\nprint (pd.datetime.now().second)\\nprint (pd.datetime.now().microsecond)\\n</code>', '<code>2017-09-22 12:44:56.092642\\n2017-09-22\\n2017\\n9\\n22\\n12\\n44\\n56\\n92693\\n</code>', '<code>import time, datetime\\n\\nprint(datetime.datetime.now().time())                         # 11:20:08.272239\\n# or in a more complicated way\\nprint(datetime.datetime.now().time().isoformat())             # 11:20:08.272239\\nprint(datetime.datetime.now().time().strftime(\\'%H:%M:%S.%f\\')) # 11:20:08.272239\\n# but do not use this\\nprint(time.strftime(\"%H:%M:%S.%f\", time.localtime()), str)    # 11:20:08.%f\\n</code>', '<code>import time\\n\\ntime.strftime(\"%H:%M:%S\", time.localtime()) + \\'.%d\\' % (time.time() % 1 * 1000)\\n# 11:34:23.751\\n</code>', '<code>def get_time_str(decimal_points=3):\\n    return time.strftime(\"%H:%M:%S\", time.localtime()) + \\'.%d\\' % (time.time() % 1 * 10**decimal_points)\\n</code>', '<code>from time import ctime\\nprint ctime().split()[3]\\n</code>', \"<code>import numpy as np\\nstr(np.datetime64('now'))\\n</code>\", \"<code>str(np.datetime64('today'))\\n</code>\", \"<code>import pandas as pd\\nstr(pd.to_datetime('now'))\\n</code>\", \"<code>str(pd.to_datetime('today'))\\n</code>\", '<code>import datetime\\ndate_time = str(datetime.datetime.now())\\ndate = date_time.split()[0]\\ntime = date_time.split()[1]\\n</code>', '<code>from time import ctime\\n\\n// Day {Mon,Tue,..}\\nprint ctime().split()[0]\\n// Month {Jan, Feb,..}\\nprint ctime().split()[1]\\n// Date {1,2,..}\\nprint ctime().split()[2]\\n// HH:MM:SS\\nprint ctime().split()[3]\\n// Year {2018,..}\\nprint ctime().split()[4]\\n</code>', '<code>ctime()</code>', \"<code>def gettime():\\n        from datetime import datetime\\n        return ((str(datetime.now())).split(' ')[1]).split('.')[0]\\n</code>\", \"<code>def gethour():\\n        from datetime import datetime\\n        return return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[0]\\n</code>\", \"<code>def getminute():\\n        from datetime import datetime\\n        return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[1]\\n</code>\", \"<code>def getsecond():\\n        from datetime import datetime\\n        return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[2]\\n</code>\", \"<code>def getmillisecond():\\n        from datetime import datetime\\n        return return (str(datetime.now())).split('.')[1]\\n</code>\", '<code>now()</code>', '<code>YYYY-MM-DD HH:MM:SS:MS</code>', '<code>getDateTime1.py</code>', '<code>import datetime\\n\\ncurrentDT = datetime.datetime.now()\\nprint (str(currentDT))\\n</code>', '<code>2018-03-01 17:03:46.759624\\n</code>']",
         "title": "How to get current time in Python?",
         "_childDocuments_": [
            {
               "up_vote_count": 1765,
               "answer_id": 415519,
               "last_activity_date": 1464546435,
               "path": "3.stack.answer",
               "body_markdown": "    &gt;&gt;&gt; import datetime\r\n    &gt;&gt;&gt; datetime.datetime.now()\r\n    datetime(2009, 1, 6, 15, 8, 24, 78915)\r\n\r\nAnd just the time:\r\n\r\n    &gt;&gt;&gt; datetime.datetime.time(datetime.datetime.now())\r\n    datetime.time(15, 8, 24, 78915)\r\n\r\nThe same but slightly more compact:\r\n\r\n    &gt;&gt;&gt; datetime.datetime.now().time()\r\n\r\nSee the [**documentation**][1] for more info.\r\n\r\n\r\nTo save typing, you can import the `datetime` object from the `datetime` module:\r\n\r\n    &gt;&gt;&gt; from datetime import datetime\r\n\r\nThen remove the leading `datetime.` from all the above.\r\n\r\n\r\n[1]: https://docs.python.org/3/library/datetime.html",
               "tags": [],
               "creation_date": 1231217825,
               "last_edit_date": 1464546435,
               "is_accepted": true,
               "id": "415519",
               "down_vote_count": 3,
               "score": 1762
            },
            {
               "up_vote_count": 717,
               "answer_id": 415525,
               "last_activity_date": 1361395037,
               "path": "3.stack.answer",
               "body_markdown": "You can use [`time.strftime()`][1]:    \r\n\r\n    &gt;&gt;&gt; from time import gmtime, strftime\r\n    &gt;&gt;&gt; strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, gmtime())\r\n    &#39;2009-01-05 22:14:39&#39;\r\n\r\n[1]: http://docs.python.org/3.3/library/time.html?highlight=time.strftime#time.strftime",
               "tags": [],
               "creation_date": 1231217992,
               "last_edit_date": 1361395037,
               "is_accepted": false,
               "id": "415525",
               "down_vote_count": 2,
               "score": 715
            },
            {
               "up_vote_count": 77,
               "answer_id": 415527,
               "last_activity_date": 1361396737,
               "path": "3.stack.answer",
               "body_markdown": "    &gt;&gt;&gt; from time import gmtime, strftime\r\n    &gt;&gt;&gt; strftime(&quot;%a, %d %b %Y %X +0000&quot;, gmtime())\r\n    &#39;Tue, 06 Jan 2009 04:54:56 +0000&#39;\r\n\r\nThat outputs the current GMT in the specified format. There is also a localtime() method. \r\n\r\nThis [page][1] has more details.\r\n\r\n\r\n  [1]: http://docs.python.org/library/time.html#module-time",
               "tags": [],
               "creation_date": 1231218163,
               "last_edit_date": 1361396737,
               "is_accepted": false,
               "id": "415527",
               "down_vote_count": 1,
               "score": 76
            },
            {
               "up_vote_count": 105,
               "answer_id": 416605,
               "last_activity_date": 1449211613,
               "path": "3.stack.answer",
               "body_markdown": "Do\r\n\r\n    from time import time\r\n\r\n    t = time()\r\n\r\n- `t` - float number, good for time interval measurement.\r\n\r\nThere is some difference for Unix and Windows platforms.",
               "tags": [],
               "creation_date": 1231250123,
               "last_edit_date": 1449211613,
               "is_accepted": false,
               "id": "416605",
               "down_vote_count": 2,
               "score": 103
            },
            {
               "up_vote_count": 31,
               "answer_id": 4538034,
               "last_activity_date": 1322071684,
               "path": "3.stack.answer",
               "body_markdown": "If you need current time as a `time` object:\r\n\r\n    &gt;&gt;&gt; import datetime\r\n    &gt;&gt;&gt; now = datetime.datetime.now()\r\n    &gt;&gt;&gt; datetime.time(now.hour, now.minute, now.second)\r\n    datetime.time(11, 23, 44)",
               "tags": [],
               "creation_date": 1293445452,
               "last_edit_date": 1322071684,
               "is_accepted": false,
               "id": "4538034",
               "down_vote_count": 1,
               "score": 30
            },
            {
               "up_vote_count": 373,
               "answer_id": 5877368,
               "last_activity_date": 1304470589,
               "path": "3.stack.answer",
               "body_markdown": "Similar to [Harley&#39;s answer][1], but use the `str()` function for a quick-n-dirty, slightly more human readable format:\r\n\r\n    &gt;&gt;&gt; from datetime import datetime\r\n    &gt;&gt;&gt; str(datetime.now())\r\n    &#39;2011-05-03 17:45:35.177000&#39;\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/415511/how-to-get-current-time-in-python/415519#415519",
               "tags": [],
               "creation_date": 1304470589,
               "last_edit_date": 1495540052,
               "is_accepted": false,
               "id": "5877368",
               "down_vote_count": 3,
               "score": 370
            },
            {
               "up_vote_count": 321,
               "answer_id": 14229023,
               "last_activity_date": 1513607239,
               "path": "3.stack.answer",
               "body_markdown": "    &gt;&gt;&gt; from datetime import datetime\r\n    &gt;&gt;&gt; datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)\r\n\r\nFor this example, the output will be like this: `&#39;2013-09-18 11:16:32&#39;`\r\n\r\nHere is the list of [strftime][1].\r\n\r\n\r\n  [1]: https://docs.python.org/3/library/time.html#time.strftime",
               "tags": [],
               "creation_date": 1357710634,
               "last_edit_date": 1513607239,
               "is_accepted": false,
               "id": "14229023",
               "down_vote_count": 2,
               "score": 319
            },
            {
               "up_vote_count": 22,
               "answer_id": 17287377,
               "last_activity_date": 1372153638,
               "path": "3.stack.answer",
               "body_markdown": "I&#39;ll contribute to this because `.isoformat()` is in the documentation but not yet here\r\n(this is mighty similar to @Ray Vega&#39;s answer):\r\n\r\n    &gt;&gt;&gt;import datetime\r\n    &gt;&gt;&gt; datetime.datetime.now().isoformat()\r\n    &#39;2013-06-24T20:35:55.982000&#39;",
               "tags": [],
               "creation_date": 1372120705,
               "last_edit_date": 1372153638,
               "is_accepted": false,
               "id": "17287377",
               "down_vote_count": 0,
               "score": 22
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 27,
               "answer_id": 18980227,
               "is_accepted": false,
               "last_activity_date": 1380021670,
               "body_markdown": "Quickest way is\r\n\r\n    &gt;&gt;&gt; import time\r\n    &gt;&gt;&gt; time.strftime(&quot;%Y%m%d&quot;)\r\n    &#39;20130924&#39;\r\n",
               "id": "18980227",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1380021670,
               "score": 26
            },
            {
               "up_vote_count": 37,
               "answer_id": 19710846,
               "last_activity_date": 1449211647,
               "path": "3.stack.answer",
               "body_markdown": "All good suggestions, but I find it easiest to use `ctime()` myself:\r\n\r\n    In [2]: from time import ctime\r\n    In [3]: ctime()\r\n    Out[3]: &#39;Thu Oct 31 11:40:53 2013&#39;\r\n\r\nThis gives a nicely formatted string representation of current local time.",
               "tags": [],
               "creation_date": 1383233979,
               "last_edit_date": 1449211647,
               "is_accepted": false,
               "id": "19710846",
               "down_vote_count": 1,
               "score": 36
            },
            {
               "up_vote_count": 9,
               "answer_id": 23753208,
               "last_activity_date": 1400655851,
               "path": "3.stack.answer",
               "body_markdown": "    &gt;&gt;&gt; import datetime, time\r\n    &gt;&gt;&gt; time = strftime(&quot;%H:%M:%S:%MS&quot;, time.localtime())\r\n    &gt;&gt;&gt; print time\r\n    &#39;00:20:58:20S&#39;",
               "tags": [],
               "creation_date": 1400570016,
               "last_edit_date": 1400655851,
               "is_accepted": false,
               "id": "23753208",
               "down_vote_count": 1,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 10,
               "answer_id": 27611645,
               "is_accepted": false,
               "last_activity_date": 1419288767,
               "body_markdown": "`datetime.now()` returns the current time as a naive datetime object that represents time in the local timezone. That value may be ambiguous e.g., during DST transitions (&quot;fall back&quot;). To avoid ambiguity either UTC timezone should be used:\r\n\r\n    from datetime import datetime\r\n    \r\n    utc_time = datetime.utcnow()\r\n    print(utc_time) # -&gt; 2014-12-22 22:48:59.916417\r\n\r\nOr a timezone-aware object that has the corresponding timezone info attached (Python 3.2+):\r\n\r\n    from datetime import datetime, timezone\r\n\r\n    now = datetime.now(timezone.utc).astimezone()\r\n    print(now) # -&gt; 2014-12-23 01:49:25.837541+03:00\r\n",
               "id": "27611645",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1419288767,
               "score": 10
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 27866830,
               "is_accepted": false,
               "last_activity_date": 1420827889,
               "body_markdown": "This is what I ended up going with: \r\n\r\n    &gt;&gt;&gt;from time import strftime\r\n    &gt;&gt;&gt;strftime(&quot;%m/%d/%Y %H:%M&quot;)\r\n    01/09/2015 13:11\r\n\r\nAlso, this table is a necessary reference for choosing the appropriate format codes to get the date formatted just the way you want it (from Python &quot;datetime&quot; documentation [here][1]).\r\n\r\n![strftime format code table][2]\r\n\r\n\r\n  [1]: https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior\r\n  [2]: http://i.stack.imgur.com/i6Hg7.jpg",
               "id": "27866830",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1420827889,
               "score": 15
            },
            {
               "up_vote_count": 10,
               "answer_id": 28371709,
               "last_activity_date": 1424072806,
               "path": "3.stack.answer",
               "body_markdown": "You can use the time module.\r\n\r\n    import time\r\n    print time.strftime(&quot;%d/%m/%Y&quot;)\r\n\r\n    &gt;&gt;&gt; 06/02/2015\r\nThe use of the captial `Y` gives the full year, using `y` would give `06/02/15`\r\n\r\nYou could also use to give a more lengthy time.\r\n\r\n    time.strftime(&quot;%a, %d %b %Y %H:%M:%S&quot;)\r\n    &gt;&gt;&gt; &#39;Fri, 06 Feb 2015 17:45:09&#39;",
               "tags": [],
               "creation_date": 1423244806,
               "last_edit_date": 1424072806,
               "is_accepted": false,
               "id": "28371709",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 138,
               "answer_id": 28576383,
               "last_activity_date": 1465591373,
               "path": "3.stack.answer",
               "body_markdown": "&gt; # How do I get the current time in Python?\r\n\r\n## The `time` module\r\n\r\nThe `time` module provides functions that tells us the time in &quot;seconds since the epoch&quot; as well as other utilities.\r\n\r\n    import time\r\n\r\n### Unix Epoch Time\r\n\r\nThis is the format you should get timestamps in for saving in databases. It is a simple floating point number that can be converted to an integer. It is also good for arithmetic in seconds, as it represents the number of seconds since Jan 1, 1970 00:00:00, and it is memory light relative to the other representations of time we&#39;ll be looking at next:\r\n\r\n    &gt;&gt;&gt; time.time()\r\n    1424233311.771502\r\n\r\nThis timestamp does not account for leap-seconds, so it&#39;s not linear - leap seconds are ignored. So while it is not equivalent to the international UTC standard, it is close, and therefore quite good for most cases of record-keeping. \r\n\r\nThis is not ideal for human scheduling, however. If you have a future event you wish to take place at a certain point in time, you&#39;ll want to store that time with a string that can be parsed into a datetime object or a serialized datetime object (these will be described later).\r\n\r\n### `time.ctime`\r\n\r\nYou can also represent the current time in the way preferred by your operating system (which means it can change when you change your system preferences, so don&#39;t rely on this to be standard across all systems, as I&#39;ve seen others expect). This is typically user friendly, but doesn&#39;t typically result in strings one can sort chronologically:\r\n\r\n    &gt;&gt;&gt; time.ctime()\r\n    &#39;Tue Feb 17 23:21:56 2015&#39;\r\n\r\nYou can hydrate timestamps into human readable form with `ctime` as well:\r\n\r\n    &gt;&gt;&gt; time.ctime(1424233311.771502)\r\n    &#39;Tue Feb 17 23:21:51 2015&#39;\r\n\r\nThis conversion is also not good for record-keeping (except in text that will only be parsed by humans - and with improved Optical Character Recognition and Artificial Intelligence, I think the number of these cases will diminish).\r\n\r\n\r\n## `datetime` module\r\n\r\nThe `datetime` module is also quite useful here:\r\n\r\n    &gt;&gt;&gt; import datetime\r\n\r\n### `datetime.datetime.now`\r\n\r\nThe `datetime.now` is a class method that returns the current time. It uses the `time.localtime` without the timezone info (if not given, otherwise see timezone aware below). It has a representation (which would allow you to recreate an equivalent object) echoed on the shell, but when printed (or coerced to a `str`), it is in human readable (and nearly ISO) format, and the lexicographic sort is equivalent to the chronological sort:\r\n\r\n    &gt;&gt;&gt; datetime.datetime.now()\r\n    datetime.datetime(2015, 2, 17, 23, 43, 49, 94252)\r\n    &gt;&gt;&gt; print(datetime.datetime.now())\r\n    2015-02-17 23:43:51.782461\r\n\r\n\r\n### datetime&#39;s `utcnow`\r\n\r\nYou can get a datetime object in UTC time, a global standard, by doing this:\r\n\r\n    &gt;&gt;&gt; datetime.datetime.utcnow()\r\n    datetime.datetime(2015, 2, 18, 4, 53, 28, 394163)\r\n    &gt;&gt;&gt; print(datetime.datetime.utcnow())\r\n    2015-02-18 04:53:31.783988\r\n\r\nUTC is a time standard that is nearly equivalent to the GMT timezone. (While GMT and UTC do not change for Daylight Savings Time, their users may switch to other timezones, like British Summer Time, during the Summer.) \r\n\r\n### datetime timezone aware\r\n\r\nHowever, none of the datetime objects we&#39;ve created so far can be easily converted to various timezones. We can solve that problem with the `pytz` module:\r\n\r\n    &gt;&gt;&gt; import pytz\r\n    &gt;&gt;&gt; then = datetime.datetime.now(pytz.utc)\r\n    &gt;&gt;&gt; then\r\n    datetime.datetime(2015, 2, 18, 4, 55, 58, 753949, tzinfo=&lt;UTC&gt;)\r\n\r\nEquivalently, in Python 3 we have the `timezone` class with a utc `timezone` instance attached, which also makes the object timezone aware (but to convert to another timezone without the handy `pytz` module is left as an exercise to the reader):\r\n\r\n    &gt;&gt;&gt; datetime.datetime.now(datetime.timezone.utc)\r\n    datetime.datetime(2015, 2, 18, 22, 31, 56, 564191, tzinfo=datetime.timezone.utc)\r\n\r\nAnd we see we can easily convert to timezones from the original utc object.\r\n\r\n    &gt;&gt;&gt; print(then)\r\n    2015-02-18 04:55:58.753949+00:00\r\n    &gt;&gt;&gt; print(then.astimezone(pytz.timezone(&#39;US/Eastern&#39;)))\r\n    2015-02-17 23:55:58.753949-05:00\r\n\r\nYou can also make a naive datetime object aware with the `pytz` timezone `localize` method, or by replacing the tzinfo attribute (with `replace`, this is done blindly), but these are more last resorts than best practices:\r\n\r\n    &gt;&gt;&gt; pytz.utc.localize(datetime.datetime.utcnow())\r\n    datetime.datetime(2015, 2, 18, 6, 6, 29, 32285, tzinfo=&lt;UTC&gt;)\r\n    &gt;&gt;&gt; datetime.datetime.utcnow().replace(tzinfo=pytz.utc)\r\n    datetime.datetime(2015, 2, 18, 6, 9, 30, 728550, tzinfo=&lt;UTC&gt;)\r\n\r\nThe `pytz` module allows us to make our `datetime` objects timezone aware and convert the times to the hundreds of timezones available in the `pytz` module.\r\n\r\nOne could ostensibly serialize this object for UTC time and store *that* in a database, but it would require far more memory and be more prone to error than simply storing the Unix Epoch time, which I demonstrated first. \r\n\r\nThe other ways of viewing times are much more error prone, especially when dealing with data that may come from different time zones. You want there to be no confusion as to which timezone a string or serialized datetime object was intended for.\r\n\r\nIf you&#39;re displaying the time with Python for the user, `ctime` works nicely, not in a table (it doesn&#39;t typically sort well), but perhaps in a clock. However, I personally recommend, when dealing with time in Python, either using Unix time, or a timezone aware UTC `datetime` object. \r\n",
               "tags": [],
               "creation_date": 1424236114,
               "last_edit_date": 1465591373,
               "is_accepted": false,
               "id": "28576383",
               "down_vote_count": 1,
               "score": 137
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 29980465,
               "is_accepted": false,
               "last_activity_date": 1430444348,
               "body_markdown": "This is what i use to get the time without having to format , some people dont like the split method but it is useful here :\r\n\r\n    from time import ctime\r\n    print ctime().split()[3]\r\n\r\nWill print in HH:MM:SS format",
               "id": "29980465",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1430444348,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 33704374,
               "is_accepted": false,
               "last_activity_date": 1447466565,
               "body_markdown": "Try the arrow module from http://crsmithdev.com/arrow/\r\n\r\n    import arrow\r\n    arrow.now()\r\n\r\nor the utc version\r\n\r\n    arrow.utcnow()\r\n\r\nto change it&#39;s output add .format()\r\n\r\n    arrow.utcnow().format(&#39;YYYY-MM-DD HH:mm:ss ZZ&#39;)\r\n\r\nfor a specific timezone?\r\n\r\n    arrow.now(&#39;US/Pacific&#39;)\r\n\r\nan hour ago\r\n\r\n    arrow.utcnow().replace(hours=-1)\r\n\r\n\r\nor if you want the gist.\r\n\r\n    arrow.get(&#39;2013-05-11T21:23:58.970460+00:00&#39;).humanize()\r\n    &gt;&gt;&gt; &#39;2 years ago&#39;\r\n\r\n",
               "id": "33704374",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1447466565,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 38366394,
               "is_accepted": false,
               "last_activity_date": 1468475420,
               "body_markdown": "    import datetime\r\n    date_time = str(datetime.datetime.now())\r\n    date = date_time.split()[0]\r\n    time = date_time.split()[1]\r\n\r\ndate will print date and time will print time.",
               "id": "38366394",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1468475420,
               "score": 1
            },
            {
               "up_vote_count": 6,
               "answer_id": 38433505,
               "last_activity_date": 1517218825,
               "path": "3.stack.answer",
               "body_markdown": "I am a simple man and I want get time with milliseconds. Simple way to get them:\r\n\r\n    import time, datetime\r\n    \r\n    print(datetime.datetime.now().time())                         # 11:20:08.272239\r\n    # or in a more complicated way\r\n    print(datetime.datetime.now().time().isoformat())             # 11:20:08.272239\r\n    print(datetime.datetime.now().time().strftime(&#39;%H:%M:%S.%f&#39;)) # 11:20:08.272239\r\n    # but do not use this\r\n    print(time.strftime(&quot;%H:%M:%S.%f&quot;, time.localtime()), str)    # 11:20:08.%f\r\n\r\nBut I want **only milliseconds**, right? Shortest way to get them:\r\n\r\n    import time\r\n\r\n    time.strftime(&quot;%H:%M:%S&quot;, time.localtime()) + &#39;.%d&#39; % (time.time() % 1 * 1000)\r\n    # 11:34:23.751\r\nAdd or remove zeroes from the last multiplication to adjust number of decimal points, or just:\r\n\r\n    def get_time_str(decimal_points=3):\r\n        return time.strftime(&quot;%H:%M:%S&quot;, time.localtime()) + &#39;.%d&#39; % (time.time() % 1 * 10**decimal_points)\r\n",
               "tags": [],
               "creation_date": 1468835150,
               "last_edit_date": 1517218825,
               "is_accepted": false,
               "id": "38433505",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 51,
               "answer_id": 38615092,
               "is_accepted": false,
               "last_activity_date": 1469627916,
               "body_markdown": "why not just keep things simple. \r\n\r\n    &gt;&gt;&gt; from datetime import datetime\r\n    &gt;&gt;&gt; datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)\r\n    &#39;2016-07-27 15:56:59&#39;\r\n    &gt;&gt;&gt;",
               "id": "38615092",
               "tags": [],
               "down_vote_count": 6,
               "creation_date": 1469627916,
               "score": 45
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 26,
               "answer_id": 39046637,
               "is_accepted": false,
               "last_activity_date": 1471635908,
               "body_markdown": "Why not ask the [U.S. Naval Observatory](http://tycho.usno.navy.mil/cgi-bin/timer.pl), the official timekeeper of the United States Navy?\r\n\r\n    import requests\r\n    from lxml import html\r\n    \r\n    page = requests.get(&#39;http://tycho.usno.navy.mil/cgi-bin/timer.pl&#39;)\r\n    tree = html.fromstring(page.content)\r\n    print(tree.xpath(&#39;//html//body//h3//pre/text()&#39;)[1])\r\n\r\nIf you live in the D.C. area (like me) the latency might not be too bad...",
               "id": "39046637",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1471635908,
               "score": 23
            },
            {
               "up_vote_count": 10,
               "answer_id": 43302976,
               "last_activity_date": 1492749565,
               "path": "3.stack.answer",
               "body_markdown": "    import datetime\r\n    date_time = datetime.datetime.now()\r\n\r\n    date = date_time.date()  # gives date\r\n    time = date_time.time()  # gives time\r\n\r\n    print date.year, date.month, date.day\r\n    print time.hour, time.minute, time.second, time.microsecond\r\n\r\ndo `dir(date)` or any variables including the package, you can get all the attributes and methods associated to the variable.",
               "tags": [],
               "creation_date": 1491711454,
               "last_edit_date": 1492749565,
               "is_accepted": false,
               "id": "43302976",
               "down_vote_count": 0,
               "score": 10
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 46365153,
               "is_accepted": false,
               "last_activity_date": 1506084357,
               "body_markdown": "using pandas to get current time, kind of over killing the problem at hand.\r\n\r\n    import pandas as pd\r\n    print (pd.datetime.now())\r\n    print (pd.datetime.now().date())\r\n    print (pd.datetime.now().year)\r\n    print (pd.datetime.now().month)\r\n    print (pd.datetime.now().day)\r\n    print (pd.datetime.now().hour)\r\n    print (pd.datetime.now().minute)\r\n    print (pd.datetime.now().second)\r\n    print (pd.datetime.now().microsecond)\r\n\r\noutput:\r\n\r\n    2017-09-22 12:44:56.092642\r\n    2017-09-22\r\n    2017\r\n    9\r\n    22\r\n    12\r\n    44\r\n    56\r\n    92693",
               "id": "46365153",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1506084357,
               "score": 8
            },
            {
               "up_vote_count": 21,
               "answer_id": 46796778,
               "last_activity_date": 1512689605,
               "path": "3.stack.answer",
               "body_markdown": "Simple and easy:\r\n\r\nUsing `datetime` package,\r\n\r\n    import datetime\r\n    print(datetime.datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;))\r\n\r\nOutput:\r\n\r\n    2017-10-17 23:48:55\r\n\r\n**OR**\r\n\r\nUsing `time`,\r\n\r\n    import time\r\n    print(time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.gmtime()))\r\n\r\nOutput:\r\n\r\n    2017-10-17 18:22:26\r\n\r\n",
               "tags": [],
               "creation_date": 1508264640,
               "last_edit_date": 1512689605,
               "is_accepted": false,
               "id": "46796778",
               "down_vote_count": 0,
               "score": 21
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 47151173,
               "is_accepted": false,
               "last_activity_date": 1510035472,
               "body_markdown": "if you are using numpy already then directly you can use numpy.datetime64() \r\nfunction.\r\n\r\n    import numpy as np\r\n    str(np.datetime64(&#39;now&#39;))\r\n\r\nfor only date:\r\n\r\n    str(np.datetime64(&#39;today&#39;))\r\n\r\n\r\nor, if you are using pandas already then you can use pandas.to_datetime() function\r\n\r\n    import pandas as pd\r\n    str(pd.to_datetime(&#39;now&#39;))\r\n\r\nor,\r\n\r\n    str(pd.to_datetime(&#39;today&#39;))",
               "id": "47151173",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1510035472,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47548082,
               "is_accepted": false,
               "last_activity_date": 1511943777,
               "body_markdown": "You Can Use This Function To Get The Time (Unfortunately It Doesn&#39;t Say AM Or PM):\r\n\r\n    def gettime():\r\n            from datetime import datetime\r\n            return ((str(datetime.now())).split(&#39; &#39;)[1]).split(&#39;.&#39;)[0]\r\n\r\nAlso To Get The Hours, Minutes, Seconds And Milliseconds To Merge Later, You Can Use These Functions:\r\n\r\n*Hour:*\r\n\r\n    def gethour():\r\n            from datetime import datetime\r\n            return return (((str(datetime.now())).split(&#39; &#39;)[1]).split(&#39;.&#39;)[0]).split(&#39;:&#39;)[0]\r\n\r\n*Minute:*\r\n\r\n    def getminute():\r\n            from datetime import datetime\r\n            return (((str(datetime.now())).split(&#39; &#39;)[1]).split(&#39;.&#39;)[0]).split(&#39;:&#39;)[1]\r\n\r\n*Second:*\r\n\r\n    def getsecond():\r\n            from datetime import datetime\r\n            return (((str(datetime.now())).split(&#39; &#39;)[1]).split(&#39;.&#39;)[0]).split(&#39;:&#39;)[2]\r\n\r\n*Millisecond:*\r\n\r\n    def getmillisecond():\r\n            from datetime import datetime\r\n            return return (str(datetime.now())).split(&#39;.&#39;)[1]",
               "id": "47548082",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1511943777,
               "score": 0
            },
            {
               "up_vote_count": 1,
               "answer_id": 48283832,
               "last_activity_date": 1516205064,
               "path": "3.stack.answer",
               "body_markdown": "    from time import ctime\r\n    \r\n    // Day {Mon,Tue,..}\r\n    print ctime().split()[0]\r\n    // Month {Jan, Feb,..}\r\n    print ctime().split()[1]\r\n    // Date {1,2,..}\r\n    print ctime().split()[2]\r\n    // HH:MM:SS\r\n    print ctime().split()[3]\r\n    // Year {2018,..}\r\n    print ctime().split()[4]\r\n\r\nWhen you call ctime() it will convert seconds to string in format &#39;Day Month Date HH:MM:SS Year&#39; (for example: &#39;Wed January 17 16:53:22 2018&#39;), then you call split() method that will make a list from your string[&#39;Wed&#39;,&#39;Jan&#39;,&#39;17&#39;,&#39;16:56:45&#39;,&#39;2018&#39;] (default delimeter is space).\r\n\r\nBrackets are used to &#39;select&#39; wanted argument in list.\r\n\r\nOne should call just one code line. One should not call them like I did, that was just an example, because in some cases you will get different values, rare but not impossible cases. \r\n",
               "tags": [],
               "creation_date": 1516113915,
               "last_edit_date": 1516205064,
               "is_accepted": false,
               "id": "48283832",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 0,
               "answer_id": 49048869,
               "last_activity_date": 1519905258,
               "path": "3.stack.answer",
               "body_markdown": "By default `now()` function returns output in `YYYY-MM-DD HH:MM:SS:MS` format. Use below sample script to get current date and time in Python script and print results on screen. Create file `getDateTime1.py` with below content.\r\n\r\n\r\n    import datetime\r\n    \r\n    currentDT = datetime.datetime.now()\r\n    print (str(currentDT))\r\n\r\n\r\nThe output looks like below:\r\n\r\n    2018-03-01 17:03:46.759624",
               "tags": [],
               "creation_date": 1519904095,
               "last_edit_date": 1519905258,
               "is_accepted": false,
               "id": "49048869",
               "down_vote_count": 0,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/415511/how-to-get-current-time-in-python",
         "id": "858127-2300"
      },
      {
         "up_vote_count": "439",
         "path": "2.stack",
         "body_markdown": "How can I write a stored procedure that imports data from a CSV file and populates the table?",
         "view_count": "557478",
         "answer_count": "12",
         "tags": "['postgresql', 'csv', 'postgresql-copy']",
         "creation_date": "1275891572",
         "last_edit_date": "1491764381",
         "code_snippet": "['<code>CREATE TABLE zip_codes \\n(ZIP char(5), LATITUDE double precision, LONGITUDE double precision, \\nCITY varchar, STATE char(2), COUNTY varchar, ZIP_CLASS varchar);\\n</code>', \"<code>COPY zip_codes FROM '/path/to/csv/ZIP_CODES.txt' WITH (FORMAT csv);\\n</code>\", \"<code>COPY zip_codes FROM '/path/to/csv/ZIP_CODES.txt' DELIMITER ',' CSV HEADER;</code>\", '<code>COPY</code>', '<code>\\\\copy</code>', '<code>CREATE TABLE zip_codes \\n(ZIP char(5), LATITUDE double precision, LONGITUDE double precision, \\nCITY varchar, STATE char(2), COUNTY varchar, ZIP_CLASS varchar);\\n</code>', \"<code>\\\\copy zip_codes FROM '/path/to/csv/ZIP_CODES.txt' DELIMITER ',' CSV\\n</code>\", \"<code>\\\\copy zip_codes(ZIP,CITY,STATE) FROM '/path/to/csv/ZIP_CODES.txt' DELIMITER ',' CSV\\n</code>\", '<code>COPY</code>', '<code>\\\\copy</code>', '<code>import pandas as pd\\ndf = pd.read_csv(\\'mypath.csv\\')\\ndf.columns = [c.lower() for c in df.columns] #postgres doesn\\'t like capitals or spaces\\n\\nfrom sqlalchemy import create_engine\\nengine = create_engine(\\'postgresql://username:password@localhost:5432/dbname\\')\\n\\ndf.to_sql(\"my_table_name\", engine)\\n</code>', '<code>#Set is so the raw sql output is logged\\nimport logging\\nlogging.basicConfig()\\nlogging.getLogger(\\'sqlalchemy.engine\\').setLevel(logging.INFO)\\n\\ndf.to_sql(\"my_table_name2\", \\n          engine, \\n          if_exists=\"append\",  #options are \u2018fail\u2019, \u2018replace\u2019, \u2018append\u2019, default \u2018fail\u2019\\n          index=False, #Do not output the index of the dataframe\\n          dtype={\\'col1\\': sqlalchemy.types.NUMERIC,\\n                 \\'col2\\': sqlalchemy.types.String}) #Datatypes should be [sqlalchemy types][1]\\n</code>', '<code>if_exists</code>', '<code>df.to_sql(\"fhrs\", engine, if_exists=\\'replace\\')</code>', '<code>CREATE TABLE my_table (\\n    /*paste data from Excel here for example ... */\\n    col_1 bigint,\\n    col_2 bigint,\\n    /* ... */\\n    col_n bigint \\n)\\n</code>', '<code>target_table</code>', '<code>create or replace function data.load_csv_file\\n(\\n    target_table text,\\n    csv_path text,\\n    col_count integer\\n)\\n\\nreturns void as $$\\n\\ndeclare\\n\\niter integer; -- dummy integer to iterate columns with\\ncol text; -- variable to keep the column name at each iteration\\ncol_first text; -- first column name, e.g., top left corner on a csv file or spreadsheet\\n\\nbegin\\n    set schema \\'your-schema\\';\\n\\n    create table temp_table ();\\n\\n    -- add just enough number of columns\\n    for iter in 1..col_count\\n    loop\\n        execute format(\\'alter table temp_table add column col_%s text;\\', iter);\\n    end loop;\\n\\n    -- copy the data from csv file\\n    execute format(\\'copy temp_table from %L with delimiter \\'\\',\\'\\' quote \\'\\'\"\\'\\' csv \\', csv_path);\\n\\n    iter := 1;\\n    col_first := (select col_1 from temp_table limit 1);\\n\\n    -- update the column names based on the first row which has the column names\\n    for col in execute format(\\'select unnest(string_to_array(trim(temp_table::text, \\'\\'()\\'\\'), \\'\\',\\'\\')) from temp_table where col_1 = %L\\', col_first)\\n    loop\\n        execute format(\\'alter table temp_table rename column col_%s to %s\\', iter, col);\\n        iter := iter + 1;\\n    end loop;\\n\\n    -- delete the columns row\\n    execute format(\\'delete from temp_table where %s = %L\\', col_first, col_first);\\n\\n    -- change the temp table name to the name given as parameter, if not blank\\n    if length(target_table) &gt; 0 then\\n        execute format(\\'alter table temp_table rename to %I\\', target_table);\\n    end if;\\n\\nend;\\n\\n$$ language plpgsql;\\n</code>', '<code>public</code>', \"<code>COPY table_name FROM 'path/to/data.csv' DELIMITER ',' CSV HEADER;\\n</code>\", \"<code>    copy table_name(atribute1,attribute2,attribute3...)\\n    from 'E:\\\\test.csv' delimiter ',' csv header\\n</code>\", \"<code>    drop table if exists ur_table;\\n    CREATE TABLE ur_table\\n    (\\n        id serial NOT NULL,\\n        log_id numeric, \\n        proc_code numeric,\\n        date timestamp,\\n        qty int,\\n        name varchar,\\n        price money\\n    );\\n    COPY \\n        ur_table(id, log_id, proc_code, date, qty, name, price)\\n    FROM '\\\\path\\\\xxx.csv' DELIMITER ',' CSV HEADER;\\n</code>\"]",
         "title": "How to import CSV file data into a PostgreSQL table?",
         "_childDocuments_": [
            {
               "up_vote_count": 640,
               "answer_id": 2987451,
               "last_activity_date": 1502435815,
               "path": "3.stack.answer",
               "body_markdown": "Take a look at this [short article][1].\r\n\r\n----------\r\n\r\nSolution paraphrased here:\r\n\r\n**Create your table:**\r\n\r\n    CREATE TABLE zip_codes \r\n    (ZIP char(5), LATITUDE double precision, LONGITUDE double precision, \r\n    CITY varchar, STATE char(2), COUNTY varchar, ZIP_CLASS varchar);\r\n\r\n\r\n**Copy data from your CSV file to the table:**\r\n\r\n    COPY zip_codes FROM &#39;/path/to/csv/ZIP_CODES.txt&#39; WITH (FORMAT csv);\r\n\r\n\r\n\r\n  [1]: http://web.archive.org/web/20101030205652/http://ensode.net/postgresql_csv_import.html",
               "tags": [],
               "creation_date": 1275891881,
               "last_edit_date": 1502435815,
               "is_accepted": false,
               "id": "2987451",
               "down_vote_count": 1,
               "score": 639
            },
            {
               "up_vote_count": 27,
               "answer_id": 26721945,
               "last_activity_date": 1415044160,
               "path": "3.stack.answer",
               "body_markdown": "You could also use pgAdmin, which offers a GUI to do the import. That&#39;s shown in this [SO thread][1]. The advantage of using pgAdmin is that it also works for remote databases.\r\n\r\nMuch like the previous solutions though, you would need to have your table on the database already. Each person has his own solution but what I usually do is open the CSV in Excel, copy the headers, paste special with transposition on a different worksheet, place the corresponding data type on the next column then just copy and paste that to a text editor together with the appropriate SQL table creation query like so:\r\n\r\n    CREATE TABLE my_table (\r\n        /*paste data from Excel here for example ... */\r\n        col_1 bigint,\r\n        col_2 bigint,\r\n        /* ... */\r\n        col_n bigint \r\n    )\r\n\r\n  [1]: https://stackoverflow.com/questions/19400173/how-should-i-import-data-from-csv-into-a-postgres-table-using-pgadmin-3",
               "tags": [],
               "creation_date": 1415044160,
               "last_edit_date": 1495540048,
               "is_accepted": false,
               "id": "26721945",
               "down_vote_count": 0,
               "score": 27
            },
            {
               "up_vote_count": 50,
               "answer_id": 29722393,
               "last_activity_date": 1454752903,
               "path": "3.stack.answer",
               "body_markdown": "One quick way of doing this is with the Python pandas library (version 0.15 or above works best).  This will handle creating the columns for you - although obviously the choices it makes for data types might not be what you want.  If it doesn&#39;t quite do what you want you can always use the &#39;create table&#39; code generated as a template.\r\n\r\nHere&#39;s a simple example:\r\n\r\n    import pandas as pd\r\n    df = pd.read_csv(&#39;mypath.csv&#39;)\r\n    df.columns = [c.lower() for c in df.columns] #postgres doesn&#39;t like capitals or spaces\r\n\r\n    from sqlalchemy import create_engine\r\n    engine = create_engine(&#39;postgresql://username:password@localhost:5432/dbname&#39;)\r\n\r\n    df.to_sql(&quot;my_table_name&quot;, engine)\r\n\r\nAnd here&#39;s some code that shows you how to set various options:\r\n\r\n    #Set is so the raw sql output is logged\r\n    import logging\r\n    logging.basicConfig()\r\n    logging.getLogger(&#39;sqlalchemy.engine&#39;).setLevel(logging.INFO)\r\n\r\n    df.to_sql(&quot;my_table_name2&quot;, \r\n              engine, \r\n              if_exists=&quot;append&quot;,  #options are \u2018fail\u2019, \u2018replace\u2019, \u2018append\u2019, default \u2018fail\u2019\r\n              index=False, #Do not output the index of the dataframe\r\n              dtype={&#39;col1&#39;: sqlalchemy.types.NUMERIC,\r\n                     &#39;col2&#39;: sqlalchemy.types.String}) #Datatypes should be [sqlalchemy types][1]\r\n\r\n\r\n  [1]: http://docs.sqlalchemy.org/en/latest/core/type_basics.html#generic-types",
               "tags": [],
               "creation_date": 1429388521,
               "last_edit_date": 1454752903,
               "is_accepted": false,
               "id": "29722393",
               "down_vote_count": 0,
               "score": 50
            },
            {
               "up_vote_count": 13,
               "answer_id": 30083588,
               "last_activity_date": 1498237270,
               "path": "3.stack.answer",
               "body_markdown": "Most other solutions here require that you create the table in advance/manually. This may not be practical in some cases (e.g., if you have a lot of columns in the destination table). So, the approach below may come handy. \r\n\r\nProviding the path and column count of your csv file, you can use the following function to load your table to a temp table that will be named as `target_table`:\r\n\r\nThe top row is assumed to have the column names.  \r\n\r\n\tcreate or replace function data.load_csv_file\r\n\t(\r\n\t\ttarget_table text,\r\n\t\tcsv_path text,\r\n\t\tcol_count integer\r\n\t)\r\n\r\n\treturns void as $$\r\n\r\n\tdeclare\r\n\r\n\titer integer; -- dummy integer to iterate columns with\r\n\tcol text; -- variable to keep the column name at each iteration\r\n\tcol_first text; -- first column name, e.g., top left corner on a csv file or spreadsheet\r\n\r\n\tbegin\r\n\t\tset schema &#39;your-schema&#39;;\r\n\r\n\t\tcreate table temp_table ();\r\n\r\n\t\t-- add just enough number of columns\r\n\t\tfor iter in 1..col_count\r\n\t\tloop\r\n\t\t\texecute format(&#39;alter table temp_table add column col_%s text;&#39;, iter);\r\n\t\tend loop;\r\n\r\n\t\t-- copy the data from csv file\r\n\t\texecute format(&#39;copy temp_table from %L with delimiter &#39;&#39;,&#39;&#39; quote &#39;&#39;&quot;&#39;&#39; csv &#39;, csv_path);\r\n\r\n\t\titer := 1;\r\n\t\tcol_first := (select col_1 from temp_table limit 1);\r\n\r\n\t\t-- update the column names based on the first row which has the column names\r\n\t\tfor col in execute format(&#39;select unnest(string_to_array(trim(temp_table::text, &#39;&#39;()&#39;&#39;), &#39;&#39;,&#39;&#39;)) from temp_table where col_1 = %L&#39;, col_first)\r\n\t\tloop\r\n\t\t\texecute format(&#39;alter table temp_table rename column col_%s to %s&#39;, iter, col);\r\n\t\t\titer := iter + 1;\r\n\t\tend loop;\r\n\r\n\t\t-- delete the columns row\r\n\t\texecute format(&#39;delete from temp_table where %s = %L&#39;, col_first, col_first);\r\n\r\n\t\t-- change the temp table name to the name given as parameter, if not blank\r\n\t\tif length(target_table) &gt; 0 then\r\n\t\t\texecute format(&#39;alter table temp_table rename to %I&#39;, target_table);\r\n\t\tend if;\r\n\r\n\tend;\r\n\r\n\t$$ language plpgsql;",
               "tags": [],
               "creation_date": 1430933055,
               "last_edit_date": 1498237270,
               "is_accepted": false,
               "id": "30083588",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 126,
               "answer_id": 30951435,
               "last_activity_date": 1485507772,
               "path": "3.stack.answer",
               "body_markdown": "If you don&#39;t have permission to use `COPY` (which work on the db server), you can use `\\copy` instead (which works in the db client). Using the same example as Bozhidar Batsov:\r\n\r\n**Create your table:**\r\n\r\n    CREATE TABLE zip_codes \r\n    (ZIP char(5), LATITUDE double precision, LONGITUDE double precision, \r\n    CITY varchar, STATE char(2), COUNTY varchar, ZIP_CLASS varchar);\r\n\r\n**Copy data from your CSV file to the table:**\r\n\r\n    \\copy zip_codes FROM &#39;/path/to/csv/ZIP_CODES.txt&#39; DELIMITER &#39;,&#39; CSV\r\n\r\n\r\nYou can also specify the columns to read:\r\n\r\n    \\copy zip_codes(ZIP,CITY,STATE) FROM &#39;/path/to/csv/ZIP_CODES.txt&#39; DELIMITER &#39;,&#39; CSV\r\n",
               "tags": [],
               "creation_date": 1434785193,
               "last_edit_date": 1485507772,
               "is_accepted": false,
               "id": "30951435",
               "down_vote_count": 0,
               "score": 126
            },
            {
               "up_vote_count": 17,
               "answer_id": 32626274,
               "last_activity_date": 1445434661,
               "path": "3.stack.answer",
               "body_markdown": "As Paul mentioned, import works in pgAdmin:\r\n\r\nright click on table -&gt; import\r\n\r\nselect local file, format and coding\r\n\r\nhere is a german pgAdmin GUI screenshot:\r\n\r\n[![pgAdmin import GUI][1]][1]\r\n\r\n\r\nsimilar thing you can do with DbVisualizer (I have a license, not sure about free version)\r\n\r\nright click on a table -&gt; Import Table Data...\r\n\r\n[![DbVisualizer import GUI][2]][2]\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/z9M5o.jpg\r\n  [2]: http://i.stack.imgur.com/107St.jpg",
               "tags": [],
               "creation_date": 1442480132,
               "last_edit_date": 1445434661,
               "is_accepted": false,
               "id": "32626274",
               "down_vote_count": 0,
               "score": 17
            },
            {
               "up_vote_count": 5,
               "answer_id": 33581389,
               "last_activity_date": 1487335269,
               "path": "3.stack.answer",
               "body_markdown": "IMHO, the most convenient way is to follow &quot;[Import CSV data into postgresql, the comfortable way ;-)][3]&quot;, using [csvsql][1] from [csvkit][2], which is a python package installable via pip.\r\n\r\n\r\n  [1]: http://csvkit.readthedocs.org/en/0.9.1/scripts/csvsql.html\r\n  [2]: https://csvkit.readthedocs.org\r\n  [3]: http://blog.aplikate.eu/2015/01/03/import-csv-data-into-postgresql-the-comfortable-way/",
               "tags": [],
               "creation_date": 1446890069,
               "last_edit_date": 1487335269,
               "is_accepted": false,
               "id": "33581389",
               "down_vote_count": 2,
               "score": 3
            },
            {
               "up_vote_count": 12,
               "answer_id": 40640306,
               "last_activity_date": 1479324063,
               "path": "3.stack.answer",
               "body_markdown": "    COPY table_name FROM &#39;path/to/data.csv&#39; DELIMITER &#39;,&#39; CSV HEADER;\r\n",
               "tags": [],
               "creation_date": 1479323209,
               "last_edit_date": 1479324063,
               "is_accepted": false,
               "id": "40640306",
               "down_vote_count": 0,
               "score": 12
            },
            {
               "up_vote_count": 4,
               "answer_id": 41607362,
               "last_activity_date": 1484730179,
               "path": "3.stack.answer",
               "body_markdown": "Use this SQL code\r\n\r\n        copy table_name(atribute1,attribute2,attribute3...)\r\n        from &#39;E:\\test.csv&#39; delimiter &#39;,&#39; csv header\r\n\r\nthe header keyword lets the DBMS know that the csv file have a header with attributes\r\n\r\nfor more visit &lt;http://www.postgresqltutorial.com/import-csv-file-into-posgresql-table/&gt;",
               "tags": [],
               "creation_date": 1484206025,
               "last_edit_date": 1484730179,
               "is_accepted": false,
               "id": "41607362",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 0,
               "answer_id": 44900705,
               "last_activity_date": 1499158807,
               "path": "3.stack.answer",
               "body_markdown": "Create table and have required columns that are used for creating table in csv file.  \r\n\r\n1. Open postgres and right click on target table which you want to load &amp; select import and Update the following steps in **file options** section\r\n\r\n2. Now browse your file in filename \r\n\r\n3. Select csv in format\r\n\r\n4. Encoding as ISO_8859_5\r\n\r\nNow goto **Misc. options** and check header and click on import.\r\n\r\n ",
               "tags": [],
               "creation_date": 1499156609,
               "last_edit_date": 1499158807,
               "is_accepted": false,
               "id": "44900705",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 4,
               "answer_id": 45333329,
               "last_activity_date": 1507582439,
               "path": "3.stack.answer",
               "body_markdown": "Personal experience with PostgreSQL, still waiting for a faster way.\r\n\r\n**1. Create table skeleton first if the file is stored locally:**\r\n\r\n        drop table if exists ur_table;\r\n        CREATE TABLE ur_table\r\n        (\r\n\t        id serial NOT NULL,\r\n\t        log_id numeric,\t\r\n    \t    proc_code numeric,\r\n\t        date timestamp,\r\n\t        qty\tint,\r\n    \t    name varchar,\r\n\t        price money\r\n        );\r\n        COPY \r\n\t        ur_table(id, log_id, proc_code, date, qty, name, price)\r\n        FROM &#39;\\path\\xxx.csv&#39; DELIMITER &#39;,&#39; CSV HEADER;\r\n\r\n\r\n**2. When the \\path\\xxx.csv is on the server, postgreSQL doesn&#39;t have the \r\npermission to access the server, you will have to import the .csv file through the pgAdmin built in functionality.**\r\n\r\nRight click the table name choose import.\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/qeQ6V.png\r\n\r\nIf you still have problem, please refer this tutorial. \r\nhttp://www.postgresqltutorial.com/import-csv-file-into-posgresql-table/",
               "tags": [],
               "creation_date": 1501088769,
               "last_edit_date": 1507582439,
               "is_accepted": false,
               "id": "45333329",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47937528,
               "is_accepted": false,
               "last_activity_date": 1513928023,
               "body_markdown": "1) create a table first\r\n\r\n2) Then use copy command to copy the table details:\r\n\r\n**copy** table_name (C1,C2,C3....)\r\n**from** &#39;path to your csv file&#39; delimiter &#39;,&#39; csv header;\r\n\r\n\r\nThanks",
               "id": "47937528",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1513928023,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/2987433/how-to-import-csv-file-data-into-a-postgresql-table",
         "id": "858127-2301"
      },
      {
         "up_vote_count": "284",
         "path": "2.stack",
         "body_markdown": "Is there a way to conveniently define a C-like structure in Python? I&#39;m tired of writing stuff like:\r\n\r\n    class MyStruct():\r\n        def __init__(self, field1, field2, field3):\r\n            self.field1 = field1\r\n            self.field2 = field2\r\n            self.field3 = field3\r\n\r\n",
         "view_count": "325081",
         "answer_count": "17",
         "tags": "['python', 'struct']",
         "creation_date": "1220106805",
         "last_edit_date": "1411220751",
         "code_snippet": "['<code>class MyStruct():\\n    def __init__(self, field1, field2, field3):\\n        self.field1 = field1\\n        self.field2 = field2\\n        self.field3 = field3\\n</code>', '<code>MyStruct = namedtuple(\"MyStruct\", \"field1 field2 field3\")</code>', '<code>pandas.Series(a=42).a</code>', '<code>from collections import namedtuple\\nMyStruct = namedtuple(\"MyStruct\", \"field1 field2 field3\")\\n</code>', '<code>m = MyStruct(\"foo\", \"bar\", \"baz\")\\n</code>', '<code>m = MyStruct(field1=\"foo\", field2=\"bar\", field3=\"baz\")\\n</code>', '<code>&gt;&gt;&gt; class Bunch:\\n...     def __init__(self, **kwds):\\n...         self.__dict__.update(kwds)\\n...\\n&gt;&gt;&gt; mystruct = Bunch(field1=value1, field2=value2)\\n</code>', '<code>TypeError: this constructor takes no arguments</code>', '<code>class Sample:\\n  name = \\'\\'\\n  average = 0.0\\n  values = None # list cannot be initialized here!\\n\\n\\ns1 = Sample()\\ns1.name = \"sample 1\"\\ns1.values = []\\ns1.values.append(1)\\ns1.values.append(2)\\ns1.values.append(3)\\n\\ns2 = Sample()\\ns2.name = \"sample 2\"\\ns2.values = []\\ns2.values.append(4)\\n\\nfor v in s1.values:   # prints 1,2,3 --&gt; OK.\\n  print v\\nprint \"***\"\\nfor v in s2.values:   # prints 4 --&gt; OK.\\n  print v\\n</code>', '<code>class Sample:</code>', '<code>Sample.name</code>', '<code>s1</code>', '<code>s2</code>', '<code>name</code>', '<code>name</code>', '<code>name</code>', '<code>Sample.name</code>', '<code>name</code>', '<code>name</code>', '<code>self</code>', \"<code>myStruct = {'field1': 'some val', 'field2': 'some val'}\\n</code>\", \"<code>print myStruct['field1']\\nmyStruct['field2'] = 'some other values'\\n</code>\", \"<code>self['member']</code>\", '<code>self.member</code>', '<code>@dataclass\\nclass Point:\\n    x: float\\n    y: float\\n    z: float = 0.0\\n\\np = Point(1.5, 2.5)\\nprint(p)   # produces \"Point(x=1.5, y=2.5, z=0.0)\"\\n</code>', '<code>from typing import NamedTuple\\n\\nclass MyStruct(NamedTuple):\\n    my_string: str\\n    my_int: int\\n    my_list: list\\n    my_dict: dict\\n    my_foo: Foo\\n</code>', \"<code>my_item = MyStruct(\\n    my_string='foo',\\n    my_int=0,\\n    my_list=['bar'],\\n    my_dict={'baz': 'qux'},\\n    my_foo=Foo('bar')\\n)\\n</code>\", \"<code>my_item = MyStruct('foo', 0, ['bar'], {'baz': 'qux'}, Foo('bar'))\\n</code>\", '<code>dict</code>', '<code>namedtuples</code>', '<code>structs</code>', '<code>namedtuples</code>', '<code># Abstract struct class       \\nclass Struct:\\n    def __init__ (self, *argv, **argd):\\n        if len(argd):\\n            # Update by dictionary\\n            self.__dict__.update (argd)\\n        else:\\n            # Update by position\\n            attrs = filter (lambda x: x[0:2] != \"__\", dir(self))\\n            for n in range(len(argv)):\\n                setattr(self, attrs[n], argv[n])\\n\\n# Specific class\\nclass Point3dStruct (Struct):\\n    x = 0\\n    y = 0\\n    z = 0\\n\\npt1 = Point3dStruct()\\npt1.x = 10\\n\\nprint pt1.x\\nprint \"-\"*10\\n\\npt2 = Point3dStruct(5, 6)\\n\\nprint pt2.x, pt2.y\\nprint \"-\"*10\\n\\npt3 = Point3dStruct (x=1, y=2, z=3)\\nprint pt3.x, pt3.y, pt3.z\\nprint \"-\"*10\\n</code>', '<code>Point3dStruct</code>', '<code>Point3dStruct(5, 6)</code>', '<code>print</code>', '<code>print()</code>', '<code>attrs[n]</code>', '<code>next(attrs)</code>', '<code>next</code>', '<code>&gt;&gt;&gt; from ctypes import *\\n&gt;&gt;&gt; class POINT(Structure):\\n...     _fields_ = [(\"x\", c_int),\\n...                 (\"y\", c_int)]\\n...\\n&gt;&gt;&gt; point = POINT(10, 20)\\n&gt;&gt;&gt; print point.x, point.y\\n10 20\\n&gt;&gt;&gt; point = POINT(y=5)\\n&gt;&gt;&gt; print point.x, point.y\\n0 5\\n&gt;&gt;&gt; POINT(1, 2, 3)\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in ?\\nValueError: too many initializers\\n&gt;&gt;&gt;\\n&gt;&gt;&gt; class RECT(Structure):\\n...     _fields_ = [(\"upperleft\", POINT),\\n...                 (\"lowerright\", POINT)]\\n...\\n&gt;&gt;&gt; rc = RECT(point)\\n&gt;&gt;&gt; print rc.upperleft.x, rc.upperleft.y\\n0 5\\n&gt;&gt;&gt; print rc.lowerright.x, rc.lowerright.y\\n0 0\\n&gt;&gt;&gt;\\n</code>', '<code>class Map(dict):\\n    def __init__(self, **kwargs):\\n        super(Map, self).__init__(**kwargs)\\n        self.__dict__ = self\\n</code>', \"<code>struct = Map(field1='foo', field2='bar', field3=42)\\n\\nself.assertEquals('bar', struct.field2)\\nself.assertEquals(42, struct['field3'])\\n</code>\", '<code>class cstruct:\\n    var_i = 0\\n    var_f = 0.0\\n    var_str = \"\"\\n</code>', '<code>obj = cstruct()\\nobj.var_i = 50\\nobj.var_f = 50.00\\nobj.var_str = \"fifty\"\\nprint \"cstruct: obj i=%d f=%f s=%s\" %(obj.var_i, obj.var_f, obj.var_str)\\n</code>', '<code>obj_array = [cstruct() for i in range(10)]\\nobj_array[0].var_i = 10\\nobj_array[0].var_f = 10.00\\nobj_array[0].var_str = \"ten\"\\n\\n#go ahead and fill rest of array instaces of struct\\n\\n#print all the value\\nfor i in range(10):\\n    print \"cstruct: obj_array i=%d f=%f s=%s\" %(obj_array[i].var_i, obj_array[i].var_f, obj_array[i].var_str)\\n</code>', '<code>def argumentsToAttributes(method):\\n    argumentNames = method.func_code.co_varnames[1:]\\n\\n    # Generate a dictionary of default values:\\n    defaultsDict = {}\\n    defaults = method.func_defaults if method.func_defaults else ()\\n    for i, default in enumerate(defaults, start = len(argumentNames) - len(defaults)):\\n        defaultsDict[argumentNames[i]] = default\\n\\n    def newMethod(self, *args, **kwargs):\\n        # Use the positional arguments.\\n        for name, value in zip(argumentNames, args):\\n            setattr(self, name, value)\\n\\n        # Add the key word arguments. If anything is missing, use the default.\\n        for name in argumentNames[len(args):]:\\n            setattr(self, name, kwargs.get(name, defaultsDict[name]))\\n\\n        # Run whatever else the method needs to do.\\n        method(self, *args, **kwargs)\\n\\n    return newMethod\\n</code>', '<code>a</code>', '<code>b</code>', '<code>c</code>', '<code>self</code>', \"<code>class A(object):\\n    @argumentsToAttributes\\n    def __init__(self, a, b = 'Invisible', c = 'Hello'):\\n        print(self.a)\\n        print(self.b)\\n        print(self.c)\\n\\nA('Why', c = 'Nothing')\\n</code>\", '<code>__init__</code>', '<code>class Struct:\\n    \"A structure that can have any fields defined.\"\\n    def __init__(self, **entries): self.__dict__.update(entries)\\n</code>', \"<code>&gt;&gt;&gt; options = Struct(answer=42, linelen=80, font='courier')\\n&gt;&gt;&gt; options.answer\\n42\\n</code>\", '<code>&gt;&gt;&gt; options.cat = \"dog\"\\n&gt;&gt;&gt; options.cat\\ndog\\n</code>', '<code>__init__</code>', '<code>class MyStruct(type):\\n    def __call__(cls, *args, **kwargs):\\n        names = cls.__init__.func_code.co_varnames[1:]\\n\\n        self = type.__call__(cls, *args, **kwargs)\\n\\n        for name, value in zip(names, args):\\n            setattr(self , name, value)\\n\\n        for name, value in kwargs.iteritems():\\n            setattr(self , name, value)\\n        return self \\n</code>', '<code>&gt;&gt;&gt; class MyClass(object):\\n    __metaclass__ = MyStruct\\n    def __init__(self, a, b, c):\\n        pass\\n\\n\\n&gt;&gt;&gt; my_instance = MyClass(1, 2, 3)\\n&gt;&gt;&gt; my_instance.a\\n1\\n&gt;&gt;&gt; \\n</code>', '<code>&gt;&gt;&gt; def init_all_args(fn):\\n    @wraps(fn)\\n    def wrapped_init(self, *args, **kwargs):\\n        names = fn.func_code.co_varnames[1:]\\n\\n        for name, value in zip(names, args):\\n            setattr(self, name, value)\\n\\n        for name, value in kwargs.iteritems():\\n            setattr(self, name, value)\\n\\n    return wrapped_init\\n\\n&gt;&gt;&gt; class Test(object):\\n    @init_all_args\\n    def __init__(self, a, b):\\n        pass\\n\\n\\n&gt;&gt;&gt; a = Test(1, 2)\\n&gt;&gt;&gt; a.a\\n1\\n&gt;&gt;&gt; \\n</code>', \"<code>class Employee:\\n    pass\\n\\njohn = Employee()  # Create an empty employee record\\n\\n# Fill the fields of the record\\njohn.name = 'John Doe'\\njohn.dept = 'computer lab'\\njohn.salary = 1000\\n</code>\", '<code>class Employee:\\n    def __init__ (self):\\n        self.name = None # or whatever\\n        self.dept = None\\n        self.salary = None\\n</code>', '<code>john.slarly = 1000</code>', '<code>class struct:\\n    def __init__(self, *sequential, **named):\\n        fields = dict(zip(sequential, [None]*len(sequential)), **named)\\n        self.__dict__.update(fields)\\n    def __repr__(self):\\n        return str(self.__dict__)\\n</code>', '<code># Struct with field1, field2, field3 that are initialized to None.\\nmystruct1 = struct(\"field1\", \"field2\", \"field3\") \\n# Struct with field1, field2, field3 that are initialized according to arguments.\\nmystruct2 = struct(field1=1, field2=2, field3=3)\\n</code>', \"<code>print(mystruct2)\\n# Prints: {'field3': 3, 'field1': 1, 'field2': 2}\\n</code>\", '<code>d = dict{}\\nd[field1] = field1\\nd[field2] = field2\\nd[field2] = field3\\n</code>', \"<code>class myStruct:\\n    def __init__(self, **kwds):\\n        self.x=0\\n        self.__dict__.update(kwds) # Must be last to accept assigned member variable.\\n    def __repr__(self):\\n        args = ['%s=%s' % (k, repr(v)) for (k,v) in vars(self).items()]\\n        return '%s(%s)' % ( self.__class__.__qualname__, ', '.join(args) )\\n\\na=myStruct()\\nb=myStruct(x=3,y='test')\\nc=myStruct(x='str')\\n\\n&gt;&gt;&gt; a\\nmyStruct(x=0)\\n&gt;&gt;&gt; b\\nmyStruct(x=3, y='test')\\n&gt;&gt;&gt; c\\nmyStruct(x='str')\\n</code>\"]",
         "title": "C-like structures in Python",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 56,
               "answer_id": 35990,
               "is_accepted": false,
               "last_activity_date": 1220106929,
               "body_markdown": "How about a dictionary?\r\n\r\nSomething like this:\r\n\r\n    myStruct = {&#39;field1&#39;: &#39;some val&#39;, &#39;field2&#39;: &#39;some val&#39;}\r\n\r\nThen you can use this to manipulate values:\r\n\r\n    print myStruct[&#39;field1&#39;]\r\n    myStruct[&#39;field2&#39;] = &#39;some other values&#39;\r\n\r\nAnd the values don&#39;t have to be strings.  They can be pretty much any other object.",
               "id": "35990",
               "tags": [],
               "down_vote_count": 4,
               "creation_date": 1220106929,
               "score": 52
            },
            {
               "up_vote_count": 88,
               "answer_id": 35993,
               "last_activity_date": 1283739871,
               "path": "3.stack.answer",
               "body_markdown": "You can use a tuple for a lot of things where you would use a struct in C (something like x,y coordinates or RGB colors for example).\r\n\r\nFor everything else you can use dictionary, or a utility class like [this one][1]:\r\n\r\n    &gt;&gt;&gt; class Bunch:\r\n    ...     def __init__(self, **kwds):\r\n    ...         self.__dict__.update(kwds)\r\n    ...\r\n    &gt;&gt;&gt; mystruct = Bunch(field1=value1, field2=value2)\r\n\r\nI think the &quot;definitive&quot; discussion is [here][2], in the published version of the Python Cookbook.\r\n\r\n\r\n  [1]: http://code.activestate.com/recipes/52308/\r\n  [2]: http://books.google.com/books?id=Q0s6Vgb98CQC&amp;lpg=PT212&amp;dq=Python%20Cookbook%20%22Collecting%20a%20Bunch%20of%20Named%20Items%22&amp;hl=en&amp;pg=PT213#v=onepage&amp;q&amp;f=false",
               "tags": [],
               "creation_date": 1220107118,
               "last_edit_date": 1283739871,
               "is_accepted": false,
               "id": "35993",
               "down_vote_count": 2,
               "score": 86
            },
            {
               "up_vote_count": 276,
               "answer_id": 36033,
               "last_activity_date": 1493235245,
               "path": "3.stack.answer",
               "body_markdown": "Use a [named tuple][3], which was added to the [collections module][2] in the standard library in Python 2.6. It&#39;s also possible to use Raymond Hettinger&#39;s [named tuple][1] recipe if you need to support Python 2.4.\r\n\r\nIt&#39;s nice for your basic example, but also covers a bunch of edge cases you might run into later as well. Your fragment above would be written as:\r\n\r\n    from collections import namedtuple\r\n    MyStruct = namedtuple(&quot;MyStruct&quot;, &quot;field1 field2 field3&quot;)\r\n\r\nThe newly created type can be used like this:\r\n\r\n    m = MyStruct(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;)\r\n\r\nYou can also use named arguments:\r\n\r\n    m = MyStruct(field1=&quot;foo&quot;, field2=&quot;bar&quot;, field3=&quot;baz&quot;)\r\n\r\n[1]: http://code.activestate.com/recipes/500261/\r\n[2]: http://docs.python.org/library/collections.html\r\n[3]: https://docs.python.org/2/library/collections.html#collections.namedtuple",
               "tags": [],
               "creation_date": 1220109539,
               "last_edit_date": 1493235245,
               "is_accepted": false,
               "id": "36033",
               "down_vote_count": 3,
               "score": 273
            },
            {
               "up_vote_count": 23,
               "answer_id": 36034,
               "last_activity_date": 1441678335,
               "path": "3.stack.answer",
               "body_markdown": "&gt; dF: that&#39;s pretty cool... I didn&#39;t\r\n&gt; know that I could access the fields in\r\n&gt; a class using dict.\r\n&gt; \r\n&gt; Mark: the situations that I wish I had\r\n&gt; this are precisely when I want a tuple\r\n&gt; but nothing as &quot;heavy&quot; as a\r\n&gt; dictionary.\r\n\r\nYou can access the fields of a class using a dictionary because the fields of a class, its methods and all its properties are stored internally using dicts (at least in CPython).\r\n\r\n...Which leads us to your second comment. Believing that Python dicts are &quot;heavy&quot; is an extremely non-pythonistic concept. And reading such comments kills my Python Zen. That&#39;s not good.\r\n\r\nYou see, when you declare a class you are actually creating a pretty complex wrapper around a dictionary - so, if anything, you are adding more overhead than by using a simple dictionary. An overhead which, by the way, is meaningless in any case. If you are working on performance critical applications, use C or something.",
               "tags": [],
               "creation_date": 1220109615,
               "last_edit_date": 1441678335,
               "is_accepted": false,
               "id": "36034",
               "down_vote_count": 1,
               "score": 22
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 15,
               "answer_id": 36061,
               "is_accepted": false,
               "last_activity_date": 1220111590,
               "body_markdown": "You can also pass the init parameters to the instance variables by position\r\n\r\n\t# Abstract struct class       \r\n\tclass Struct:\r\n\t    def __init__ (self, *argv, **argd):\r\n\t        if len(argd):\r\n\t            # Update by dictionary\r\n\t            self.__dict__.update (argd)\r\n\t        else:\r\n\t            # Update by position\r\n\t            attrs = filter (lambda x: x[0:2] != &quot;__&quot;, dir(self))\r\n\t            for n in range(len(argv)):\r\n\t                setattr(self, attrs[n], argv[n])\r\n\t\r\n\t# Specific class\r\n\tclass Point3dStruct (Struct):\r\n\t    x = 0\r\n\t    y = 0\r\n\t    z = 0\r\n\t\r\n\tpt1 = Point3dStruct()\r\n\tpt1.x = 10\r\n\t\r\n\tprint pt1.x\r\n\tprint &quot;-&quot;*10\r\n\t\r\n\tpt2 = Point3dStruct(5, 6)\r\n\t\r\n\tprint pt2.x, pt2.y\r\n\tprint &quot;-&quot;*10\r\n\t\r\n\tpt3 = Point3dStruct (x=1, y=2, z=3)\r\n\tprint pt3.x, pt3.y, pt3.z\r\n\tprint &quot;-&quot;*10\r\n\t\r\n",
               "id": "36061",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1220111590,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 80,
               "answer_id": 3761729,
               "is_accepted": false,
               "last_activity_date": 1285082133,
               "body_markdown": "Perhaps you are looking for Structs without constructors:\r\n\r\n    class Sample:\r\n      name = &#39;&#39;\r\n      average = 0.0\r\n      values = None # list cannot be initialized here!\r\n    \r\n    \r\n    s1 = Sample()\r\n    s1.name = &quot;sample 1&quot;\r\n    s1.values = []\r\n    s1.values.append(1)\r\n    s1.values.append(2)\r\n    s1.values.append(3)\r\n    \r\n    s2 = Sample()\r\n    s2.name = &quot;sample 2&quot;\r\n    s2.values = []\r\n    s2.values.append(4)\r\n    \r\n    for v in s1.values:   # prints 1,2,3 --&gt; OK.\r\n      print v\r\n    print &quot;***&quot;\r\n    for v in s2.values:   # prints 4 --&gt; OK.\r\n      print v\r\n\r\n",
               "id": "3761729",
               "tags": [],
               "down_vote_count": 6,
               "creation_date": 1285082133,
               "score": 74
            },
            {
               "up_vote_count": 10,
               "answer_id": 18792190,
               "last_activity_date": 1379113593,
               "path": "3.stack.answer",
               "body_markdown": "Whenever I need an &quot;instant data object that also behaves like a dictionary&quot; (I _don&#39;t_ think of C structs!), I think of this cute hack:\r\n\r\n    class Map(dict):\r\n        def __init__(self, **kwargs):\r\n            super(Map, self).__init__(**kwargs)\r\n            self.__dict__ = self\r\n\r\nNow you can just say:\r\n\r\n    struct = Map(field1=&#39;foo&#39;, field2=&#39;bar&#39;, field3=42)\r\n    \r\n    self.assertEquals(&#39;bar&#39;, struct.field2)\r\n    self.assertEquals(42, struct[&#39;field3&#39;])\r\n\r\nPerfectly handy for those times when you need a &quot;data bag that&#39;s NOT a class&quot;, and for when namedtuples are incomprehensible...",
               "tags": [],
               "creation_date": 1379094045,
               "last_edit_date": 1379113593,
               "is_accepted": false,
               "id": "18792190",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 10,
               "answer_id": 26826089,
               "last_activity_date": 1483542299,
               "path": "3.stack.answer",
               "body_markdown": "You access C-Style struct in python in following way.\r\n\r\n    class cstruct:\r\n        var_i = 0\r\n        var_f = 0.0\r\n        var_str = &quot;&quot;\r\n             \r\n#if you just want use object of cstruct\r\n\r\n    obj = cstruct()\r\n    obj.var_i = 50\r\n    obj.var_f = 50.00\r\n    obj.var_str = &quot;fifty&quot;\r\n    print &quot;cstruct: obj i=%d f=%f s=%s&quot; %(obj.var_i, obj.var_f, obj.var_str)\r\n\r\n#if you want to create an array of objects of cstruct\r\n\r\n    obj_array = [cstruct() for i in range(10)]\r\n    obj_array[0].var_i = 10\r\n    obj_array[0].var_f = 10.00\r\n    obj_array[0].var_str = &quot;ten&quot;\r\n    \r\n    #go ahead and fill rest of array instaces of struct\r\n    \r\n    #print all the value\r\n    for i in range(10):\r\n        print &quot;cstruct: obj_array i=%d f=%f s=%s&quot; %(obj_array[i].var_i, obj_array[i].var_f, obj_array[i].var_str)\r\n\r\n\r\n\r\nNote:\r\ninstead of &#39;cstruct&#39; name, please use your struct name\r\ninstead of var_i, var_f, var_str, please define your structure&#39;s member variable. \r\n\r\n",
               "tags": [],
               "creation_date": 1415518667,
               "last_edit_date": 1483542299,
               "is_accepted": false,
               "id": "26826089",
               "down_vote_count": 3,
               "score": 7
            },
            {
               "up_vote_count": 3,
               "answer_id": 29212925,
               "last_activity_date": 1427365767,
               "path": "3.stack.answer",
               "body_markdown": "This might be a bit late but I made a solution using Python Meta-Classes (decorator version below too).\r\n\r\nWhen `__init__` is called during run time, it grabs each of the arguments and their value and assigns them as instance variables to your class. This way you can make a struct-like class without having to assign every value manually.\r\n\r\nMy example has no error checking so it is easier to follow.\r\n\r\n    class MyStruct(type):\r\n        def __call__(cls, *args, **kwargs):\r\n            names = cls.__init__.func_code.co_varnames[1:]\r\n            \r\n            self = type.__call__(cls, *args, **kwargs)\r\n\r\n            for name, value in zip(names, args):\r\n                setattr(self , name, value)\r\n            \r\n            for name, value in kwargs.iteritems():\r\n                setattr(self , name, value)\r\n            return self \r\n\r\n\r\nHere it is in action.\r\n\r\n    &gt;&gt;&gt; class MyClass(object):\r\n    \t__metaclass__ = MyStruct\r\n    \tdef __init__(self, a, b, c):\r\n    \t\tpass\r\n    \r\n    \t\r\n    &gt;&gt;&gt; my_instance = MyClass(1, 2, 3)\r\n    &gt;&gt;&gt; my_instance.a\r\n    1\r\n    &gt;&gt;&gt; \r\n\r\nI [posted it on reddit][1] and [/u/matchu][2] posted a decorator version which is cleaner. I&#39;d encourage you to use it unless you want to expand the metaclass version.\r\n\r\n    &gt;&gt;&gt; def init_all_args(fn):\r\n        @wraps(fn)\r\n        def wrapped_init(self, *args, **kwargs):\r\n            names = fn.func_code.co_varnames[1:]\r\n    \r\n            for name, value in zip(names, args):\r\n                setattr(self, name, value)\r\n    \r\n            for name, value in kwargs.iteritems():\r\n                setattr(self, name, value)\r\n    \r\n        return wrapped_init\r\n    \r\n    &gt;&gt;&gt; class Test(object):\r\n    \t@init_all_args\r\n    \tdef __init__(self, a, b):\r\n    \t\tpass\r\n    \r\n    \t\r\n    &gt;&gt;&gt; a = Test(1, 2)\r\n    &gt;&gt;&gt; a.a\r\n    1\r\n    &gt;&gt;&gt; \r\n\r\n\r\n  [1]: http://www.reddit.com/r/Python/comments/300psq/i_made_a_cstyle_struct_using_a_metaclass_to_save/\r\n  [2]: http://www.reddit.com/user/matchu",
               "tags": [],
               "creation_date": 1427121151,
               "last_edit_date": 1427365767,
               "is_accepted": false,
               "id": "29212925",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 31062667,
               "is_accepted": false,
               "last_activity_date": 1435276217,
               "body_markdown": "You can subclass the C structure that is available in the standard library. The [ctypes](https://docs.python.org/2/library/ctypes.html) module provides a [Structure class](https://docs.python.org/2/library/ctypes.html#structures-and-unions). The example from the docs:\r\n\r\n    &gt;&gt;&gt; from ctypes import *\r\n    &gt;&gt;&gt; class POINT(Structure):\r\n    ...     _fields_ = [(&quot;x&quot;, c_int),\r\n    ...                 (&quot;y&quot;, c_int)]\r\n    ...\r\n    &gt;&gt;&gt; point = POINT(10, 20)\r\n    &gt;&gt;&gt; print point.x, point.y\r\n    10 20\r\n    &gt;&gt;&gt; point = POINT(y=5)\r\n    &gt;&gt;&gt; print point.x, point.y\r\n    0 5\r\n    &gt;&gt;&gt; POINT(1, 2, 3)\r\n    Traceback (most recent call last):\r\n      File &quot;&lt;stdin&gt;&quot;, line 1, in ?\r\n    ValueError: too many initializers\r\n    &gt;&gt;&gt;\r\n    &gt;&gt;&gt; class RECT(Structure):\r\n    ...     _fields_ = [(&quot;upperleft&quot;, POINT),\r\n    ...                 (&quot;lowerright&quot;, POINT)]\r\n    ...\r\n    &gt;&gt;&gt; rc = RECT(point)\r\n    &gt;&gt;&gt; print rc.upperleft.x, rc.upperleft.y\r\n    0 5\r\n    &gt;&gt;&gt; print rc.lowerright.x, rc.lowerright.y\r\n    0 0\r\n    &gt;&gt;&gt;",
               "id": "31062667",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1435276217,
               "score": 12
            },
            {
               "up_vote_count": 4,
               "answer_id": 32448434,
               "last_activity_date": 1441680401,
               "path": "3.stack.answer",
               "body_markdown": "I wrote a decorator which you can use on any method to make it so that all of the arguments passed in, or any defaults, are assigned to the instance.\r\n\r\n    def argumentsToAttributes(method):\r\n        argumentNames = method.func_code.co_varnames[1:]\r\n\r\n        # Generate a dictionary of default values:\r\n        defaultsDict = {}\r\n        defaults = method.func_defaults if method.func_defaults else ()\r\n        for i, default in enumerate(defaults, start = len(argumentNames) - len(defaults)):\r\n            defaultsDict[argumentNames[i]] = default\r\n\r\n        def newMethod(self, *args, **kwargs):\r\n            # Use the positional arguments.\r\n            for name, value in zip(argumentNames, args):\r\n                setattr(self, name, value)\r\n\r\n            # Add the key word arguments. If anything is missing, use the default.\r\n            for name in argumentNames[len(args):]:\r\n                setattr(self, name, kwargs.get(name, defaultsDict[name]))\r\n\r\n            # Run whatever else the method needs to do.\r\n            method(self, *args, **kwargs)\r\n\r\n        return newMethod\r\n\r\nA quick demonstration. Note that I use a positional argument `a`, use the default value for `b`, and a named argument `c`. I then print all 3 referencing `self`, to show that they&#39;ve been properly assigned before the method is entered.\r\n\r\n    class A(object):\r\n        @argumentsToAttributes\r\n        def __init__(self, a, b = &#39;Invisible&#39;, c = &#39;Hello&#39;):\r\n            print(self.a)\r\n            print(self.b)\r\n            print(self.c)\r\n\r\n    A(&#39;Why&#39;, c = &#39;Nothing&#39;)\r\n\r\nNote that my decorator should work with any method, not just `__init__`.",
               "tags": [],
               "creation_date": 1441679898,
               "last_edit_date": 1441680401,
               "is_accepted": false,
               "id": "32448434",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 40488966,
               "is_accepted": false,
               "last_activity_date": 1478613965,
               "body_markdown": "I don&#39;t see this answer here, so I figure I&#39;ll add it since I&#39;m leaning Python right now and just discovered it. The [Python tutorial](https://docs.python.org/2/tutorial/classes.html) (Python 2 in this case) gives the following simple and effective example:\r\n\r\n    class Employee:\r\n        pass\r\n    \r\n    john = Employee()  # Create an empty employee record\r\n    \r\n    # Fill the fields of the record\r\n    john.name = &#39;John Doe&#39;\r\n    john.dept = &#39;computer lab&#39;\r\n    john.salary = 1000\r\n\r\nThat is, an empty class object is created, then instantiated, and the fields are added dynamically.\r\n\r\nThe up-side to this is its really simple. The downside is it isn&#39;t particularly self-documenting (the intended members aren&#39;t listed anywhere in the class &quot;definition&quot;), and unset fields can cause problems when accessed. Those two problems can be solved by:\r\n\r\n    class Employee:\r\n        def __init__ (self):\r\n            self.name = None # or whatever\r\n            self.dept = None\r\n            self.salary = None\r\n\r\nNow at a glance you can at least see what fields the program will be expecting.\r\n\r\nBoth are prone to typos, `john.slarly = 1000` will succeed. Still, it works.",
               "id": "40488966",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1478613965,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 44989925,
               "is_accepted": false,
               "last_activity_date": 1499541476,
               "body_markdown": "I think Python structure dictionary is suitable for this requirement.\r\n\r\n&lt;!-- language: python --&gt;\r\n\r\n    d = dict{}\r\n    d[field1] = field1\r\n    d[field2] = field2\r\n    d[field2] = field3\r\n\r\n",
               "id": "44989925",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1499541476,
               "score": 0
            },
            {
               "up_vote_count": 18,
               "answer_id": 45426493,
               "last_activity_date": 1519660986,
               "path": "3.stack.answer",
               "body_markdown": "### *Update*: Data Classes\r\n\r\nWith **Python 3.7** we get very close. They introduced *Data Classes*!\r\n\r\nThe following example is similar to the *NamedTuple* example below, but the resulting object is mutable and it allows for default values.\r\n\r\n\r\n    @dataclass\r\n    class Point:\r\n        x: float\r\n        y: float\r\n        z: float = 0.0\r\n\r\n    p = Point(1.5, 2.5)\r\n    print(p)   # produces &quot;Point(x=1.5, y=2.5, z=0.0)&quot;\r\n\r\nI&#39;ve been waiting desperately for this! If you ask me, *Data Classes* and the new *NamedTuple* declaration, combined with the *typing* module are a godsend!\r\n\r\n### Improved NamedTuple declartaion\r\n\r\nSince **Python 3.6** it became quite simple and beautiful (IMHO), as long as you can live with immutability.\r\n\r\nThey introduced a new way of declaring NamedTuples, which allows for [type annotations][1] as well:\r\n\r\n\tfrom typing import NamedTuple\r\n\r\n\tclass MyStruct(NamedTuple):\r\n        my_string: str\r\n        my_int: int\r\n        my_list: list\r\n        my_dict: dict\r\n        my_foo: Foo\r\n\r\n\r\nUse it like this:\r\n\r\n    my_item = MyStruct(\r\n        my_string=&#39;foo&#39;,\r\n        my_int=0,\r\n        my_list=[&#39;bar&#39;],\r\n        my_dict={&#39;baz&#39;: &#39;qux&#39;},\r\n        my_foo=Foo(&#39;bar&#39;)\r\n    )\r\n\r\nOr like this if you really want to:\r\n\r\n    my_item = MyStruct(&#39;foo&#39;, 0, [&#39;bar&#39;], {&#39;baz&#39;: &#39;qux&#39;}, Foo(&#39;bar&#39;))\r\n\r\n\r\n\r\n\r\n  [1]: https://docs.python.org/3/library/typing.html",
               "tags": [],
               "creation_date": 1501541672,
               "last_edit_date": 1519660986,
               "is_accepted": false,
               "id": "45426493",
               "down_vote_count": 1,
               "score": 17
            },
            {
               "up_vote_count": 4,
               "answer_id": 45517161,
               "last_activity_date": 1501893898,
               "path": "3.stack.answer",
               "body_markdown": "Some the answers here are massively elaborate. The simplest option I&#39;ve found is (from: http://norvig.com/python-iaq.html):\r\n\r\n    class Struct:\r\n        &quot;A structure that can have any fields defined.&quot;\r\n        def __init__(self, **entries): self.__dict__.update(entries)\r\n\r\n\r\nInitialising:\r\n\r\n    &gt;&gt;&gt; options = Struct(answer=42, linelen=80, font=&#39;courier&#39;)\r\n    &gt;&gt;&gt; options.answer\r\n    42\r\n\r\nadding more:\r\n\r\n    &gt;&gt;&gt; options.cat = &quot;dog&quot;\r\n    &gt;&gt;&gt; options.cat\r\n    dog\r\n\r\n**edit:** Sorry didn&#39;t see this example already further down.",
               "tags": [],
               "creation_date": 1501893590,
               "last_edit_date": 1501893898,
               "is_accepted": false,
               "id": "45517161",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47016739,
               "is_accepted": false,
               "last_activity_date": 1509370839,
               "body_markdown": "&lt;https://stackoverflow.com/a/32448434/159695&gt; does not work in Python3.\r\n\r\n&lt;https://stackoverflow.com/a/35993/159695&gt; works in Python3.\r\n\r\nAnd I extends it to add default values.\r\n\r\n    class myStruct:\r\n        def __init__(self, **kwds):\r\n            self.x=0\r\n            self.__dict__.update(kwds) # Must be last to accept assigned member variable.\r\n        def __repr__(self):\r\n            args = [&#39;%s=%s&#39; % (k, repr(v)) for (k,v) in vars(self).items()]\r\n            return &#39;%s(%s)&#39; % ( self.__class__.__qualname__, &#39;, &#39;.join(args) )\r\n\r\n    a=myStruct()\r\n    b=myStruct(x=3,y=&#39;test&#39;)\r\n    c=myStruct(x=&#39;str&#39;)\r\n\r\n    &gt;&gt;&gt; a\r\n    myStruct(x=0)\r\n    &gt;&gt;&gt; b\r\n    myStruct(x=3, y=&#39;test&#39;)\r\n    &gt;&gt;&gt; c\r\n    myStruct(x=&#39;str&#39;)\r\n",
               "id": "47016739",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1509370839,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47140549,
               "is_accepted": false,
               "last_activity_date": 1509982724,
               "body_markdown": "Personally, I like this variant too. It extends [@dF&#39;s answer][1].\r\n\r\n    class struct:\r\n        def __init__(self, *sequential, **named):\r\n            fields = dict(zip(sequential, [None]*len(sequential)), **named)\r\n            self.__dict__.update(fields)\r\n        def __repr__(self):\r\n            return str(self.__dict__)\r\n\r\nIt supports two modes of initialization (that can be blended):\r\n\r\n    # Struct with field1, field2, field3 that are initialized to None.\r\n    mystruct1 = struct(&quot;field1&quot;, &quot;field2&quot;, &quot;field3&quot;) \r\n    # Struct with field1, field2, field3 that are initialized according to arguments.\r\n    mystruct2 = struct(field1=1, field2=2, field3=3)\r\n\r\nAlso, it prints nicer: \r\n\r\n    print(mystruct2)\r\n    # Prints: {&#39;field3&#39;: 3, &#39;field1&#39;: 1, &#39;field2&#39;: 2}\r\n     \r\n  [1]: https://stackoverflow.com/a/35993/3388962\r\n",
               "id": "47140549",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1509982724,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/35988/c-like-structures-in-python",
         "id": "858127-2302"
      },
      {
         "up_vote_count": "537",
         "path": "2.stack",
         "body_markdown": "Pretty much I need to write a program to check if a list has any duplicates and if it does it removes them and returns a new list with the items that werent duplicated/removed. This is what I have but to be honest I do not know what to do.\r\n\r\n    def remove_duplicates():\r\n        t = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]\r\n        t2 = [&#39;a&#39;, &#39;c&#39;, &#39;d&#39;]\r\n        for t in t2:\r\n            t.append(t.remove())\r\n        return t\r\n\r\n  \r\n\r\n   ",
         "view_count": "783661",
         "answer_count": "34",
         "tags": "['python', 'algorithm', 'list', 'duplicates', 'intersection']",
         "creation_date": "1320108324",
         "last_edit_date": "1487658213",
         "code_snippet": "[\"<code>def remove_duplicates():\\n    t = ['a', 'b', 'c', 'd']\\n    t2 = ['a', 'c', 'd']\\n    for t in t2:\\n        t.append(t.remove())\\n    return t\\n</code>\", '<code>set</code>', '<code>set()</code>', '<code>list()</code>', '<code>&gt;&gt;&gt; t = [1, 2, 3, 1, 2, 5, 6, 7, 8]\\n&gt;&gt;&gt; t\\n[1, 2, 3, 1, 2, 5, 6, 7, 8]\\n&gt;&gt;&gt; list(set(t))\\n[1, 2, 3, 5, 6, 7, 8]\\n&gt;&gt;&gt; s = [1, 2, 3]\\n&gt;&gt;&gt; list(set(t) - set(s))\\n[8, 5, 6, 7]\\n</code>', '<code>OrderedDict</code>', '<code>&gt;&gt;&gt; from collections import OrderedDict\\n&gt;&gt;&gt; list(OrderedDict.fromkeys(t))\\n[1, 2, 3, 5, 6, 7, 8]\\n</code>', '<code>set</code>', '<code>OrderedDict</code>', \"<code>&gt;&gt;&gt; from collections import OrderedDict\\n&gt;&gt;&gt; list(OrderedDict.fromkeys('abracadabra'))\\n['a', 'b', 'r', 'c', 'd']\\n</code>\", \"<code>&gt;&gt;&gt; list(dict.fromkeys('abracadabra'))\\n['a', 'b', 'r', 'c', 'd']\\n</code>\", \"<code>&gt;&gt;&gt; list(dict.fromkeys('abracadabra'))\\n['a', 'b', 'r', 'c', 'd']\\n</code>\", '<code>list(set(source_list))</code>', '<code>set</code>', '<code>from collections import OrderedDict\\nOrderedDict((x, True) for x in source_list).keys()\\n</code>', '<code>OrderedDict</code>', '<code>True</code>', '<code>set</code>', '<code>dict</code>', '<code>&gt;&gt;&gt; t = [1, 2, 3, 1, 2, 5, 6, 7, 8]\\n&gt;&gt;&gt; t\\n[1, 2, 3, 1, 2, 5, 6, 7, 8]\\n&gt;&gt;&gt; s = []\\n&gt;&gt;&gt; for i in t:\\n       if i not in s:\\n          s.append(i)\\n&gt;&gt;&gt; s\\n[1, 2, 3, 5, 6, 7, 8]\\n</code>', '<code>frozenset</code>', '<code>def remove_duplicates(l):\\n    return list(set(l))\\n</code>', '<code>set</code>', '<code>L</code>', '<code>newlist=[ii for n,ii in enumerate(L) if ii not in L[:n]]</code>', '<code>if L=[1, 2, 2, 3, 4, 2, 4, 3, 5]</code>', '<code>newlist</code>', '<code>[1,2,3,4,5]</code>', '<code>set</code>', '<code>OrderedDict</code>', \"<code>&gt;&gt;&gt; seq = [1,2,3,'a', 'a', 1,2]\\n&gt;&gt; dict.fromkeys(seq).keys()\\n['a', 1, 2, 3]\\n</code>\", '<code>keys()</code>', '<code>def ordered_set(in_list):\\n    out_list = []\\n    added = set()\\n    for val in in_list:\\n        if not val in added:\\n            out_list.append(val)\\n            added.add(val)\\n    return out_list\\n</code>', '<code>from random import randint\\nx = [randint(0,100) for _ in xrange(100)]\\n\\nIn [131]: len(set(x))\\nOut[131]: 62\\n</code>', '<code>In [129]: %timeit list(OrderedDict.fromkeys(x))\\n10000 loops, best of 3: 86.4 us per loop\\n\\nIn [130]: %timeit ordered_set(x)\\n100000 loops, best of 3: 15.1 us per loop\\n</code>', '<code>def ordered_set(inlist):\\n    out_list = []\\n    for val in inlist:\\n        if not val in out_list:\\n            out_list.append(val)\\n    return out_list\\n</code>', '<code>In [136]: %timeit ordered_set(x)\\n10000 loops, best of 3: 52.6 us per loop\\n</code>', '<code>TypeError: unhashable type:\\n</code>', '<code>def make_unique(original_list):\\n    unique_list = []\\n    [unique_list.append(obj) for obj in original_list if obj not in unique_list]\\n    return unique_list\\n</code>', '<code>def make_unique(original_list):\\n    unique_list = []\\n    map(lambda x: unique_list.append(x) if (x not in unique_list) else False, original_list)\\n    return unique_list\\n</code>', '<code>map</code>', '<code>lambda x: unique_list.append(x)</code>', '<code>unique_list.append</code>', '<code>myList = [1, 2, 3, 1, 2, 5, 6, 7, 8]\\ncleanlist = []\\n[cleanlist.append(x) for x in myList if x not in cleanlist]\\n</code>', '<code>&gt;&gt;&gt; cleanlist \\n[1, 2, 3, 5, 6, 7, 8]\\n</code>', '<code>in</code>', '<code>cleanlist</code>', '<code>n</code>', \"<code>import sets\\nt = sets.Set(['a', 'b', 'c', 'd'])\\nt1 = sets.Set(['a', 'b', 'c'])\\n\\nprint t | t1\\nprint t - t1\\n</code>\", '<code>&gt;&gt;&gt; t = [1, 2, 3, 3, 2, 4, 5, 6]\\n&gt;&gt;&gt; s = [x for i, x in enumerate(t) if i == t.index(x)]\\n&gt;&gt;&gt; s\\n[1, 2, 3, 4, 5, 6]\\n</code>', '<code>index</code>', '<code>.tolist()</code>', \"<code>t=['a','a','b','b','b','c','c','c']\\nt2= ['c','c','b','b','b','a','a','a']\\n</code>\", '<code>unique()</code>', \"<code>import pandas as pd\\npd.unique(t).tolist()\\n&gt;&gt;&gt;['a','b','c']\\npd.unique(t2).tolist()\\n&gt;&gt;&gt;['c','b','a']\\n</code>\", '<code>unique()</code>', \"<code>import numpy as np\\nnp.unique(t).tolist()\\n&gt;&gt;&gt;['a','b','c']\\nnp.unique(t2).tolist()\\n&gt;&gt;&gt;['a','b','c']\\n</code>\", '<code>t2</code>', \"<code>_, idx = np.unique(t2, return_index=True)\\nt2[np.sort(idx)].tolist()\\n&gt;&gt;&gt;['c','b','a']\\n</code>\", \"<code>def remove_duplicates(list):\\n    ''' Removes duplicate items from a list '''\\n    singles_list = []\\n    for element in list:\\n        if element not in singles_list:\\n            singles_list.append(element)\\n    return singles_list\\n</code>\", '<code>list</code>', '<code>list</code>', '<code>def remove_duplicates(x):\\n    a = []\\n    for i in x:\\n        if i not in a:\\n            a.append(i)\\n    return a\\n\\nprint remove_duplicates([1,2,2,3,3,4])\\n</code>', '<code>list(set(..))</code>', '<code>list(set(..))</code>', '<code>OrderedDicts</code>', '<code>set</code>', '<code>list</code>', '<code>key</code>', '<code># from functools import reduce &lt;-- add this import on Python 3\\n\\ndef uniq(iterable, key=lambda x: x):\\n    \"\"\"\\n    Remove duplicates from an iterable. Preserves order. \\n    :type iterable: Iterable[Ord =&gt; A]\\n    :param iterable: an iterable of objects of any orderable type\\n    :type key: Callable[A] -&gt; (Ord =&gt; B)\\n    :param key: optional argument; by default an item (A) is discarded \\n    if another item (B), such that A == B, has already been encountered and taken. \\n    If you provide a key, this condition changes to key(A) == key(B); the callable \\n    must return orderable objects.\\n    \"\"\"\\n    # Enumerate the list to restore order lately; reduce the sorted list; restore order\\n    def append_unique(acc, item):\\n        return acc if key(acc[-1][1]) == key(item[1]) else acc.append(item) or acc \\n    srt_enum = sorted(enumerate(iterable), key=lambda item: key(item[1]))\\n    return [item[1] for item in sorted(reduce(append_unique, srt_enum, [srt_enum[0]]))] \\n</code>', '<code>tuple()</code>', '<code>l = [5, 6, 6, 1, 1, 2, 2, 3, 4]\\n</code>', '<code>&gt;&gt;&gt; reduce(lambda r, v: v in r and r or r + [v], l, [])\\n[5, 6, 1, 2, 3, 4]\\n</code>', '<code>&gt;&gt;&gt; reduce(lambda r, v: v in r[1] and r or (r[0].append(v) or r[1].add(v)) or r, l, ([], set()))[0]\\n[5, 6, 1, 2, 3, 4]\\n</code>', '<code>default = (list(), set())\\n# user list to keep order\\n# use set to make lookup faster\\n\\ndef reducer(result, item):\\n    if item not in result[1]:\\n        result[0].append(item)\\n        result[1].add(item)\\n    return result\\n\\nreduce(reducer, l, default)[0]\\n</code>', '<code>def uniqify(iterable):\\n    seen = set()\\n    for item in iterable:\\n        if item not in seen:\\n            seen.add(item)\\n            yield item\\n</code>', \"<code>for unique_item in uniqify([1, 2, 3, 4, 3, 2, 4, 5, 6, 7, 6, 8, 8]):\\n    print(unique_item, end=' ')\\n\\nprint()\\n</code>\", '<code>1 2 3 4 5 6 7 8\\n</code>', '<code>list</code>', '<code>unique_list = list(uniqify([1, 2, 3, 4, 3, 2, 4, 5, 6, 7, 6, 8, 8]))\\n\\nprint(unique_list)\\n</code>', '<code>[1, 2, 3, 4, 5, 6, 7, 8]\\n</code>', '<code>seen = set(iterable); for item in seen: yield item</code>', '<code>visited.add(item)</code>', '<code>None</code>', '<code>False</code>', '<code>or</code>', '<code>def deduplicate(sequence):\\n    visited = set()\\n    adder = visited.add  # get rid of qualification overhead\\n    out = [adder(item) or item for item in sequence if item not in visited]\\n    return out\\n</code>', \"<code>In [2]: some_list = ['a','a','v','v','v','c','c','d']\\nIn [3]: list(set(some_list))\\nOut[3]: ['a', 'c', 'd', 'v']\\n</code>\", '<code>data=[1, 2, 3, 1, 2, 5, 6, 7, 8]\\nuni_data=[]\\nfor dat in data:\\n    if dat not in uni_data:\\n        uni_data.append(dat)\\n\\nprint(uni_data) \\n</code>', '<code>&gt;&gt;&gt; import collections\\n&gt;&gt;&gt; c = collections.Counter([1, 2, 3, 4, 5, 6, 1, 1, 1, 1])\\n&gt;&gt;&gt; c.keys()\\ndict_keys([1, 2, 3, 4, 5, 6])\\n</code>', '<code>dict</code>', '<code>def GetListWithoutRepetitions(loInput):\\n    # return list, consisting of elements of list/tuple loInput, without repetitions.\\n    # Example: GetListWithoutRepetitions([None,None,1,1,2,2,3,3,3])\\n    # Returns: [None, 1, 2, 3]\\n\\n    if loInput==[]:\\n        return []\\n\\n    loOutput = []\\n\\n    if loInput[0] is None:\\n        oGroupElement=1\\n    else: # loInput[0]&lt;&gt;None\\n        oGroupElement=None\\n\\n    for oElement in loInput:\\n        if oElement&lt;&gt;oGroupElement:\\n            loOutput.append(oElement)\\n            oGroupElement = oElement\\n    return loOutput\\n</code>', '<code>&gt;&gt;&gt; t = [1, 2, 3, 1, 2, 5, 6, 7, 8]\\n&gt;&gt;&gt; for i in t:\\n...     if i in t[t.index(i)+1:]:\\n...         t.remove(i)\\n... \\n&gt;&gt;&gt; t\\n[3, 1, 2, 5, 6, 7, 8]\\n</code>', '<code>enumerate()</code>', '<code>for i, value in enumerate(t): if value in t[i + 1:]: t.remove(value)</code>', '<code>[1,1,1]</code>', '<code>list1 = [1,2,1]\\nlist1 = list(set(list1))\\nprint list1\\n</code>', '<code>a = [0,1,2,3,4,3,3,4]\\na = list(set(a))\\nprint a\\n</code>', '<code>import numpy as np\\na = [0,1,2,3,4,3,3,4]\\na = np.unique(a).tolist()\\nprint a\\n</code>', '<code>a = [1,2,3,4,5,9,11,15]\\nb = [4,5,6,7,8]\\nc=a+b\\nprint c\\nprint list(set(c)) #one line for getting unique elements of c\\n</code>', '<code>[1, 2, 3, 4, 5, 9, 11, 15, 4, 5, 6, 7, 8]  #simple list addition with duplicates\\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 15] #duplicates removed!!\\n</code>', '<code>In [1]: a = [\"apples\", \"bananas\", \"cucumbers\"]\\n\\nIn [2]: b = [\"pears\", \"apples\", \"watermelons\"]\\n\\nIn [3]: set(a).symmetric_difference(b).union(set(a).intersection(b))\\nOut[3]: {\\'apples\\', \\'bananas\\', \\'cucumbers\\', \\'pears\\', \\'watermelons\\'}\\n</code>', '<code>def remove_duplicates(A):\\n   [A.pop(count) for count,elem in enumerate(A) if A.count(elem)!=1]\\n   return A\\n</code>', \"<code>def remove_dup(arr):\\n    size = len(arr)\\n    j = 0    # To store index of next unique element\\n    for i in range(0, size-1):\\n        # If current element is not equal\\n        # to next element then store that\\n        # current element\\n        if(arr[i] != arr[i+1]):\\n            arr[j] = arr[i]\\n            j+=1\\n\\n    arr[j] = arr[size-1] # Store the last element as whether it is unique or repeated, it hasn't stored previously\\n\\n    return arr[0:j+1]\\n\\nif __name__ == '__main__':\\n    arr = [10, 10, 1, 1, 1, 3, 3, 4, 5, 6, 7, 8, 8, 9]\\n    print(remove_dup(sorted(arr)))\\n</code>\"]",
         "title": "Removing duplicates in lists",
         "_childDocuments_": [
            {
               "up_vote_count": 941,
               "answer_id": 7961390,
               "last_activity_date": 1517430325,
               "path": "3.stack.answer",
               "body_markdown": "The common approach to get a unique collection of items is to use a [`set`](http://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset). Sets are *unordered* collections of *distinct* objects. To create a set from any iterable, you can simply pass it to the built-in [`set()`](http://docs.python.org/3/library/functions.html#func-set) function. If you later need a real list again, you can similarly pass the set to the [`list()`](http://docs.python.org/3/library/functions.html#func-list) function.\r\n\r\nThe following example should cover whatever you are trying to do:\r\n\r\n    &gt;&gt;&gt; t = [1, 2, 3, 1, 2, 5, 6, 7, 8]\r\n    &gt;&gt;&gt; t\r\n    [1, 2, 3, 1, 2, 5, 6, 7, 8]\r\n    &gt;&gt;&gt; list(set(t))\r\n    [1, 2, 3, 5, 6, 7, 8]\r\n    &gt;&gt;&gt; s = [1, 2, 3]\r\n    &gt;&gt;&gt; list(set(t) - set(s))\r\n    [8, 5, 6, 7]\r\n\r\nAs you can see from the example result, the original order is not maintained. As mentioned above, sets themselves are unordered collections, so the order is lost. When converting a set back to a list, an arbitrary order is created.\r\n\r\nIf order is important to you, then you will have to use a different mechanism. A very common solution for this is to rely on [`OrderedDict`](https://docs.python.org/3/library/collections.html#collections.OrderedDict) to keep the order of keys during insertion:\r\n\r\n    &gt;&gt;&gt; from collections import OrderedDict\r\n    &gt;&gt;&gt; list(OrderedDict.fromkeys(t))\r\n    [1, 2, 3, 5, 6, 7, 8]\r\n\r\nNote that this has the overhead of creating a dictionary first, and then creating a list from it. So if you don\u2019t actually need to preserve the order, you\u2019re better off using a set. Check out [this question](https://stackoverflow.com/q/480214/216074) for more details and alternative ways to preserve the order when removing duplicates.\r\n\r\n---\r\n\r\nFinally note that both the `set` as well as the `OrderedDict` solution require your items to be *hashable*. This usually means that they have to be immutable. If you have to deal with items that are not hashable (e.g. list objects), then you will have to use a slow approach in which you will basically have to compare every item with every other item in a nested loop.",
               "tags": [],
               "creation_date": 1320108544,
               "last_edit_date": 1517430325,
               "is_accepted": true,
               "id": "7961390",
               "down_vote_count": 5,
               "score": 936
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 59,
               "answer_id": 7961391,
               "is_accepted": false,
               "last_activity_date": 1320108548,
               "body_markdown": "If you don&#39;t care about the order, just do this:\r\n\r\n    def remove_duplicates(l):\r\n        return list(set(l))\r\n\r\nA `set` is guaranteed to not have duplicates.",
               "id": "7961391",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1320108548,
               "score": 58
            },
            {
               "up_vote_count": 148,
               "answer_id": 7961393,
               "last_activity_date": 1496680757,
               "path": "3.stack.answer",
               "body_markdown": "It&#39;s a one-liner: `list(set(source_list))` will do the trick.\r\n\r\nA `set` is something that can&#39;t possibly have duplicates.\r\n\r\nUpdate: an order-preserving approach is two lines:\r\n\r\n    from collections import OrderedDict\r\n    OrderedDict((x, True) for x in source_list).keys()\r\n\r\nHere we use the fact that `OrderedDict` remembers the insertion order of keys, and does not change it when a value at a particular key is updated. We insert `True` as values, but we could insert anything, values are just not used. (`set` works a lot like a `dict` with ignored values, too.)",
               "tags": [],
               "creation_date": 1320108573,
               "last_edit_date": 1496680757,
               "is_accepted": false,
               "id": "7961393",
               "down_vote_count": 1,
               "score": 147
            },
            {
               "up_vote_count": 266,
               "answer_id": 7961425,
               "last_activity_date": 1513931201,
               "path": "3.stack.answer",
               "body_markdown": "**In Python 2.7**, the new way of removing duplicates from an iterable while keeping it in the original order is:\r\n\r\n    &gt;&gt;&gt; from collections import OrderedDict\r\n    &gt;&gt;&gt; list(OrderedDict.fromkeys(&#39;abracadabra&#39;))\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;r&#39;, &#39;c&#39;, &#39;d&#39;]\r\n\r\n**In Python 3.5**, the OrderedDict has a C implementation. My timings show that this is now both the fastest and shortest of the various approaches for Python 3.5.\r\n\r\n**In Python 3.6**, the regular dict became both ordered and compact.  (This feature is holds for CPython and PyPy but may not present in other implementations).  That gives us a new fastest way of deduping while retaining order:\r\n\r\n    &gt;&gt;&gt; list(dict.fromkeys(&#39;abracadabra&#39;))\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;r&#39;, &#39;c&#39;, &#39;d&#39;]\r\n\r\n**In Python 3.7**, the regular dict is guaranteed to both ordered across all implementations.  **So, the shortest and fastest solution is:**\r\n\r\n    &gt;&gt;&gt; list(dict.fromkeys(&#39;abracadabra&#39;))\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;r&#39;, &#39;c&#39;, &#39;d&#39;]",
               "tags": [],
               "creation_date": 1320108835,
               "last_edit_date": 1513931201,
               "is_accepted": false,
               "id": "7961425",
               "down_vote_count": 2,
               "score": 264
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 7961428,
               "is_accepted": false,
               "last_activity_date": 1320108853,
               "body_markdown": "Try using sets:\r\n\r\n    import sets\r\n    t = sets.Set([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])\r\n    t1 = sets.Set([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\r\n    \r\n    print t | t1\r\n    print t - t1",
               "id": "7961428",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1320108853,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 66,
               "answer_id": 16543406,
               "is_accepted": false,
               "last_activity_date": 1368535195,
               "body_markdown": "\r\n    &gt;&gt;&gt; t = [1, 2, 3, 1, 2, 5, 6, 7, 8]\r\n    &gt;&gt;&gt; t\r\n    [1, 2, 3, 1, 2, 5, 6, 7, 8]\r\n    &gt;&gt;&gt; s = []\r\n    &gt;&gt;&gt; for i in t:\r\n           if i not in s:\r\n              s.append(i)\r\n    &gt;&gt;&gt; s\r\n    [1, 2, 3, 5, 6, 7, 8]\r\n",
               "id": "16543406",
               "tags": [],
               "down_vote_count": 4,
               "creation_date": 1368535195,
               "score": 62
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 17167096,
               "is_accepted": false,
               "last_activity_date": 1371552894,
               "body_markdown": "Nowadays you might use Counter class:\r\n\r\n    &gt;&gt;&gt; import collections\r\n    &gt;&gt;&gt; c = collections.Counter([1, 2, 3, 4, 5, 6, 1, 1, 1, 1])\r\n    &gt;&gt;&gt; c.keys()\r\n    dict_keys([1, 2, 3, 4, 5, 6])\r\n    ",
               "id": "17167096",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1371552894,
               "score": 2
            },
            {
               "up_vote_count": 18,
               "answer_id": 20870217,
               "last_activity_date": 1480735403,
               "path": "3.stack.answer",
               "body_markdown": "Another way of doing:\r\n\r\n    &gt;&gt;&gt; seq = [1,2,3,&#39;a&#39;, &#39;a&#39;, 1,2]\r\n    &gt;&gt; dict.fromkeys(seq).keys()\r\n    [&#39;a&#39;, 1, 2, 3]",
               "tags": [],
               "creation_date": 1388590749,
               "last_edit_date": 1480735403,
               "is_accepted": false,
               "id": "20870217",
               "down_vote_count": 1,
               "score": 17
            },
            {
               "up_vote_count": 13,
               "answer_id": 24085464,
               "last_activity_date": 1414407518,
               "path": "3.stack.answer",
               "body_markdown": "I had a dict in my list, so I could not use the above approach. I got the error:\r\n\r\n    TypeError: unhashable type:\r\n\r\nSo if you care about **order** and/or some items are **unhashable**. Then you might find this useful:\r\n    \r\n    def make_unique(original_list):\r\n        unique_list = []\r\n        [unique_list.append(obj) for obj in original_list if obj not in unique_list]\r\n        return unique_list\r\n\r\nSome may consider list comprehension with a side effect to not be a good solution. Here&#39;s an alternative:\r\n\r\n    def make_unique(original_list):\r\n        unique_list = []\r\n        map(lambda x: unique_list.append(x) if (x not in unique_list) else False, original_list)\r\n        return unique_list",
               "tags": [],
               "creation_date": 1402068356,
               "last_edit_date": 1414407518,
               "is_accepted": false,
               "id": "24085464",
               "down_vote_count": 2,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 24118409,
               "is_accepted": false,
               "last_activity_date": 1402309998,
               "body_markdown": "Here is an example, returning list without repetiotions preserving order. Does not need any external imports.\r\n\r\n    def GetListWithoutRepetitions(loInput):\r\n        # return list, consisting of elements of list/tuple loInput, without repetitions.\r\n        # Example: GetListWithoutRepetitions([None,None,1,1,2,2,3,3,3])\r\n        # Returns: [None, 1, 2, 3]\r\n    \r\n        if loInput==[]:\r\n            return []\r\n    \r\n        loOutput = []\r\n    \r\n        if loInput[0] is None:\r\n            oGroupElement=1\r\n        else: # loInput[0]&lt;&gt;None\r\n            oGroupElement=None\r\n    \r\n        for oElement in loInput:\r\n            if oElement&lt;&gt;oGroupElement:\r\n                loOutput.append(oElement)\r\n                oGroupElement = oElement\r\n        return loOutput",
               "id": "24118409",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1402309998,
               "score": 2
            },
            {
               "up_vote_count": 9,
               "answer_id": 24554087,
               "last_activity_date": 1512557484,
               "path": "3.stack.answer",
               "body_markdown": "There are also solutions using Pandas and Numpy. They both return numpy array so you have to use the function [`.tolist()`][1] if you want a list.\r\n\r\n    t=[&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;,&#39;c&#39;,&#39;c&#39;]\r\n    t2= [&#39;c&#39;,&#39;c&#39;,&#39;b&#39;,&#39;b&#39;,&#39;b&#39;,&#39;a&#39;,&#39;a&#39;,&#39;a&#39;]\r\n\r\n## Pandas solution\r\nUsing Pandas function [`unique()`][2]:\r\n\r\n    import pandas as pd\r\n    pd.unique(t).tolist()\r\n    &gt;&gt;&gt;[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]\r\n    pd.unique(t2).tolist()\r\n    &gt;&gt;&gt;[&#39;c&#39;,&#39;b&#39;,&#39;a&#39;]\r\n\r\n## Numpy solution\r\nUsing numpy function [`unique()`][3].\r\n\r\n    import numpy as np\r\n    np.unique(t).tolist()\r\n    &gt;&gt;&gt;[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]\r\n    np.unique(t2).tolist()\r\n    &gt;&gt;&gt;[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]\r\n\r\n**Note that numpy.unique() also sort the values**. So the list `t2` is returned sorted. If you want to have the order preserved use as in [this answer][4]:\r\n    \r\n    _, idx = np.unique(t2, return_index=True)\r\n    t2[np.sort(idx)].tolist()\r\n    &gt;&gt;&gt;[&#39;c&#39;,&#39;b&#39;,&#39;a&#39;]\r\n\r\nThe solution is not so elegant compared to the others, however, compared to pandas.unique(), numpy.unique() allows you also to check if nested arrays are unique along one selected axis.\r\n\r\n\r\n  [1]: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.tolist.html\r\n  [2]: https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.unique.html\r\n  [3]: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html\r\n  [4]: https://stackoverflow.com/questions/15637336/numpy-unique-with-order-preserved",
               "tags": [],
               "creation_date": 1404391551,
               "last_edit_date": 1512557484,
               "is_accepted": false,
               "id": "24554087",
               "down_vote_count": 2,
               "score": 7
            },
            {
               "up_vote_count": 26,
               "answer_id": 24582741,
               "last_activity_date": 1409181259,
               "path": "3.stack.answer",
               "body_markdown": "To make a new list  retaining the order of first elements of duplicates in ```L```\r\n\r\n```newlist=[ii for n,ii in enumerate(L) if ii not in L[:n]]```\r\n\r\nfor example ```if L=[1, 2, 2, 3, 4, 2, 4, 3, 5]``` then ```newlist``` will be ```[1,2,3,4,5]```\r\n\r\nThis checks each new element has not appeared previously in the list before adding it. \r\nAlso it does not need imports. ",
               "tags": [],
               "creation_date": 1404531566,
               "last_edit_date": 1409181259,
               "is_accepted": false,
               "id": "24582741",
               "down_vote_count": 1,
               "score": 25
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 25622503,
               "is_accepted": false,
               "last_activity_date": 1409657874,
               "body_markdown": "This one cares about the order without too much hassle (OrderdDict &amp; others). Probably not the most Pythonic way, nor shortest way, but does the trick:\r\n\r\n    def remove_duplicates(list):\r\n        &#39;&#39;&#39; Removes duplicate items from a list &#39;&#39;&#39;\r\n        singles_list = []\r\n        for element in list:\r\n            if element not in singles_list:\r\n                singles_list.append(element)\r\n        return singles_list",
               "id": "25622503",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1409657874,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 25887387,
               "is_accepted": false,
               "last_activity_date": 1410947565,
               "body_markdown": "A colleague have sent the accepted answer as part of his code to me for a codereview today.\r\nWhile I certainly admire the elegance of the answer in question, I am not happy with the performance.\r\nI have tried this solution (I use *set* to reduce lookup time)\r\n\r\n    def ordered_set(in_list):\r\n        out_list = []\r\n        added = set()\r\n        for val in in_list:\r\n            if not val in added:\r\n                out_list.append(val)\r\n                added.add(val)\r\n        return out_list\r\n\r\nTo compare efficiency, I used a random sample of 100 integers - 62 were unique\r\n\r\n    from random import randint\r\n    x = [randint(0,100) for _ in xrange(100)]\r\n    \r\n    In [131]: len(set(x))\r\n    Out[131]: 62\r\n\r\nHere are the results of the measurements\r\n\r\n    In [129]: %timeit list(OrderedDict.fromkeys(x))\r\n    10000 loops, best of 3: 86.4 us per loop\r\n    \r\n    In [130]: %timeit ordered_set(x)\r\n    100000 loops, best of 3: 15.1 us per loop\r\n\r\nWell, what happens if set is removed from the solution?\r\n\r\n    def ordered_set(inlist):\r\n        out_list = []\r\n        for val in inlist:\r\n            if not val in out_list:\r\n                out_list.append(val)\r\n        return out_list\r\nThe result is not as bad as with the *OrderedDict*, but still more than 3 times of the original solution\r\n\r\n    In [136]: %timeit ordered_set(x)\r\n    10000 loops, best of 3: 52.6 us per loop\r\n\r\n\r\n",
               "id": "25887387",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1410947565,
               "score": 15
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 29639138,
               "is_accepted": false,
               "last_activity_date": 1429054420,
               "body_markdown": "Simple and easy:\r\n\r\n    myList = [1, 2, 3, 1, 2, 5, 6, 7, 8]\r\n    cleanlist = []\r\n    [cleanlist.append(x) for x in myList if x not in cleanlist]\r\nOutput:\r\n\r\n    &gt;&gt;&gt; cleanlist \r\n    [1, 2, 3, 5, 6, 7, 8]",
               "id": "29639138",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1429054420,
               "score": 10
            },
            {
               "up_vote_count": 3,
               "answer_id": 29898868,
               "last_activity_date": 1430146595,
               "path": "3.stack.answer",
               "body_markdown": "Reduce variant with ordering preserve:\r\n\r\nAssume that we have list:\r\n\r\n    l = [5, 6, 6, 1, 1, 2, 2, 3, 4]\r\n\r\nReduce variant (unefficient):\r\n\r\n    &gt;&gt;&gt; reduce(lambda r, v: v in r and r or r + [v], l, [])\r\n    [5, 6, 1, 2, 3, 4]\r\n\r\n5 x faster but more sophisticated\r\n\r\n    &gt;&gt;&gt; reduce(lambda r, v: v in r[1] and r or (r[0].append(v) or r[1].add(v)) or r, l, ([], set()))[0]\r\n    [5, 6, 1, 2, 3, 4]\r\n\r\nExplanation:\r\n\r\n    default = (list(), set())\r\n    # user list to keep order\r\n    # use set to make lookup faster\r\n    \r\n    def reducer(result, item):\r\n        if item not in result[1]:\r\n            result[0].append(item)\r\n            result[1].add(item)\r\n        return result\r\n\r\n    reduce(reducer, l, default)[0]",
               "tags": [],
               "creation_date": 1430145723,
               "last_edit_date": 1430146595,
               "is_accepted": false,
               "id": "29898868",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 31999082,
               "is_accepted": false,
               "last_activity_date": 1439502867,
               "body_markdown": "below code is simple for removing duplicate in list\r\n\r\n    def remove_duplicates(x):\r\n        a = []\r\n        for i in x:\r\n            if i not in a:\r\n                a.append(i)\r\n        return a\r\n    \r\n    print remove_duplicates([1,2,2,3,3,4])\r\n\r\n    \r\n\r\nit returns [1,2,3,4]",
               "id": "31999082",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1439502867,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 32199313,
               "is_accepted": false,
               "last_activity_date": 1440491924,
               "body_markdown": "To remove the duplicates, make it a SET and then again make it a LIST and print/use it.\r\nA set is guaranteed to have unique elements. For example : \r\n\r\n    a = [1,2,3,4,5,9,11,15]\r\n    b = [4,5,6,7,8]\r\n    c=a+b\r\n    print c\r\n    print list(set(c)) #one line for getting unique elements of c\r\n\r\nThe output will be as follows (checked in python 2.7)\r\n\r\n    [1, 2, 3, 4, 5, 9, 11, 15, 4, 5, 6, 7, 8]  #simple list addition with duplicates\r\n    [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 15] #duplicates removed!!\r\n   \r\n\r\n\r\n ",
               "id": "32199313",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1440491924,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 32215911,
               "is_accepted": false,
               "last_activity_date": 1440546717,
               "body_markdown": "There are many other answers suggesting different ways to do this, but they&#39;re all batch operations, and some of them throw away the original order. That might be okay depending on what you need, but if you want to iterate over the values in the order of the first instance of each value, and you want to remove the duplicates on-the-fly versus all at once, you could use this generator:\r\n\r\n    def uniqify(iterable):\r\n        seen = set()\r\n        for item in iterable:\r\n            if item not in seen:\r\n                seen.add(item)\r\n                yield item\r\n    \r\nThis returns a generator/iterator, so you can use it anywhere that you can use an iterator.\r\n\r\n    for unique_item in uniqify([1, 2, 3, 4, 3, 2, 4, 5, 6, 7, 6, 8, 8]):\r\n        print(unique_item, end=&#39; &#39;)\r\n    \r\n    print()\r\n\r\nOutput:\r\n\r\n    1 2 3 4 5 6 7 8\r\n\r\n----------\r\n\r\nIf you do want a `list`, you can do this:\r\n\r\n    unique_list = list(uniqify([1, 2, 3, 4, 3, 2, 4, 5, 6, 7, 6, 8, 8]))\r\n    \r\n    print(unique_list)\r\n\r\nOutput:\r\n\r\n    [1, 2, 3, 4, 5, 6, 7, 8]",
               "id": "32215911",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1440546717,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 33830196,
               "is_accepted": false,
               "last_activity_date": 1448032840,
               "body_markdown": "Check this if you want to remove duplicates (in-place edit rather than returning new list) without using inbuilt set, dict.keys, uniqify, counter\r\n    \r\n    &gt;&gt;&gt; t = [1, 2, 3, 1, 2, 5, 6, 7, 8]\r\n    &gt;&gt;&gt; for i in t:\r\n    ...     if i in t[t.index(i)+1:]:\r\n    ...         t.remove(i)\r\n    ... \r\n    &gt;&gt;&gt; t\r\n    [3, 1, 2, 5, 6, 7, 8]",
               "id": "33830196",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1448032840,
               "score": 2
            },
            {
               "up_vote_count": 5,
               "answer_id": 34775128,
               "last_activity_date": 1518447550,
               "path": "3.stack.answer",
               "body_markdown": "All the order-preserving approaches I&#39;ve seen here so far either use naive  comparison (with O(n^2) time-complexity at best) or heavy-weight `OrderedDicts`/`set`+`list` combinations that are limited to hashable inputs. Here is a hash-independent O(nlogn) solution:\r\n\r\n**Update** added the `key` argument, documentation and Python 3 compatibility.\r\n\r\n    # from functools import reduce &lt;-- add this import on Python 3\r\n\r\n    def uniq(iterable, key=lambda x: x):\r\n        &quot;&quot;&quot;\r\n        Remove duplicates from an iterable. Preserves order. \r\n        :type iterable: Iterable[Ord =&gt; A]\r\n        :param iterable: an iterable of objects of any orderable type\r\n        :type key: Callable[A] -&gt; (Ord =&gt; B)\r\n        :param key: optional argument; by default an item (A) is discarded \r\n        if another item (B), such that A == B, has already been encountered and taken. \r\n        If you provide a key, this condition changes to key(A) == key(B); the callable \r\n        must return orderable objects.\r\n        &quot;&quot;&quot;\r\n        # Enumerate the list to restore order lately; reduce the sorted list; restore order\r\n        def append_unique(acc, item):\r\n            return acc if key(acc[-1][1]) == key(item[1]) else acc.append(item) or acc \r\n        srt_enum = sorted(enumerate(iterable), key=lambda item: key(item[1]))\r\n        return [item[1] for item in sorted(reduce(append_unique, srt_enum, [srt_enum[0]]))] \r\n  ",
               "tags": [],
               "creation_date": 1452712379,
               "last_edit_date": 1518447550,
               "is_accepted": false,
               "id": "34775128",
               "down_vote_count": 1,
               "score": 4
            },
            {
               "up_vote_count": 0,
               "answer_id": 40499263,
               "last_activity_date": 1516267861,
               "path": "3.stack.answer",
               "body_markdown": "It requires installing a 3rd-party module but the package `iteration_utilities` contains a [`unique_everseen`](http://iteration-utilities.readthedocs.io/en/latest/api/cfuncs.html#iteration_utilities.unique_everseen)&lt;sup&gt;1&lt;/sup&gt; function that can remove all duplicates while preserving the order:\r\n\r\n    &gt;&gt;&gt; from iteration_utilities import unique_everseen\r\n    \r\n    &gt;&gt;&gt; list(unique_everseen([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] + [&#39;a&#39;, &#39;c&#39;, &#39;d&#39;]))\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]\r\n\r\nIn case you want to avoid the overhead of the list addition operation you can use [`itertools.chain`](https://docs.python.org/library/itertools.html#itertools.chain) instead:\r\n\r\n    &gt;&gt;&gt; from itertools import chain\r\n    &gt;&gt;&gt; list(unique_everseen(chain([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;], [&#39;a&#39;, &#39;c&#39;, &#39;d&#39;])))\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]\r\n\r\nThe `unique_everseen` also works if you have unhashable items (for example lists) in the lists:\r\n\r\n    &gt;&gt;&gt; from iteration_utilities import unique_everseen\r\n    &gt;&gt;&gt; list(unique_everseen([[&#39;a&#39;], [&#39;b&#39;], &#39;c&#39;, &#39;d&#39;] + [&#39;a&#39;, &#39;c&#39;, &#39;d&#39;]))\r\n    [[&#39;a&#39;], [&#39;b&#39;], &#39;c&#39;, &#39;d&#39;, &#39;a&#39;]\r\n\r\nHowever that will be (much) slower than if the items are hashable.\r\n\r\n---\r\n\r\n&lt;sup&gt;1&lt;/sup&gt; Disclosure: I&#39;m the author of the `iteration_utilities`-library.",
               "tags": [],
               "creation_date": 1478656582,
               "last_edit_date": 1516267861,
               "is_accepted": false,
               "id": "40499263",
               "down_vote_count": 1,
               "score": -1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 42694683,
               "is_accepted": false,
               "last_activity_date": 1489060235,
               "body_markdown": "For completeness, and since this is a very popular question, the [toolz][1] library offers a [`unique`][2] function:\r\n\r\n    &gt;&gt;&gt; tuple(unique((1, 2, 3)))\r\n    (1, 2, 3)\r\n    &gt;&gt;&gt; tuple(unique((1, 2, 1, 3)))\r\n    (1, 2, 3)\r\n\r\n  [1]: http://toolz.readthedocs.io/en/latest/\r\n  [2]: http://toolz.readthedocs.io/en/latest/api.html#toolz.itertoolz.unique\r\n",
               "id": "42694683",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1489060235,
               "score": 0
            },
            {
               "up_vote_count": 3,
               "answer_id": 43161258,
               "last_activity_date": 1491076561,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s the fastest pythonic solution comaring to others listed in replies.\r\n\r\nUsing implementation details of short-circuit evaluation allows to use list comprehension, which is fast enough. `visited.add(item)` always returns `None` as a result, which is evaluated as `False`, so the right-side of `or` would always be the result of such an expression.\r\n\r\nTime it yourself\r\n\r\n    def deduplicate(sequence):\r\n        visited = set()\r\n        adder = visited.add  # get rid of qualification overhead\r\n        out = [adder(item) or item for item in sequence if item not in visited]\r\n        return out",
               "tags": [],
               "creation_date": 1491075991,
               "last_edit_date": 1491076561,
               "is_accepted": false,
               "id": "43161258",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 44385914,
               "is_accepted": false,
               "last_activity_date": 1496740346,
               "body_markdown": "You could also do this:\r\n\r\n    &gt;&gt;&gt; t = [1, 2, 3, 3, 2, 4, 5, 6]\r\n    &gt;&gt;&gt; s = [x for i, x in enumerate(t) if i == t.index(x)]\r\n    &gt;&gt;&gt; s\r\n    [1, 2, 3, 4, 5, 6]\r\n\r\nThe reason that above works is that `index` method returns only the first index of an element. Duplicate elements have higher indices. Refer to [here][1]:\r\n\r\n&gt; **list.index(x[, start[, end]])**   \r\n&gt; Return zero-based index in the list of\r\n&gt; the first item whose value is x.    Raises a ValueError if there is no\r\n&gt; such item.\r\n\r\n\r\n\r\n  [1]: https://docs.python.org/3/tutorial/datastructures.html#more-on-lists",
               "id": "44385914",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1496740346,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 45384089,
               "is_accepted": false,
               "last_activity_date": 1501288381,
               "body_markdown": "I think converting to set is the easiest way to remove duplicate:\r\n\r\n    list1 = [1,2,1]\r\n    list1 = list(set(list1))\r\n    print list1",
               "id": "45384089",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1501288381,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 45384125,
               "is_accepted": false,
               "last_activity_date": 1501288754,
               "body_markdown": "Using ***set*** :\r\n\r\n    a = [0,1,2,3,4,3,3,4]\r\n    a = list(set(a))\r\n    print a\r\nUsing ***unique*** :\r\n\r\n    import numpy as np\r\n    a = [0,1,2,3,4,3,3,4]\r\n    a = np.unique(a).tolist()\r\n    print a",
               "id": "45384125",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1501288754,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 45729214,
               "is_accepted": false,
               "last_activity_date": 1502955565,
               "body_markdown": "Best approach of removing duplicates from a list is using **set()** function, available in python, again converting that **set into list**\r\n\r\n    In [2]: some_list = [&#39;a&#39;,&#39;a&#39;,&#39;v&#39;,&#39;v&#39;,&#39;v&#39;,&#39;c&#39;,&#39;c&#39;,&#39;d&#39;]\r\n    In [3]: list(set(some_list))\r\n    Out[3]: [&#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;v&#39;]\r\n\r\n\r\n ",
               "id": "45729214",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1502955565,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 45755275,
               "is_accepted": false,
               "last_activity_date": 1503054714,
               "body_markdown": "You can do this simply by using sets.\r\n\r\n**Step1:** Get Different elements of lists &lt;br&gt;\r\n**Step2** Get Common elements of lists &lt;br&gt;\r\n**Step3** Combine them\r\n\r\n    In [1]: a = [&quot;apples&quot;, &quot;bananas&quot;, &quot;cucumbers&quot;]\r\n    \r\n    In [2]: b = [&quot;pears&quot;, &quot;apples&quot;, &quot;watermelons&quot;]\r\n    \r\n    In [3]: set(a).symmetric_difference(b).union(set(a).intersection(b))\r\n    Out[3]: {&#39;apples&#39;, &#39;bananas&#39;, &#39;cucumbers&#39;, &#39;pears&#39;, &#39;watermelons&#39;}\r\n\r\n",
               "id": "45755275",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1503054714,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 45900479,
               "is_accepted": false,
               "last_activity_date": 1503789822,
               "body_markdown": "    def remove_duplicates(A):\r\n       [A.pop(count) for count,elem in enumerate(A) if A.count(elem)!=1]\r\n       return A\r\nA list comprehesion to remove duplicates",
               "id": "45900479",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1503789822,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 46272738,
               "is_accepted": false,
               "last_activity_date": 1505715598,
               "body_markdown": "If you don&#39;t care about order and want something different than the pythonic ways suggested above (that is, it can be used in interviews) then : \r\n\r\n    def remove_dup(arr):\r\n        size = len(arr)\r\n        j = 0    # To store index of next unique element\r\n        for i in range(0, size-1):\r\n            # If current element is not equal\r\n            # to next element then store that\r\n            # current element\r\n            if(arr[i] != arr[i+1]):\r\n                arr[j] = arr[i]\r\n                j+=1\r\n    \r\n        arr[j] = arr[size-1] # Store the last element as whether it is unique or repeated, it hasn&#39;t stored previously\r\n    \r\n        return arr[0:j+1]\r\n    \r\n    if __name__ == &#39;__main__&#39;:\r\n        arr = [10, 10, 1, 1, 1, 3, 3, 4, 5, 6, 7, 8, 8, 9]\r\n        print(remove_dup(sorted(arr)))\r\n\r\nTime Complexity : O(n)\r\n\r\nAuxiliary Space : O(n)\r\n\r\nReference: http://www.geeksforgeeks.org/remove-duplicates-sorted-array/\r\n\r\n",
               "id": "46272738",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1505715598,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 46707294,
               "is_accepted": false,
               "last_activity_date": 1507804097,
               "body_markdown": "Without using set \r\n\r\n    data=[1, 2, 3, 1, 2, 5, 6, 7, 8]\r\n    uni_data=[]\r\n    for dat in data:\r\n        if dat not in uni_data:\r\n            uni_data.append(dat)\r\n        \r\n    print(uni_data) \r\n\r\n   ",
               "id": "46707294",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1507804097,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 47885660,
               "is_accepted": false,
               "last_activity_date": 1513681834,
               "body_markdown": "There are a lot of answers here that use a `set(..)` (which is fast given the elements are *hashable*), or a list (which has the downside that it results in an *O(n&lt;sup&gt;2&lt;/sup&gt;)* algorithm.\r\n\r\nThe function I propose is a hybrid one: we use a `set(..)` for items that *are* hashable, and a `list(..)` for the ones that are not. Furthermore it is implemented as a *generator* such that we can for instance limit the number of items, or do some additional filtering.\r\n\r\nFinally we also can use a `key` argument to specify in what way the elements should be unique. For instance we can use this if we want to filter a list of strings such that every string in the output has a different length.\r\n\r\n&lt;pre&gt;&lt;code&gt;def uniq(iterable, key=lambda x: x):\r\n    seens = set()\r\n    seenl = []\r\n    for item in iterable:\r\n        k = key(item)\r\n        try:\r\n            seen = k in seens\r\n        except TypeError:\r\n            seen = k in seenl\r\n        if not seen:\r\n            yield item\r\n            try:\r\n                seens.add(k)\r\n            except TypeError:\r\n                seenl.append(k)&lt;/code&gt;&lt;/pre&gt;\r\n\r\nWe can now for instance use this like:\r\n\r\n    &gt;&gt;&gt; list(uniq([&quot;apple&quot;, &quot;pear&quot;, &quot;banana&quot;, &quot;lemon&quot;], len))\r\n    [&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;]\r\n    &gt;&gt;&gt; list(uniq([&quot;apple&quot;, &quot;pear&quot;, &quot;lemon&quot;, &quot;banana&quot;], len))\r\n    [&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;]\r\n    &gt;&gt;&gt; list(uniq([&quot;apple&quot;, &quot;pear&quot;, {}, &quot;lemon&quot;, [], &quot;banana&quot;], len))\r\n    [&#39;apple&#39;, &#39;pear&#39;, {}, &#39;banana&#39;]\r\n    &gt;&gt;&gt; list(uniq([&quot;apple&quot;, &quot;pear&quot;, {}, &quot;lemon&quot;, [], &quot;banana&quot;]))\r\n    [&#39;apple&#39;, &#39;pear&#39;, {}, &#39;lemon&#39;, [], &#39;banana&#39;]\r\n    &gt;&gt;&gt; list(uniq([&quot;apple&quot;, &quot;pear&quot;, {}, &quot;lemon&quot;, {}, &quot;banana&quot;]))\r\n    [&#39;apple&#39;, &#39;pear&#39;, {}, &#39;lemon&#39;, &#39;banana&#39;]\r\n\r\nIt is thus a uniqeness filter that can work on any iterable and filter out uniques, regardless whether these are hashable or not.\r\n\r\nIt makes one assumption: that if one object is hashable, and another one is not, the two objects are never equal. This can strictly speaking happen, although it would be very uncommon.",
               "id": "47885660",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1513681834,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48933395,
               "is_accepted": false,
               "last_activity_date": 1519319431,
               "body_markdown": "Another solution might be the following. Create a dictionary out of the list with item as key and index as value, and then print the dictionary keys.\r\n\r\n\r\n    &gt;&gt;&gt; lst = [1, 3, 4, 2, 1, 21, 1, 32, 21, 1, 6, 5, 7, 8, 2]\r\n    &gt;&gt;&gt;\r\n    &gt;&gt;&gt; dict_enum = {item:index for index, item in enumerate(lst)}\r\n    &gt;&gt;&gt; print dict_enum.keys()\r\n    [32, 1, 2, 3, 4, 5, 6, 7, 8, 21]",
               "id": "48933395",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519319431,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists",
         "id": "858127-2303"
      },
      {
         "up_vote_count": "1290",
         "path": "2.stack",
         "body_markdown": "Short and simple. I&#39;ve got a huge list of date-times like this as strings:\r\n\r\n    Jun 1 2005  1:33PM\r\n    Aug 28 1999 12:00AM\r\n\r\nI&#39;m going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. \r\n\r\nAny help (even if it&#39;s just a kick in the right direction) would be appreciated.\r\n\r\nEdit: This is going through Django&#39;s ORM so I can&#39;t use SQL to do the conversion on insert.",
         "view_count": "1532664",
         "answer_count": "17",
         "tags": "['python', 'datetime']",
         "creation_date": "1232560829",
         "last_edit_date": "1465659192",
         "code_snippet": "['<code>Jun 1 2005  1:33PM\\nAug 28 1999 12:00AM\\n</code>', '<code>strptime()</code>', \"<code>from datetime import datetime\\n\\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\\n</code>\", '<code>datetime</code>', '<code>strptime</code>', '<code>strftime</code>', '<code>strptime</code>', '<code>strftime</code>', '<code>date</code>', '<code>datetime</code>', '<code>datetime</code>', \"<code>datetime.strptime('Jun 1 2005', '%b %d %Y').date() == date(2005, 6, 1)</code>\", '<code>datetime</code>', '<code>from datetime import timezone; datetime_object = datetime_object.replace(tzinfo=timezone.utc)</code>', '<code>\"%Y-%m-%d %H:%M:%S\"</code>', '<code>from dateutil import parser\\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\\n</code>', '<code>pip install python-dateutil\\n</code>', \"<code>$ python\\n&gt;&gt;&gt; import time\\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\\n                 tm_hour=13, tm_min=33, tm_sec=0,\\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\\n</code>\", '<code>parse</code>', '<code>format</code>', '<code>pip install timestring</code>', \"<code>&gt;&gt;&gt; import timestring\\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\\ndatetime.datetime(2015, 8, 15, 20, 40)\\n&gt;&gt;&gt; timestring.Range('next week')\\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\\n</code>\", '<code>import timestring</code>', \"<code>timestring.Date('27 Mar 2014 12:32:29 GMT').year</code>\", '<code>&gt;&gt;&gt; datetime.datetime.strptime(\\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\\n</code>', '<code>mktime()</code>', '<code>datetime.strptime()</code>', '<code>calendar.timegm</code>', '<code>(datetime(1970,1,1)+timedelta(seconds=timegm(time.strptime(..)))).replace(tzinfo=timezone(timedelta(-3)))</code>', '<code>strptime</code>', '<code>strftime</code>', '<code>Jun 1 2005  1:33PM</code>', '<code>%b %d %Y %I:%M%p</code>', '<code>string</code>', \"<code>&gt;&gt;&gt; dates = []\\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\\n&gt;&gt;&gt; from datetime import datetime\\n&gt;&gt;&gt; for d in dates:\\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\\n...     print type(date)\\n...     print date\\n... \\n</code>\", \"<code>&lt;type 'datetime.datetime'&gt;\\n2005-06-01 13:33:00\\n&lt;type 'datetime.datetime'&gt;\\n1999-08-28 00:00:00\\n</code>\", \"<code>&gt;&gt;&gt; import dateutil\\n&gt;&gt;&gt; dates = []\\n&gt;&gt;&gt; dates.append('12 1 2017')\\n&gt;&gt;&gt; dates.append('1 1 2017')\\n&gt;&gt;&gt; dates.append('1 12 2017')\\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\\n</code>\", '<code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\\n</code>', '<code>import time\\n\\ndef num_suffix(n):\\n    \\'\\'\\'\\n    Returns the suffix for any given int\\n    \\'\\'\\'\\n    suf = (\\'th\\',\\'st\\', \\'nd\\', \\'rd\\')\\n    n = abs(n) # wise guy\\n    tens = int(str(n)[-2:])\\n    units = n % 10\\n    if tens &gt; 10 and tens &lt; 20:\\n        return suf[0] # teens with \\'th\\'\\n    elif units &lt;= 3:\\n        return suf[units]\\n    else:\\n        return suf[0] # \\'th\\'\\n\\ndef day_suffix(t):\\n    \\'\\'\\'\\n    Returns the suffix of the given struct_time day\\n    \\'\\'\\'\\n    return num_suffix(t.tm_mday)\\n\\n# Examples\\nprint num_suffix(123)\\nprint num_suffix(3431)\\nprint num_suffix(1234)\\nprint \\'\\'\\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\n</code>', \"<code>import pandas as pd\\n\\ndates = ['2015-12-25', '2015-12-26']\\n\\n# 1) Use a list comprehension.\\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\\n\\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\\n</code>\", \"<code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\\n\\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\\n# 100 loops, best of 3: 3.11 ms per loop\\n\\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\\n# 100 loops, best of 3: 6.85 ms per loop\\n</code>\", \"<code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\\n\\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\\n[datetime.datetime(2005, 6, 1, 13, 33), \\n datetime.datetime(1999, 8, 28, 0, 0)]\\n</code>\", '<code>to_datetime</code>', '<code>.date</code>', '<code>In [34]: import datetime\\n\\nIn [35]: _now = datetime.datetime.now()\\n\\nIn [36]: _now\\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\\n\\nIn [37]: print _now\\n2016-01-19 09:47:00.432000\\n\\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\\n\\nIn [39]: _parsed\\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\\n\\nIn [40]: assert _now == _parsed\\n</code>', \"<code>import datetime\\nfrom django.utils.timezone import get_current_timezone\\ntz = get_current_timezone()\\n\\nformat = '%b %d %Y %I:%M%p'\\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\\ndate_obj = tz.localize(date_object)\\n</code>\", '<code>USE_TZ = True</code>', '<code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\\n</code>', '<code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\\n    from datetime import datetime\\n    if not datestr:\\n        return datetime.today().date()\\n    return datetime.strptime(datestr, format).date()\\n</code>', '<code>format</code>', \"<code>import datetime\\n\\nnew_date_format1 = datetime.datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\\nnew_date_format2 = datetime.datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p').strftime('%Y/%m/%d %I:%M%p')\\nprint new_date_format1\\nprint new_date_format2\\n</code>\", '<code>2005-06-01 13:33:00\\n2005/06/01 01:33PM\\n</code>', \"<code>&gt;&gt;&gt; import arrow\\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\\n&gt;&gt;&gt; for dateString in dateStrings:\\n...     dateString\\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\\n...     \\n'Jun 1  2005 1:33PM'\\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\\n'Wed, 1st Jun 2005 13:33'\\n'vor 11 Jahren'\\n'Aug 28 1999 12:00AM'\\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\\n'Sat, 28th Aug 1999 00:00'\\n'vor 17 Jahren'\\n</code>\", \"<code>import date_converter\\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\\n</code>\", '<code>parser</code>', '<code>90%</code>', '<code>strptime</code>', '<code>from dateutil import parser\\nfrom datetime import datetime\\nimport timeit\\n\\ndef dt():\\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\\ndef strptime():\\n    datetime_object = datetime.strptime(\\'Jun 1 2005  1:33PM\\', \\'%b %d %Y %I:%M%p\\')\\n\\nprint(timeit.timeit(stmt=dt, number=10**5))\\nprint(timeit.timeit(stmt=strptime, number=10**5))\\n&gt;10.70296801342902\\n&gt;1.3627995655316933\\n</code>', '<code>a million</code>', '<code>parser</code>', '<code>strptime()</code>', \"<code>&gt;&gt;&gt; import datetime\\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\\n&gt;&gt;&gt; date\\ndatetime.date(2017, 12, 21)\\n&gt;&gt;&gt; type(date)\\n&lt;type 'datetime.date'&gt;\\n</code>\", \"<code>selected_month_rec = '2017-09-01'\\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\\n</code>\"]",
         "title": "Converting string into datetime",
         "_childDocuments_": [
            {
               "up_vote_count": 473,
               "answer_id": 466366,
               "last_activity_date": 1483037418,
               "path": "3.stack.answer",
               "body_markdown": "Check out [strptime][1] in the [time][2] module.  It is the inverse of [strftime][3].\r\n\r\n    $ python\r\n    &gt;&gt;&gt; import time\r\n    &gt;&gt;&gt; time.strptime(&#39;Jun 1 2005  1:33PM&#39;, &#39;%b %d %Y %I:%M%p&#39;)\r\n    time.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\r\n                     tm_hour=13, tm_min=33, tm_sec=0,\r\n                     tm_wday=2, tm_yday=152, tm_isdst=-1)\r\n\r\n  [1]: http://docs.python.org/3/library/time.html#time.strptime\r\n  [2]: http://docs.python.org/3/library/time.html\r\n  [3]: http://docs.python.org/3/library/time.html#time.strftime",
               "tags": [],
               "creation_date": 1232561237,
               "last_edit_date": 1483037418,
               "is_accepted": false,
               "id": "466366",
               "down_vote_count": 22,
               "score": 451
            },
            {
               "up_vote_count": 2121,
               "answer_id": 466376,
               "last_activity_date": 1511060288,
               "path": "3.stack.answer",
               "body_markdown": "    from datetime import datetime\r\n    \r\n    datetime_object = datetime.strptime(&#39;Jun 1 2005  1:33PM&#39;, &#39;%b %d %Y %I:%M%p&#39;)\r\n\r\nThe resulting `datetime` object is timezone-naive.\r\n\r\nLinks:\r\n\r\n- Python documentation for `strptime`: [Python 2][1], [Python 3][2]\r\n\r\n- Python documentation for `strftime` format mask: [Python 2][3], [Python 3][4]\r\n\r\n- [strftime.org][5] is also a really nice reference for strftime\r\n\r\nNotes:\r\n\r\n- `strptime` = &quot;string parse time&quot;\r\n- `strftime` = &quot;string format time&quot;\r\n- Pronounce it out loud today &amp; you won&#39;t have to search for it again in 6 months.\r\n\r\n  [1]: https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime &quot;datetime.datetime.strptime&quot;\r\n  [2]: https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\r\n  [3]: https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior &quot;strftime-and-strptime-behavior&quot;\r\n  [4]: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\r\n  [5]: http://strftime.org/",
               "tags": [],
               "creation_date": 1232561332,
               "last_edit_date": 1511060288,
               "is_accepted": true,
               "id": "466376",
               "down_vote_count": 0,
               "score": 2121
            },
            {
               "up_vote_count": 561,
               "answer_id": 470303,
               "last_activity_date": 1458750954,
               "path": "3.stack.answer",
               "body_markdown": "Use the third party [dateutil][1] library:\r\n\r\n    from dateutil import parser\r\n    dt = parser.parse(&quot;Aug 28 1999 12:00AM&quot;)\r\n\r\nIt can handle most date formats, including the one you need to parse. It&#39;s more convenient than strptime as it can guess the correct format most of the time.\r\n\r\nIt very useful for writing tests, where readability is more important that performance.\r\n\r\nYou can install it with:\r\n\r\n    pip install python-dateutil\r\n\r\n\r\n  [1]: http://labix.org/python-dateutil",
               "tags": [],
               "creation_date": 1232648838,
               "last_edit_date": 1458750954,
               "is_accepted": false,
               "id": "470303",
               "down_vote_count": 2,
               "score": 559
            },
            {
               "up_vote_count": 22,
               "answer_id": 7761860,
               "last_activity_date": 1318552120,
               "path": "3.stack.answer",
               "body_markdown": "Something that isn&#39;t mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.\r\n\r\n    import time\r\n        \r\n    def num_suffix(n):\r\n        &#39;&#39;&#39;\r\n        Returns the suffix for any given int\r\n        &#39;&#39;&#39;\r\n        suf = (&#39;th&#39;,&#39;st&#39;, &#39;nd&#39;, &#39;rd&#39;)\r\n        n = abs(n) # wise guy\r\n        tens = int(str(n)[-2:])\r\n        units = n % 10\r\n        if tens &gt; 10 and tens &lt; 20:\r\n            return suf[0] # teens with &#39;th&#39;\r\n        elif units &lt;= 3:\r\n            return suf[units]\r\n        else:\r\n            return suf[0] # &#39;th&#39;\r\n    \r\n    def day_suffix(t):\r\n        &#39;&#39;&#39;\r\n        Returns the suffix of the given struct_time day\r\n        &#39;&#39;&#39;\r\n        return num_suffix(t.tm_mday)\r\n    \r\n    # Examples\r\n    print num_suffix(123)\r\n    print num_suffix(3431)\r\n    print num_suffix(1234)\r\n    print &#39;&#39;\r\n    print day_suffix(time.strptime(&quot;1 Dec 00&quot;, &quot;%d %b %y&quot;))\r\n    print day_suffix(time.strptime(&quot;2 Nov 01&quot;, &quot;%d %b %y&quot;))\r\n    print day_suffix(time.strptime(&quot;3 Oct 02&quot;, &quot;%d %b %y&quot;))\r\n    print day_suffix(time.strptime(&quot;4 Sep 03&quot;, &quot;%d %b %y&quot;))\r\n    print day_suffix(time.strptime(&quot;13 Nov 90&quot;, &quot;%d %b %y&quot;))\r\n    print day_suffix(time.strptime(&quot;14 Oct 10&quot;, &quot;%d %b %y&quot;))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\r\n",
               "tags": [],
               "creation_date": 1318551208,
               "last_edit_date": 1318552120,
               "is_accepted": false,
               "id": "7761860",
               "down_vote_count": 2,
               "score": 20
            },
            {
               "up_vote_count": 79,
               "answer_id": 22128786,
               "last_activity_date": 1483039982,
               "path": "3.stack.answer",
               "body_markdown": "I have put together a project that can convert some really neat expressions. Check out **[timestring](http://github.com/stevepeak/timestring)**. \r\n\r\n## Here are some examples below:\r\n####`pip install timestring`\r\n\r\n\r\n    &gt;&gt;&gt; import timestring\r\n    &gt;&gt;&gt; timestring.Date(&#39;monday, aug 15th 2015 at 8:40 pm&#39;)\r\n    &lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\r\n    &gt;&gt;&gt; timestring.Date(&#39;monday, aug 15th 2015 at 8:40 pm&#39;).date\r\n    datetime.datetime(2015, 8, 15, 20, 40)\r\n    &gt;&gt;&gt; timestring.Range(&#39;next week&#39;)\r\n    &lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\r\n    &gt;&gt;&gt; (timestring.Range(&#39;next week&#39;).start.date, timestring.Range(&#39;next week&#39;).end.date)\r\n    (datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))",
               "tags": [],
               "creation_date": 1393770164,
               "last_edit_date": 1483039982,
               "is_accepted": false,
               "id": "22128786",
               "down_vote_count": 0,
               "score": 79
            },
            {
               "up_vote_count": 31,
               "answer_id": 22223725,
               "last_activity_date": 1433507698,
               "path": "3.stack.answer",
               "body_markdown": "Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.\r\n\r\nPython 3.2+:\r\n\r\n    &gt;&gt;&gt; datetime.datetime.strptime(\r\n    ...     &quot;March 5, 2014, 20:13:50&quot;, &quot;%B %d, %Y, %H:%M:%S&quot;\r\n    ... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))",
               "tags": [],
               "creation_date": 1394106785,
               "last_edit_date": 1433507698,
               "is_accepted": false,
               "id": "22223725",
               "down_vote_count": 2,
               "score": 29
            },
            {
               "up_vote_count": 10,
               "answer_id": 27046382,
               "last_activity_date": 1457012680,
               "path": "3.stack.answer",
               "body_markdown": "Django Timezone aware datetime object example.\r\n\r\n    import datetime\r\n    from django.utils.timezone import get_current_timezone\r\n    tz = get_current_timezone()\r\n    \r\n    format = &#39;%b %d %Y %I:%M%p&#39;\r\n    date_object = datetime.datetime.strptime(&#39;Jun 1 2005  1:33PM&#39;, format)\r\n    date_obj = tz.localize(date_object)\r\n\r\nThis conversion is very important for Django and Python when you have `USE_TZ = True`:\r\n\r\n    RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.",
               "tags": [],
               "creation_date": 1416506281,
               "last_edit_date": 1457012680,
               "is_accepted": false,
               "id": "27046382",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 27,
               "answer_id": 27401685,
               "last_activity_date": 1485760819,
               "path": "3.stack.answer",
               "body_markdown": "Remember this and you didn&#39;t need to get confused in datetime conversion again.\r\n\r\nString to datetime object = `strptime`\r\n\r\ndatetime object to other formats = `strftime`\r\n\r\n\r\n\r\n`Jun 1 2005  1:33PM`\r\n\r\nis equals to\r\n\r\n`%b %d %Y %I:%M%p`\r\n\r\n&gt; %b\tMonth as locale\u2019s abbreviated name(Jun)\r\n&gt; \r\n&gt; %d\tDay of the month as a zero-padded decimal number(1)\r\n&gt; \r\n&gt; %Y\tYear with century as a decimal number(2015)\r\n&gt; \r\n&gt; %I\tHour (12-hour clock) as a zero-padded decimal number(01)\r\n&gt; \r\n&gt; %M\tMinute as a zero-padded decimal number(33)\r\n&gt; \r\n&gt; %p\tLocale\u2019s equivalent of either AM or PM(PM)\r\n\r\nso you need strptime i-e converting `string` to \r\n\r\n\r\n    &gt;&gt;&gt; dates = []\r\n    &gt;&gt;&gt; dates.append(&#39;Jun 1 2005  1:33PM&#39;)\r\n    &gt;&gt;&gt; dates.append(&#39;Aug 28 1999 12:00AM&#39;)\r\n    &gt;&gt;&gt; from datetime import datetime\r\n    &gt;&gt;&gt; for d in dates:\r\n    ...     date = datetime.strptime(d, &#39;%b %d %Y %I:%M%p&#39;)\r\n    ...     print type(date)\r\n    ...     print date\r\n    ... \r\n\r\nOutput\r\n\r\n    &lt;type &#39;datetime.datetime&#39;&gt;\r\n    2005-06-01 13:33:00\r\n    &lt;type &#39;datetime.datetime&#39;&gt;\r\n    1999-08-28 00:00:00\r\n\r\n\r\nWhat if you have different format of dates you can use panda or dateutil.parse\r\n\r\n    &gt;&gt;&gt; import dateutil\r\n    &gt;&gt;&gt; dates = []\r\n    &gt;&gt;&gt; dates.append(&#39;12 1 2017&#39;)\r\n    &gt;&gt;&gt; dates.append(&#39;1 1 2017&#39;)\r\n    &gt;&gt;&gt; dates.append(&#39;1 12 2017&#39;)\r\n    &gt;&gt;&gt; dates.append(&#39;June 1 2017 1:30:00AM&#39;)\r\n    &gt;&gt;&gt; [parser.parse(x) for x in dates]\r\n\r\nOutPut\r\n\r\n    [datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\r\n\r\n",
               "tags": [],
               "creation_date": 1418216449,
               "last_edit_date": 1485760819,
               "is_accepted": false,
               "id": "27401685",
               "down_vote_count": 0,
               "score": 27
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 30577110,
               "is_accepted": false,
               "last_activity_date": 1433171702,
               "body_markdown": "You can use [easy_date][1] to make it easy:\r\n\r\n    import date_converter\r\n    converted_date = date_converter.string_to_datetime(&#39;Jun 1 2005  1:33PM&#39;, &#39;%b %d %Y %I:%M%p&#39;)\r\n\r\n\r\n  [1]: https://github.com/ralphavalon/easy_date",
               "id": "30577110",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1433171702,
               "score": 4
            },
            {
               "up_vote_count": 17,
               "answer_id": 34377575,
               "last_activity_date": 1512198529,
               "path": "3.stack.answer",
               "body_markdown": "Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.\r\n\r\n    import pandas as pd\r\n\r\n    dates = [&#39;2015-12-25&#39;, &#39;2015-12-26&#39;]\r\n    \r\n    # 1) Use a list comprehension.\r\n    &gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\r\n    [datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\r\n\r\n    # 2) Convert the dates to a DatetimeIndex and extract the python dates.\r\n    &gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\r\n    [datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\r\n\r\n**Timings**\r\n\r\n    dates = pd.DatetimeIndex(start=&#39;2000-1-1&#39;, end=&#39;2010-1-1&#39;, freq=&#39;d&#39;).date.tolist()\r\n\r\n    &gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\r\n    # 100 loops, best of 3: 3.11 ms per loop\r\n\r\n    &gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\r\n    # 100 loops, best of 3: 6.85 ms per loop\r\n\r\nAnd here is how to convert the OP&#39;s original date-time examples:\r\n\r\n    datetimes = [&#39;Jun 1 2005  1:33PM&#39;, &#39;Aug 28 1999 12:00AM&#39;]\r\n\r\n    &gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\r\n    [datetime.datetime(2005, 6, 1, 13, 33), \r\n     datetime.datetime(1999, 8, 28, 0, 0)]\r\n\r\nThere are many options for converting from the strings to Pandas Timestamps using `to_datetime`, so check the [docs][1] if you need anything special.\r\n\r\nLikewise, Timestamps have many [properties and methods][2] that can be accessed in addition to `.date`\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\r\n  [2]: http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties",
               "tags": [],
               "creation_date": 1450580605,
               "last_edit_date": 1512198529,
               "is_accepted": false,
               "id": "34377575",
               "down_vote_count": 0,
               "score": 17
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 34871180,
               "is_accepted": false,
               "last_activity_date": 1453189727,
               "body_markdown": "    In [34]: import datetime\r\n    \r\n    In [35]: _now = datetime.datetime.now()\r\n    \r\n    In [36]: _now\r\n    Out[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\r\n    \r\n    In [37]: print _now\r\n    2016-01-19 09:47:00.432000\r\n    \r\n    In [38]: _parsed = datetime.datetime.strptime(str(_now),&quot;%Y-%m-%d %H:%M:%S.%f&quot;)\r\n    \r\n    In [39]: _parsed\r\n    Out[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\r\n    \r\n    In [40]: assert _now == _parsed",
               "id": "34871180",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1453189727,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 35202640,
               "is_accepted": false,
               "last_activity_date": 1454593387,
               "body_markdown": "Create a small utility function like:\r\n\r\n    def date(datestr=&quot;&quot;, format=&quot;%Y-%m-%d&quot;):\r\n    \tfrom datetime import datetime\r\n    \tif not datestr:\r\n    \t\treturn datetime.today().date()\r\n    \treturn datetime.strptime(datestr, format).date()\r\n\r\nThis is versatile enough:\r\n\r\n - If you dont pass any arguments it will return today&#39;s date.\r\n - theres a date format as default that you can override.\r\n - You can easily modify it to return a datetime.",
               "id": "35202640",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1454593387,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 38005442,
               "is_accepted": false,
               "last_activity_date": 1466741247,
               "body_markdown": "The [datetime][1] python module is good for get date time and convert date time formats.\r\n\r\n    import datetime\r\n    \r\n    new_date_format1 = datetime.datetime.strptime(&#39;Jun 1 2005  1:33PM&#39;, &#39;%b %d %Y %I:%M%p&#39;)\r\n    new_date_format2 = datetime.datetime.strptime(&#39;Jun 1 2005  1:33PM&#39;, &#39;%b %d %Y %I:%M%p&#39;).strftime(&#39;%Y/%m/%d %I:%M%p&#39;)\r\n    print new_date_format1\r\n    print new_date_format2\r\n\r\noutput :\r\n\r\n    2005-06-01 13:33:00\r\n    2005/06/01 01:33PM\r\n\r\n  [1]: https://docs.python.org/2/library/datetime.html",
               "id": "38005442",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1466741247,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 42515333,
               "is_accepted": false,
               "last_activity_date": 1488303459,
               "body_markdown": "This question has been visited many times but **arrow** hasn&#39;t been mentioned, and it offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.\r\n\r\n    &gt;&gt;&gt; import arrow\r\n    &gt;&gt;&gt; dateStrings = [ &#39;Jun 1  2005 1:33PM&#39;, &#39;Aug 28 1999 12:00AM&#39; ]\r\n    &gt;&gt;&gt; for dateString in dateStrings:\r\n    ... \tdateString\r\n    ... \tarrow.get(dateString.replace(&#39;  &#39;,&#39; &#39;), &#39;MMM D YYYY H:mmA&#39;).datetime\r\n    ... \tarrow.get(dateString.replace(&#39;  &#39;,&#39; &#39;), &#39;MMM D YYYY H:mmA&#39;).format(&#39;ddd, Do MMM YYYY HH:mm&#39;)\r\n    ... \tarrow.get(dateString.replace(&#39;  &#39;,&#39; &#39;), &#39;MMM D YYYY H:mmA&#39;).humanize(locale=&#39;de&#39;)\r\n    ... \t\r\n    &#39;Jun 1  2005 1:33PM&#39;\r\n    datetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\r\n    &#39;Wed, 1st Jun 2005 13:33&#39;\r\n    &#39;vor 11 Jahren&#39;\r\n    &#39;Aug 28 1999 12:00AM&#39;\r\n    datetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\r\n    &#39;Sat, 28th Aug 1999 00:00&#39;\r\n    &#39;vor 17 Jahren&#39;\r\n\r\nSee http://arrow.readthedocs.io/en/latest/ for more.",
               "id": "42515333",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1488303459,
               "score": 6
            },
            {
               "up_vote_count": 1,
               "answer_id": 47876476,
               "last_activity_date": 1513655177,
               "path": "3.stack.answer",
               "body_markdown": "See [my answer](https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446).\r\n\r\nIn real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It&#39;s not ok for production code to fail, let alone go exception-happy like a fox.\r\n\r\nWe need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from `strptime()`) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). See [my solution](https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446)",
               "tags": [],
               "creation_date": 1513631523,
               "last_edit_date": 1513655177,
               "is_accepted": false,
               "id": "47876476",
               "down_vote_count": 1,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47925290,
               "is_accepted": false,
               "last_activity_date": 1513860251,
               "body_markdown": "if you want only date format then you can manually convert it my passing your individual fields like:\r\n     \r\n    &gt;&gt;&gt; import datetime\r\n    &gt;&gt;&gt; date = datetime.date(int(&#39;2017&#39;),int(&#39;12&#39;),int(&#39;21&#39;))\r\n    &gt;&gt;&gt; date\r\n    datetime.date(2017, 12, 21)\r\n    &gt;&gt;&gt; type(date)\r\n    &lt;type &#39;datetime.date&#39;&gt;\r\n\r\nyou can pass your splited string values to convert it into date type like:\r\n\r\n    selected_month_rec = &#39;2017-09-01&#39;\r\n    date_formate = datetime.date(int(selected_month_rec.split(&#39;-&#39;)[0]),int(selected_month_rec.split(&#39;-&#39;)[1]),int(selected_month_rec.split(&#39;-&#39;)[2]))\r\n\r\nyou will get the resultent value in date formate.",
               "id": "47925290",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1513860251,
               "score": -1
            },
            {
               "up_vote_count": 3,
               "answer_id": 48054268,
               "last_activity_date": 1515209405,
               "path": "3.stack.answer",
               "body_markdown": "I personally like the solution using the `parser` module, which is the second Answer to this Question and is beautiful, as you don&#39;t have to construct any string literals to get it working. However, one downside,  it is `90%` slower then the accepted answer with `strptime`. \r\n\r\n    from dateutil import parser\r\n    from datetime import datetime\r\n    import timeit\r\n    \r\n    def dt():\r\n    \tdt = parser.parse(&quot;Jun 1 2005  1:33PM&quot;)\r\n    def strptime():\r\n    \tdatetime_object = datetime.strptime(&#39;Jun 1 2005  1:33PM&#39;, &#39;%b %d %Y %I:%M%p&#39;)\r\n    \r\n    print(timeit.timeit(stmt=dt, number=10**5))\r\n    print(timeit.timeit(stmt=strptime, number=10**5))\r\n    &gt;10.70296801342902\r\n    &gt;1.3627995655316933\r\n\r\nAs long as you are not doing this `a million` times over and over again, i stil ll  think the `parser` method is more convenient and will handle most of the time formats automatically. ",
               "tags": [],
               "creation_date": 1514851459,
               "last_edit_date": 1515209405,
               "is_accepted": false,
               "id": "48054268",
               "down_vote_count": 0,
               "score": 3
            }
         ],
         "link": "https://stackoverflow.com/questions/466345/converting-string-into-datetime",
         "id": "858127-2304"
      },
      {
         "up_vote_count": "251",
         "path": "2.stack",
         "body_markdown": "I have a Python pandas DataFrame `rpt`:\r\n\r\n    rpt\r\n    &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\r\n    MultiIndex: 47518 entries, (&#39;000002&#39;, &#39;20120331&#39;) to (&#39;603366&#39;, &#39;20091231&#39;)\r\n    Data columns:\r\n    STK_ID                    47518  non-null values\r\n    STK_Name                  47518  non-null values\r\n    RPT_Date                  47518  non-null values\r\n    sales                     47518  non-null values\r\n\r\nI can filter the rows whose stock id is `&#39;600809&#39;` like this: `rpt[rpt[&#39;STK_ID&#39;] == &#39;600809&#39;]`\r\n\r\n    &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\r\n    MultiIndex: 25 entries, (&#39;600809&#39;, &#39;20120331&#39;) to (&#39;600809&#39;, &#39;20060331&#39;)\r\n    Data columns:\r\n    STK_ID                    25  non-null values\r\n    STK_Name                  25  non-null values\r\n    RPT_Date                  25  non-null values\r\n    sales                     25  non-null values\r\n\r\n\r\nand I want to get all the rows of some stocks together, such as `[&#39;600809&#39;,&#39;600141&#39;,&#39;600329&#39;]`. That means I want a syntax like this: \r\n\r\n    stk_list = [&#39;600809&#39;,&#39;600141&#39;,&#39;600329&#39;]\r\n    \r\n    rst = rpt[rpt[&#39;STK_ID&#39;] in stk_list] # this does not works in pandas \r\n\r\nSince pandas not accept above command, how to achieve the target? ",
         "view_count": "157761",
         "answer_count": "7",
         "tags": "['python', 'pandas', 'dataframe']",
         "creation_date": "1345605416",
         "last_edit_date": "1493299689",
         "code_snippet": "['<code>rpt</code>', \"<code>rpt\\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nMultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')\\nData columns:\\nSTK_ID                    47518  non-null values\\nSTK_Name                  47518  non-null values\\nRPT_Date                  47518  non-null values\\nsales                     47518  non-null values\\n</code>\", \"<code>'600809'</code>\", \"<code>rpt[rpt['STK_ID'] == '600809']</code>\", \"<code>&lt;class 'pandas.core.frame.DataFrame'&gt;\\nMultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')\\nData columns:\\nSTK_ID                    25  non-null values\\nSTK_Name                  25  non-null values\\nRPT_Date                  25  non-null values\\nsales                     25  non-null values\\n</code>\", \"<code>['600809','600141','600329']</code>\", \"<code>stk_list = ['600809','600141','600329']\\n\\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \\n</code>\", '<code>isin</code>', \"<code>rpt[rpt['STK_ID'].isin(stk_list)]</code>\", '<code>!isin()</code>', '<code>~</code>', \"<code>rpt[~rpt['STK_ID'].isin(stk_list)]</code>\", '<code>isin</code>', '<code>df[df.index.isin(ls)]</code>', '<code>isin()</code>', '<code>str.contains</code>', \"<code>'600'</code>\", \"<code>&gt;&gt;&gt; rpt[rpt['STK_ID'].str.contains(r'^600[0-9]{3}$')] # ^ means start of string\\n...   STK_ID   ...                                    # [0-9]{3} means any three digits\\n...  '600809'  ...                                    # $ means end of string\\n...  '600141'  ...\\n...  '600329'  ...\\n...      ...   ...\\n</code>\", \"<code>'STK_ID'</code>\", \"<code>endstrings = ['01$', '02$', '05$']\\n</code>\", '<code>|</code>', '<code>str.contains</code>', \"<code>&gt;&gt;&gt; rpt[rpt['STK_ID'].str.contains('|'.join(endstrings)]\\n...   STK_ID   ...\\n...  '155905'  ...\\n...  '633101'  ...\\n...  '210302'  ...\\n...      ...   ...\\n</code>\", '<code>contains</code>', '<code>case=False</code>', \"<code>str.contains('pandas', case=False)\\n</code>\", '<code>PANDAS</code>', '<code>PanDAs</code>', '<code>paNdAs123</code>', \"<code>b = df[(df['a'] &gt; 1) &amp; (df['a'] &lt; 5)]\\n</code>\", \"<code>rpt.query('STK_ID in (600809,600141,600329)')\\n</code>\", \"<code>rpt.query('60000 &lt; STK_ID &lt; 70000')\\n</code>\", '<code>my_list</code>', \"<code>rpt.query('STK_ID in @my_list')</code>\", '<code>    RPT_Date  STK_ID STK_Name  sales\\n0 1980-01-01       0   Arthur      0\\n1 1980-01-02       1    Beate      4\\n2 1980-01-03       2    Cecil      2\\n3 1980-01-04       3     Dana      8\\n4 1980-01-05       4     Eric      4\\n5 1980-01-06       5    Fidel      5\\n6 1980-01-07       6   George      4\\n7 1980-01-08       7     Hans      7\\n8 1980-01-09       8   Ingrid      7\\n9 1980-01-10       9    Jones      4\\n</code>', '<code>.isin</code>', '<code>True</code>', '<code>False</code>', \"<code>mask = df['STK_ID'].isin([4, 2, 6])\\n\\nmask\\n0    False\\n1    False\\n2     True\\n3    False\\n4     True\\n5    False\\n6     True\\n7    False\\n8    False\\n9    False\\nName: STK_ID, dtype: bool\\n\\ndf[mask]\\n    RPT_Date  STK_ID STK_Name  sales\\n2 1980-01-03       2    Cecil      2\\n4 1980-01-05       4     Eric      4\\n6 1980-01-07       6   George      4\\n</code>\", '<code>STK_ID</code>', '<code>.loc</code>', \"<code>df.set_index('STK_ID', inplace=True)\\n         RPT_Date STK_Name  sales\\nSTK_ID                           \\n0      1980-01-01   Arthur      0\\n1      1980-01-02    Beate      4\\n2      1980-01-03    Cecil      2\\n3      1980-01-04     Dana      8\\n4      1980-01-05     Eric      4\\n5      1980-01-06    Fidel      5\\n6      1980-01-07   George      4\\n7      1980-01-08     Hans      7\\n8      1980-01-09   Ingrid      7\\n9      1980-01-10    Jones      4\\n\\ndf.loc[[4, 2, 6]]\\n         RPT_Date STK_Name  sales\\nSTK_ID                           \\n4      1980-01-05     Eric      4\\n2      1980-01-03    Cecil      2\\n6      1980-01-07   George      4\\n</code>\", '<code>stkid_df = pd.DataFrame({\"STK_ID\": [4,2,6]})\\ndf.merge(stkid_df, on=\\'STK_ID\\')\\n   STK_ID   RPT_Date STK_Name  sales\\n0       2 1980-01-03    Cecil      2\\n1       4 1980-01-05     Eric      4\\n2       6 1980-01-07   George      4\\n</code>', \"<code>'STK_ID'</code>\", '<code>query</code>', \"<code>b = df.query('a &gt; 1 &amp; a &lt; 5')\\n</code>\", '<code>df = pd.DataFrame({\\'A\\': [1, 2, 3], \\'B\\': [\\'a\\', \\'b\\', \\'f\\']})\\ndf = pd.DataFrame({\\'A\\' : [5,6,3,4], \\'B\\' : [1,2,3, 5]})\\nlist_of_values = [3,6]\\nresult= df.query(\"A in @list_of_values\")\\nresult\\n   A  B\\n1  6  2\\n2  3  3\\n</code>']",
         "title": "Filter dataframe rows if value in column is in a set list of values",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 370,
               "answer_id": 12065904,
               "is_accepted": true,
               "last_activity_date": 1345605672,
               "body_markdown": "Use the `isin` method.  `rpt[rpt[&#39;STK_ID&#39;].isin(stk_list)]`.",
               "id": "12065904",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1345605672,
               "score": 370
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 37,
               "answer_id": 19295726,
               "is_accepted": false,
               "last_activity_date": 1381407989,
               "body_markdown": "you can also use ranges by using:\r\n\r\n    b = df[(df[&#39;a&#39;] &gt; 1) &amp; (df[&#39;a&#39;] &lt; 5)]\r\n",
               "id": "19295726",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1381407989,
               "score": 37
            },
            {
               "up_vote_count": 78,
               "answer_id": 26724725,
               "last_activity_date": 1428863367,
               "path": "3.stack.answer",
               "body_markdown": "`isin()` is ideal if you have a list of exact matches, but if you have a list of partial matches or substrings to look for, you can filter using the [`str.contains`](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.strings.StringMethods.contains.html) method and regular expressions.\r\n\r\nFor example, if we want to return a DataFrame where all of the stock IDs which begin with `&#39;600&#39;` and then are followed by any three digits:\r\n\r\n    &gt;&gt;&gt; rpt[rpt[&#39;STK_ID&#39;].str.contains(r&#39;^600[0-9]{3}$&#39;)] # ^ means start of string\r\n    ...   STK_ID   ...                                    # [0-9]{3} means any three digits\r\n    ...  &#39;600809&#39;  ...                                    # $ means end of string\r\n    ...  &#39;600141&#39;  ...\r\n    ...  &#39;600329&#39;  ...\r\n    ...      ...   ...\r\n\r\nSuppose now we have a list of strings which we want the values in `&#39;STK_ID&#39;` to end with, e.g.\r\n \r\n    endstrings = [&#39;01$&#39;, &#39;02$&#39;, &#39;05$&#39;]\r\n\r\nWe can join these strings with the regex &#39;or&#39; character `|` and pass the string to `str.contains` to filter the DataFrame: \r\n\r\n    &gt;&gt;&gt; rpt[rpt[&#39;STK_ID&#39;].str.contains(&#39;|&#39;.join(endstrings)]\r\n    ...   STK_ID   ...\r\n    ...  &#39;155905&#39;  ...\r\n    ...  &#39;633101&#39;  ...\r\n    ...  &#39;210302&#39;  ...\r\n    ...      ...   ...\r\n\r\nFinally, `contains` can ignore case (by setting `case=False`), allowing you to be more general when specifying the strings you want to match.\r\n\r\nFor example,\r\n\r\n    str.contains(&#39;pandas&#39;, case=False)\r\n\r\nwould match `PANDAS`, `PanDAs`, `paNdAs123`, and so on.\r\n\r\n",
               "tags": [],
               "creation_date": 1415054967,
               "last_edit_date": 1428863367,
               "is_accepted": false,
               "id": "26724725",
               "down_vote_count": 0,
               "score": 78
            },
            {
               "up_vote_count": 33,
               "answer_id": 29108799,
               "last_activity_date": 1445960391,
               "path": "3.stack.answer",
               "body_markdown": "You can also directly [query](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html) your DataFrame for this information.\r\n\r\n    rpt.query(&#39;STK_ID in (600809,600141,600329)&#39;)\r\n\r\nOr similarly search for ranges:\r\n\r\n    rpt.query(&#39;60000 &lt; STK_ID &lt; 70000&#39;)",
               "tags": [],
               "creation_date": 1426623130,
               "last_edit_date": 1445960391,
               "is_accepted": false,
               "id": "29108799",
               "down_vote_count": 0,
               "score": 33
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 43643654,
               "is_accepted": false,
               "last_activity_date": 1493237352,
               "body_markdown": "You can use `query`, i.e.:\r\n\r\n    b = df.query(&#39;a &gt; 1 &amp; a &lt; 5&#39;)",
               "id": "43643654",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1493237352,
               "score": 3
            },
            {
               "up_vote_count": 11,
               "answer_id": 43659192,
               "last_activity_date": 1493299850,
               "path": "3.stack.answer",
               "body_markdown": "#Slicing data with pandas\r\n\r\nGiven a dataframe like this:\r\n\r\n        RPT_Date  STK_ID STK_Name  sales\r\n    0 1980-01-01       0   Arthur      0\r\n    1 1980-01-02       1    Beate      4\r\n    2 1980-01-03       2    Cecil      2\r\n    3 1980-01-04       3     Dana      8\r\n    4 1980-01-05       4     Eric      4\r\n    5 1980-01-06       5    Fidel      5\r\n    6 1980-01-07       6   George      4\r\n    7 1980-01-08       7     Hans      7\r\n    8 1980-01-09       8   Ingrid      7\r\n    9 1980-01-10       9    Jones      4\r\n\r\nThere are multiple ways of selecting or slicing the data.\r\n\r\n###Using .isin\r\nThe most obvious is the `.isin` feature. You can create a mask that gives you a series of `True`/`False` statements, which can be applied to a dataframe like this:\r\n\r\n    mask = df[&#39;STK_ID&#39;].isin([4, 2, 6])\r\n    \r\n    mask\r\n    0    False\r\n    1    False\r\n    2     True\r\n    3    False\r\n    4     True\r\n    5    False\r\n    6     True\r\n    7    False\r\n    8    False\r\n    9    False\r\n    Name: STK_ID, dtype: bool\r\n    \r\n    df[mask]\r\n        RPT_Date  STK_ID STK_Name  sales\r\n    2 1980-01-03       2    Cecil      2\r\n    4 1980-01-05       4     Eric      4\r\n    6 1980-01-07       6   George      4\r\n\r\nMasking is the ad-hoc solution to the problem, but does not always perform well in terms of speed and memory.\r\n\r\n###With indexing\r\n\r\nBy setting the index to the `STK_ID` column, we can use the pandas builtin slicing object `.loc`\r\n\r\n\r\n    df.set_index(&#39;STK_ID&#39;, inplace=True)\r\n             RPT_Date STK_Name  sales\r\n    STK_ID                           \r\n    0      1980-01-01   Arthur      0\r\n    1      1980-01-02    Beate      4\r\n    2      1980-01-03    Cecil      2\r\n    3      1980-01-04     Dana      8\r\n    4      1980-01-05     Eric      4\r\n    5      1980-01-06    Fidel      5\r\n    6      1980-01-07   George      4\r\n    7      1980-01-08     Hans      7\r\n    8      1980-01-09   Ingrid      7\r\n    9      1980-01-10    Jones      4\r\n\r\n    df.loc[[4, 2, 6]]\r\n             RPT_Date STK_Name  sales\r\n    STK_ID                           \r\n    4      1980-01-05     Eric      4\r\n    2      1980-01-03    Cecil      2\r\n    6      1980-01-07   George      4\r\n\r\nThis is the fast way of doing it, even if the indexing can take a little while, it saves time if you want to do multiple queries like this.\r\n\r\n###Merging dataframes\r\n\r\nThis can also be done by merging dataframes. This would fit more for a scenario where you have a lot more data than in these examples.\r\n\r\n    stkid_df = pd.DataFrame({&quot;STK_ID&quot;: [4,2,6]})\r\n    df.merge(stkid_df, on=&#39;STK_ID&#39;)\r\n       STK_ID   RPT_Date STK_Name  sales\r\n    0       2 1980-01-03    Cecil      2\r\n    1       4 1980-01-05     Eric      4\r\n    2       6 1980-01-07   George      4\r\n\r\n# Note\r\n\r\nAll the above methods work even if there are multiple rows with the same `&#39;STK_ID&#39;`",
               "tags": [],
               "creation_date": 1493299457,
               "last_edit_date": 1493299850,
               "is_accepted": false,
               "id": "43659192",
               "down_vote_count": 0,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 46460307,
               "is_accepted": false,
               "last_activity_date": 1506567879,
               "body_markdown": "You can also achieve similar results by using &#39;query&#39; and @&lt;your list of values&gt;:\r\n\r\neg:\r\n\r\n\r\n    df = pd.DataFrame({&#39;A&#39;: [1, 2, 3], &#39;B&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;f&#39;]})\r\n    df = pd.DataFrame({&#39;A&#39; : [5,6,3,4], &#39;B&#39; : [1,2,3, 5]})\r\n    list_of_values = [3,6]\r\n    result= df.query(&quot;A in @list_of_values&quot;)\r\n    result\r\n       A  B\r\n    1  6  2\r\n    2  3  3\r\n",
               "id": "46460307",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1506567879,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/12065885/filter-dataframe-rows-if-value-in-column-is-in-a-set-list-of-values",
         "id": "858127-2305"
      },
      {
         "up_vote_count": "570",
         "path": "2.stack",
         "body_markdown": "How to select rows from a DataFrame based on values in some column in pandas?  \r\nIn SQL I would use: \r\n\r\n\r\n    select * from table where colume_name = some_value. \r\n\r\n\r\n*I tried to look at pandas documentation but did not immediately find the answer.*\r\n",
         "view_count": "689214",
         "answer_count": "11",
         "tags": "['python', 'pandas', 'dataframe']",
         "creation_date": "1371058925",
         "last_edit_date": "1460929746",
         "code_snippet": "['<code>select * from table where colume_name = some_value. \\n</code>', '<code>some_value</code>', '<code>==</code>', \"<code>df.loc[df['column_name'] == some_value]\\n</code>\", '<code>some_values</code>', '<code>isin</code>', \"<code>df.loc[df['column_name'].isin(some_values)]\\n</code>\", '<code>&amp;</code>', \"<code>df.loc[(df['column_name'] == some_value) &amp; df['other_column'].isin(some_values)]\\n</code>\", '<code>some_value</code>', '<code>!=</code>', \"<code>df.loc[df['column_name'] != some_value]\\n</code>\", '<code>isin</code>', '<code>some_values</code>', '<code>~</code>', \"<code>df.loc[~df['column_name'].isin(some_values)]\\n</code>\", \"<code>import pandas as pd\\nimport numpy as np\\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\\n                   'B': 'one one two three two two one three'.split(),\\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\\nprint(df)\\n#      A      B  C   D\\n# 0  foo    one  0   0\\n# 1  bar    one  1   2\\n# 2  foo    two  2   4\\n# 3  bar  three  3   6\\n# 4  foo    two  4   8\\n# 5  bar    two  5  10\\n# 6  foo    one  6  12\\n# 7  foo  three  7  14\\n\\nprint(df.loc[df['A'] == 'foo'])\\n</code>\", '<code>     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>', '<code>isin</code>', \"<code>print(df.loc[df['B'].isin(['one','three'])])\\n</code>\", '<code>     A      B  C   D\\n0  foo    one  0   0\\n1  bar    one  1   2\\n3  bar  three  3   6\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>', '<code>df.loc</code>', \"<code>df = df.set_index(['B'])\\nprint(df.loc['one'])\\n</code>\", '<code>       A  C   D\\nB              \\none  foo  0   0\\none  bar  1   2\\none  foo  6  12\\n</code>', '<code>df.index.isin</code>', \"<code>df.loc[df.index.isin(['one','two'])]\\n</code>\", '<code>       A  C   D\\nB              \\none  foo  0   0\\none  bar  1   2\\ntwo  foo  2   4\\ntwo  foo  4   8\\ntwo  bar  5  10\\none  foo  6  12\\n</code>', '<code>df.where(condition)</code>', '<code>df</code>', '<code>select * from table where column_name = some_value\\n</code>', '<code>table[table.column_name == some_value]\\n</code>', '<code>table((table.column_name == some_value) | (table.column_name2 == some_value2))\\n</code>', \"<code>table.query('column_name == some_value | column_name2 == some_value2')\\n</code>\", \"<code>import pandas as pd\\n\\n# Create data set\\nd = {'foo':[100, 111, 222], \\n     'bar':[333, 444, 555]}\\ndf = pd.DataFrame(d)\\n\\n# Full dataframe:\\ndf\\n\\n# Shows:\\n#    bar   foo \\n# 0  333   100\\n# 1  444   111\\n# 2  555   222\\n\\n# Output only the row(s) in df where foo is 222:\\ndf[df.foo == 222]\\n\\n# Shows:\\n#    bar  foo\\n# 2  555  222\\n</code>\", '<code>df[df.foo == 222]</code>', '<code>222</code>', '<code>df[(df.foo == 222) | (df.bar == 444)]\\n#    bar  foo\\n# 1  444  111\\n# 2  555  222\\n</code>', \"<code>df.query('foo == 222 | bar == 444')\\n</code>\", '<code>numpy</code>', '<code>column_name == some_value</code>', \"<code>import pandas as pd, numpy as np\\n\\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\\n                   'B': 'one one two three two two one three'.split(),\\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\\n</code>\", \"<code>'A'</code>\", \"<code>'foo'</code>\", \"<code>'A'</code>\", \"<code>'foo'</code>\", '<code>mask</code>', \"<code>mask = df['A'] == 'foo'\\n</code>\", '<code>df[mask]\\n\\n     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>', '<code>mask</code>', \"<code>mask = df['A'] == 'foo'\\npos = np.flatnonzero(mask)\\ndf.iloc[pos]\\n\\n     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>\", \"<code>df.set_index('A', append=True, drop=False).xs('foo', level=1)\\n\\n     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>\", '<code>pd.DataFrame.query</code>', '<code>df.query(\\'A == \"foo\"\\')\\n\\n     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>', '<code>Boolean</code>', '<code>mask</code>', '<code>Boolean</code>', '<code>mask</code>', '<code>mask</code>', '<code>numpy</code>', '<code>pd.Series</code>', \"<code>mask = df['A'].values == 'foo'\\n</code>\", '<code>mask</code>', \"<code>%timeit mask = df['A'].values == 'foo'\\n%timeit mask = df['A'] == 'foo'\\n\\n5.84 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\\n166 \u00b5s \u00b1 4.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\\n</code>\", '<code>mask</code>', '<code>numpy</code>', '<code>numpy</code>', '<code>pd.Series</code>', '<code>mask</code>', \"<code>mask = df['A'].values == 'foo'\\n%timeit df[mask]\\nmask = df['A'] == 'foo'\\n%timeit df[mask]\\n\\n219 \u00b5s \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\\n239 \u00b5s \u00b1 7.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\\n</code>\", '<code>mask</code>', '<code>dtypes</code>', '<code>df[mask]</code>', '<code>pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\\n</code>', '<code>df.values</code>', '<code>dtype</code>', '<code>object</code>', '<code>dtype</code>', '<code>object</code>', '<code>astype(df.dtypes)</code>', '<code>%timeit df[m]\\n%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\\n\\n216 \u00b5s \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\\n1.43 ms \u00b1 39.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\\n</code>', \"<code>np.random.seed([3,1415])\\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\\n\\nd1\\n\\n   A  B  C  D  E\\n0  0  2  7  3  8\\n1  7  0  6  8  6\\n2  0  2  0  4  9\\n3  7  3  2  4  3\\n4  3  6  7  7  4\\n5  5  3  7  5  9\\n6  8  7  6  4  7\\n7  6  2  6  6  5\\n8  2  8  7  5  8\\n9  4  7  6  1  5    \\n</code>\", \"<code>%%timeit\\nmask = d1['A'].values == 7\\nd1[mask]\\n\\n179 \u00b5s \u00b1 8.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\\n</code>\", \"<code>%%timeit\\nmask = d1['A'].values == 7\\npd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)\\n\\n87 \u00b5s \u00b1 5.12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\\n</code>\", '<code>mask</code>', '<code>pd.Series.isin</code>', \"<code>df['A']</code>\", \"<code>'foo'</code>\", \"<code>mask = df['A'].isin(['foo'])\\ndf[mask]\\n\\n     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>\", '<code>numpy</code>', '<code>np.in1d</code>', \"<code>mask = np.in1d(df['A'].values, ['foo'])\\ndf[mask]\\n\\n     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>\", '<code>1.0</code>', '<code>res.div(res.min())\\n\\n                         10        30        100       300       1000      3000      10000     30000\\nmask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151\\nmask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103\\nmask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919\\nmask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000\\nquery                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190\\nxs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255\\nmask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760\\nmask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175\\n</code>', '<code>mask_with_values</code>', '<code>mask_with_in1d</code>', '<code>res.T.plot(loglog=True)\\n</code>', '<code>def mask_standard(df):\\n    mask = df[\\'A\\'] == \\'foo\\'\\n    return df[mask]\\n\\ndef mask_standard_loc(df):\\n    mask = df[\\'A\\'] == \\'foo\\'\\n    return df.loc[mask]\\n\\ndef mask_with_values(df):\\n    mask = df[\\'A\\'].values == \\'foo\\'\\n    return df[mask]\\n\\ndef mask_with_values_loc(df):\\n    mask = df[\\'A\\'].values == \\'foo\\'\\n    return df.loc[mask]\\n\\ndef query(df):\\n    return df.query(\\'A == \"foo\"\\')\\n\\ndef xs_label(df):\\n    return df.set_index(\\'A\\', append=True, drop=False).xs(\\'foo\\', level=-1)\\n\\ndef mask_with_isin(df):\\n    mask = df[\\'A\\'].isin([\\'foo\\'])\\n    return df[mask]\\n\\ndef mask_with_in1d(df):\\n    mask = np.in1d(df[\\'A\\'].values, [\\'foo\\'])\\n    return df[mask]\\n</code>', \"<code>res = pd.DataFrame(\\n    index=[\\n        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',\\n        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'\\n    ],\\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\\n    dtype=float\\n)\\n\\nfor j in res.columns:\\n    d = pd.concat([df] * j, ignore_index=True)\\n    for i in res.index:a\\n        stmt = '{}(d)'.format(i)\\n        setp = 'from __main__ import d, {}'.format(i)\\n        res.at[i, j] = timeit(stmt, setp, number=50)\\n</code>\", '<code>dtype</code>', '<code>spec.div(spec.min())\\n\\n                     10        30        100       300       1000      3000      10000     30000\\nmask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000\\nmask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100\\nreconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735\\n</code>', '<code>spec.T.plot(loglog=True)\\n</code>', \"<code>np.random.seed([3,1415])\\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\\n\\ndef mask_with_values(df):\\n    mask = df['A'].values == 'foo'\\n    return df[mask]\\n\\ndef mask_with_in1d(df):\\n    mask = np.in1d(df['A'].values, ['foo'])\\n    return df[mask]\\n\\ndef reconstruct(df):\\n    v = df.values\\n    mask = np.in1d(df['A'].values, ['foo'])\\n    return pd.DataFrame(v[mask], df.index[mask], df.columns)\\n\\nspec = pd.DataFrame(\\n    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],\\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\\n    dtype=float\\n)\\n</code>\", \"<code>for j in spec.columns:\\n    d = pd.concat([df] * j, ignore_index=True)\\n    for i in spec.index:\\n        stmt = '{}(d)'.format(i)\\n        setp = 'from __main__ import d, {}'.format(i)\\n        spec.at[i, j] = timeit(stmt, setp, number=50)\\n</code>\", '<code>query()</code>', \"<code>df.query('col == val')</code>\", \"<code>In [167]: n = 10\\n\\nIn [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\\n\\nIn [169]: df\\nOut[169]: \\n          a         b         c\\n0  0.687704  0.582314  0.281645\\n1  0.250846  0.610021  0.420121\\n2  0.624328  0.401816  0.932146\\n3  0.011763  0.022921  0.244186\\n4  0.590198  0.325680  0.890392\\n5  0.598892  0.296424  0.007312\\n6  0.634625  0.803069  0.123872\\n7  0.924168  0.325076  0.303746\\n8  0.116822  0.364564  0.454607\\n9  0.986142  0.751953  0.561512\\n\\n# pure python\\nIn [170]: df[(df.a &lt; df.b) &amp; (df.b &lt; df.c)]\\nOut[170]: \\n          a         b         c\\n3  0.011763  0.022921  0.244186\\n8  0.116822  0.364564  0.454607\\n\\n# query\\nIn [171]: df.query('(a &lt; b) &amp; (b &lt; c)')\\nOut[171]: \\n          a         b         c\\n3  0.011763  0.022921  0.244186\\n8  0.116822  0.364564  0.454607\\n</code>\", '<code>@</code>', \"<code>exclude = ('red', 'orange')\\ndf.query('color not in @exclude')\\n</code>\", '<code>numexpr</code>', '<code>from pandas import DataFrame\\n\\n# Create data set\\nd = {\\'Revenue\\':[100,111,222], \\n     \\'Cost\\':[333,444,555]}\\ndf = DataFrame(d)\\n\\n\\n# mask = Return True when the value in column \"Revenue\" is equal to 111\\nmask = df[\\'Revenue\\'] == 111\\n\\nprint mask\\n\\n# Result:\\n# 0    False\\n# 1     True\\n# 2    False\\n# Name: Revenue, dtype: bool\\n\\n\\n# Select * FROM df WHERE Revenue = 111\\ndf[mask]\\n\\n# Result:\\n#    Cost    Revenue\\n# 1  444     111\\n</code>', \"<code>df.loc[df['column_name'] == some_value]\\n</code>\", \"<code>df.loc[df['column_name'].isin(some_values)]\\n</code>\", \"<code>import pandas as pd\\nimport numpy as np\\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\\n               'B': 'one one two three two two one three'.split(),\\n               'C': np.arange(8), 'D': np.arange(8) * 2})\\nprint(df)\\n#      A      B  C   D\\n# 0  foo    one  0   0\\n# 1  bar    one  1   2\\n# 2  foo    two  2   4\\n# 3  bar  three  3   6\\n# 4  foo    two  4   8\\n# 5  bar    two  5  10\\n# 6  foo    one  6  12\\n# 7  foo  three  7  14\\n\\nprint(df.loc[df['A'] == 'foo'])\\n</code>\", '<code>     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>', \"<code>print(df.loc[df['B'].isin(['one','three'])])\\n</code>\", '<code>      A      B  C   D\\n0  foo    one  0   0\\n1  bar    one  1   2\\n3  bar  three  3   6\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>', \"<code>df = df.set_index(['A'])\\nprint(df.loc['foo'])\\n</code>\", '<code>  A      B  C   D\\nfoo    one  0   0\\nfoo    two  2   4\\nfoo    two  4   8\\nfoo    one  6  12\\nfoo  three  7  14\\n</code>', \"<code>In [76]: df.iloc[np.where(df.A.values=='foo')]\\nOut[76]: \\n     A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>\", '<code>In [68]: %timeit df.iloc[np.where(df.A.values==\\'foo\\')]  # fastest\\n1000 loops, best of 3: 380 \u00b5s per loop\\n\\nIn [69]: %timeit df.loc[df[\\'A\\'] == \\'foo\\']\\n1000 loops, best of 3: 745 \u00b5s per loop\\n\\nIn [71]: %timeit df.loc[df[\\'A\\'].isin([\\'foo\\'])]\\n1000 loops, best of 3: 562 \u00b5s per loop\\n\\nIn [72]: %timeit df[df.A==\\'foo\\']\\n1000 loops, best of 3: 796 \u00b5s per loop\\n\\nIn [74]: %timeit df.query(\\'(A==\"foo\")\\')  # slowest\\n1000 loops, best of 3: 1.71 ms per loop\\n</code>', \"<code>df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\\n                   'B': 'one one two three two two one three'.split(),\\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\\ndf[df['A']=='foo']\\n\\nOUTPUT:\\n   A      B  C   D\\n0  foo    one  0   0\\n2  foo    two  2   4\\n4  foo    two  4   8\\n6  foo    one  6  12\\n7  foo  three  7  14\\n</code>\", \"<code>df.loc[~df['column_name'].isin(some_values)]\\n</code>\", '<code>!=</code>', \"<code>import pandas as pd\\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\\n                   'B': 'one one two three two two one three'.split()})\\nprint(df)\\n</code>\", '<code>     A      B\\n0  foo    one\\n1  bar    one\\n2  foo    two\\n3  bar  three\\n4  foo    two\\n5  bar    two\\n6  foo    one\\n7  foo  three    \\n</code>', '<code>one</code>', '<code>three</code>', '<code>B</code>', \"<code>df.loc[~df['B'].isin(['one', 'three'])]\\n</code>\", '<code>     A    B\\n2  foo  two\\n4  foo  two\\n5  bar  two\\n</code>', \"<code>df.groupby('column_name').get_group('column_desired_value').reset_index()</code>\", '<code>import pandas as pd\\ndf = pd.DataFrame({\\'A\\': \\'foo bar foo bar foo bar foo foo\\'.split(),\\n                   \\'B\\': \\'one one two three two two one three\\'.split()})\\nprint(\"Original dataframe:\")\\nprint(df)\\n\\nb_is_two_dataframe = pd.DataFrame(df.groupby(\\'B\\').get_group(\\'two\\').reset_index()).drop(\\'index\\', axis = 1) \\n#NOTE: the final drop is to remove the extra index column returned by groupby object\\nprint(\\'Sub dataframe where B is two:\\')\\nprint(b_is_two_dataframe)\\n</code>', '<code>Original dataframe:\\n     A      B\\n0  foo    one\\n1  bar    one\\n2  foo    two\\n3  bar  three\\n4  foo    two\\n5  bar    two\\n6  foo    one\\n7  foo  three\\nSub dataframe where B is two:\\n     A    B\\n0  foo  two\\n1  foo  two\\n2  bar  two\\n</code>', '<code>select col_name1, col_name2 from table where column_name = some_value.\\n</code>', \"<code>df.loc[df['column_name'] == some_value][[col_name1, col_name2]]\\n</code>\", '<code>df.query[\\'column_name\\' == \"some_value\"\\'][[col_name1, col_name2]]\\n</code>']",
         "title": "Select rows from a DataFrame based on values in a column in pandas",
         "_childDocuments_": [
            {
               "up_vote_count": 1174,
               "answer_id": 17071908,
               "last_activity_date": 1489177008,
               "path": "3.stack.answer",
               "body_markdown": "To select rows whose column value equals a scalar, `some_value`, use `==`:\r\n\r\n    df.loc[df[&#39;column_name&#39;] == some_value]\r\n\r\nTo select rows whose column value is in an iterable, `some_values`, use `isin`:\r\n\r\n    df.loc[df[&#39;column_name&#39;].isin(some_values)]\r\n\r\nCombine multiple conditions with `&amp;`: \r\n\r\n    df.loc[(df[&#39;column_name&#39;] == some_value) &amp; df[&#39;other_column&#39;].isin(some_values)]\r\n\r\n----------\r\n\r\nTo select rows whose column value *does not equal* `some_value`, use `!=`:\r\n\r\n    df.loc[df[&#39;column_name&#39;] != some_value]\r\n\r\n\r\n`isin` returns a boolean Series, so to select rows whose value is *not* in `some_values`, negate the boolean Series using `~`:\r\n\r\n    df.loc[~df[&#39;column_name&#39;].isin(some_values)]\r\n\r\n----------\r\n\r\nFor example,\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    df = pd.DataFrame({&#39;A&#39;: &#39;foo bar foo bar foo bar foo foo&#39;.split(),\r\n                       &#39;B&#39;: &#39;one one two three two two one three&#39;.split(),\r\n                       &#39;C&#39;: np.arange(8), &#39;D&#39;: np.arange(8) * 2})\r\n    print(df)\r\n    #      A      B  C   D\r\n    # 0  foo    one  0   0\r\n    # 1  bar    one  1   2\r\n    # 2  foo    two  2   4\r\n    # 3  bar  three  3   6\r\n    # 4  foo    two  4   8\r\n    # 5  bar    two  5  10\r\n    # 6  foo    one  6  12\r\n    # 7  foo  three  7  14\r\n\r\n    print(df.loc[df[&#39;A&#39;] == &#39;foo&#39;])\r\n\r\nyields\r\n\r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\n----------\r\n\r\nIf you have multiple values you want to include, put them in a\r\nlist (or more generally, any iterable) and use `isin`:\r\n\r\n    print(df.loc[df[&#39;B&#39;].isin([&#39;one&#39;,&#39;three&#39;])])\r\n\r\nyields\r\n\r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    1  bar    one  1   2\r\n    3  bar  three  3   6\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\n\r\n----------\r\n\r\nNote, however, that if you wish to do this many times, it is more efficient to\r\nmake an index first, and then use `df.loc`:\r\n\r\n    df = df.set_index([&#39;B&#39;])\r\n    print(df.loc[&#39;one&#39;])\r\n\r\nyields\r\n\r\n           A  C   D\r\n    B              \r\n    one  foo  0   0\r\n    one  bar  1   2\r\n    one  foo  6  12\r\n\r\nor, to include multiple values from the index use `df.index.isin`:\r\n\r\n    df.loc[df.index.isin([&#39;one&#39;,&#39;two&#39;])]\r\n\r\nyields\r\n\r\n           A  C   D\r\n    B              \r\n    one  foo  0   0\r\n    one  bar  1   2\r\n    two  foo  2   4\r\n    two  foo  4   8\r\n    two  bar  5  10\r\n    one  foo  6  12\r\n",
               "tags": [],
               "creation_date": 1371059060,
               "last_edit_date": 1489177008,
               "is_accepted": true,
               "id": "17071908",
               "down_vote_count": 2,
               "score": 1172
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 17086321,
               "is_accepted": false,
               "last_activity_date": 1371124140,
               "body_markdown": "Here is a simple example  \r\n\r\n    from pandas import DataFrame\r\n    \r\n    # Create data set\r\n    d = {&#39;Revenue&#39;:[100,111,222], \r\n         &#39;Cost&#39;:[333,444,555]}\r\n    df = DataFrame(d)\r\n    \r\n    \r\n    # mask = Return True when the value in column &quot;Revenue&quot; is equal to 111\r\n    mask = df[&#39;Revenue&#39;] == 111\r\n    \r\n    print mask\r\n    \r\n    # Result:\r\n    # 0    False\r\n    # 1     True\r\n    # 2    False\r\n    # Name: Revenue, dtype: bool\r\n    \r\n    \r\n    # Select * FROM df WHERE Revenue = 111\r\n    df[mask]\r\n    \r\n    # Result:\r\n    #    Cost\t Revenue\r\n    # 1\t 444\t 111",
               "id": "17086321",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1371124140,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 28142820,
               "is_accepted": false,
               "last_activity_date": 1422228431,
               "body_markdown": "I just tried editing this, but I wasn&#39;t logged in, so I&#39;m not sure where my edit went. I was trying to incorporate multiple selection. So I think a better answer is:\r\n\r\n\r\nFor a single value, the most straightforward (human readable) is probably:\r\n\r\n    df.loc[df[&#39;column_name&#39;] == some_value]\r\n\r\nFor lists of values you can also use:\r\n\r\n    df.loc[df[&#39;column_name&#39;].isin(some_values)]\r\n\r\nFor example,\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    df = pd.DataFrame({&#39;A&#39;: &#39;foo bar foo bar foo bar foo foo&#39;.split(),\r\n                   &#39;B&#39;: &#39;one one two three two two one three&#39;.split(),\r\n                   &#39;C&#39;: np.arange(8), &#39;D&#39;: np.arange(8) * 2})\r\n    print(df)\r\n    #      A      B  C   D\r\n    # 0  foo    one  0   0\r\n    # 1  bar    one  1   2\r\n    # 2  foo    two  2   4\r\n    # 3  bar  three  3   6\r\n    # 4  foo    two  4   8\r\n    # 5  bar    two  5  10\r\n    # 6  foo    one  6  12\r\n    # 7  foo  three  7  14\r\n\r\n    print(df.loc[df[&#39;A&#39;] == &#39;foo&#39;])\r\n\r\nyields\r\n\r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\nIf you have multiple criteria you want to select against, you can put them in a list and use &#39;isin&#39;:\r\n\r\n    print(df.loc[df[&#39;B&#39;].isin([&#39;one&#39;,&#39;three&#39;])])\r\n\r\nyields\r\n\r\n          A      B  C   D\r\n    0  foo    one  0   0\r\n    1  bar    one  1   2\r\n    3  bar  three  3   6\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\nNote, however, that if you wish to do this many times, it is more efficient to make A the index first, and then use df.loc:\r\n\r\n    df = df.set_index([&#39;A&#39;])\r\n    print(df.loc[&#39;foo&#39;])\r\n\r\nyields\r\n\r\n      A      B  C   D\r\n    foo    one  0   0\r\n    foo    two  2   4\r\n    foo    two  4   8\r\n    foo    one  6  12\r\n    foo  three  7  14",
               "id": "28142820",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1422228431,
               "score": 6
            },
            {
               "up_vote_count": 83,
               "answer_id": 31296878,
               "last_activity_date": 1489399025,
               "path": "3.stack.answer",
               "body_markdown": "### tl;dr\r\n\r\nThe pandas equivalent to \r\n\r\n    select * from table where column_name = some_value\r\n\r\nis\r\n\r\n    table[table.column_name == some_value]\r\n\r\n\r\nMultiple conditions:\r\n\r\n    table((table.column_name == some_value) | (table.column_name2 == some_value2))\r\n\r\nor\r\n\r\n    table.query(&#39;column_name == some_value | column_name2 == some_value2&#39;)\r\n\r\n### Code example\r\n\r\n    import pandas as pd\r\n\r\n    # Create data set\r\n    d = {&#39;foo&#39;:[100, 111, 222], \r\n         &#39;bar&#39;:[333, 444, 555]}\r\n    df = pd.DataFrame(d)\r\n    \r\n    # Full dataframe:\r\n    df\r\n    \r\n    # Shows:\r\n    #    bar   foo \r\n    # 0  333   100\r\n    # 1  444   111\r\n    # 2  555   222\r\n\r\n    # Output only the row(s) in df where foo is 222:\r\n    df[df.foo == 222]\r\n    \r\n    # Shows:\r\n    #    bar  foo\r\n    # 2  555  222\r\n\r\nIn the above code it is the line `df[df.foo == 222]` that gives the rows based on the column value, `222` in this case.\r\n\r\n\r\nMultiple conditions are also possible:\r\n\r\n    df[(df.foo == 222) | (df.bar == 444)]\r\n    #    bar  foo\r\n    # 1  444  111\r\n    # 2  555  222\r\n\r\nBut at that point I would recommend using the query function, since it&#39;s less verbose and yields the same result:\r\n\r\n    df.query(&#39;foo == 222 | bar == 444&#39;)\r\n\r\n    \r\n\r\n",
               "tags": [],
               "creation_date": 1436368658,
               "last_edit_date": 1489399025,
               "is_accepted": false,
               "id": "31296878",
               "down_vote_count": 0,
               "score": 83
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 33680329,
               "is_accepted": false,
               "last_activity_date": 1447358612,
               "body_markdown": "If you came here looking to select rows from a dataframe by including those whose column&#39;s value is NOT any of a list of values, here&#39;s how to flip around unutbu&#39;s answer for a list of values above:\r\n\r\n    df.loc[~df[&#39;column_name&#39;].isin(some_values)]\r\n\r\n(To not include a single value, of course, you just use the regular not equals operator, `!=`.)\r\n\r\nExample:\r\n\r\n    import pandas as pd\r\n    df = pd.DataFrame({&#39;A&#39;: &#39;foo bar foo bar foo bar foo foo&#39;.split(),\r\n                       &#39;B&#39;: &#39;one one two three two two one three&#39;.split()})\r\n    print(df)\r\n\r\ngives us\r\n\r\n         A      B\r\n    0  foo    one\r\n    1  bar    one\r\n    2  foo    two\r\n    3  bar  three\r\n    4  foo    two\r\n    5  bar    two\r\n    6  foo    one\r\n    7  foo  three    \r\n\r\nTo subset to just those rows that AREN&#39;T `one` or `three` in column `B`:\r\n\r\n    df.loc[~df[&#39;B&#39;].isin([&#39;one&#39;, &#39;three&#39;])]\r\n\r\nyields\r\n\r\n         A    B\r\n    2  foo  two\r\n    4  foo  two\r\n    5  bar  two",
               "id": "33680329",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1447358612,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 37,
               "answer_id": 35282530,
               "is_accepted": false,
               "last_activity_date": 1454981809,
               "body_markdown": "I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the `query()` method in v0.13 and I much prefer it. For your question, you could do `df.query(&#39;col == val&#39;)`\r\n\r\nReproduced from http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-query\r\n\r\n\r\n    In [167]: n = 10\r\n    \r\n    In [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list(&#39;abc&#39;))\r\n    \r\n    In [169]: df\r\n    Out[169]: \r\n              a         b         c\r\n    0  0.687704  0.582314  0.281645\r\n    1  0.250846  0.610021  0.420121\r\n    2  0.624328  0.401816  0.932146\r\n    3  0.011763  0.022921  0.244186\r\n    4  0.590198  0.325680  0.890392\r\n    5  0.598892  0.296424  0.007312\r\n    6  0.634625  0.803069  0.123872\r\n    7  0.924168  0.325076  0.303746\r\n    8  0.116822  0.364564  0.454607\r\n    9  0.986142  0.751953  0.561512\r\n    \r\n    # pure python\r\n    In [170]: df[(df.a &lt; df.b) &amp; (df.b &lt; df.c)]\r\n    Out[170]: \r\n              a         b         c\r\n    3  0.011763  0.022921  0.244186\r\n    8  0.116822  0.364564  0.454607\r\n    \r\n    # query\r\n    In [171]: df.query(&#39;(a &lt; b) &amp; (b &lt; c)&#39;)\r\n    Out[171]: \r\n              a         b         c\r\n    3  0.011763  0.022921  0.244186\r\n    8  0.116822  0.364564  0.454607\r\n\r\nYou can also access variables in the environment by prepending an `@`.\r\n\r\n    exclude = (&#39;red&#39;, &#39;orange&#39;)\r\n    df.query(&#39;color not in @exclude&#39;)",
               "id": "35282530",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1454981809,
               "score": 37
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 35823724,
               "is_accepted": false,
               "last_activity_date": 1457244141,
               "body_markdown": "    df = pd.DataFrame({&#39;A&#39;: &#39;foo bar foo bar foo bar foo foo&#39;.split(),\r\n                       &#39;B&#39;: &#39;one one two three two two one three&#39;.split(),\r\n                       &#39;C&#39;: np.arange(8), &#39;D&#39;: np.arange(8) * 2})\r\n    df[df[&#39;A&#39;]==&#39;foo&#39;]\r\n    \r\n    OUTPUT:\r\n       A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\n",
               "id": "35823724",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1457244141,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 40676816,
               "is_accepted": false,
               "last_activity_date": 1479471042,
               "body_markdown": "To append to this famous question (though a bit too late): You can also do `df.groupby(&#39;column_name&#39;).get_group(&#39;column_desired_value&#39;).reset_index()` to make a new data frame with specified column having a particular value. E.g.\r\n\r\n    import pandas as pd\r\n    df = pd.DataFrame({&#39;A&#39;: &#39;foo bar foo bar foo bar foo foo&#39;.split(),\r\n                       &#39;B&#39;: &#39;one one two three two two one three&#39;.split()})\r\n    print(&quot;Original dataframe:&quot;)\r\n    print(df)\r\n    \r\n    b_is_two_dataframe = pd.DataFrame(df.groupby(&#39;B&#39;).get_group(&#39;two&#39;).reset_index()).drop(&#39;index&#39;, axis = 1) \r\n    #NOTE: the final drop is to remove the extra index column returned by groupby object\r\n    print(&#39;Sub dataframe where B is two:&#39;)\r\n    print(b_is_two_dataframe)\r\n\r\n\r\nRun this gives:\r\n\r\n    Original dataframe:\r\n         A      B\r\n    0  foo    one\r\n    1  bar    one\r\n    2  foo    two\r\n    3  bar  three\r\n    4  foo    two\r\n    5  bar    two\r\n    6  foo    one\r\n    7  foo  three\r\n    Sub dataframe where B is two:\r\n         A    B\r\n    0  foo  two\r\n    1  foo  two\r\n    2  bar  two\r\n\r\n\r\n",
               "id": "40676816",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1479471042,
               "score": 1
            },
            {
               "up_vote_count": 6,
               "answer_id": 44931669,
               "last_activity_date": 1507047441,
               "path": "3.stack.answer",
               "body_markdown": "Faster results can be achieved using [numpy.where](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html). \r\n\r\nFor example, with [unubtu&#39;s setup](https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas/17071908#17071908) -\r\n\r\n    In [76]: df.iloc[np.where(df.A.values==&#39;foo&#39;)]\r\n    Out[76]: \r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\nTiming comparisons:\r\n\r\n    In [68]: %timeit df.iloc[np.where(df.A.values==&#39;foo&#39;)]  # fastest\r\n    1000 loops, best of 3: 380 &#181;s per loop\r\n    \r\n    In [69]: %timeit df.loc[df[&#39;A&#39;] == &#39;foo&#39;]\r\n    1000 loops, best of 3: 745 &#181;s per loop\r\n    \r\n    In [71]: %timeit df.loc[df[&#39;A&#39;].isin([&#39;foo&#39;])]\r\n    1000 loops, best of 3: 562 &#181;s per loop\r\n    \r\n    In [72]: %timeit df[df.A==&#39;foo&#39;]\r\n    1000 loops, best of 3: 796 &#181;s per loop\r\n    \r\n    In [74]: %timeit df.query(&#39;(A==&quot;foo&quot;)&#39;)  # slowest\r\n    1000 loops, best of 3: 1.71 ms per loop\r\n\r\n",
               "tags": [],
               "creation_date": 1499272497,
               "last_edit_date": 1507047441,
               "is_accepted": false,
               "id": "44931669",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "up_vote_count": 47,
               "answer_id": 46165056,
               "last_activity_date": 1505194351,
               "path": "3.stack.answer",
               "body_markdown": "There are a few basic ways to select rows from a pandas dataframe.\r\n\r\n1. Boolean indexing\r\n2. Positional indexing\r\n3. Label indexing\r\n4. API\r\n\r\nFor each base type, we can keep things simple by restricting ourselves to the pandas API or we can venture outside the API, usually into `numpy`, and speed things up.\r\n\r\nI&#39;ll show you examples of each and guide you as to when to use certain techniques.\r\n\r\n___\r\n\r\n**Setup**  \r\nThe first thing we&#39;ll need is to identify a condition that will act as our criterion for selecting rows.  The OP offers up `column_name == some_value`.  We&#39;ll start there and include some other common use cases.\r\n\r\nBorrowing from @unutbu:\r\n\r\n    import pandas as pd, numpy as np\r\n\r\n    df = pd.DataFrame({&#39;A&#39;: &#39;foo bar foo bar foo bar foo foo&#39;.split(),\r\n                       &#39;B&#39;: &#39;one one two three two two one three&#39;.split(),\r\n                       &#39;C&#39;: np.arange(8), &#39;D&#39;: np.arange(8) * 2})\r\n                       \r\n                       \r\n___\r\n\r\nAssume our criterion is column `&#39;A&#39;` = `&#39;foo&#39;`\r\n\r\n**1.**  \r\n*Boolean* indexing requires finding the truth value of each row&#39;s `&#39;A&#39;` column being equal to `&#39;foo&#39;`, then using those truth values to identify which rows to keep.  Typically, we&#39;d name this series, an array of truth values, `mask`.  We&#39;ll do so here as well.\r\n\r\n    mask = df[&#39;A&#39;] == &#39;foo&#39;\r\n    \r\nWe can then use this mask to slice or index the dataframe\r\n\r\n    df[mask]\r\n    \r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n    \r\nThis is one of the simplest ways to accomplish this task and if performance or intuitiveness isn&#39;t an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the `mask`.\r\n\r\n___\r\n\r\n**2.**  \r\n*Positional* indexing has its use cases, but this isn&#39;t one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.\r\n\r\n    mask = df[&#39;A&#39;] == &#39;foo&#39;\r\n    pos = np.flatnonzero(mask)\r\n    df.iloc[pos]\r\n    \r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\n**3.**  \r\n*Label* indexing can be very handy, but in this case, we are again doing more work for no benefit\r\n\r\n    df.set_index(&#39;A&#39;, append=True, drop=False).xs(&#39;foo&#39;, level=1)\r\n\r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n \r\n**4.**  \r\n*`pd.DataFrame.query`* is a very elegant/intuitive way to perform this task.  But is often slower.  **However**, if you pay attention to the timings below, for large data, query is very efficient.  More so than the standard approach and of similar magnitude as my best suggestion.\r\n\r\n    df.query(&#39;A == &quot;foo&quot;&#39;)\r\n    \r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\n___\r\n\r\nMy preference is to use the `Boolean` `mask` \r\n\r\nActual improvements can be made by modifying how we create our `Boolean` `mask`.\r\n\r\n**`mask` alternative 1**  \r\n*Use the underlying `numpy` array and forgo the overhead of creating another `pd.Series`*  \r\n\r\n    mask = df[&#39;A&#39;].values == &#39;foo&#39;\r\n    \r\nI&#39;ll show more complete time tests at the end, but just take a look at the performance gains we get using the sample dataframe.  First we look at the difference in creating the `mask`\r\n\r\n    %timeit mask = df[&#39;A&#39;].values == &#39;foo&#39;\r\n    %timeit mask = df[&#39;A&#39;] == &#39;foo&#39;\r\n    \r\n    5.84 &#181;s &#177; 195 ns per loop (mean &#177; std. dev. of 7 runs, 100000 loops each)\r\n    166 &#181;s &#177; 4.45 &#181;s per loop (mean &#177; std. dev. of 7 runs, 10000 loops each)\r\n\r\nEvaluating the `mask` with the `numpy` array is ~ 30 times faster.  This is partly due to `numpy` evaluation often being faster.  It is also partly due to the lack of overhead necessary to build an index and a corresponding `pd.Series` object.\r\n    \r\nNext we&#39;ll look at the timing for slicing with one `mask` versus the other.\r\n\r\n    mask = df[&#39;A&#39;].values == &#39;foo&#39;\r\n    %timeit df[mask]\r\n    mask = df[&#39;A&#39;] == &#39;foo&#39;\r\n    %timeit df[mask]\r\n\r\n    219 &#181;s &#177; 12.3 &#181;s per loop (mean &#177; std. dev. of 7 runs, 1000 loops each)\r\n    239 &#181;s &#177; 7.03 &#181;s per loop (mean &#177; std. dev. of 7 runs, 1000 loops each)\r\n    \r\nThe performance gains aren&#39;t as pronounced.  We&#39;ll see if this holds up over more robust testing.\r\n\r\n___\r\n\r\n**`mask` alternative 2**  \r\nWe could have reconstructed the dataframe as well.  There is a big caveat when reconstructing a dataframe\u2014you must take care of the `dtypes` when doing so!\r\n\r\nInstead of `df[mask]` we will do this\r\n\r\n    pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\r\n    \r\nIf the dataframe is of mixed type, which our example is, then when we get `df.values` the resulting array is of `dtype` `object` and consequently, all columns of the new dataframe will be of `dtype` `object`.  Thus requiring the `astype(df.dtypes)` and killing any potential performance gains.\r\n\r\n    %timeit df[m]\r\n    %timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\r\n\r\n    216 &#181;s &#177; 10.4 &#181;s per loop (mean &#177; std. dev. of 7 runs, 1000 loops each)\r\n    1.43 ms &#177; 39.6 &#181;s per loop (mean &#177; std. dev. of 7 runs, 1000 loops each)\r\n    \r\nHowever, if the dataframe is not of mixed type, this is a very useful way to do it.\r\n\r\nGiven\r\n\r\n    np.random.seed([3,1415])\r\n    d1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list(&#39;ABCDE&#39;))\r\n\r\n    d1\r\n    \r\n       A  B  C  D  E\r\n    0  0  2  7  3  8\r\n    1  7  0  6  8  6\r\n    2  0  2  0  4  9\r\n    3  7  3  2  4  3\r\n    4  3  6  7  7  4\r\n    5  5  3  7  5  9\r\n    6  8  7  6  4  7\r\n    7  6  2  6  6  5\r\n    8  2  8  7  5  8\r\n    9  4  7  6  1  5    \r\n    \r\n___\r\n\r\n    %%timeit\r\n    mask = d1[&#39;A&#39;].values == 7\r\n    d1[mask]\r\n\r\n    179 &#181;s &#177; 8.73 &#181;s per loop (mean &#177; std. dev. of 7 runs, 10000 loops each)\r\n\r\nVersus\r\n\r\n    %%timeit\r\n    mask = d1[&#39;A&#39;].values == 7\r\n    pd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)\r\n\r\n    87 &#181;s &#177; 5.12 &#181;s per loop (mean &#177; std. dev. of 7 runs, 10000 loops each)\r\n\r\nWe cut the time in half.\r\n\r\n___\r\n**`mask` alternative 3**  \r\n@unutbu also shows us how to use `pd.Series.isin` to account for each element of `df[&#39;A&#39;]` being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely `&#39;foo&#39;`.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.\r\n\r\n    mask = df[&#39;A&#39;].isin([&#39;foo&#39;])\r\n    df[mask]\r\n    \r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\nHowever, as before, we can utilize `numpy` to improve performance while sacraficing virtually anything.  We&#39;ll use `np.in1d`\r\n\r\n    mask = np.in1d(df[&#39;A&#39;].values, [&#39;foo&#39;])\r\n    df[mask]\r\n\r\n         A      B  C   D\r\n    0  foo    one  0   0\r\n    2  foo    two  2   4\r\n    4  foo    two  4   8\r\n    6  foo    one  6  12\r\n    7  foo  three  7  14\r\n\r\n___\r\n\r\n**Timing**  \r\nI&#39;ll include other concepts mentioned in other posts as well for reference.  \r\n*Code Below*  \r\n\r\nEach Column in this table represents a different length dataframe over which we test each function. Each column shows relative time taken, with the fastest function given a base index of `1.0`.\r\n\r\n    res.div(res.min())\r\n\r\n                             10        30        100       300       1000      3000      10000     30000\r\n    mask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151\r\n    mask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103\r\n    mask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919\r\n    mask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000\r\n    query                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190\r\n    xs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255\r\n    mask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760\r\n    mask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175\r\n    \r\nYou&#39;ll notice that fastest times seem to be shared between `mask_with_values` and `mask_with_in1d`\r\n\r\n    res.T.plot(loglog=True)\r\n\r\n[![enter image description here][1]][1]\r\n\r\n**Functions**  \r\n\r\n    def mask_standard(df):\r\n        mask = df[&#39;A&#39;] == &#39;foo&#39;\r\n        return df[mask]\r\n\r\n    def mask_standard_loc(df):\r\n        mask = df[&#39;A&#39;] == &#39;foo&#39;\r\n        return df.loc[mask]\r\n\r\n    def mask_with_values(df):\r\n        mask = df[&#39;A&#39;].values == &#39;foo&#39;\r\n        return df[mask]\r\n\r\n    def mask_with_values_loc(df):\r\n        mask = df[&#39;A&#39;].values == &#39;foo&#39;\r\n        return df.loc[mask]\r\n\r\n    def query(df):\r\n        return df.query(&#39;A == &quot;foo&quot;&#39;)\r\n\r\n    def xs_label(df):\r\n        return df.set_index(&#39;A&#39;, append=True, drop=False).xs(&#39;foo&#39;, level=-1)\r\n\r\n    def mask_with_isin(df):\r\n        mask = df[&#39;A&#39;].isin([&#39;foo&#39;])\r\n        return df[mask]\r\n\r\n    def mask_with_in1d(df):\r\n        mask = np.in1d(df[&#39;A&#39;].values, [&#39;foo&#39;])\r\n        return df[mask]\r\n\r\n___\r\n**Testing**  \r\n\r\n    res = pd.DataFrame(\r\n        index=[\r\n            &#39;mask_standard&#39;, &#39;mask_standard_loc&#39;, &#39;mask_with_values&#39;, &#39;mask_with_values_loc&#39;,\r\n            &#39;query&#39;, &#39;xs_label&#39;, &#39;mask_with_isin&#39;, &#39;mask_with_in1d&#39;\r\n        ],\r\n        columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\r\n        dtype=float\r\n    )\r\n\r\n    for j in res.columns:\r\n        d = pd.concat([df] * j, ignore_index=True)\r\n        for i in res.index:a\r\n            stmt = &#39;{}(d)&#39;.format(i)\r\n            setp = &#39;from __main__ import d, {}&#39;.format(i)\r\n            res.at[i, j] = timeit(stmt, setp, number=50)\r\n            \r\n___\r\n**Special Timing**  \r\nLooking at the special case when we have a single non-object `dtype` for the entire dataframe.\r\n*Code Below*  \r\n\r\n    spec.div(spec.min())\r\n\r\n                         10        30        100       300       1000      3000      10000     30000\r\n    mask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000\r\n    mask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100\r\n    reconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735\r\n    \r\nTurns out, reconstruction isn&#39;t worth it past a few hundred rows.\r\n    \r\n    spec.T.plot(loglog=True)\r\n    \r\n[![enter image description here][2]][2]\r\n\r\n**Functions**  \r\n\r\n    np.random.seed([3,1415])\r\n    d1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list(&#39;ABCDE&#39;))\r\n\r\n    def mask_with_values(df):\r\n        mask = df[&#39;A&#39;].values == &#39;foo&#39;\r\n        return df[mask]\r\n\r\n    def mask_with_in1d(df):\r\n        mask = np.in1d(df[&#39;A&#39;].values, [&#39;foo&#39;])\r\n        return df[mask]\r\n\r\n    def reconstruct(df):\r\n        v = df.values\r\n        mask = np.in1d(df[&#39;A&#39;].values, [&#39;foo&#39;])\r\n        return pd.DataFrame(v[mask], df.index[mask], df.columns)\r\n\r\n    spec = pd.DataFrame(\r\n        index=[&#39;mask_with_values&#39;, &#39;mask_with_in1d&#39;, &#39;reconstruct&#39;],\r\n        columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\r\n        dtype=float\r\n    )\r\n\r\n**Testing**  \r\n\r\n    for j in spec.columns:\r\n        d = pd.concat([df] * j, ignore_index=True)\r\n        for i in spec.index:\r\n            stmt = &#39;{}(d)&#39;.format(i)\r\n            setp = &#39;from __main__ import d, {}&#39;.format(i)\r\n            spec.at[i, j] = timeit(stmt, setp, number=50)\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/ljeTd.png\r\n  [2]: https://i.stack.imgur.com/K1bNc.png",
               "tags": [],
               "creation_date": 1505168068,
               "last_edit_date": 1505194351,
               "is_accepted": false,
               "id": "46165056",
               "down_vote_count": 0,
               "score": 47
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47693145,
               "is_accepted": false,
               "last_activity_date": 1512643190,
               "body_markdown": "For selecting only specific columns out of multiple columns for a given value in pandas:\r\n\r\n    select col_name1, col_name2 from table where column_name = some_value.\r\n\r\nOptions:\r\n\r\n    df.loc[df[&#39;column_name&#39;] == some_value][[col_name1, col_name2]]\r\n\r\nor \r\n\r\n    df.query[&#39;column_name&#39; == &quot;some_value&quot;&#39;][[col_name1, col_name2]]\r\n",
               "id": "47693145",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512643190,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas",
         "id": "858127-2306"
      },
      {
         "up_vote_count": "353",
         "path": "2.stack",
         "body_markdown": "This is what I normally do in order to ascertain that the input is a `list`/`tuple` - but not a `str`. Because many times I stumbled upon bugs where a function passes a `str` object by mistake, and the target function does `for x in lst` assuming that `lst` is actually a `list` or `tuple`.\r\n\r\n    assert isinstance(lst, (list, tuple))\r\n\r\nMy question is: is there a better way of achieving this?\r\n",
         "view_count": "428728",
         "answer_count": "15",
         "tags": "['python', 'list', 'types', 'assert']",
         "creation_date": "1259779987",
         "last_edit_date": "1493582558",
         "code_snippet": "['<code>list</code>', '<code>tuple</code>', '<code>str</code>', '<code>str</code>', '<code>for x in lst</code>', '<code>lst</code>', '<code>list</code>', '<code>tuple</code>', '<code>assert isinstance(lst, (list, tuple))\\n</code>', '<code>assert not isinstance(lst, basestring)\\n</code>', '<code>list</code>', '<code>tuple</code>', '<code>basestring</code>', '<code>isinstance(lst, str)</code>', '<code>set</code>', '<code>mmap</code>', '<code>array</code>', '<code>lst</code>', '<code>assert isinstance(lst, (list, tuple)) and assert not isinstance(lst, basestring)</code>', '<code>repr()</code>', '<code>def srepr(arg):\\n    if isinstance(arg, basestring): # Python 3: isinstance(arg, str)\\n        return repr(arg)\\n    try:\\n        return \\'&lt;\\' + \", \".join(srepr(x) for x in arg) + \\'&gt;\\'\\n    except TypeError: # catch when for loop fails\\n        return repr(arg) # not a sequence so just return repr\\n</code>', '<code>isinstance()</code>', '<code>try</code>', '<code>except</code>', '<code>arg</code>', '<code>.strip()</code>', '<code>def is_sequence(arg):\\n    return (not hasattr(arg, \"strip\") and\\n            hasattr(arg, \"__getitem__\") or\\n            hasattr(arg, \"__iter__\"))\\n\\ndef srepr(arg):\\n    if is_sequence(arg):\\n        return \\'&lt;\\' + \", \".join(srepr(x) for x in arg) + \\'&gt;\\'\\n    return repr(arg)\\n</code>', '<code>__getslice__()</code>', '<code>collections</code>', '<code>__getitem__()</code>', '<code>__getslice__()</code>', '<code>srepr</code>', '<code>str</code>', '<code>str</code>', '<code>srepr</code>', '<code>str</code>', '<code>srepr()</code>', '<code>\"foo\"</code>', \"<code>&lt;'f', 'o', 'o'&gt;</code>\", '<code>H = \"Hello\"\\n\\nif type(H) is list or type(H) is tuple:\\n    ## Do Something.\\nelse\\n    ## Do Something.\\n</code>', '<code>import collections\\n\\nif isinstance(obj, collections.Sequence) and not isinstance(obj, basestring):\\n    print \"obj is a sequence (list, tuple, etc) but not a string or unicode\"\\n</code>', '<code>collections.Sequence</code>', '<code>xrange</code>', '<code>dict</code>', '<code>__getitem__</code>', '<code>__iter__</code>', '<code>inspect.getmro(list)</code>', '<code>Sequence</code>', '<code>isinstance</code>', '<code>getmro</code>', '<code>Sequence</code>', '<code>def is_array(var):\\n    return isinstance(var, (list, tuple))\\n</code>', '<code>__getitem__</code>', '<code>__getitem__</code>', '<code>str</code>', '<code>__getitem__</code>', '<code>str</code>', '<code>__getitem__</code>', '<code>isinstance</code>', '<code>f(\"abc\")</code>', '<code>f([\"abc\"])</code>', '<code>str</code>', '<code>__iter__</code>', \"<code>&gt;&gt;&gt; hasattr('', '__iter__')\\nFalse \\n</code>\", \"<code>assert hasattr(x, '__iter__')\\n</code>\", '<code>AssertionError</code>', \"<code>hasattr('','__iter__')</code>\", '<code>True</code>', '<code>str</code>', '<code>str</code>', '<code>srepr</code>', '<code>str</code>', '<code>srepr</code>', '<code>srepr</code>', '<code>srepr</code>', '<code>list(arg) == [arg]</code>', '<code>str</code>', '<code>isinstance</code>', '<code>list(arg) == [arg]</code>', '<code>str</code>', '<code>str</code>', '<code>str</code>', '<code>.__atomic__</code>', '<code>atomic()</code>', '<code>from collections import atomic</code>', '<code>import types\\nif isinstance(lst, types.ListType) or isinstance(lst, types.TupleType):\\n   # Do something\\n</code>', '<code>def assertIsIterable(self, item):\\n    #add types here you don\\'t want to mistake as iterables\\n    if isinstance(item, basestring): \\n        raise AssertionError(\"type %s is not iterable\" % type(item))\\n\\n    #Fake an iteration.\\n    try:\\n        for x in item:\\n            break;\\n    except TypeError:\\n        raise AssertionError(\"type %s is not iterable\" % type(item))\\n</code>', '<code>def is_sequence(seq):\\n  \"\"\"Returns a true if its input is a collections.Sequence (except strings).\\n  Args:\\n    seq: an input sequence.\\n  Returns:\\n    True if the sequence is a not a string and is a collections.Sequence.\\n  \"\"\"\\n  return (isinstance(seq, collections.Sequence)\\nand not isinstance(seq, six.string_types))\\n</code>', '<code>any</code>', '<code>isinstance</code>', \"<code>&gt;&gt;&gt; console_routers = 'x'\\n&gt;&gt;&gt; any([isinstance(console_routers, list), isinstance(console_routers, tuple)])\\nFalse\\n&gt;&gt;&gt;\\n&gt;&gt;&gt; console_routers = ('x',)\\n&gt;&gt;&gt; any([isinstance(console_routers, list), isinstance(console_routers, tuple)])\\nTrue\\n&gt;&gt;&gt; console_routers = list('x',)\\n&gt;&gt;&gt; any([isinstance(console_routers, list), isinstance(console_routers, tuple)])\\nTrue\\n</code>\", '<code>if type(lst) in (list, tuple):\\n    # Do stuff\\n</code>', \"<code>for i in some_var:\\n   if type(i) == type(list()):\\n       #do something with a list\\n   elif type(i) == type(tuple()):\\n       #do something with a tuple\\n   elif type(i) == type(str()):\\n       #here's your string\\n</code>\", '<code>some_var</code>', '<code>list()</code>', '<code>type(tuple())</code>', '<code>tuple</code>', '<code>str</code>', '<code>unicode</code>', '<code>basestring</code>', '<code>type(i) is list</code>', '<code>type(list())</code>', '<code>list</code>', '<code>i</code>', '<code>variable = pd.Series(variable).tolist()\\n</code>']",
         "title": "How to check if an object is a list or tuple (but not string)?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 302,
               "answer_id": 1835044,
               "is_accepted": true,
               "last_activity_date": 1259780211,
               "body_markdown": "I think\r\n\r\n    assert not isinstance(lst, basestring)\r\n\r\nIs actually what you want, otherwise you&#39;ll miss out on a lot of things which act like lists, but aren&#39;t subclasses of `list` or `tuple`.",
               "id": "1835044",
               "tags": [],
               "down_vote_count": 14,
               "creation_date": 1259780211,
               "score": 288
            },
            {
               "up_vote_count": 6,
               "answer_id": 1835162,
               "last_activity_date": 1259788191,
               "path": "3.stack.answer",
               "body_markdown": "The `str` object doesn&#39;t have an `__iter__` attribute\r\n\r\n    &gt;&gt;&gt; hasattr(&#39;&#39;, &#39;__iter__&#39;)\r\n    False \r\n\r\nso you can do a check\r\n\r\n    assert hasattr(x, &#39;__iter__&#39;)\r\n\r\nand this will also raise a nice `AssertionError` for any other non-iterable object too.\r\n\r\n*Edit:*\r\nAs Tim mentions in the comments, this will only work in python 2.x, not 3.x",
               "tags": [],
               "creation_date": 1259781352,
               "last_edit_date": 1259788191,
               "is_accepted": false,
               "id": "1835162",
               "down_vote_count": 1,
               "score": 5
            },
            {
               "up_vote_count": 152,
               "answer_id": 1835259,
               "last_activity_date": 1310019956,
               "path": "3.stack.answer",
               "body_markdown": "Remember that in Python we want to use &quot;duck typing&quot;.  So, anything that acts like a list can be treated as a list.  So, don&#39;t check for the type of a list, just see if it acts like a list.\r\n\r\nBut strings act like a list too, and often that is not what we want.  There are times when it is even a problem!  So, check explicitly for a string, but then use duck typing.\r\n\r\nHere is a function I wrote for fun.  It is a special version of `repr()` that prints any sequence in angle brackets (&#39;&lt;&#39;, &#39;&gt;&#39;).\r\n\r\n\r\n    def srepr(arg):\r\n        if isinstance(arg, basestring): # Python 3: isinstance(arg, str)\r\n            return repr(arg)\r\n        try:\r\n            return &#39;&lt;&#39; + &quot;, &quot;.join(srepr(x) for x in arg) + &#39;&gt;&#39;\r\n        except TypeError: # catch when for loop fails\r\n            return repr(arg) # not a sequence so just return repr\r\n\r\nThis is clean and elegant, overall.  But what&#39;s that `isinstance()` check doing there?  That&#39;s kind of a hack.  But it is essential.\r\n\r\nThis function calls itself recursively on anything that acts like a list.  If we didn&#39;t handle the string specially, then it would be treated like a list, and split up one character at a time.  But then the recursive call would try to treat each character as a list -- and it would work!  Even a one-character string works as a list!  The function would keep on calling itself recursively until stack overflow.\r\n\r\nFunctions like this one, that depend on each recursive call breaking down the work to be done, have to special-case strings--because you can&#39;t break down a string below the level of a one-character string, and even a one-character string acts like a list.\r\n\r\nNote: the `try`/`except` is the cleanest way to express our intentions.  But if this code were somehow time-critical, we might want to replace it with some sort of test to see if `arg` is a sequence.  Rather than testing the type, we should probably test behaviors.  If it has a `.strip()` method, it&#39;s a string, so don&#39;t consider it a sequence; otherwise, if it is indexable or iterable, it&#39;s a sequence:\r\n\r\n    def is_sequence(arg):\r\n        return (not hasattr(arg, &quot;strip&quot;) and\r\n                hasattr(arg, &quot;__getitem__&quot;) or\r\n                hasattr(arg, &quot;__iter__&quot;))\r\n\r\n    def srepr(arg):\r\n        if is_sequence(arg):\r\n            return &#39;&lt;&#39; + &quot;, &quot;.join(srepr(x) for x in arg) + &#39;&gt;&#39;\r\n        return repr(arg)\r\n\r\n\r\nEDIT: I originally wrote the above with a check for `__getslice__()` but I noticed that in the `collections` module documentation, the interesting method is `__getitem__()`; this makes sense, that&#39;s how you index an object.  That seems more fundamental than `__getslice__()` so I changed the above.",
               "tags": [],
               "creation_date": 1259782213,
               "last_edit_date": 1310019956,
               "is_accepted": false,
               "id": "1835259",
               "down_vote_count": 2,
               "score": 150
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 1835402,
               "is_accepted": false,
               "last_activity_date": 1259783982,
               "body_markdown": "I tend to do this (if I really, really had to):\r\n\r\n    for i in some_var:\r\n       if type(i) == type(list()):\r\n           #do something with a list\r\n       elif type(i) == type(tuple()):\r\n           #do something with a tuple\r\n       elif type(i) == type(str()):\r\n           #here&#39;s your string",
               "id": "1835402",
               "tags": [],
               "down_vote_count": 9,
               "creation_date": 1259783982,
               "score": -4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 1835599,
               "is_accepted": false,
               "last_activity_date": 1259785982,
               "body_markdown": "Generally speaking, the fact that a function which iterates over an object works on strings as well as tuples and lists is more feature than bug.  You certainly *can* use `isinstance` or duck typing to check an argument, but why should you?\r\n\r\nThat sounds like a rhetorical question, but it isn&#39;t.  The answer to &quot;why should I check the argument&#39;s type?&quot; is probably going to suggest a solution to the real problem, not the perceived problem.  Why is it a bug when a string is passed to the function?  Also:  if it&#39;s a bug when a string is passed to this function, is it also a bug if some other non-list/tuple iterable is passed to it?  Why, or why not?\r\n\r\nI think that the most common answer to the question is likely to be that developers who write `f(&quot;abc&quot;)` are expecting the function to behave as though they&#39;d written `f([&quot;abc&quot;])`.  There are probably circumstances where it makes more sense to protect developers from themselves than it does to support the use case of iterating across the characters in a string.  But I&#39;d think long and hard about it first.",
               "id": "1835599",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1259785982,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 30,
               "answer_id": 5971720,
               "is_accepted": false,
               "last_activity_date": 1305156604,
               "body_markdown": "Python with PHP flavor:\r\n\r\n    def is_array(var):\r\n        return isinstance(var, (list, tuple))",
               "id": "5971720",
               "tags": [],
               "down_vote_count": 4,
               "creation_date": 1305156604,
               "score": 26
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 13117648,
               "is_accepted": false,
               "last_activity_date": 1351496540,
               "body_markdown": "This is not intended to directly answer the OP, but I wanted to share some related ideas.\r\n\r\nI was very interested in @steveha answer above, which seemed to give an example where duck typing seems to break. On second thought, however, his example suggests that duck typing is hard to conform to, but it does *not* suggest that `str` deserves any special handling.\r\n\r\nAfter all, a non-`str` type (e.g., a user-defined type that maintains some complicated recursive structures) may cause @steveha `srepr` function to cause an infinite recursion. While this is admittedly rather unlikely, we can&#39;t ignore this possibility. Therefore, rather than special-casing `str` in `srepr`, we should clarify what we want `srepr` to do when an infinite recursion results.\r\n\r\nIt may seem that one reasonable approach is to simply break the recursion in `srepr` the moment `list(arg) == [arg]`. This would, in fact, completely solve the problem with `str`, without any `isinstance`.\r\n\r\nHowever, a really complicated recursive structure may cause an infinite loop where `list(arg) == [arg]` never happens. Therefore, while the above check is useful, it&#39;s not sufficient. We need something like a hard limit on the recursion depth.\r\n\r\nMy point is that if you plan to handle arbitrary argument types, handling `str` via duck typing is far, far easier than handling the more general types you may (theoretically) encounter. So if you feel the need to exclude `str` instances, you should instead demand that the argument is an instance of one of the few types that you explicitly specify.",
               "id": "13117648",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1351496540,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 14657269,
               "is_accepted": false,
               "last_activity_date": 1359764726,
               "body_markdown": "I do this in my testcases.\r\n\r\n    def assertIsIterable(self, item):\r\n        #add types here you don&#39;t want to mistake as iterables\r\n        if isinstance(item, basestring): \r\n            raise AssertionError(&quot;type %s is not iterable&quot; % type(item))\r\n\r\n        #Fake an iteration.\r\n        try:\r\n            for x in item:\r\n                break;\r\n        except TypeError:\r\n            raise AssertionError(&quot;type %s is not iterable&quot; % type(item))\r\n\r\nUntested on generators, I think you are left at the next &#39;yield&#39; if passed in a generator, which may screw things up downstream.  But then again, this is a &#39;unittest&#39;",
               "id": "14657269",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1359764726,
               "score": 1
            },
            {
               "up_vote_count": 100,
               "answer_id": 21522971,
               "last_activity_date": 1395917878,
               "path": "3.stack.answer",
               "body_markdown": "    H = &quot;Hello&quot;\r\n    \r\n    if type(H) is list or type(H) is tuple:\r\n        ## Do Something.\r\n    else\r\n        ## Do Something.\r\n         ",
               "tags": [],
               "creation_date": 1391417325,
               "last_edit_date": 1395917878,
               "is_accepted": false,
               "id": "21522971",
               "down_vote_count": 9,
               "score": 91
            },
            {
               "up_vote_count": 34,
               "answer_id": 37842328,
               "last_activity_date": 1490319386,
               "path": "3.stack.answer",
               "body_markdown": "    import collections\r\n\r\n    if isinstance(obj, collections.Sequence) and not isinstance(obj, basestring):\r\n        print &quot;obj is a sequence (list, tuple, etc) but not a string or unicode&quot;\r\n\r\nFor Python 3, note per the [docs](https://docs.python.org/3/library/collections.html#module-collections):\r\n\r\n&gt; Changed in version 3.3: Moved Collections Abstract Base Classes to the collections.abc module. For backwards compatibility, they continue to be visible in this module as well.",
               "tags": [],
               "creation_date": 1466012807,
               "last_edit_date": 1490319386,
               "is_accepted": false,
               "id": "37842328",
               "down_vote_count": 0,
               "score": 34
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 40452424,
               "is_accepted": false,
               "last_activity_date": 1478454072,
               "body_markdown": "If you have pandas already available you can just do this:\r\n\r\n    variable = pd.Series(variable).tolist()\r\nThis is what I do to ensure a list.",
               "id": "40452424",
               "tags": [],
               "down_vote_count": 4,
               "creation_date": 1478454072,
               "score": -4
            },
            {
               "up_vote_count": 1,
               "answer_id": 44066538,
               "last_activity_date": 1495187277,
               "path": "3.stack.answer",
               "body_markdown": "I find such a function named [is_sequence in tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/nest.py#L77). \r\n\r\n    def is_sequence(seq):\r\n      &quot;&quot;&quot;Returns a true if its input is a collections.Sequence (except strings).\r\n      Args:\r\n        seq: an input sequence.\r\n      Returns:\r\n        True if the sequence is a not a string and is a collections.Sequence.\r\n      &quot;&quot;&quot;\r\n      return (isinstance(seq, collections.Sequence)\r\n    and not isinstance(seq, six.string_types))\r\n\r\nAnd I have verified that it meets your needs. ",
               "tags": [],
               "creation_date": 1495186757,
               "last_edit_date": 1495187277,
               "is_accepted": false,
               "id": "44066538",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 44506995,
               "is_accepted": false,
               "last_activity_date": 1497294469,
               "body_markdown": "Just do this\r\n\r\n    if type(lst) in (list, tuple):\r\n        # Do stuff",
               "id": "44506995",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1497294469,
               "score": -2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47590245,
               "is_accepted": false,
               "last_activity_date": 1512120439,
               "body_markdown": "simplest way... using `any` and `isinstance`\r\n\r\n\r\n    &gt;&gt;&gt; console_routers = &#39;x&#39;\r\n    &gt;&gt;&gt; any([isinstance(console_routers, list), isinstance(console_routers, tuple)])\r\n    False\r\n    &gt;&gt;&gt;\r\n    &gt;&gt;&gt; console_routers = (&#39;x&#39;,)\r\n    &gt;&gt;&gt; any([isinstance(console_routers, list), isinstance(console_routers, tuple)])\r\n    True\r\n    &gt;&gt;&gt; console_routers = list(&#39;x&#39;,)\r\n    &gt;&gt;&gt; any([isinstance(console_routers, list), isinstance(console_routers, tuple)])\r\n    True\r\n",
               "id": "47590245",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1512120439,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 48047774,
               "is_accepted": false,
               "last_activity_date": 1514790183,
               "body_markdown": "Try this for readability and best practices:\r\n\r\n    import types\r\n    if isinstance(lst, types.ListType) or isinstance(lst, types.TupleType):\r\n       # Do something\r\n\r\nHope it helps.",
               "id": "48047774",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1514790183,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/1835018/how-to-check-if-an-object-is-a-list-or-tuple-but-not-string",
         "id": "858127-2307"
      },
      {
         "up_vote_count": "738",
         "path": "2.stack",
         "body_markdown": "I&#39;m trying to install version 1.2.2 of the MySQL_python adaptor, using a fresh virtualenv created with the `--no-site-packages` option. The current version shown in PyPi is [1.2.3][1]. Is there a way to install the older version? I found an article stating that this should do it:\r\n\r\n    pip install MySQL_python==1.2.2\r\n\r\nWhen installed, however, it still shows MySQL_python-1.2.3-py2.6.egg-info in the site packages. Is this a problem specific to this package, or am I doing something wrong?\r\n\r\n  [1]: http://pypi.python.org/pypi/MySQL-python/1.2.3",
         "view_count": "617218",
         "answer_count": "4",
         "tags": "['python', 'mysql', 'pip', 'pypi', 'mysql-python']",
         "creation_date": "1299538693",
         "last_edit_date": "1496123232",
         "code_snippet": "['<code>--no-site-packages</code>', '<code>pip install MySQL_python==1.2.2\\n</code>', '<code>pip install MySQL_python==1.8.9</code>', '<code>pip install -Iv pandas==0.12.0</code>', '<code>pip install -I MySQL_python==1.2.2</code>', '<code>pip install -Iv MySQL_python==1.2.2</code>', '<code>pip uninstall MySQL_python\\npip install -Iv http://sourceforge.net/projects/mysql-python/files/mysql-python/1.2.2/MySQL-python-1.2.2.tar.gz/download\\n</code>', '<code>pip install</code>', \"<code>pip install 'stevedore&gt;=1.3.0,&lt;1.4.0'\\n</code>\", \"<code>$ pip install 'xkcdpass==1.2.5' --force-reinstall</code>\", '<code>pip</code>', '<code>pip install -Iv MySQL_python==1.2.2</code>', '<code>==</code>', '<code>-I, --ignore-installed</code>', '<code>-I</code>', '<code>unset PYTHONPATH</code>', '<code>pip</code>']",
         "title": "Installing specific package versions with pip",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 32,
               "answer_id": 5226452,
               "is_accepted": false,
               "last_activity_date": 1299539602,
               "body_markdown": "I believe that if you already have a package it installed, pip will not overwrite it with another version.  Use `-I` to ignore previous versions.",
               "id": "5226452",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1299539602,
               "score": 32
            },
            {
               "up_vote_count": 565,
               "answer_id": 5226504,
               "last_activity_date": 1343216172,
               "path": "3.stack.answer",
               "body_markdown": "First, I see two issues with what you&#39;re trying to do. Since you already have an installed version, you should either uninstall the current existing driver or use ``pip install -I MySQL_python==1.2.2``\r\n\r\nHowever, you&#39;ll soon find out that this doesn&#39;t work. If you look at pip&#39;s installation log, or if you do a ``pip install -Iv MySQL_python==1.2.2`` you&#39;ll find that the PyPI URL link does not work for MySQL_python v1.2.2. You can verify this here: http://pypi.python.org/pypi/MySQL-python/1.2.2\r\n\r\nThe download link 404s and the fallback URL links are re-directing infinitely due to sourceforge.net&#39;s recent upgrade and PyPI&#39;s stale URL.\r\n\r\nSo to properly install the driver, you can follow these steps:\r\n\r\n    pip uninstall MySQL_python\r\n    pip install -Iv http://sourceforge.net/projects/mysql-python/files/mysql-python/1.2.2/MySQL-python-1.2.2.tar.gz/download\r\n\r\n",
               "tags": [],
               "creation_date": 1299539933,
               "last_edit_date": 1343216172,
               "is_accepted": true,
               "id": "5226504",
               "down_vote_count": 1,
               "score": 564
            },
            {
               "up_vote_count": 200,
               "answer_id": 33812968,
               "last_activity_date": 1447962683,
               "path": "3.stack.answer",
               "body_markdown": "You can even use a version range with `pip install` command. Something like this:\r\n\r\n    pip install &#39;stevedore&gt;=1.3.0,&lt;1.4.0&#39;",
               "tags": [],
               "creation_date": 1447962159,
               "last_edit_date": 1447962683,
               "is_accepted": false,
               "id": "33812968",
               "down_vote_count": 1,
               "score": 199
            },
            {
               "up_vote_count": 78,
               "answer_id": 36399566,
               "last_activity_date": 1490673249,
               "path": "3.stack.answer",
               "body_markdown": "One way as suggested in [this post][1] is to mention version in `pip` as \r\n\r\n`pip install -Iv MySQL_python==1.2.2`\r\n\r\ni.e. Use `==` and mention the version number to install only that version. `-I, --ignore-installed` ignores already installed packages.\r\n\r\n  [1]: https://stackoverflow.com/questions/13916820/how-to-install-a-specific-version-of-a-package-with-pip",
               "tags": [],
               "creation_date": 1459763898,
               "last_edit_date": 1495541448,
               "is_accepted": false,
               "id": "36399566",
               "down_vote_count": 0,
               "score": 78
            }
         ],
         "link": "https://stackoverflow.com/questions/5226311/installing-specific-package-versions-with-pip",
         "id": "858127-2308"
      },
      {
         "up_vote_count": "174",
         "path": "2.stack",
         "body_markdown": "I am opening a file which has 100,000 url&#39;s.  I need to send an http request to each url and print the status code. I am using Python 2.6, and so far looked at the many confusing ways Python implements threading/concurrency.  I have even looked at the python [concurrence][1] library, but cannot figure out how to write this program correctly.  Has anyone come across a similar problem?  I guess generally I need to know how to perform thousands of tasks in Python as fast as possible - I suppose that means &#39;concurrently&#39;.\r\n\r\n  [1]: http://opensource.hyves.org/concurrence",
         "view_count": "103322",
         "answer_count": "13",
         "tags": "['python', 'http', 'concurrency']",
         "creation_date": "1271186390",
         "last_edit_date": "1511096154",
         "code_snippet": "['<code>requests.get</code>', '<code>requests.head</code>', '<code>from urlparse import urlparse\\nfrom threading import Thread\\nimport httplib, sys\\nfrom Queue import Queue\\n\\nconcurrent = 200\\n\\ndef doWork():\\n    while True:\\n        url = q.get()\\n        status, url = getStatus(url)\\n        doSomethingWithResult(status, url)\\n        q.task_done()\\n\\ndef getStatus(ourl):\\n    try:\\n        url = urlparse(ourl)\\n        conn = httplib.HTTPConnection(url.netloc)   \\n        conn.request(\"HEAD\", url.path)\\n        res = conn.getresponse()\\n        return res.status, ourl\\n    except:\\n        return \"error\", ourl\\n\\ndef doSomethingWithResult(status, url):\\n    print status, url\\n\\nq = Queue(concurrent * 2)\\nfor i in range(concurrent):\\n    t = Thread(target=doWork)\\n    t.daemon = True\\n    t.start()\\ntry:\\n    for url in open(\\'urllist.txt\\'):\\n        q.put(url.strip())\\n    q.join()\\nexcept KeyboardInterrupt:\\n    sys.exit(1)\\n</code>', '<code>concurrent*2</code>', '<code>conn.close()</code>', '<code>Queue</code>', '<code>queue</code>', \"<code>from tornado import ioloop, httpclient\\n\\ni = 0\\n\\ndef handle_request(response):\\n    print(response.code)\\n    global i\\n    i -= 1\\n    if i == 0:\\n        ioloop.IOLoop.instance().stop()\\n\\nhttp_client = httpclient.AsyncHTTPClient()\\nfor url in open('urls.txt'):\\n    i += 1\\n    http_client.fetch(url.strip(), handle_request, method='HEAD')\\nioloop.IOLoop.instance().start()\\n</code>\", '<code>twisted</code>', '<code>HTTP</code>', \"<code>import grequests\\n\\nurls = [\\n   'http://www.heroku.com',\\n   'http://tablib.org',\\n   'http://httpbin.org',\\n   'http://python-requests.org',\\n   'http://kennethreitz.com'\\n]\\n</code>\", '<code>&gt;&gt;&gt; rs = (grequests.get(u) for u in urls)\\n</code>', '<code>&gt;&gt;&gt; grequests.map(rs)\\n[&lt;Response [200]&gt;, &lt;Response [200]&gt;, &lt;Response [200]&gt;, &lt;Response [200]&gt;, &lt;Response [200]&gt;]\\n</code>', '<code>list</code>', '<code>dict</code>', '<code>from twisted.internet import reactor, threads\\nfrom urlparse import urlparse\\nimport httplib\\nimport itertools\\n\\n\\nconcurrent = 200\\nfinished=itertools.count(1)\\nreactor.suggestThreadPoolSize(concurrent)\\n\\ndef getStatus(ourl):\\n    url = urlparse(ourl)\\n    conn = httplib.HTTPConnection(url.netloc)   \\n    conn.request(\"HEAD\", url.path)\\n    res = conn.getresponse()\\n    return res.status\\n\\ndef processResponse(response,url):\\n    print response, url\\n    processedOne()\\n\\ndef processError(error,url):\\n    print \"error\", url#, error\\n    processedOne()\\n\\ndef processedOne():\\n    if finished.next()==added:\\n        reactor.stop()\\n\\ndef addTask(url):\\n    req = threads.deferToThread(getStatus, url)\\n    req.addCallback(processResponse, url)\\n    req.addErrback(processError, url)   \\n\\nadded=0\\nfor url in open(\\'urllist.txt\\'):\\n    added+=1\\n    addTask(url.strip())\\n\\ntry:\\n    reactor.run()\\nexcept KeyboardInterrupt:\\n    reactor.stop()\\n</code>', '<code>[kalmi@ubi1:~] wc -l urllist.txt\\n10000 urllist.txt\\n[kalmi@ubi1:~] time python f.py &gt; /dev/null \\n\\nreal    1m10.682s\\nuser    0m16.020s\\nsys 0m10.330s\\n[kalmi@ubi1:~] head -n 6 urllist.txt\\nhttp://www.google.com\\nhttp://www.bix.hu\\nhttp://www.godaddy.com\\nhttp://www.google.com\\nhttp://www.bix.hu\\nhttp://www.godaddy.com\\n[kalmi@ubi1:~] python f.py | head -n 6\\n200 http://www.bix.hu\\n200 http://www.bix.hu\\n200 http://www.bix.hu\\n200 http://www.bix.hu\\n200 http://www.bix.hu\\n200 http://www.bix.hu\\n</code>', '<code>bix.hu is ~10 ms away from me\\ngodaddy.com: ~170 ms\\ngoogle.com: ~30 ms\\n</code>', '<code>import pandas as pd\\nimport concurrent.futures\\nimport requests\\nimport time\\n\\nout = []\\nCONNECTIONS = 100\\nTIMEOUT = 5\\ntime1 = None\\ntime2 = None\\n\\ntlds = open(\\'../data/sample_1k.txt\\').read().splitlines()\\nurls = [\\'http://{}\\'.format(x) for x in tlds[1:]]\\n\\ndef load_url(url, timeout):\\n    ans = requests.head(url, timeout=timeout)\\n    return ans.status_code\\n\\nwith concurrent.futures.ThreadPoolExecutor(max_workers=CONNECTIONS) as executor:\\n    future_to_url = {executor.submit(load_url, url, TIMEOUT): url for url in urls}\\n    for future in concurrent.futures.as_completed(future_to_url):\\n        try:\\n            data = future.result()\\n        except Exception as exc:\\n            data = str(type(exc))\\n        finally:\\n            out.append(data)\\n\\n            print(str(len(out)),end=\"\\\\r\")\\n\\n            if time1 == None:\\n                time1 = time.time()\\n            if len(out)/len(urls)&gt;=1 and time2==None:\\n                time2 = time.time()\\n\\nprint(\\'Took {:.2f} s\\'.format((time2-time1)))\\nprint(str(pd.Series(out).value_counts()))\\n</code>', '<code>pool = ThreadPool(poolsize)\\nrequests = makeRequests(some_callable, list_of_args, callback)\\n[pool.putRequest(req) for req in requests]\\npool.wait()\\n</code>', '<code>q_size</code>', '<code>putRequest</code>', '<code>timeout</code>', '<code>putRequest</code>', \"<code>#!/usr/bin/python2.7\\n\\nfrom twisted.internet import reactor\\nfrom twisted.internet.defer import Deferred, DeferredList, DeferredLock\\nfrom twisted.internet.defer import inlineCallbacks\\nfrom twisted.web.client import Agent, HTTPConnectionPool\\nfrom twisted.web.http_headers import Headers\\nfrom pprint import pprint\\nfrom collections import defaultdict\\nfrom urlparse import urlparse\\nfrom random import randrange\\nimport fileinput\\n\\npool = HTTPConnectionPool(reactor)\\npool.maxPersistentPerHost = 16\\nagent = Agent(reactor, pool)\\nlocks = defaultdict(DeferredLock)\\ncodes = {}\\n\\ndef getLock(url, simultaneous = 1):\\n    return locks[urlparse(url).netloc, randrange(simultaneous)]\\n\\n@inlineCallbacks\\ndef getMapping(url):\\n    # Limit ourselves to 4 simultaneous connections per host\\n    # Tweak this number, but it should be no larger than pool.maxPersistentPerHost \\n    lock = getLock(url,4)\\n    yield lock.acquire()\\n    try:\\n        resp = yield agent.request('HEAD', url)\\n        codes[url] = resp.code\\n    except Exception as e:\\n        codes[url] = str(e)\\n    finally:\\n        lock.release()\\n\\n\\ndl = DeferredList(getMapping(url.strip()) for url in fileinput.input())\\ndl.addCallback(lambda _: reactor.stop())\\n\\nreactor.run()\\npprint(codes)\\n</code>\"]",
         "title": "What is the fastest way to send 100,000 HTTP requests in Python?",
         "_childDocuments_": [
            {
               "up_vote_count": 2,
               "answer_id": 2632583,
               "last_activity_date": 1514234535,
               "path": "3.stack.answer",
               "body_markdown": "The easiest way would be to use Python&#39;s built-in threading library. &lt;s&gt;They&#39;re not &quot;real&quot; / kernel threads&lt;/s&gt; They have issues (like serialization), but are good enough. You&#39;d want a queue &amp; thread pool. One option is [here][1], but it&#39;s trivial to write your own. You can&#39;t parallelize all 100,000 calls, but you can fire off 100 (or so) of them at the same time.\r\n\r\n  [1]: http://pypi.python.org/pypi/threadpool/1.2.5",
               "tags": [],
               "creation_date": 1271187025,
               "last_edit_date": 1514234535,
               "is_accepted": false,
               "id": "2632583",
               "down_vote_count": 4,
               "score": -2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 2632661,
               "is_accepted": false,
               "last_activity_date": 1271187761,
               "body_markdown": "Using a &lt;a href=&quot;http://en.wikipedia.org/wiki/Thread_pool&quot;&gt;thread pool&lt;/a&gt; is a good option, and will make this fairly easy. Unfortunately, python doesn&#39;t have a standard library that makes thread pools ultra easy. But here is a decent library that should get you started:\r\nhttp://www.chrisarndt.de/projects/threadpool/\r\n\r\nCode example from their site:\r\n\r\n    pool = ThreadPool(poolsize)\r\n    requests = makeRequests(some_callable, list_of_args, callback)\r\n    [pool.putRequest(req) for req in requests]\r\n    pool.wait()\r\n\r\nHope this helps.",
               "id": "2632661",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1271187761,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 2632662,
               "is_accepted": false,
               "last_activity_date": 1271187782,
               "body_markdown": "For your case, threading will probably do the trick as you&#39;ll probably be spending most time waiting for a response. There are helpful modules like [Queue][1] in the standard library that might help.\r\n\r\nI did a similar thing with parallel downloading of files before and it was good enough for me, but it wasn&#39;t on the scale you are talking about.\r\n\r\nIf your task was more CPU-bound, you might want to look at the [multiprocessing][2] module, which will allow you to utilize more CPUs/cores/threads (more processes that won&#39;t block each other since the locking is per process)\r\n\r\n  [1]: http://docs.python.org/library/queue.html\r\n  [2]: http://docs.python.org/library/multiprocessing.html",
               "id": "2632662",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1271187782,
               "score": 0
            },
            {
               "up_vote_count": 10,
               "answer_id": 2632847,
               "last_activity_date": 1272663739,
               "path": "3.stack.answer",
               "body_markdown": "A good approach to solving this problem is to first write the code required to get one result, then incorporate threading code to parallelize the application.\r\n\r\nIn a perfect world this would simply mean simultaneously starting 100,000 threads which output their results into a dictionary or list for later processing, but in practice you are limited in how many parallel HTTP requests you can issue in this fashion.  Locally, you have limits in how many sockets you can open concurrently, how many threads of execution your Python interpreter will allow.  Remotely, you may be limited in the number of simultaneous connections if all the requests are against one server, or many.  These limitations will probably necessitate that you write the script in such a way as to only poll a small fraction of the URLs at any one time (100, as another poster mentioned, is probably a decent thread pool size, although you may find that you can successfully deploy many more).\r\n\r\nYou can follow this design pattern to resolve the above issue:\r\n\r\n 1. Start a thread which launches new request threads until the number of currently running threads (you can track them via threading.active_count() or by pushing the thread objects into a data structure) is &gt;= your maximum number of simultaneous requests (say 100), then sleeps for a short timeout.  This thread should terminate when there is are no more URLs to process.  Thus, the thread will keep waking up, launching new threads, and sleeping until your are finished.\r\n 2. Have the request threads store their results in some data structure for later retrieval and output.  If the structure you are storing the results in is a `list` or `dict` in CPython, you can [safely append or insert unique items from your threads without locks][1], but if you write to a file or require in more complex cross-thread data interaction *you should use a mutual exclusion lock to protect this state from corruption*.\r\n\r\nI would suggest you use the [threading][2] module.  You can use it to launch and track running threads.  Python&#39;s threading support is bare, but the description of your problem suggests that it is completely sufficient for your needs.\r\n\r\nFinally, if you&#39;d like to see a pretty straightforward application of a parallel network application written in Python, check out [ssh.py][3].  It&#39;s a small library which uses Python threading to parallelize many SSH connections.  The design is close enough to your requirements that you may find it to be a good resource.\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/2740435/are-there-some-cases-where-python-threads-can-safely-manipulate-shared-state\r\n  [2]: http://docs.python.org/library/threading.html\r\n  [3]: http://github.com/ekg/ssh.py/blob/master/ssh.py",
               "tags": [],
               "creation_date": 1271189389,
               "last_edit_date": 1495540969,
               "is_accepted": false,
               "id": "2632847",
               "down_vote_count": 3,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 2632871,
               "is_accepted": false,
               "last_activity_date": 1271189567,
               "body_markdown": "If you&#39;re looking to get the best performance possible, you might want to consider using Asynchronous I/O rather than threads. The overhead associated with thousands of OS threads is non-trivial and the context switching within the Python interpreter adds even more on top of it. Threading will certainly get the job done but I suspect that an asynchronous route will provide better overall performance.\r\n\r\nSpecifically, I&#39;d suggest the async web client in the Twisted library (http://www.twistedmatrix.com). It has an admittedly steep learning curve but it quite easy to use once you get a good handle on Twisted&#39;s style of asynchronous programming.\r\n\r\nA HowTo on Twisted&#39;s asynchronous web client API is available at:\r\n\r\nhttp://twistedmatrix.com/documents/current/web/howto/client.html",
               "id": "2632871",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1271189567,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 31,
               "answer_id": 2632885,
               "is_accepted": false,
               "last_activity_date": 1271189648,
               "body_markdown": "Threads are absolutely not the answer here. They will provide both process and kernel bottlenecks, as well as throughput limits that are not acceptable if the overall goal is &quot;the fastest way&quot;.\r\n\r\nA little bit of `twisted` and its asynchronous `HTTP` client would give you much better results.",
               "id": "2632885",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1271189648,
               "score": 30
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 2632928,
               "is_accepted": false,
               "last_activity_date": 1271190002,
               "body_markdown": "Consider using [Windmill][1] , although Windmill probably cant do that many threads.  \r\n\r\n You could do it with a hand rolled Python script on 5 machines, each one connecting outbound using ports 40000-60000,   opening 100,000 port connections.  \r\n\r\nAlso, it might help to do a sample test with a nicely threaded QA app such as [OpenSTA][2]  in order to get an idea of how much each server can handle.\r\n\r\nAlso, try looking into just using simple Perl with the LWP::ConnCache  class.  You&#39;ll probably get more performance (more connections) that way.\r\n\r\n\r\n  [1]: http://www.getwindmill.com/\r\n  [2]: http://www.opensta.org/",
               "id": "2632928",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1271190002,
               "score": 0
            },
            {
               "up_vote_count": 5,
               "answer_id": 2634565,
               "last_activity_date": 1271222585,
               "path": "3.stack.answer",
               "body_markdown": "A solution:\r\n\r\n    from twisted.internet import reactor, threads\r\n    from urlparse import urlparse\r\n    import httplib\r\n    import itertools\r\n    \r\n    \r\n    concurrent = 200\r\n    finished=itertools.count(1)\r\n    reactor.suggestThreadPoolSize(concurrent)\r\n    \r\n    def getStatus(ourl):\r\n    \turl = urlparse(ourl)\r\n    \tconn = httplib.HTTPConnection(url.netloc)\t\r\n    \tconn.request(&quot;HEAD&quot;, url.path)\r\n    \tres = conn.getresponse()\r\n    \treturn res.status\r\n    \r\n    def processResponse(response,url):\r\n    \tprint response, url\r\n    \tprocessedOne()\r\n    \t\r\n    def processError(error,url):\r\n    \tprint &quot;error&quot;, url#, error\r\n    \tprocessedOne()\r\n    \r\n    def processedOne():\r\n    \tif finished.next()==added:\r\n    \t\treactor.stop()\r\n    \r\n    def addTask(url):\r\n    \treq = threads.deferToThread(getStatus, url)\r\n    \treq.addCallback(processResponse, url)\r\n    \treq.addErrback(processError, url)\t\r\n    \r\n    added=0\r\n    for url in open(&#39;urllist.txt&#39;):\r\n    \tadded+=1\r\n    \taddTask(url.strip())\r\n    \t\r\n    try:\r\n    \treactor.run()\r\n    except KeyboardInterrupt:\r\n    \treactor.stop()\r\n\r\n\r\nTesttime:\r\n\r\n\r\n    [kalmi@ubi1:~] wc -l urllist.txt\r\n    10000 urllist.txt\r\n    [kalmi@ubi1:~] time python f.py &gt; /dev/null \r\n    \r\n    real\t1m10.682s\r\n    user\t0m16.020s\r\n    sys\t0m10.330s\r\n    [kalmi@ubi1:~] head -n 6 urllist.txt\r\n    http://www.google.com\r\n    http://www.bix.hu\r\n    http://www.godaddy.com\r\n    http://www.google.com\r\n    http://www.bix.hu\r\n    http://www.godaddy.com\r\n    [kalmi@ubi1:~] python f.py | head -n 6\r\n    200 http://www.bix.hu\r\n    200 http://www.bix.hu\r\n    200 http://www.bix.hu\r\n    200 http://www.bix.hu\r\n    200 http://www.bix.hu\r\n    200 http://www.bix.hu\r\n\r\nPingtime:\r\n\r\n    bix.hu is ~10 ms away from me\r\n    godaddy.com: ~170 ms\r\n    google.com: ~30 ms",
               "tags": [],
               "creation_date": 1271213199,
               "last_edit_date": 1271222585,
               "is_accepted": false,
               "id": "2634565",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "up_vote_count": 144,
               "answer_id": 2635066,
               "last_activity_date": 1409229526,
               "path": "3.stack.answer",
               "body_markdown": "Twistedless solution:\r\n\r\n    from urlparse import urlparse\r\n    from threading import Thread\r\n    import httplib, sys\r\n    from Queue import Queue\r\n    \r\n    concurrent = 200\r\n    \r\n    def doWork():\r\n    \twhile True:\r\n    \t\turl = q.get()\r\n    \t\tstatus, url = getStatus(url)\r\n    \t\tdoSomethingWithResult(status, url)\r\n    \t\tq.task_done()\r\n    \r\n    def getStatus(ourl):\r\n    \ttry:\r\n    \t\turl = urlparse(ourl)\r\n    \t\tconn = httplib.HTTPConnection(url.netloc)\t\r\n    \t\tconn.request(&quot;HEAD&quot;, url.path)\r\n    \t\tres = conn.getresponse()\r\n    \t\treturn res.status, ourl\r\n    \texcept:\r\n    \t\treturn &quot;error&quot;, ourl\r\n    \r\n    def doSomethingWithResult(status, url):\r\n    \tprint status, url\r\n    \r\n    q = Queue(concurrent * 2)\r\n    for i in range(concurrent):\r\n    \tt = Thread(target=doWork)\r\n    \tt.daemon = True\r\n    \tt.start()\r\n    try:\r\n    \tfor url in open(&#39;urllist.txt&#39;):\r\n    \t\tq.put(url.strip())\r\n    \tq.join()\r\n    except KeyboardInterrupt:\r\n    \tsys.exit(1)\r\n\r\nThis one is slighty faster than the twisted solution and uses less CPU.",
               "tags": [],
               "creation_date": 1271222563,
               "last_edit_date": 1409229526,
               "is_accepted": true,
               "id": "2635066",
               "down_vote_count": 1,
               "score": 143
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 10,
               "answer_id": 24731721,
               "is_accepted": false,
               "last_activity_date": 1405323685,
               "body_markdown": "Use [grequests](https://github.com/kennethreitz/grequests) , it&#39;s a combination of requests + Gevent module .\r\n\r\nGRequests allows you to use Requests with Gevent to make asyncronous HTTP Requests easily.\r\n\r\nUsage is simple:\r\n\r\n    import grequests\r\n\r\n    urls = [\r\n       &#39;http://www.heroku.com&#39;,\r\n       &#39;http://tablib.org&#39;,\r\n       &#39;http://httpbin.org&#39;,\r\n       &#39;http://python-requests.org&#39;,\r\n       &#39;http://kennethreitz.com&#39;\r\n    ]\r\n\r\nCreate a set of unsent Requests:\r\n\r\n    &gt;&gt;&gt; rs = (grequests.get(u) for u in urls)\r\n\r\nSend them all at the same time:\r\n\r\n    &gt;&gt;&gt; grequests.map(rs)\r\n    [&lt;Response [200]&gt;, &lt;Response [200]&gt;, &lt;Response [200]&gt;, &lt;Response [200]&gt;, &lt;Response [200]&gt;]",
               "id": "24731721",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1405323685,
               "score": 10
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 25255485,
               "is_accepted": false,
               "last_activity_date": 1407812420,
               "body_markdown": "This twisted async web client goes pretty fast.\r\n\r\n    #!/usr/bin/python2.7\r\n    \r\n    from twisted.internet import reactor\r\n    from twisted.internet.defer import Deferred, DeferredList, DeferredLock\r\n    from twisted.internet.defer import inlineCallbacks\r\n    from twisted.web.client import Agent, HTTPConnectionPool\r\n    from twisted.web.http_headers import Headers\r\n    from pprint import pprint\r\n    from collections import defaultdict\r\n    from urlparse import urlparse\r\n    from random import randrange\r\n    import fileinput\r\n    \r\n    pool = HTTPConnectionPool(reactor)\r\n    pool.maxPersistentPerHost = 16\r\n    agent = Agent(reactor, pool)\r\n    locks = defaultdict(DeferredLock)\r\n    codes = {}\r\n    \r\n    def getLock(url, simultaneous = 1):\r\n        return locks[urlparse(url).netloc, randrange(simultaneous)]\r\n    \r\n    @inlineCallbacks\r\n    def getMapping(url):\r\n        # Limit ourselves to 4 simultaneous connections per host\r\n        # Tweak this number, but it should be no larger than pool.maxPersistentPerHost \r\n        lock = getLock(url,4)\r\n        yield lock.acquire()\r\n        try:\r\n            resp = yield agent.request(&#39;HEAD&#39;, url)\r\n            codes[url] = resp.code\r\n        except Exception as e:\r\n            codes[url] = str(e)\r\n        finally:\r\n            lock.release()\r\n    \r\n    \r\n    dl = DeferredList(getMapping(url.strip()) for url in fileinput.input())\r\n    dl.addCallback(lambda _: reactor.stop())\r\n    \r\n    reactor.run()\r\n    pprint(codes)\r\n\r\n",
               "id": "25255485",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1407812420,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 37,
               "answer_id": 25549675,
               "is_accepted": false,
               "last_activity_date": 1409231506,
               "body_markdown": "A solution using [tornado][1] asynchronous networking library\r\n\r\n    from tornado import ioloop, httpclient\r\n    \r\n    i = 0\r\n    \r\n    def handle_request(response):\r\n        print(response.code)\r\n        global i\r\n        i -= 1\r\n        if i == 0:\r\n            ioloop.IOLoop.instance().stop()\r\n    \r\n    http_client = httpclient.AsyncHTTPClient()\r\n    for url in open(&#39;urls.txt&#39;):\r\n        i += 1\r\n        http_client.fetch(url.strip(), handle_request, method=&#39;HEAD&#39;)\r\n    ioloop.IOLoop.instance().start()\r\n\r\n\r\n  [1]: http://www.tornadoweb.org",
               "id": "25549675",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1409231506,
               "score": 36
            },
            {
               "up_vote_count": 5,
               "answer_id": 46144596,
               "last_activity_date": 1505071335,
               "path": "3.stack.answer",
               "body_markdown": "Things have changed quite a bit since 2010 when this was posted and I haven&#39;t tried all the other answers but I have tried a few, and I found this to work the best for me using python3.6. \r\n\r\nI was able to fetch about ~150 unique domains per second running on AWS.\r\n\r\n    import pandas as pd\r\n    import concurrent.futures\r\n    import requests\r\n    import time\r\n    \r\n    out = []\r\n    CONNECTIONS = 100\r\n    TIMEOUT = 5\r\n    time1 = None\r\n    time2 = None\r\n    \r\n    tlds = open(&#39;../data/sample_1k.txt&#39;).read().splitlines()\r\n    urls = [&#39;http://{}&#39;.format(x) for x in tlds[1:]]\r\n    \r\n    def load_url(url, timeout):\r\n        ans = requests.head(url, timeout=timeout)\r\n        return ans.status_code\r\n    \r\n    with concurrent.futures.ThreadPoolExecutor(max_workers=CONNECTIONS) as executor:\r\n        future_to_url = {executor.submit(load_url, url, TIMEOUT): url for url in urls}\r\n        for future in concurrent.futures.as_completed(future_to_url):\r\n            try:\r\n                data = future.result()\r\n            except Exception as exc:\r\n                data = str(type(exc))\r\n            finally:\r\n                out.append(data)\r\n    \r\n                print(str(len(out)),end=&quot;\\r&quot;)\r\n    \r\n                if time1 == None:\r\n                    time1 = time.time()\r\n                if len(out)/len(urls)&gt;=1 and time2==None:\r\n                    time2 = time.time()\r\n    \r\n    print(&#39;Took {:.2f} s&#39;.format((time2-time1)))\r\n    print(str(pd.Series(out).value_counts()))",
               "tags": [],
               "creation_date": 1505070827,
               "last_edit_date": 1505071335,
               "is_accepted": false,
               "id": "46144596",
               "down_vote_count": 0,
               "score": 5
            }
         ],
         "link": "https://stackoverflow.com/questions/2632520/what-is-the-fastest-way-to-send-100-000-http-requests-in-python",
         "id": "858127-2309"
      },
      {
         "up_vote_count": "241",
         "path": "2.stack",
         "body_markdown": "How can I find the duplicates in a Python list and create another list of the duplicates?  The list is just integers.",
         "view_count": "304718",
         "answer_count": "21",
         "tags": "['python', 'list', 'duplicates']",
         "creation_date": "1332489599",
         "last_edit_date": "1487001392",
         "code_snippet": "['<code>set(a)</code>', '<code>a = [1,2,3,2,1,5,6,5,5,5]\\n\\nimport collections\\nprint [item for item, count in collections.Counter(a).items() if count &gt; 1]\\n\\n## [1, 2, 5]\\n</code>', '<code>Counter</code>', '<code>set</code>', '<code>seen = set()\\nuniq = []\\nfor x in a:\\n    if x not in seen:\\n        uniq.append(x)\\n        seen.add(x)\\n</code>', '<code>seen = set()\\nuniq = [x for x in a if x not in seen and not seen.add(x)]    \\n</code>', '<code>not seen.add(x)</code>', '<code>add()</code>', '<code>None</code>', '<code>not</code>', '<code>a = [ [1], [2], [3], [1], [5], [3] ]\\n\\nno_dupes = [x for n, x in enumerate(a) if x not in a[:n]]\\nprint no_dupes # [[1], [2], [3], [5]]\\n\\ndupes = [x for n, x in enumerate(a) if x in a[:n]]\\nprint dupes # [[1], [3]]\\n</code>', '<code>O(n)</code>', '<code>O(1)</code>', '<code>dup = []</code>', '<code>else:     dup.append(x)</code>', '<code>print()</code>', '<code>&gt;&gt;&gt; l = [1,2,3,4,4,5,5,6,1]\\n&gt;&gt;&gt; set([x for x in l if l.count(x) &gt; 1])\\nset([1, 4, 5])\\n</code>', '<code>l</code>', '<code>set(l)</code>', \"<code>def list_duplicates(seq):\\n  seen = set()\\n  seen_add = seen.add\\n  # adds all elements it doesn't know yet to seen and all other to seen_twice\\n  seen_twice = set( x for x in seq if x in seen or seen_add(x) )\\n  # turn the set into a list (as requested)\\n  return list( seen_twice )\\n\\na = [1,2,3,2,1,5,6,5,5,5]\\nlist_duplicates(a) # yields [1, 2, 5]\\n</code>\", \"<code># file: test.py\\nimport collections\\n\\ndef thg435(l):\\n    return [x for x, y in collections.Counter(l).items() if y &gt; 1]\\n\\ndef moooeeeep(l):\\n    seen = set()\\n    seen_add = seen.add\\n    # adds all elements it doesn't know yet to seen and all other to seen_twice\\n    seen_twice = set( x for x in l if x in seen or seen_add(x) )\\n    # turn the set into a list (as requested)\\n    return list( seen_twice )\\n\\ndef RiteshKumar(l):\\n    return list(set([x for x in l if l.count(x) &gt; 1]))\\n\\ndef JohnLaRooy(L):\\n    seen = set()\\n    seen2 = set()\\n    seen_add = seen.add\\n    seen2_add = seen2.add\\n    for item in L:\\n        if item in seen:\\n            seen2_add(item)\\n        else:\\n            seen_add(item)\\n    return list(seen2)\\n\\nl = [1,2,3,2,1,5,6,5,5,5]*100\\n</code>\", \"<code>$ python -mtimeit -s 'import test' 'test.JohnLaRooy(test.l)'\\n10000 loops, best of 3: 74.6 usec per loop\\n$ python -mtimeit -s 'import test' 'test.moooeeeep(test.l)'\\n10000 loops, best of 3: 91.3 usec per loop\\n$ python -mtimeit -s 'import test' 'test.thg435(test.l)'\\n1000 loops, best of 3: 266 usec per loop\\n$ python -mtimeit -s 'import test' 'test.RiteshKumar(test.l)'\\n100 loops, best of 3: 8.35 msec per loop\\n</code>\", \"<code>$ pypy -mtimeit -s 'import test' 'test.JohnLaRooy(test.l)'\\n100000 loops, best of 3: 17.8 usec per loop\\n$ pypy -mtimeit -s 'import test' 'test.thg435(test.l)'\\n10000 loops, best of 3: 23 usec per loop\\n$ pypy -mtimeit -s 'import test' 'test.moooeeeep(test.l)'\\n10000 loops, best of 3: 39.3 usec per loop\\n</code>\", '<code>l = [random.randrange(1000000) for i in xrange(10000)]</code>', \"<code>$ pypy -mtimeit -s 'import test' 'test.moooeeeep(test.l)'\\n1000 loops, best of 3: 495 usec per loop\\n$ pypy -mtimeit -s 'import test' 'test.JohnLaRooy(test.l)'\\n1000 loops, best of 3: 499 usec per loop\\n$ pypy -mtimeit -s 'import test' 'test.thg435(test.l)'\\n1000 loops, best of 3: 1.68 msec per loop\\n</code>\", '<code>add</code>', '<code>pypy</code>', '<code>&gt;&gt;&gt; print list(getDupes_9([1,2,3,2,1,5,6,5,5,5]))\\n[1, 2, 5]\\n</code>', \"<code>def getDupes(c):\\n        '''sort/tee/izip'''\\n        a, b = itertools.tee(sorted(c))\\n        next(b, None)\\n        r = None\\n        for k, g in itertools.izip(a, b):\\n            if k != g: continue\\n            if k != r:\\n                yield k\\n                r = k\\n</code>\", '<code>import itertools\\nimport time\\nimport random\\n\\ndef getDupes_1(c):\\n    \\'\\'\\'naive\\'\\'\\'\\n    for i in xrange(0, len(c)):\\n        if c[i] in c[:i]:\\n            yield c[i]\\n\\ndef getDupes_2(c):\\n    \\'\\'\\'set len change\\'\\'\\'\\n    s = set()\\n    for i in c:\\n        l = len(s)\\n        s.add(i)\\n        if len(s) == l:\\n            yield i\\n\\ndef getDupes_3(c):\\n    \\'\\'\\'in dict\\'\\'\\'\\n    d = {}\\n    for i in c:\\n        if i in d:\\n            if d[i]:\\n                yield i\\n                d[i] = False\\n        else:\\n            d[i] = True\\n\\ndef getDupes_4(c):\\n    \\'\\'\\'in set\\'\\'\\'\\n    s,r = set(),set()\\n    for i in c:\\n        if i not in s:\\n            s.add(i)\\n        elif i not in r:\\n            r.add(i)\\n            yield i\\n\\ndef getDupes_5(c):\\n    \\'\\'\\'sort/adjacent\\'\\'\\'\\n    c = sorted(c)\\n    r = None\\n    for i in xrange(1, len(c)):\\n        if c[i] == c[i - 1]:\\n            if c[i] != r:\\n                yield c[i]\\n                r = c[i]\\n\\ndef getDupes_6(c):\\n    \\'\\'\\'sort/groupby\\'\\'\\'\\n    def multiple(x):\\n        try:\\n            x.next()\\n            x.next()\\n            return True\\n        except:\\n            return False\\n    for k, g in itertools.ifilter(lambda x: multiple(x[1]), itertools.groupby(sorted(c))):\\n        yield k\\n\\ndef getDupes_7(c):\\n    \\'\\'\\'sort/zip\\'\\'\\'\\n    c = sorted(c)\\n    r = None\\n    for k, g in zip(c[:-1],c[1:]):\\n        if k == g:\\n            if k != r:\\n                yield k\\n                r = k\\n\\ndef getDupes_8(c):\\n    \\'\\'\\'sort/izip\\'\\'\\'\\n    c = sorted(c)\\n    r = None\\n    for k, g in itertools.izip(c[:-1],c[1:]):\\n        if k == g:\\n            if k != r:\\n                yield k\\n                r = k\\n\\ndef getDupes_9(c):\\n    \\'\\'\\'sort/tee/izip\\'\\'\\'\\n    a, b = itertools.tee(sorted(c))\\n    next(b, None)\\n    r = None\\n    for k, g in itertools.izip(a, b):\\n        if k != g: continue\\n        if k != r:\\n            yield k\\n            r = k\\n\\ndef getDupes_a(l):\\n    \\'\\'\\'moooeeeep\\'\\'\\'\\n    seen = set()\\n    seen_add = seen.add\\n    # adds all elements it doesn\\'t know yet to seen and all other to seen_twice\\n    for x in l:\\n        if x in seen or seen_add(x):\\n            yield x\\n\\ndef getDupes_b(x):\\n    \\'\\'\\'iter*/sorted\\'\\'\\'\\n    x = sorted(x)\\n    def _matches():\\n        for k,g in itertools.izip(x[:-1],x[1:]):\\n            if k == g:\\n                yield k\\n    for k, n in itertools.groupby(_matches()):\\n        yield k\\n\\ndef getDupes_c(a):\\n    \\'\\'\\'pandas\\'\\'\\'\\n    import pandas as pd\\n    vc = pd.Series(a).value_counts()\\n    i = vc[vc &gt; 1].index\\n    for _ in i:\\n        yield _\\n\\ndef hasDupes(fn,c):\\n    try:\\n        if fn(c).next(): return True    # Found a dupe\\n    except StopIteration:\\n        pass\\n    return False\\n\\ndef getDupes(fn,c):\\n    return list(fn(c))\\n\\nSTABLE = True\\nif STABLE:\\n    print \\'Finding FIRST then ALL duplicates, single dupe of \"nth\" placed element in 1m element array\\'\\nelse:\\n    print \\'Finding FIRST then ALL duplicates, single dupe of \"n\" included in randomised 1m element array\\'\\nfor location in (50,250000,500000,750000,999999):\\n    for test in (getDupes_2, getDupes_3, getDupes_4, getDupes_5, getDupes_6,\\n                 getDupes_8, getDupes_9, getDupes_a, getDupes_b, getDupes_c):\\n        print \\'Test %-15s:%10d - \\'%(test.__doc__ or test.__name__,location),\\n        deltas = []\\n        for FIRST in (True,False):\\n            for i in xrange(0, 5):\\n                c = range(0,1000000)\\n                if STABLE:\\n                    c[0] = location\\n                else:\\n                    c.append(location)\\n                    random.shuffle(c)\\n                start = time.time()\\n                if FIRST:\\n                    print \\'.\\' if location == test(c).next() else \\'!\\',\\n                else:\\n                    print \\'.\\' if [location] == list(test(c)) else \\'!\\',\\n                deltas.append(time.time()-start)\\n            print \\' -- %0.3f  \\'%(sum(deltas)/len(deltas)),\\n        print\\n    print\\n</code>', '<code>Finding FIRST then ALL duplicates, single dupe of \"nth\" placed element in 1m element array\\nTest set len change :    500000 -  . . . . .  -- 0.264   . . . . .  -- 0.402  \\nTest in dict        :    500000 -  . . . . .  -- 0.163   . . . . .  -- 0.250  \\nTest in set         :    500000 -  . . . . .  -- 0.163   . . . . .  -- 0.249  \\nTest sort/adjacent  :    500000 -  . . . . .  -- 0.159   . . . . .  -- 0.229  \\nTest sort/groupby   :    500000 -  . . . . .  -- 0.860   . . . . .  -- 1.286  \\nTest sort/izip      :    500000 -  . . . . .  -- 0.165   . . . . .  -- 0.229  \\nTest sort/tee/izip  :    500000 -  . . . . .  -- 0.145   . . . . .  -- 0.206  *\\nTest moooeeeep      :    500000 -  . . . . .  -- 0.149   . . . . .  -- 0.232  \\nTest iter*/sorted   :    500000 -  . . . . .  -- 0.160   . . . . .  -- 0.221  \\nTest pandas         :    500000 -  . . . . .  -- 0.493   . . . . .  -- 0.499  \\n</code>', '<code>Finding FIRST then ALL duplicates, single dupe of \"n\" included in randomised 1m element array\\nTest set len change :    500000 -  . . . . .  -- 0.321   . . . . .  -- 0.473  \\nTest in dict        :    500000 -  . . . . .  -- 0.285   . . . . .  -- 0.360  \\nTest in set         :    500000 -  . . . . .  -- 0.309   . . . . .  -- 0.365  \\nTest sort/adjacent  :    500000 -  . . . . .  -- 0.756   . . . . .  -- 0.823  \\nTest sort/groupby   :    500000 -  . . . . .  -- 1.459   . . . . .  -- 1.896  \\nTest sort/izip      :    500000 -  . . . . .  -- 0.786   . . . . .  -- 0.845  \\nTest sort/tee/izip  :    500000 -  . . . . .  -- 0.743   . . . . .  -- 0.804  \\nTest moooeeeep      :    500000 -  . . . . .  -- 0.234   . . . . .  -- 0.311  *\\nTest iter*/sorted   :    500000 -  . . . . .  -- 0.776   . . . . .  -- 0.840  \\nTest pandas         :    500000 -  . . . . .  -- 0.539   . . . . .  -- 0.540  \\n</code>', '<code>random.shuffle(c)</code>', '<code>\\nPython 2.5.4 (r254:67916, May 31 2010, 15:03:39) \\n[GCC 4.1.2 20080704 (Red Hat 4.1.2-46)] on linux2\\na = [1,2,3,2,1,5,6,5,5,5]\\nimport collections\\nprint [x for x, y in collections.Counter(a).items() if y &gt; 1]\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n  File \"\", line 1, in \\nAttributeError: \\'module\\' object has no attribute \\'Counter\\'\\n&gt;&gt;&gt; \\n</code>', '<code>a = [1,2,3,2,1,5,6,5,5,5]\\nd = {}\\nfor elem in a:\\n    if elem in d:\\n        d[elem] += 1\\n    else:\\n        d[elem] = 1\\n\\nprint [x for x, y in d.items() if y &gt; 1]\\n</code>', '<code>&gt;&gt;&gt; import pandas as pd\\n&gt;&gt;&gt; a = [1, 2, 1, 3, 3, 3, 0]\\n&gt;&gt;&gt; pd.Series(a)[pd.Series(a).duplicated()].values\\narray([1, 3, 3])\\n</code>', '<code>import pandas as pd\\na = [1,2,3,3,3,4,5,6,6,7]\\nvc = pd.Series(a).value_counts()\\nvc[vc &gt; 1].index.tolist()\\n</code>', '<code>[3,6]\\n</code>', '<code>pda = pd.Series(a)</code>', '<code>print list(pda[pda.duplicated()])</code>', '<code>for x in set(li):\\n    li.remove(x)\\n\\nli = list(set(li))\\n</code>', '<code>l=[1,2,3,5,4,1,3,1]\\ns=set(l)\\nd=[]\\nfor x in l:\\n    if x in s:\\n        s.remove(x)\\n    else:\\n        d.append(x)\\nd\\n[1,3,1]\\n</code>', \"<code>testList = ['red', 'blue', 'red', 'green', 'blue', 'blue']\\n\\ntestListDict = {}\\n\\nfor item in testList:\\n  try:\\n    testListDict[item] += 1\\n  except:\\n    testListDict[item] = 1\\n\\nprint testListDict\\n</code>\", \"<code>&gt;&gt;&gt; print testListDict\\n{'blue': 3, 'green': 1, 'red': 2}\\n</code>\", '<code>myList  = [2 ,4 , 6, 8, 4, 6, 12];\\nnewList = set()\\n\\nfor i in myList:\\n    if myList.count(i) &gt;= 2:\\n        newList.add(i)\\n\\nprint(list(newList))\\n## [4 , 6]\\n</code>', '<code>number_lst = [1, 1, 2, 3, 5, ...]\\n\\nseen_set = set()\\nduplicate_set = set(x for x in number_lst if x in seen_set or seen_set.add(x))\\nunique_set = seen_set - duplicate_set\\n</code>', '<code>iteration_utilities.duplicates</code>', '<code>&gt;&gt;&gt; from iteration_utilities import duplicates\\n\\n&gt;&gt;&gt; list(duplicates([1,1,2,1,2,3,4,2]))\\n[1, 1, 2, 2]\\n</code>', '<code>iteration_utilities.unique_everseen</code>', '<code>&gt;&gt;&gt; from iteration_utilities import unique_everseen\\n\\n&gt;&gt;&gt; list(unique_everseen(duplicates([1,1,2,1,2,3,4,2])))\\n[1, 2]\\n</code>', '<code>iteration_utilities</code>', '<code>itertools.groupby</code>', '<code>from itertools import groupby\\n\\nmyList  = [2, 4, 6, 8, 4, 6, 12]\\n# when the list is sorted, groupby groups by consecutive elements which are similar\\nfor x, y in groupby(sorted(myList)):\\n    #  list(y) returns all the occurences of item x\\n    if len(list(y)) &gt; 1:\\n        print x  \\n</code>', '<code>4\\n6\\n</code>', '<code>list2 = [1, 2, 3, 4, 1, 2, 3]\\nlset = set()\\n[(lset.add(item), list2.append(item))\\n for item in list2 if item not in lset]\\nprint list(lset)\\n</code>', '<code>set([i for i in list if sum([1 for a in list if a == i]) &gt; 1])\\n</code>', '<code>def get_duplicates(sorted_list):\\n    duplicates = []\\n    last = sorted_list[0]\\n    for x in sorted_list[1:]:\\n        if x == last:\\n            duplicates.append(x)\\n        last = x\\n    return set(duplicates)\\n</code>', '<code>def gen_dupes(array):\\n    unique = {}\\n    for value in array:\\n        if value in unique and unique[value]:\\n            unique[value] = False\\n            yield value\\n        else:\\n            unique[value] = True\\n\\narray = [1, 2, 2, 3, 4, 1, 5, 2, 6, 6]\\nprint(list(gen_dupes(array)))\\n# =&gt; [2, 1, 6]\\n</code>', '<code>def gen_dupes(array):\\n    unique = {}\\n    for value in array:\\n        is_list = False\\n        if type(value) is list:\\n            value = tuple(value)\\n            is_list = True\\n\\n        if value in unique and unique[value]:\\n            unique[value] = False\\n            if is_list:\\n                value = list(value)\\n\\n            yield value\\n        else:\\n            unique[value] = True\\n\\narray = [1, 2, 2, [1, 2], 3, 4, [1, 2], 5, 2, 6, 6]\\nprint(list(gen_dupes(array)))\\n# =&gt; [2, [1, 2], 6]\\n</code>', '<code>def removeduplicates(a):\\n  seen = set()\\n\\n  for i in a:\\n    if i not in seen:\\n      seen.add(i)\\n  return seen \\n\\nprint(removeduplicates([1,1,2,2]))\\n</code>', \"<code>def dupList(oldlist):\\n    if type(oldlist)==type((2,2)):\\n        oldlist=[x for x in oldlist]\\n    newList=[]\\n    newList=newList+oldlist\\n    oldlist=oldlist\\n    forbidden=[]\\n    checkPoint=0\\n    for i in range(len(oldlist)):\\n        #print 'start i', i\\n        if i in forbidden:\\n            continue\\n        else:\\n            for j in range(len(oldlist)):\\n                #print 'start j', j\\n                if j in forbidden:\\n                    continue\\n                else:\\n                    #print 'after Else'\\n                    if i!=j: \\n                        #print 'i,j', i,j\\n                        #print oldlist\\n                        #print newList\\n                        if oldlist[j]==oldlist[i]:\\n                            #print 'oldlist[i],oldlist[j]', oldlist[i],oldlist[j]\\n                            forbidden.append(j)\\n                            #print 'forbidden', forbidden\\n                            del newList[j-checkPoint]\\n                            #print newList\\n                            checkPoint=checkPoint+1\\n    return newList\\n</code>\", '<code>&gt;&gt;&gt;a = [1,2,3,3,3,4,5,6,6,7]\\n&gt;&gt;&gt;dupList(a)\\n[1, 2, 3, 4, 5, 6, 7]\\n</code>', '<code>duplist = list(set(a))</code>', '<code>sort()</code>', '<code>l1[i] == l1[i+1]</code>']",
         "title": "Find and list duplicates in a list?",
         "_childDocuments_": [
            {
               "up_vote_count": 279,
               "answer_id": 9835819,
               "last_activity_date": 1502466671,
               "path": "3.stack.answer",
               "body_markdown": "To remove duplicates use `set(a)`, to print duplicates - something like\r\n\r\n    a = [1,2,3,2,1,5,6,5,5,5]\r\n    \r\n    import collections\r\n    print [item for item, count in collections.Counter(a).items() if count &gt; 1]\r\n    \r\n    ## [1, 2, 5]\r\n\r\nNote that `Counter` is not particularly efficient ([timings](https://stackoverflow.com/a/25706298/989121)) and probably an overkill here, `set` will perform better:\r\n\r\n    seen = set()\r\n    uniq = []\r\n    for x in a:\r\n        if x not in seen:\r\n            uniq.append(x)\r\n            seen.add(x)\r\n\r\nor, more concisely:\r\n\r\n    seen = set()\r\n    uniq = [x for x in a if x not in seen and not seen.add(x)]    \r\n\r\nI don&#39;t recommend the latter style, because it is not obvious what `not seen.add(x)` is doing  (the set `add()` method always returns `None`, hence the need for `not`).\r\n\r\nIf list elements are not hashable, you cannot use set/dicts and have to resort to a quadratic time solution (compare each which each), for example:\r\n\r\n    a = [ [1], [2], [3], [1], [5], [3] ]\r\n    \r\n    no_dupes = [x for n, x in enumerate(a) if x not in a[:n]]\r\n    print no_dupes # [[1], [2], [3], [5]]\r\n    \r\n    dupes = [x for n, x in enumerate(a) if x in a[:n]]\r\n    print dupes # [[1], [3]]\r\n\r\n",
               "tags": [],
               "creation_date": 1332489944,
               "last_edit_date": 1502466671,
               "is_accepted": true,
               "id": "9835819",
               "down_vote_count": 4,
               "score": 275
            },
            {
               "up_vote_count": 11,
               "answer_id": 9836287,
               "last_activity_date": 1332494974,
               "path": "3.stack.answer",
               "body_markdown": "collections.Counter is new in python 2.7:\r\n&lt;pre&gt;&lt;code&gt;\r\nPython 2.5.4 (r254:67916, May 31 2010, 15:03:39) \r\n[GCC 4.1.2 20080704 (Red Hat 4.1.2-46)] on linux2\r\na = [1,2,3,2,1,5,6,5,5,5]\r\nimport collections\r\nprint [x for x, y in collections.Counter(a).items() if y &gt; 1]\r\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\r\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\nAttributeError: &#39;module&#39; object has no attribute &#39;Counter&#39;\r\n&gt;&gt;&gt; \r\n&lt;/code&gt;&lt;/pre&gt;\r\n\r\nIn an earlier version you can use a conventional dict instead:\r\n\r\n    a = [1,2,3,2,1,5,6,5,5,5]\r\n    d = {}\r\n    for elem in a:\r\n        if elem in d:\r\n            d[elem] += 1\r\n        else:\r\n            d[elem] = 1\r\n            \r\n    print [x for x, y in d.items() if y &gt; 1]\r\n\r\n",
               "tags": [],
               "creation_date": 1332492748,
               "last_edit_date": 1332494974,
               "is_accepted": false,
               "id": "9836287",
               "down_vote_count": 2,
               "score": 9
            },
            {
               "up_vote_count": 58,
               "answer_id": 9836685,
               "last_activity_date": 1450424955,
               "path": "3.stack.answer",
               "body_markdown": "You don&#39;t need the count, just whether or not the item was seen before. Adapted [that answer][1] to this problem:\r\n\r\n    def list_duplicates(seq):\r\n      seen = set()\r\n      seen_add = seen.add\r\n      # adds all elements it doesn&#39;t know yet to seen and all other to seen_twice\r\n      seen_twice = set( x for x in seq if x in seen or seen_add(x) )\r\n      # turn the set into a list (as requested)\r\n      return list( seen_twice )\r\n\r\n    a = [1,2,3,2,1,5,6,5,5,5]\r\n    list_duplicates(a) # yields [1, 2, 5]\r\n\r\n---\r\nJust in case speed matters, here are some timings:\r\n\r\n    # file: test.py\r\n    import collections\r\n\r\n    def thg435(l):\r\n        return [x for x, y in collections.Counter(l).items() if y &gt; 1]\r\n        \r\n    def moooeeeep(l):\r\n        seen = set()\r\n        seen_add = seen.add\r\n        # adds all elements it doesn&#39;t know yet to seen and all other to seen_twice\r\n        seen_twice = set( x for x in l if x in seen or seen_add(x) )\r\n        # turn the set into a list (as requested)\r\n        return list( seen_twice )\r\n\r\n    def RiteshKumar(l):\r\n        return list(set([x for x in l if l.count(x) &gt; 1]))\r\n\r\n    def JohnLaRooy(L):\r\n        seen = set()\r\n        seen2 = set()\r\n        seen_add = seen.add\r\n        seen2_add = seen2.add\r\n        for item in L:\r\n            if item in seen:\r\n                seen2_add(item)\r\n            else:\r\n                seen_add(item)\r\n        return list(seen2)\r\n\r\n    l = [1,2,3,2,1,5,6,5,5,5]*100\r\n\r\nHere are the results: (well done @JohnLaRooy!)\r\n\r\n    $ python -mtimeit -s &#39;import test&#39; &#39;test.JohnLaRooy(test.l)&#39;\r\n    10000 loops, best of 3: 74.6 usec per loop\r\n    $ python -mtimeit -s &#39;import test&#39; &#39;test.moooeeeep(test.l)&#39;\r\n    10000 loops, best of 3: 91.3 usec per loop\r\n    $ python -mtimeit -s &#39;import test&#39; &#39;test.thg435(test.l)&#39;\r\n    1000 loops, best of 3: 266 usec per loop\r\n    $ python -mtimeit -s &#39;import test&#39; &#39;test.RiteshKumar(test.l)&#39;\r\n    100 loops, best of 3: 8.35 msec per loop\r\n\r\nInterestingly, besides the timings itself, also the ranking slightly changes when pypy is used. Most interestingly, the Counter-based approach benefits hugely from pypy&#39;s optimizations, whereas the method caching approach I have suggested seems to have almost no effect.\r\n\r\n    $ pypy -mtimeit -s &#39;import test&#39; &#39;test.JohnLaRooy(test.l)&#39;\r\n    100000 loops, best of 3: 17.8 usec per loop\r\n    $ pypy -mtimeit -s &#39;import test&#39; &#39;test.thg435(test.l)&#39;\r\n    10000 loops, best of 3: 23 usec per loop\r\n    $ pypy -mtimeit -s &#39;import test&#39; &#39;test.moooeeeep(test.l)&#39;\r\n    10000 loops, best of 3: 39.3 usec per loop\r\n\r\nApparantly this effect is related to the &quot;duplicatedness&quot; of the input data. I have set `l = [random.randrange(1000000) for i in xrange(10000)]` and got these results:\r\n\r\n    $ pypy -mtimeit -s &#39;import test&#39; &#39;test.moooeeeep(test.l)&#39;\r\n    1000 loops, best of 3: 495 usec per loop\r\n    $ pypy -mtimeit -s &#39;import test&#39; &#39;test.JohnLaRooy(test.l)&#39;\r\n    1000 loops, best of 3: 499 usec per loop\r\n    $ pypy -mtimeit -s &#39;import test&#39; &#39;test.thg435(test.l)&#39;\r\n    1000 loops, best of 3: 1.68 msec per loop\r\n\r\n  [1]: https://stackoverflow.com/a/480227/1025391\r\n\r\n",
               "tags": [],
               "creation_date": 1332494706,
               "last_edit_date": 1495542394,
               "is_accepted": false,
               "id": "9836685",
               "down_vote_count": 0,
               "score": 58
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 214,
               "answer_id": 15155286,
               "is_accepted": false,
               "last_activity_date": 1362133160,
               "body_markdown": "    &gt;&gt;&gt; l = [1,2,3,4,4,5,5,6,1]\r\n    &gt;&gt;&gt; set([x for x in l if l.count(x) &gt; 1])\r\n    set([1, 4, 5])",
               "id": "15155286",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1362133160,
               "score": 212
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 28454355,
               "is_accepted": false,
               "last_activity_date": 1423657685,
               "body_markdown": "    list2 = [1, 2, 3, 4, 1, 2, 3]\r\n    lset = set()\r\n    [(lset.add(item), list2.append(item))\r\n     for item in list2 if item not in lset]\r\n    print list(lset)\r\n",
               "id": "28454355",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1423657685,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 30473012,
               "is_accepted": false,
               "last_activity_date": 1432699796,
               "body_markdown": "A bit late, but maybe helpful for some.\r\nFor a largish list, I found this worked for me.\r\n\r\n    l=[1,2,3,5,4,1,3,1]\r\n    s=set(l)\r\n    d=[]\r\n    for x in l:\r\n        if x in s:\r\n            s.remove(x)\r\n        else:\r\n            d.append(x)\r\n    d\r\n    [1,3,1]\r\nShows just and all duplicates and preserves order.\r\n\r\n",
               "id": "30473012",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1432699796,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 31359697,
               "is_accepted": false,
               "last_activity_date": 1436634623,
               "body_markdown": "One line solution:\r\n\r\n    set([i for i in list if sum([1 for a in list if a == i]) &gt; 1])",
               "id": "31359697",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1436634623,
               "score": 0
            },
            {
               "up_vote_count": 19,
               "answer_id": 31439372,
               "last_activity_date": 1451428851,
               "path": "3.stack.answer",
               "body_markdown": "I came across this question whilst looking in to something related - and wonder why no-one offered a generator based solution?  Solving this problem would be:\r\n\r\n    &gt;&gt;&gt; print list(getDupes_9([1,2,3,2,1,5,6,5,5,5]))\r\n    [1, 2, 5]\r\nI was concerned with scalability, so tested several approaches, including naive items that work well on small lists, but scale horribly as lists get larger (note- would have been better to use timeit, but this is illustrative).\r\n\r\nI included @moooeeeep for comparison (it is impressively fast: fastest if the input list is completely random) and an itertools approach that is even faster again for mostly sorted lists...  Now includes pandas approach from @firelynx -- slow, but not horribly so, and simple. Note - sort/tee/zip approach is consistently fastest on my machine for large mostly ordered lists, moooeeeep is fastest for shuffled lists, but your mileage may vary.\r\n\r\n**Advantages**\r\n\r\n - very quick simple to test for &#39;any&#39; duplicates using the same code\r\n\r\n**Assumptions**\r\n\r\n - Duplicates should be reported once only\r\n - Duplicate order does not need to be preserved\r\n - Duplicate might be anywhere in the list\r\n\r\n---\r\nFastest solution, 1m entries:\r\n\r\n    def getDupes(c):\r\n\t\t\t&#39;&#39;&#39;sort/tee/izip&#39;&#39;&#39;\r\n\t\t\ta, b = itertools.tee(sorted(c))\r\n\t\t\tnext(b, None)\r\n\t\t\tr = None\r\n\t\t\tfor k, g in itertools.izip(a, b):\r\n\t\t\t\tif k != g: continue\r\n\t\t\t\tif k != r:\r\n\t\t\t\t\tyield k\r\n\t\t\t\t\tr = k\r\n\r\n---\r\n\r\n**Approaches tested**\r\n\r\n    import itertools\r\n\timport time\r\n\timport random\r\n\r\n\tdef getDupes_1(c):\r\n\t\t&#39;&#39;&#39;naive&#39;&#39;&#39;\r\n\t\tfor i in xrange(0, len(c)):\r\n\t\t\tif c[i] in c[:i]:\r\n\t\t\t\tyield c[i]\r\n\r\n\tdef getDupes_2(c):\r\n\t\t&#39;&#39;&#39;set len change&#39;&#39;&#39;\r\n\t\ts = set()\r\n\t\tfor i in c:\r\n\t\t\tl = len(s)\r\n\t\t\ts.add(i)\r\n\t\t\tif len(s) == l:\r\n\t\t\t\tyield i\r\n\r\n\tdef getDupes_3(c):\r\n\t\t&#39;&#39;&#39;in dict&#39;&#39;&#39;\r\n\t\td = {}\r\n\t\tfor i in c:\r\n\t\t\tif i in d:\r\n\t\t\t\tif d[i]:\r\n\t\t\t\t\tyield i\r\n\t\t\t\t\td[i] = False\r\n\t\t\telse:\r\n\t\t\t\td[i] = True\r\n\r\n\tdef getDupes_4(c):\r\n\t\t&#39;&#39;&#39;in set&#39;&#39;&#39;\r\n\t\ts,r = set(),set()\r\n\t\tfor i in c:\r\n\t\t\tif i not in s:\r\n\t\t\t\ts.add(i)\r\n\t\t\telif i not in r:\r\n\t\t\t\tr.add(i)\r\n\t\t\t\tyield i\r\n\r\n\tdef getDupes_5(c):\r\n\t\t&#39;&#39;&#39;sort/adjacent&#39;&#39;&#39;\r\n\t\tc = sorted(c)\r\n\t\tr = None\r\n\t\tfor i in xrange(1, len(c)):\r\n\t\t\tif c[i] == c[i - 1]:\r\n\t\t\t\tif c[i] != r:\r\n\t\t\t\t\tyield c[i]\r\n\t\t\t\t\tr = c[i]\r\n\r\n\tdef getDupes_6(c):\r\n\t\t&#39;&#39;&#39;sort/groupby&#39;&#39;&#39;\r\n\t\tdef multiple(x):\r\n\t\t\ttry:\r\n\t\t\t\tx.next()\r\n\t\t\t\tx.next()\r\n\t\t\t\treturn True\r\n\t\t\texcept:\r\n\t\t\t\treturn False\r\n\t\tfor k, g in itertools.ifilter(lambda x: multiple(x[1]), itertools.groupby(sorted(c))):\r\n\t\t\tyield k\r\n\r\n\tdef getDupes_7(c):\r\n\t\t&#39;&#39;&#39;sort/zip&#39;&#39;&#39;\r\n\t\tc = sorted(c)\r\n\t\tr = None\r\n\t\tfor k, g in zip(c[:-1],c[1:]):\r\n\t\t\tif k == g:\r\n\t\t\t\tif k != r:\r\n\t\t\t\t\tyield k\r\n\t\t\t\t\tr = k\r\n\r\n\tdef getDupes_8(c):\r\n\t\t&#39;&#39;&#39;sort/izip&#39;&#39;&#39;\r\n\t\tc = sorted(c)\r\n\t\tr = None\r\n\t\tfor k, g in itertools.izip(c[:-1],c[1:]):\r\n\t\t\tif k == g:\r\n\t\t\t\tif k != r:\r\n\t\t\t\t\tyield k\r\n\t\t\t\t\tr = k\r\n\r\n\tdef getDupes_9(c):\r\n\t\t&#39;&#39;&#39;sort/tee/izip&#39;&#39;&#39;\r\n\t\ta, b = itertools.tee(sorted(c))\r\n\t\tnext(b, None)\r\n\t\tr = None\r\n\t\tfor k, g in itertools.izip(a, b):\r\n\t\t\tif k != g: continue\r\n\t\t\tif k != r:\r\n\t\t\t\tyield k\r\n\t\t\t\tr = k\r\n\r\n\tdef getDupes_a(l):\r\n\t\t&#39;&#39;&#39;moooeeeep&#39;&#39;&#39;\r\n\t\tseen = set()\r\n\t\tseen_add = seen.add\r\n\t\t# adds all elements it doesn&#39;t know yet to seen and all other to seen_twice\r\n\t\tfor x in l:\r\n\t\t\tif x in seen or seen_add(x):\r\n\t\t\t\tyield x\r\n\r\n\tdef getDupes_b(x):\r\n\t\t&#39;&#39;&#39;iter*/sorted&#39;&#39;&#39;\r\n\t\tx = sorted(x)\r\n\t\tdef _matches():\r\n\t\t\tfor k,g in itertools.izip(x[:-1],x[1:]):\r\n\t\t\t\tif k == g:\r\n\t\t\t\t\tyield k\r\n\t\tfor k, n in itertools.groupby(_matches()):\r\n\t\t\tyield k\r\n\r\n\tdef getDupes_c(a):\r\n\t\t&#39;&#39;&#39;pandas&#39;&#39;&#39;\r\n\t\timport pandas as pd\r\n\t\tvc = pd.Series(a).value_counts()\r\n\t\ti = vc[vc &gt; 1].index\r\n\t\tfor _ in i:\r\n\t\t\tyield _\r\n\r\n\tdef hasDupes(fn,c):\r\n\t\ttry:\r\n\t\t\tif fn(c).next(): return True    # Found a dupe\r\n\t\texcept StopIteration:\r\n\t\t\tpass\r\n\t\treturn False\r\n\r\n\tdef getDupes(fn,c):\r\n\t\treturn list(fn(c))\r\n\r\n\tSTABLE = True\r\n\tif STABLE:\r\n\t\tprint &#39;Finding FIRST then ALL duplicates, single dupe of &quot;nth&quot; placed element in 1m element array&#39;\r\n\telse:\r\n\t\tprint &#39;Finding FIRST then ALL duplicates, single dupe of &quot;n&quot; included in randomised 1m element array&#39;\r\n\tfor location in (50,250000,500000,750000,999999):\r\n\t\tfor test in (getDupes_2, getDupes_3, getDupes_4, getDupes_5, getDupes_6,\r\n\t\t\t\t\t getDupes_8, getDupes_9, getDupes_a, getDupes_b, getDupes_c):\r\n\t\t\tprint &#39;Test %-15s:%10d - &#39;%(test.__doc__ or test.__name__,location),\r\n\t\t\tdeltas = []\r\n\t\t\tfor FIRST in (True,False):\r\n\t\t\t\tfor i in xrange(0, 5):\r\n\t\t\t\t\tc = range(0,1000000)\r\n\t\t\t\t\tif STABLE:\r\n\t\t\t\t\t\tc[0] = location\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tc.append(location)\r\n\t\t\t\t\t\trandom.shuffle(c)\r\n\t\t\t\t\tstart = time.time()\r\n\t\t\t\t\tif FIRST:\r\n\t\t\t\t\t\tprint &#39;.&#39; if location == test(c).next() else &#39;!&#39;,\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tprint &#39;.&#39; if [location] == list(test(c)) else &#39;!&#39;,\r\n\t\t\t\t\tdeltas.append(time.time()-start)\r\n\t\t\t\tprint &#39; -- %0.3f  &#39;%(sum(deltas)/len(deltas)),\r\n\t\t\tprint\r\n\t\tprint\r\n\r\n\r\nThe results for the &#39;all dupes&#39; test were consistent, finding &quot;first&quot; duplicate then &quot;all&quot; duplicates in this array:\r\n\r\n    Finding FIRST then ALL duplicates, single dupe of &quot;nth&quot; placed element in 1m element array\r\n\tTest set len change :    500000 -  . . . . .  -- 0.264   . . . . .  -- 0.402  \r\n\tTest in dict        :    500000 -  . . . . .  -- 0.163   . . . . .  -- 0.250  \r\n\tTest in set         :    500000 -  . . . . .  -- 0.163   . . . . .  -- 0.249  \r\n\tTest sort/adjacent  :    500000 -  . . . . .  -- 0.159   . . . . .  -- 0.229  \r\n\tTest sort/groupby   :    500000 -  . . . . .  -- 0.860   . . . . .  -- 1.286  \r\n\tTest sort/izip      :    500000 -  . . . . .  -- 0.165   . . . . .  -- 0.229  \r\n\tTest sort/tee/izip  :    500000 -  . . . . .  -- 0.145   . . . . .  -- 0.206  *\r\n\tTest moooeeeep      :    500000 -  . . . . .  -- 0.149   . . . . .  -- 0.232  \r\n\tTest iter*/sorted   :    500000 -  . . . . .  -- 0.160   . . . . .  -- 0.221  \r\n\tTest pandas         :    500000 -  . . . . .  -- 0.493   . . . . .  -- 0.499  \r\n\r\nWhen the lists are shuffled first, the price of the sort becomes apparent - the efficiency drops noticeably and the @moooeeeep approach dominates, with set &amp; dict approaches being similar but lessor performers:\r\n\r\n    Finding FIRST then ALL duplicates, single dupe of &quot;n&quot; included in randomised 1m element array\r\n    Test set len change :    500000 -  . . . . .  -- 0.321   . . . . .  -- 0.473  \r\n    Test in dict        :    500000 -  . . . . .  -- 0.285   . . . . .  -- 0.360  \r\n    Test in set         :    500000 -  . . . . .  -- 0.309   . . . . .  -- 0.365  \r\n    Test sort/adjacent  :    500000 -  . . . . .  -- 0.756   . . . . .  -- 0.823  \r\n    Test sort/groupby   :    500000 -  . . . . .  -- 1.459   . . . . .  -- 1.896  \r\n    Test sort/izip      :    500000 -  . . . . .  -- 0.786   . . . . .  -- 0.845  \r\n    Test sort/tee/izip  :    500000 -  . . . . .  -- 0.743   . . . . .  -- 0.804  \r\n    Test moooeeeep      :    500000 -  . . . . .  -- 0.234   . . . . .  -- 0.311  *\r\n    Test iter*/sorted   :    500000 -  . . . . .  -- 0.776   . . . . .  -- 0.840  \r\n    Test pandas         :    500000 -  . . . . .  -- 0.539   . . . . .  -- 0.540  \r\n",
               "tags": [],
               "creation_date": 1436988484,
               "last_edit_date": 1451428851,
               "is_accepted": false,
               "id": "31439372",
               "down_vote_count": 0,
               "score": 19
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 34175661,
               "is_accepted": false,
               "last_activity_date": 1449654763,
               "body_markdown": "I would do this with pandas, because I use pandas a lot\r\n\r\n    import pandas as pd\r\n    a = [1,2,3,3,3,4,5,6,6,7]\r\n    vc = pd.Series(a).value_counts()\r\n    vc[vc &gt; 1].index.tolist()\r\n\r\nGives\r\n\r\n    [3,6]\r\n\r\nProbably isn&#39;t very efficient, but it sure is less code than a lot of the other answers, so I thought I would contribute\r\n",
               "id": "34175661",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1449654763,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 35226712,
               "is_accepted": false,
               "last_activity_date": 1454682995,
               "body_markdown": "this is the way I had to do it because I challenged myself not to use other methods:\r\n\r\n\r\n    def dupList(oldlist):\r\n        if type(oldlist)==type((2,2)):\r\n            oldlist=[x for x in oldlist]\r\n        newList=[]\r\n        newList=newList+oldlist\r\n        oldlist=oldlist\r\n        forbidden=[]\r\n        checkPoint=0\r\n        for i in range(len(oldlist)):\r\n            #print &#39;start i&#39;, i\r\n            if i in forbidden:\r\n                continue\r\n            else:\r\n                for j in range(len(oldlist)):\r\n                    #print &#39;start j&#39;, j\r\n                    if j in forbidden:\r\n                        continue\r\n                    else:\r\n                        #print &#39;after Else&#39;\r\n                        if i!=j: \r\n                            #print &#39;i,j&#39;, i,j\r\n                            #print oldlist\r\n                            #print newList\r\n                            if oldlist[j]==oldlist[i]:\r\n                                #print &#39;oldlist[i],oldlist[j]&#39;, oldlist[i],oldlist[j]\r\n                                forbidden.append(j)\r\n                                #print &#39;forbidden&#39;, forbidden\r\n                                del newList[j-checkPoint]\r\n                                #print newList\r\n                                checkPoint=checkPoint+1\r\n        return newList\r\n\r\nso your sample works as:\r\n\r\n    &gt;&gt;&gt;a = [1,2,3,3,3,4,5,6,6,7]\r\n    &gt;&gt;&gt;dupList(a)\r\n    [1, 2, 3, 4, 5, 6, 7]",
               "id": "35226712",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1454682995,
               "score": -1
            },
            {
               "up_vote_count": 0,
               "answer_id": 35319181,
               "last_activity_date": 1483476849,
               "path": "3.stack.answer",
               "body_markdown": "Use the `sort()` function. Duplicates can be identified by looping over it and checking `l1[i] == l1[i+1]`.\r\n",
               "tags": [],
               "creation_date": 1455117948,
               "last_edit_date": 1483476849,
               "is_accepted": false,
               "id": "35319181",
               "down_vote_count": 3,
               "score": -3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 35686067,
               "is_accepted": false,
               "last_activity_date": 1456681866,
               "body_markdown": "the third example of the accepted answer give an erroneous answer and does not attempt to give duplicates. Here is the correct version :\r\n\r\n    number_lst = [1, 1, 2, 3, 5, ...]\r\n\r\n\tseen_set = set()\r\n\tduplicate_set = set(x for x in number_lst if x in seen_set or seen_set.add(x))\r\n\tunique_set = seen_set - duplicate_set",
               "id": "35686067",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1456681866,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 35928376,
               "is_accepted": false,
               "last_activity_date": 1457647932,
               "body_markdown": "There are a lot of answers up here, but I think this is relatively a very readable and easy to understand approach:\r\n\r\n    def get_duplicates(sorted_list):\r\n        duplicates = []\r\n        last = sorted_list[0]\r\n        for x in sorted_list[1:]:\r\n            if x == last:\r\n                duplicates.append(x)\r\n            last = x\r\n        return set(duplicates)\r\n\r\nNotes:\r\n\r\n - If you wish to preserve duplication count, get rid of the cast\r\n   to &#39;set&#39; at the bottom to get the full list\r\n - If you prefer to use generators, replace *duplicates.append(x)* with *yield x* and the return statement at the bottom (you can cast to set later)\r\n",
               "id": "35928376",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1457647932,
               "score": 0
            },
            {
               "up_vote_count": 0,
               "answer_id": 37403193,
               "last_activity_date": 1486871575,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s a fast generator that uses a dict to store each element as a key with a boolean value for checking if the duplicate item has already been yielded.\r\n\r\nFor lists with all elements that are hashable types:\r\n\r\n    def gen_dupes(array):\r\n        unique = {}\r\n        for value in array:\r\n            if value in unique and unique[value]:\r\n                unique[value] = False\r\n                yield value\r\n            else:\r\n                unique[value] = True\r\n\r\n    array = [1, 2, 2, 3, 4, 1, 5, 2, 6, 6]\r\n    print(list(gen_dupes(array)))\r\n    # =&gt; [2, 1, 6]\r\n\r\nFor lists that might contain lists:\r\n\r\n    def gen_dupes(array):\r\n        unique = {}\r\n        for value in array:\r\n            is_list = False\r\n            if type(value) is list:\r\n                value = tuple(value)\r\n                is_list = True\r\n    \r\n            if value in unique and unique[value]:\r\n                unique[value] = False\r\n                if is_list:\r\n                    value = list(value)\r\n    \r\n                yield value\r\n            else:\r\n                unique[value] = True\r\n\r\n    array = [1, 2, 2, [1, 2], 3, 4, [1, 2], 5, 2, 6, 6]\r\n    print(list(gen_dupes(array)))\r\n    # =&gt; [2, [1, 2], 6]",
               "tags": [],
               "creation_date": 1464054940,
               "last_edit_date": 1486871575,
               "is_accepted": false,
               "id": "37403193",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 37934666,
               "is_accepted": false,
               "last_activity_date": 1466476339,
               "body_markdown": "Very simple and quick way of finding dupes with one iteration in Python is:\r\n\r\n    testList = [&#39;red&#39;, &#39;blue&#39;, &#39;red&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;blue&#39;]\r\n    \r\n    testListDict = {}\r\n    \r\n    for item in testList:\r\n      try:\r\n        testListDict[item] += 1\r\n      except:\r\n        testListDict[item] = 1\r\n    \r\n    print testListDict\r\n\r\nOutput will be as follows:\r\n\r\n    &gt;&gt;&gt; print testListDict\r\n    {&#39;blue&#39;: 3, &#39;green&#39;: 1, &#39;red&#39;: 2}\r\n\r\nThis and more in my blog http://www.howtoprogramwithpython.com",
               "id": "37934666",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1466476339,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 38402030,
               "is_accepted": false,
               "last_activity_date": 1468603787,
               "body_markdown": "Here&#39;s a neat and concise solution -\r\n\r\n    for x in set(li):\r\n        li.remove(x)\r\n    \r\n    li = list(set(li))",
               "id": "38402030",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1468603787,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 39931653,
               "is_accepted": false,
               "last_activity_date": 1475925045,
               "body_markdown": "Using pandas:\r\n\r\n    &gt;&gt;&gt; import pandas as pd\r\n    &gt;&gt;&gt; a = [1, 2, 1, 3, 3, 3, 0]\r\n    &gt;&gt;&gt; pd.Series(a)[pd.Series(a).duplicated()].values\r\n    array([1, 3, 3])",
               "id": "39931653",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1475925045,
               "score": 6
            },
            {
               "up_vote_count": 1,
               "answer_id": 41817537,
               "last_activity_date": 1491334743,
               "path": "3.stack.answer",
               "body_markdown": "You can use [`iteration_utilities.duplicates`][0]:\r\n\r\n    &gt;&gt;&gt; from iteration_utilities import duplicates\r\n    \r\n    &gt;&gt;&gt; list(duplicates([1,1,2,1,2,3,4,2]))\r\n    [1, 1, 2, 2]\r\n\r\nor if you only want one of each duplicate this can be combined with [`iteration_utilities.unique_everseen`][2]:\r\n\r\n    &gt;&gt;&gt; from iteration_utilities import unique_everseen\r\n    \r\n    &gt;&gt;&gt; list(unique_everseen(duplicates([1,1,2,1,2,3,4,2])))\r\n    [1, 2]\r\n\r\n---\r\n\r\n&lt;sup&gt;1 This is from a third-party library I have written: [`iteration_utilities`][1].&lt;/sup&gt;\r\n\r\n[0]: https://iteration-utilities.readthedocs.io/en/latest/generated/duplicates.html\r\n[1]: https://github.com/MSeifert04/iteration_utilities\r\n[2]: https://iteration-utilities.readthedocs.io/en/latest/generated/unique_everseen.html",
               "tags": [],
               "creation_date": 1485214201,
               "last_edit_date": 1491334743,
               "is_accepted": false,
               "id": "41817537",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 44099311,
               "is_accepted": false,
               "last_activity_date": 1495384854,
               "body_markdown": "How about simply loop through each element in the list by checking the number of occurrences, then adding them to a set which will then print the duplicates. Hope this helps someone out there.\r\n\r\n    myList  = [2 ,4 , 6, 8, 4, 6, 12];\r\n    newList = set()\r\n\r\n    for i in myList:\r\n        if myList.count(i) &gt;= 2:\r\n            newList.add(i)\r\n\r\n    print(list(newList))\r\n    ## [4 , 6]\r\n",
               "id": "44099311",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1495384854,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 45242753,
               "is_accepted": false,
               "last_activity_date": 1500655355,
               "body_markdown": "We can use [`itertools.groupby`][1] in order to find all the items that have dups:\r\n\r\n    from itertools import groupby\r\n    \r\n    myList  = [2, 4, 6, 8, 4, 6, 12]\r\n    # when the list is sorted, groupby groups by consecutive elements which are similar\r\n    for x, y in groupby(sorted(myList)):\r\n        #  list(y) returns all the occurences of item x\r\n        if len(list(y)) &gt; 1:\r\n            print x  \r\n\r\nThe output will be:\r\n\r\n    4\r\n    6\r\n\r\n  [1]: https://docs.python.org/2/library/itertools.html#itertools.groupby\r\n",
               "id": "45242753",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1500655355,
               "score": 1
            },
            {
               "up_vote_count": 0,
               "answer_id": 45686513,
               "last_activity_date": 1506249476,
               "path": "3.stack.answer",
               "body_markdown": "    def removeduplicates(a):\r\n      seen = set()\r\n      \r\n      for i in a:\r\n        if i not in seen:\r\n          seen.add(i)\r\n      return seen \r\n      \r\n    print(removeduplicates([1,1,2,2]))",
               "tags": [],
               "creation_date": 1502771439,
               "last_edit_date": 1506249476,
               "is_accepted": false,
               "id": "45686513",
               "down_vote_count": 0,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/9835762/find-and-list-duplicates-in-a-list",
         "id": "858127-2310"
      },
      {
         "up_vote_count": "227",
         "path": "2.stack",
         "body_markdown": "Can you tell me when to use these vectorization methods with basic examples? \r\n\r\nI see that `map` is a `Series` method whereas the rest are `DataFrame` methods. I got confused about `apply` and `applymap` methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!\r\n\r\n",
         "view_count": "125868",
         "answer_count": "6",
         "tags": "['python', 'numpy', 'pandas', 'vectorization']",
         "creation_date": "1383682814",
         "last_edit_date": "1513758843",
         "code_snippet": "['<code>map</code>', '<code>Series</code>', '<code>DataFrame</code>', '<code>apply</code>', '<code>applymap</code>', \"<code>In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\n\\nIn [117]: frame\\nOut[117]: \\n               b         d         e\\nUtah   -0.029638  1.081563  1.280300\\nOhio    0.647747  0.831136 -1.549481\\nTexas   0.513416 -0.884417  0.195343\\nOregon -0.485454 -0.477388 -0.309548\\n\\nIn [118]: f = lambda x: x.max() - x.min()\\n\\nIn [119]: frame.apply(f)\\nOut[119]: \\nb    1.133201\\nd    1.965980\\ne    2.829781\\ndtype: float64\\n</code>\", \"<code>In [120]: format = lambda x: '%.2f' % x\\n\\nIn [121]: frame.applymap(format)\\nOut[121]: \\n            b      d      e\\nUtah    -0.03   1.08   1.28\\nOhio     0.65   0.83  -1.55\\nTexas    0.51  -0.88   0.20\\nOregon  -0.49  -0.48  -0.31\\n</code>\", \"<code>In [122]: frame['e'].map(format)\\nOut[122]: \\nUtah       1.28\\nOhio      -1.55\\nTexas      0.20\\nOregon    -0.31\\nName: e, dtype: object\\n</code>\", '<code>apply</code>', '<code>applymap</code>', '<code>map</code>', '<code>func</code>', '<code>lambda x: [func(y) for y in x]</code>', '<code>map</code>', '<code>applymap</code>', '<code>map</code>', '<code>applymap</code>', '<code>applymap</code>', '<code>groupby</code>', '<code>Series</code>', '<code>In [40]: p=pd.Series([1,2,3])\\nIn [41]: p\\nOut[31]:\\n0    1\\n1    2\\n2    3\\ndtype: int64\\n\\nIn [42]: p.apply(lambda x: pd.Series([x, x]))\\nOut[42]: \\n   0  1\\n0  1  1\\n1  2  2\\n2  3  3\\n\\nIn [43]: p.map(lambda x: pd.Series([x, x]))\\nOut[43]: \\n0    0    1\\n1    1\\ndtype: int64\\n1    0    2\\n1    2\\ndtype: int64\\n2    0    3\\n1    3\\ndtype: int64\\ndtype: object\\n</code>', '<code>apply</code>', '<code>series.apply(download_file_for_every_element) \\n</code>', '<code>Map</code>', '<code>1 2 3 4 5\\n2 1 4 5 3\\n</code>', '<code>1 2 3 4 5\\n1 2 5 3 4\\n</code>', '<code>map</code>', '<code>0.15.1</code>', '<code>In [39]: p=pd.Series([1,0,3,4,2])\\n\\nIn [40]: p.map(p)\\nOut[40]: \\n0    0\\n1    1\\n2    4\\n3    2\\n4    3\\ndtype: int64\\n</code>', '<code>    frame.apply(np.sqrt)\\n    Out[102]: \\n                   b         d         e\\n    Utah         NaN  1.435159       NaN\\n    Ohio    1.098164  0.510594  0.729748\\n    Texas        NaN  0.456436  0.697337\\n    Oregon  0.359079       NaN       NaN\\n\\n    frame.applymap(np.sqrt)\\n    Out[103]: \\n                   b         d         e\\n    Utah         NaN  1.435159       NaN\\n    Ohio    1.098164  0.510594  0.729748\\n    Texas        NaN  0.456436  0.697337\\n    Oregon  0.359079       NaN       NaN\\n</code>', '<code>DataFrame.apply</code>', '<code>DataFrame.applymap</code>', '<code>Series.apply</code>', '<code>Series.map</code>', '<code>Series.apply</code>', '<code>Series.map</code>', '<code>def f(x):\\n    if x &lt; 0:\\n        x = 0\\n    elif x &gt; 100000:\\n        x = 100000\\n    return x\\n\\ndf.applymap(f)\\ndf.describe()\\n</code>', '<code>df = df.applymap(f)\\ndf.describe()\\n</code>', '<code>df = modified_df</code>', '<code>inplace=True</code>', '<code>.ix</code>', '<code>.where</code>']",
         "title": "Difference between map, applymap and apply methods in Pandas",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 265,
               "answer_id": 19798528,
               "is_accepted": true,
               "last_activity_date": 1383684033,
               "body_markdown": "Straight from Wes McKinney&#39;s [Python for Data Analysis][1] book, pg. 132 (I highly recommended this book):\r\n\r\n&gt; Another frequent operation is applying a function on 1D arrays to each column or row. DataFrame\u2019s apply method does exactly this:\r\n    \r\n    In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list(&#39;bde&#39;), index=[&#39;Utah&#39;, &#39;Ohio&#39;, &#39;Texas&#39;, &#39;Oregon&#39;])\r\n    \r\n    In [117]: frame\r\n    Out[117]: \r\n                   b         d         e\r\n    Utah   -0.029638  1.081563  1.280300\r\n    Ohio    0.647747  0.831136 -1.549481\r\n    Texas   0.513416 -0.884417  0.195343\r\n    Oregon -0.485454 -0.477388 -0.309548\r\n    \r\n    In [118]: f = lambda x: x.max() - x.min()\r\n    \r\n    In [119]: frame.apply(f)\r\n    Out[119]: \r\n    b    1.133201\r\n    d    1.965980\r\n    e    2.829781\r\n    dtype: float64\r\n    \r\n&gt; Many of the most common array statistics (like sum and mean) are DataFrame methods,\r\n    so using apply is not necessary.\r\n\r\n&gt; Element-wise Python functions can be used, too. Suppose you wanted to compute a formatted string from each floating point value in frame. You can do this with applymap:\r\n    \r\n    In [120]: format = lambda x: &#39;%.2f&#39; % x\r\n    \r\n    In [121]: frame.applymap(format)\r\n    Out[121]: \r\n                b      d      e\r\n    Utah    -0.03   1.08   1.28\r\n    Ohio     0.65   0.83  -1.55\r\n    Texas    0.51  -0.88   0.20\r\n    Oregon  -0.49  -0.48  -0.31\r\n    \r\n&gt; The reason for the name applymap is that Series has a map method for applying an element-wise function:\r\n    \r\n    In [122]: frame[&#39;e&#39;].map(format)\r\n    Out[122]: \r\n    Utah       1.28\r\n    Ohio      -1.55\r\n    Texas      0.20\r\n    Oregon    -0.31\r\n    Name: e, dtype: object\r\n\r\nSumming up, `apply` works on a row / column basis of a DataFrame, `applymap` works element-wise on a DataFrame, and `map` works element-wise on a Series.\r\n\r\n  [1]: http://shop.oreilly.com/product/0636920023784.do",
               "id": "19798528",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1383684033,
               "score": 265
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 13,
               "answer_id": 20687887,
               "is_accepted": false,
               "last_activity_date": 1387473698,
               "body_markdown": "@jeremiahbuddha mentioned that apply works on row/columns, while applymap works element-wise. But it seems you can still use apply for element-wise computation.... \r\n\r\n        frame.apply(np.sqrt)\r\n        Out[102]: \r\n                       b         d         e\r\n        Utah         NaN  1.435159       NaN\r\n        Ohio    1.098164  0.510594  0.729748\r\n        Texas        NaN  0.456436  0.697337\r\n        Oregon  0.359079       NaN       NaN\r\n\r\n        frame.applymap(np.sqrt)\r\n        Out[103]: \r\n                       b         d         e\r\n        Utah         NaN  1.435159       NaN\r\n        Ohio    1.098164  0.510594  0.729748\r\n        Texas        NaN  0.456436  0.697337\r\n        Oregon  0.359079       NaN       NaN\r\n",
               "id": "20687887",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1387473698,
               "score": 12
            },
            {
               "up_vote_count": 14,
               "answer_id": 27368948,
               "last_activity_date": 1509554950,
               "path": "3.stack.answer",
               "body_markdown": "Adding to the other answers, in a `Series` there are also [map][1] and [apply][2]. \r\n\r\n**Apply can make a DataFrame out of a series**; however, map will just put a series in every cell of another series, which is probably not what you want.\r\n\r\n    In [40]: p=pd.Series([1,2,3])\r\n    In [41]: p\r\n    Out[31]:\r\n    0    1\r\n    1    2\r\n    2    3\r\n    dtype: int64\r\n    \r\n    In [42]: p.apply(lambda x: pd.Series([x, x]))\r\n    Out[42]: \r\n       0  1\r\n    0  1  1\r\n    1  2  2\r\n    2  3  3\r\n    \r\n    In [43]: p.map(lambda x: pd.Series([x, x]))\r\n    Out[43]: \r\n    0    0    1\r\n    1    1\r\n    dtype: int64\r\n    1    0    2\r\n    1    2\r\n    dtype: int64\r\n    2    0    3\r\n    1    3\r\n    dtype: int64\r\n    dtype: object\r\n\r\nAlso if I had a function with side effects, such as &quot;connect to a web server&quot;, I&#39;d probably use `apply` just for the sake of clarity.\r\n\r\n    series.apply(download_file_for_every_element) \r\n\r\n**`Map` can use not only a function, but also a dictionary or another series.** Let&#39;s say you want to manipulate [permutations][3].\r\n\r\nTake\r\n\r\n    1 2 3 4 5\r\n    2 1 4 5 3\r\n\r\nThe square of this permutation is\r\n\r\n    1 2 3 4 5\r\n    1 2 5 3 4\r\n\r\nYou can compute it using `map`. Not sure if self-application is documented, but it works in `0.15.1`. \r\n    \r\n    In [39]: p=pd.Series([1,0,3,4,2])\r\n    \r\n    In [40]: p.map(p)\r\n    Out[40]: \r\n    0    0\r\n    1    1\r\n    2    4\r\n    3    2\r\n    4    3\r\n    dtype: int64\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html\r\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html\r\n  [3]: http://en.wikipedia.org/wiki/Permutation\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1418081411,
               "last_edit_date": 1509554950,
               "is_accepted": false,
               "id": "27368948",
               "down_vote_count": 0,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 32792734,
               "is_accepted": false,
               "last_activity_date": 1443231024,
               "body_markdown": "Just wanted to point out, as I struggled with this for a bit\r\n\r\n    def f(x):\r\n        if x &lt; 0:\r\n            x = 0\r\n        elif x &gt; 100000:\r\n            x = 100000\r\n        return x\r\n    \r\n    df.applymap(f)\r\n    df.describe()\r\n# this does not modify the dataframe itself, has to be reassigned\r\n   \r\n    df = df.applymap(f)\r\n    df.describe()",
               "id": "32792734",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1443231024,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 37336872,
               "is_accepted": false,
               "last_activity_date": 1463710204,
               "body_markdown": "Probably simplest explanation the difference between apply and applymap:\r\n\r\n**apply** takes the whole column as a parameter and then assign the result to this column\r\n\r\n**applymap** takes the separate cell value as a parameter and assign the result back to this cell.\r\n\r\nNB If apply returns the single value you will have this value instead of the column after assigning and eventually will have just a row instead of matrix.",
               "id": "37336872",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1463710204,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 10,
               "answer_id": 38900352,
               "is_accepted": false,
               "last_activity_date": 1470928832,
               "body_markdown": "There&#39;s great information in these answers, but I&#39;m adding my own to clearly summarize which methods work array-wise versus element-wise. jeremiahbuddha mostly did this but did not mention Series.apply.  I don&#39;t have the rep to comment.\r\n\r\n - `DataFrame.apply` operates on entire rows or columns at a time.\r\n   \r\n - `DataFrame.applymap`, `Series.apply`, and `Series.map` operate on one\r\n   element at time.\r\n\r\nThere is a lot of overlap between the capabilities of `Series.apply` and `Series.map`, meaning that either one will work in most cases.  They do have some slight differences though, some of which were discussed in osa&#39;s answer.",
               "id": "38900352",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1470928832,
               "score": 10
            }
         ],
         "link": "https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas",
         "id": "858127-2311"
      },
      {
         "up_vote_count": "1217",
         "path": "2.stack",
         "body_markdown": "I got a list of dictionaries and want that to be sorted by a value of that dictionary.\r\n\r\nThis\r\n\r\n    [{&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}]\r\n\r\nsorted by name, should become\r\n\r\n    [{&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}, {&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}]\r\n",
         "view_count": "462959",
         "answer_count": "17",
         "tags": "['python', 'list', 'sorting', 'dictionary', 'data-structures']",
         "creation_date": "1221575267",
         "last_edit_date": "1508719921",
         "code_snippet": "[\"<code>[{'name':'Homer', 'age':39}, {'name':'Bart', 'age':10}]\\n</code>\", \"<code>[{'name':'Bart', 'age':10}, {'name':'Homer', 'age':39}]\\n</code>\", \"<code>newlist = sorted(list_to_be_sorted, key=lambda k: k['name']) \\n</code>\", \"<code>from operator import itemgetter\\nnewlist = sorted(list_to_be_sorted, key=itemgetter('name')) \\n</code>\", '<code>reverse=True</code>', \"<code>newlist = sorted(l, key=itemgetter('name'), reverse=True)\\n</code>\", \"<code>lambda k: k['name']</code>\", '<code>itemgetter</code>', '<code>itemgetter(1,2,3)</code>', '<code>obj[1], obj[2], obj[3]</code>', '<code>import operator\\n</code>', \"<code>list_of_dicts.sort(key=operator.itemgetter('name'))\\n</code>\", \"<code>list_of_dicts.sort(key=operator.itemgetter('age'))\\n</code>\", \"<code>key=lambda k: (k['name'], k['age'])</code>\", \"<code>key=itemgetter('name', 'age')</code>\", '<code>cmp</code>', '<code>key</code>', '<code>list.sort()</code>', '<code>list</code>', '<code>my_list = [{\\'name\\':\\'Homer\\', \\'age\\':39}, {\\'name\\':\\'Milhouse\\', \\'age\\':10}, {\\'name\\':\\'Bart\\', \\'age\\':10} ]\\nsortedlist = sorted(my_list , key=lambda elem: \"%02d %s\" % (elem[\\'age\\'], elem[\\'name\\']))\\n</code>', \"<code>my_list = [{'name':'Homer', 'age':39}, {'name':'Bart', 'age':10}]\\n\\nmy_list.sort(lambda x,y : cmp(x['name'], y['name']))\\n</code>\", '<code>my_list</code>', '<code>key</code>', \"<code>my_list = sorted(my_list, key=lambda k: k['name'])\\n</code>\", '<code>operator.itemgetter</code>', \"<code>import operator\\na_list_of_dicts.sort(key=operator.itemgetter('name'))\\n</code>\", \"<code>[{'name':'Homer', 'age':39}, {'name':'Bart', 'age':10}]\\n</code>\", \"<code>sorted(l,cmp=lambda x,y: cmp(x['name'],y['name']))\\n</code>\", \"<code>def mykey(adict): return adict['name']\\nx = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age':10}]\\nsorted(x, key=mykey)\\n</code>\", '<code>itemgetter</code>', \"<code>from operator import itemgetter\\nx = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age':10}]\\nsorted(x, key=itemgetter('name'))\\n</code>\", \"<code>py = [{'name':'Homer', 'age':39}, {'name':'Bart', 'age':10}]\\n</code>\", '<code>sort_on = \"name\"\\ndecorated = [(dict_[sort_on], dict_) for dict_ in py]\\ndecorated.sort()\\nresult = [dict_ for (key, dict_) in decorated]\\n</code>', \"<code>&gt;&gt;&gt; result\\n[{'age': 10, 'name': 'Bart'}, {'age': 39, 'name': 'Homer'}]\\n</code>\", '<code>key=</code>', '<code>.sort</code>', \"<code>a = [{'name':'Homer', 'age':39}, ...]\\n\\n# This changes the list a\\na.sort(key=lambda k : k['name'])\\n\\n# This returns a new list (a is not modified)\\nsorted(a, key=lambda k : k['name']) \\n</code>\", \"<code>my_list.sort(key=lambda x: x['name'])\\n</code>\", '<code>def sort_key_func(item):\\n    \"\"\" helper function used to sort list of dicts\\n\\n    :param item: dict\\n    :return: sorted list of tuples (k, v)\\n    \"\"\"\\n    pairs = []\\n    for k, v in item.items():\\n        pairs.append((k, v))\\n    return sorted(pairs)\\nsorted(A, key=sort_key_func)\\n</code>', '<code>lower()</code>', \"<code>lists = [{'name':'Homer', 'age':39},\\n  {'name':'Bart', 'age':10},\\n  {'name':'abby', 'age':9}]\\n\\nlists = sorted(lists, key=lambda k: k['name'])\\nprint(lists)\\n# [{'name':'Bart', 'age':10}, {'name':'Homer', 'age':39}, {'name':'abby', 'age':9}]\\n\\nlists = sorted(lists, key=lambda k: k['name'].lower())\\nprint(lists)\\n# [ {'name':'abby', 'age':9}, {'name':'Bart', 'age':10}, {'name':'Homer', 'age':39}]\\n</code>\", \"<code>D = {'eggs': 3, 'ham': 1, 'spam': 2}\\n\\ndef get_count(tuple):\\n    return tuple[1]\\n\\nsorted(D.items(), key = get_count, reverse=True)\\nor\\nsorted(D.items(), key = lambda x: x[1], reverse=True)  avoiding get_count function call\\n</code>\", \"<code>import pandas as pd\\n\\nlistOfDicts = [{'name':'Homer', 'age':39}, {'name':'Bart', 'age':10}]\\ndf = pd.DataFrame(listOfDicts)\\ndf = df.sort_values('name')\\nsorted_listOfDicts = df.T.to_dict().values()\\n</code>\", '<code>setup_large = \"listOfDicts = [];\\\\\\n[listOfDicts.extend(({\\'name\\':\\'Homer\\', \\'age\\':39}, {\\'name\\':\\'Bart\\', \\'age\\':10})) for _ in range(50000)];\\\\\\nfrom operator import itemgetter;import pandas as pd;\\\\\\ndf = pd.DataFrame(listOfDicts);\"\\n\\nsetup_small = \"listOfDicts = [];\\\\\\nlistOfDicts.extend(({\\'name\\':\\'Homer\\', \\'age\\':39}, {\\'name\\':\\'Bart\\', \\'age\\':10}));\\\\\\nfrom operator import itemgetter;import pandas as pd;\\\\\\ndf = pd.DataFrame(listOfDicts);\"\\n\\nmethod1 = \"newlist = sorted(listOfDicts, key=lambda k: k[\\'name\\'])\"\\nmethod2 = \"newlist = sorted(listOfDicts, key=itemgetter(\\'name\\')) \"\\nmethod3 = \"df = df.sort_values(\\'name\\');\\\\\\nsorted_listOfDicts = df.T.to_dict().values()\"\\n\\nimport timeit\\nt = timeit.Timer(method1, setup_small)\\nprint(\\'Small Method LC: \\' + str(t.timeit(100)))\\nt = timeit.Timer(method2, setup_small)\\nprint(\\'Small Method LC2: \\' + str(t.timeit(100)))\\nt = timeit.Timer(method3, setup_small)\\nprint(\\'Small Method Pandas: \\' + str(t.timeit(100)))\\n\\nt = timeit.Timer(method1, setup_large)\\nprint(\\'Large Method LC: \\' + str(t.timeit(100)))\\nt = timeit.Timer(method2, setup_large)\\nprint(\\'Large Method LC2: \\' + str(t.timeit(100)))\\nt = timeit.Timer(method3, setup_large)\\nprint(\\'Large Method Pandas: \\' + str(t.timeit(1)))\\n\\n#Small Method LC: 0.000163078308105\\n#Small Method LC2: 0.000134944915771\\n#Small Method Pandas: 0.0712950229645\\n#Large Method LC: 0.0321750640869\\n#Large Method LC2: 0.0206089019775\\n#Large Method Pandas: 5.81405615807\\n</code>', '<code>list</code>', '<code>dictionaries</code>', '<code>sort()</code>', '<code>def get_name(d):\\n    \"\"\" Return the value of a key in a dictionary. \"\"\"\\n\\n    return d[\"name\"]\\n</code>', '<code>list</code>', \"<code>data_one = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\\n</code>\", '<code>data_one.sort(key=get_name)\\n</code>', '<code>list</code>', '<code>sorted()</code>', '<code>list</code>', '<code>list</code>', \"<code>data_two = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\\nnew_data = sorted(data_two, key=get_name)\\n</code>\", '<code>data_one</code>', '<code>new_data</code>', \"<code>&gt;&gt;&gt; print(data_one)\\n[{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\\n&gt;&gt;&gt; print(new_data)\\n[{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\\n</code>\"]",
         "title": "How do I sort a list of dictionaries by values of the dictionary in Python?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 72950,
               "is_accepted": false,
               "last_activity_date": 1221575512,
               "body_markdown": "You have to implement your own comparison function that will compare the dictionaries by values of name keys. See [Sorting Mini-HOW TO from PythonInfo Wiki][1]\r\n\r\n\r\n  [1]: http://wiki.python.org/moin/HowTo/Sorting",
               "id": "72950",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1221575512,
               "score": 11
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 15,
               "answer_id": 73019,
               "is_accepted": false,
               "last_activity_date": 1221575814,
               "body_markdown": "I guess you&#39;ve meant:\r\n\r\n    [{&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}]\r\n\r\nThis would be sorted like this:\r\n\r\n    sorted(l,cmp=lambda x,y: cmp(x[&#39;name&#39;],y[&#39;name&#39;]))",
               "id": "73019",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1221575814,
               "score": 13
            },
            {
               "up_vote_count": 24,
               "answer_id": 73044,
               "last_activity_date": 1465220535,
               "path": "3.stack.answer",
               "body_markdown": "    my_list = [{&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}]\r\n    \r\n    my_list.sort(lambda x,y : cmp(x[&#39;name&#39;], y[&#39;name&#39;]))\r\n\r\n`my_list` will now be what you want.\r\n\r\n**(3 years later) Edited to add:**\r\n\r\n   The new `key` argument is more efficient and neater.  A better answer now looks like:\r\n\r\n    my_list = sorted(my_list, key=lambda k: k[&#39;name&#39;])\r\n\r\n...the lambda is, IMO, easier to understand than `operator.itemgetter`, but YMMV.",
               "tags": [],
               "creation_date": 1221575951,
               "last_edit_date": 1465220535,
               "is_accepted": false,
               "id": "73044",
               "down_vote_count": 0,
               "score": 24
            },
            {
               "up_vote_count": 1652,
               "answer_id": 73050,
               "last_activity_date": 1420054789,
               "path": "3.stack.answer",
               "body_markdown": "It may look cleaner using a key instead a cmp:\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    newlist = sorted(list_to_be_sorted, key=lambda k: k[&#39;name&#39;]) \r\n\r\nor as J.F.Sebastian and others suggested,\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    from operator import itemgetter\r\n    newlist = sorted(list_to_be_sorted, key=itemgetter(&#39;name&#39;)) \r\n\r\nFor completeness (as pointed out in comments by fitzgeraldsteele), add `reverse=True` to sort descending\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    newlist = sorted(l, key=itemgetter(&#39;name&#39;), reverse=True)\r\n\r\n",
               "tags": [],
               "creation_date": 1221575984,
               "last_edit_date": 1420054789,
               "is_accepted": true,
               "id": "73050",
               "down_vote_count": 1,
               "score": 1651
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 19,
               "answer_id": 73098,
               "is_accepted": false,
               "last_activity_date": 1221576231,
               "body_markdown": "    import operator\r\n    a_list_of_dicts.sort(key=operator.itemgetter(&#39;name&#39;))\r\n\r\n&#39;key&#39; is used to sort by an arbitrary value and &#39;itemgetter&#39; sets that value to each item&#39;s &#39;name&#39; attribute.",
               "id": "73098",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1221576231,
               "score": 19
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 73186,
               "is_accepted": false,
               "last_activity_date": 1221576734,
               "body_markdown": "You could use a custom comparison function, or you could pass in a function that calculates a custom sort key. That&#39;s usually more efficient as the key is only calculated once per item, while the comparison function would be called many more times.\r\n\r\nYou could do it this way:\r\n\r\n    def mykey(adict): return adict[&#39;name&#39;]\r\n    x = [{&#39;name&#39;: &#39;Homer&#39;, &#39;age&#39;: 39}, {&#39;name&#39;: &#39;Bart&#39;, &#39;age&#39;:10}]\r\n    sorted(x, key=mykey)\r\n\r\nBut the standard library contains a generic routine for getting items of arbitrary objects: `itemgetter`. So try this instead:\r\n\r\n    from operator import itemgetter\r\n    x = [{&#39;name&#39;: &#39;Homer&#39;, &#39;age&#39;: 39}, {&#39;name&#39;: &#39;Bart&#39;, &#39;age&#39;:10}]\r\n    sorted(x, key=itemgetter(&#39;name&#39;))\r\n",
               "id": "73186",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1221576734,
               "score": 12
            },
            {
               "up_vote_count": 98,
               "answer_id": 73465,
               "last_activity_date": 1367419325,
               "path": "3.stack.answer",
               "body_markdown": "    import operator\r\n\r\nTo sort the list of dictionaries by key=&#39;name&#39;:\r\n\r\n    list_of_dicts.sort(key=operator.itemgetter(&#39;name&#39;))\r\n\r\nTo sort the list of dictionaries by key=&#39;age&#39;:\r\n\r\n    list_of_dicts.sort(key=operator.itemgetter(&#39;age&#39;))",
               "tags": [],
               "creation_date": 1221578294,
               "last_edit_date": 1367419325,
               "is_accepted": false,
               "id": "73465",
               "down_vote_count": 0,
               "score": 98
            },
            {
               "up_vote_count": 3,
               "answer_id": 1144907,
               "last_activity_date": 1247854928,
               "path": "3.stack.answer",
               "body_markdown": "Here is [my answer to a related question on sorting by multiple columns][1]. It also works for the degenerate case where the number of columns is only one.\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/1143671/python-sorting-list-of-dictionaries-by-multiple-keys/1144405",
               "tags": [],
               "creation_date": 1247854928,
               "last_edit_date": 1495542891,
               "is_accepted": false,
               "id": "1144907",
               "down_vote_count": 1,
               "score": 2
            },
            {
               "up_vote_count": 41,
               "answer_id": 2858683,
               "last_activity_date": 1365593475,
               "path": "3.stack.answer",
               "body_markdown": "If you want to sort the list by multiple keys you can do the following:\r\n\r\n    my_list = [{&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;Milhouse&#39;, &#39;age&#39;:10}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10} ]\r\n    sortedlist = sorted(my_list , key=lambda elem: &quot;%02d %s&quot; % (elem[&#39;age&#39;], elem[&#39;name&#39;]))\r\n\r\nIt is rather hackish, since it relies on converting the values into a single string representation for comparison, but it works as expected for numbers including negative ones (although you will need to format your string appropriately with zero paddings if you are using numbers)\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1274196491,
               "last_edit_date": 1365593475,
               "is_accepted": false,
               "id": "2858683",
               "down_vote_count": 4,
               "score": 37
            },
            {
               "up_vote_count": 6,
               "answer_id": 12420427,
               "last_activity_date": 1365593429,
               "path": "3.stack.answer",
               "body_markdown": "I tried something like this:\r\n\r\n    my_list.sort(key=lambda x: x[&#39;name&#39;])\r\n\r\nIt worked for integers as well.",
               "tags": [],
               "creation_date": 1347609947,
               "last_edit_date": 1365593429,
               "is_accepted": false,
               "id": "12420427",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 12,
               "answer_id": 16772049,
               "is_accepted": false,
               "last_activity_date": 1369653663,
               "body_markdown": "Using Schwartzian transform from Perl,\r\n\r\n    py = [{&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}]\r\n\r\ndo\r\n\r\n    sort_on = &quot;name&quot;\r\n    decorated = [(dict_[sort_on], dict_) for dict_ in py]\r\n    decorated.sort()\r\n    result = [dict_ for (key, dict_) in decorated]\r\n\r\ngives\r\n\r\n    &gt;&gt;&gt; result\r\n    [{&#39;age&#39;: 10, &#39;name&#39;: &#39;Bart&#39;}, {&#39;age&#39;: 39, &#39;name&#39;: &#39;Homer&#39;}]\r\n\r\nMore on [Perl Schwartzian transform][1]\r\n\r\n&gt; In computer science, the Schwartzian transform is a Perl programming\r\n&gt; idiom used to improve the efficiency of sorting a list of items. This\r\n&gt; idiom is appropriate for comparison-based sorting when the ordering is\r\n&gt; actually based on the ordering of a certain property (the key) of the\r\n&gt; elements, where computing that property is an intensive operation that\r\n&gt; should be performed a minimal number of times. The Schwartzian\r\n&gt; Transform is notable in that it does not use named temporary arrays.\r\n\r\n\r\n  [1]: http://en.wikipedia.org/wiki/Schwartzian_transform",
               "id": "16772049",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1369653663,
               "score": 12
            },
            {
               "up_vote_count": 3,
               "answer_id": 23102554,
               "last_activity_date": 1397633710,
               "path": "3.stack.answer",
               "body_markdown": "Lets Say I h&#39;v a Dictionary D with elements below. To sort just use key argument in sorted to pass custom function as below\r\n\r\n    D = {&#39;eggs&#39;: 3, &#39;ham&#39;: 1, &#39;spam&#39;: 2}\r\n\r\n    def get_count(tuple):\r\n        return tuple[1]\r\n\r\n    sorted(D.items(), key = get_count, reverse=True)\r\n    or\r\n    sorted(D.items(), key = lambda x: x[1], reverse=True)  avoiding get_count function call\r\n\r\nhttps://wiki.python.org/moin/HowTo/Sorting/#Key_Functions",
               "tags": [],
               "creation_date": 1397632701,
               "last_edit_date": 1397633710,
               "is_accepted": false,
               "id": "23102554",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 28094888,
               "is_accepted": false,
               "last_activity_date": 1421947277,
               "body_markdown": "Here is the alternative general solution - it sorts elements of dict by keys and values.\r\nThe advantage of it - no need to specify keys, and it would still work if some keys are missing in some of dictionaries.\r\n\r\n    def sort_key_func(item):\r\n        &quot;&quot;&quot; helper function used to sort list of dicts\r\n    \r\n        :param item: dict\r\n        :return: sorted list of tuples (k, v)\r\n        &quot;&quot;&quot;\r\n        pairs = []\r\n        for k, v in item.items():\r\n            pairs.append((k, v))\r\n        return sorted(pairs)\r\n    sorted(A, key=sort_key_func)",
               "id": "28094888",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1421947277,
               "score": 5
            },
            {
               "up_vote_count": 3,
               "answer_id": 39281050,
               "last_activity_date": 1478710737,
               "path": "3.stack.answer",
               "body_markdown": "Using the pandas package is another method, though it&#39;s runtime at large scale is much slower than the more traditional methods proposed by others:\r\n\r\n    import pandas as pd\r\n\r\n    listOfDicts = [{&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}]\r\n    df = pd.DataFrame(listOfDicts)\r\n    df = df.sort_values(&#39;name&#39;)\r\n    sorted_listOfDicts = df.T.to_dict().values()\r\n\r\n\r\n\r\nHere are some benchmark values for a tiny list and a large (100k+) list of dicts:\r\n\r\n    setup_large = &quot;listOfDicts = [];\\\r\n    [listOfDicts.extend(({&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10})) for _ in range(50000)];\\\r\n    from operator import itemgetter;import pandas as pd;\\\r\n    df = pd.DataFrame(listOfDicts);&quot;\r\n    \r\n    setup_small = &quot;listOfDicts = [];\\\r\n    listOfDicts.extend(({&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}));\\\r\n    from operator import itemgetter;import pandas as pd;\\\r\n    df = pd.DataFrame(listOfDicts);&quot;\r\n    \r\n    method1 = &quot;newlist = sorted(listOfDicts, key=lambda k: k[&#39;name&#39;])&quot;\r\n    method2 = &quot;newlist = sorted(listOfDicts, key=itemgetter(&#39;name&#39;)) &quot;\r\n    method3 = &quot;df = df.sort_values(&#39;name&#39;);\\\r\n    sorted_listOfDicts = df.T.to_dict().values()&quot;\r\n    \r\n    import timeit\r\n    t = timeit.Timer(method1, setup_small)\r\n    print(&#39;Small Method LC: &#39; + str(t.timeit(100)))\r\n    t = timeit.Timer(method2, setup_small)\r\n    print(&#39;Small Method LC2: &#39; + str(t.timeit(100)))\r\n    t = timeit.Timer(method3, setup_small)\r\n    print(&#39;Small Method Pandas: &#39; + str(t.timeit(100)))\r\n    \r\n    t = timeit.Timer(method1, setup_large)\r\n    print(&#39;Large Method LC: &#39; + str(t.timeit(100)))\r\n    t = timeit.Timer(method2, setup_large)\r\n    print(&#39;Large Method LC2: &#39; + str(t.timeit(100)))\r\n    t = timeit.Timer(method3, setup_large)\r\n    print(&#39;Large Method Pandas: &#39; + str(t.timeit(1)))\r\n\r\n    #Small Method LC: 0.000163078308105\r\n    #Small Method LC2: 0.000134944915771\r\n    #Small Method Pandas: 0.0712950229645\r\n    #Large Method LC: 0.0321750640869\r\n    #Large Method LC2: 0.0206089019775\r\n    #Large Method Pandas: 5.81405615807",
               "tags": [],
               "creation_date": 1472764871,
               "last_edit_date": 1478710737,
               "is_accepted": false,
               "id": "39281050",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 42855105,
               "is_accepted": false,
               "last_activity_date": 1489746590,
               "body_markdown": "    a = [{&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, ...]\r\n\r\n    # This changes the list a\r\n    a.sort(key=lambda k : k[&#39;name&#39;])\r\n\r\n    # This returns a new list (a is not modified)\r\n    sorted(a, key=lambda k : k[&#39;name&#39;]) \r\n",
               "id": "42855105",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1489746590,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 45094029,
               "is_accepted": false,
               "last_activity_date": 1500002468,
               "body_markdown": "sometime we need to use `lower()` for example\r\n\r\n    lists = [{&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39},\r\n      {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10},\r\n      {&#39;name&#39;:&#39;abby&#39;, &#39;age&#39;:9}]\r\n\r\n    lists = sorted(lists, key=lambda k: k[&#39;name&#39;])\r\n    print(lists)\r\n    # [{&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}, {&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}, {&#39;name&#39;:&#39;abby&#39;, &#39;age&#39;:9}]\r\n\r\n    lists = sorted(lists, key=lambda k: k[&#39;name&#39;].lower())\r\n    print(lists)\r\n    # [ {&#39;name&#39;:&#39;abby&#39;, &#39;age&#39;:9}, {&#39;name&#39;:&#39;Bart&#39;, &#39;age&#39;:10}, {&#39;name&#39;:&#39;Homer&#39;, &#39;age&#39;:39}]",
               "id": "45094029",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1500002468,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47892332,
               "is_accepted": false,
               "last_activity_date": 1513704690,
               "body_markdown": "If you do not need the original `list` of `dictionaries`, you could modify it in-place with `sort()` method using a custom key function.\r\n\r\nKey function:\r\n\r\n    def get_name(d):\r\n        &quot;&quot;&quot; Return the value of a key in a dictionary. &quot;&quot;&quot;\r\n        \r\n        return d[&quot;name&quot;]\r\n\r\nThe `list` to be sorted:\r\n\r\n    data_one = [{&#39;name&#39;: &#39;Homer&#39;, &#39;age&#39;: 39}, {&#39;name&#39;: &#39;Bart&#39;, &#39;age&#39;: 10}]\r\n\r\nSorting it in-place:\r\n\r\n    data_one.sort(key=get_name)\r\n\r\n\r\nIf you need the original `list`, call the `sorted()` function passing it the `list` and the key function, then assign the returned sorted `list` to a new variable:\r\n\r\n    data_two = [{&#39;name&#39;: &#39;Homer&#39;, &#39;age&#39;: 39}, {&#39;name&#39;: &#39;Bart&#39;, &#39;age&#39;: 10}]\r\n    new_data = sorted(data_two, key=get_name)\r\n\r\nPrinting `data_one` and `new_data`.\r\n    \r\n    &gt;&gt;&gt; print(data_one)\r\n    [{&#39;name&#39;: &#39;Bart&#39;, &#39;age&#39;: 10}, {&#39;name&#39;: &#39;Homer&#39;, &#39;age&#39;: 39}]\r\n    &gt;&gt;&gt; print(new_data)\r\n    [{&#39;name&#39;: &#39;Bart&#39;, &#39;age&#39;: 10}, {&#39;name&#39;: &#39;Homer&#39;, &#39;age&#39;: 39}]\r\n\r\n\r\n",
               "id": "47892332",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1513704690,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-values-of-the-dictionary-in-python",
         "id": "858127-2312"
      },
      {
         "up_vote_count": "384",
         "path": "2.stack",
         "body_markdown": "I have data in different columns but I don&#39;t know how to extract it to save it in another variable.\r\n\r\n    index  a   b   c\r\n    1      2   3   4\r\n    2      3   4   5\r\n\r\nHow do I select `&#39;b&#39;`, `&#39;c&#39;` and save it in to df1?\r\n\r\nI tried \r\n\r\n    df1 = df[&#39;a&#39;:&#39;b&#39;]\r\n    df1 = df.ix[:, &#39;a&#39;:&#39;b&#39;]\r\n\r\nNone seem to work.",
         "view_count": "650540",
         "answer_count": "9",
         "tags": "['python', 'pandas', 'dataframe']",
         "creation_date": "1341176596",
         "last_edit_date": "1519377102",
         "code_snippet": "['<code>index  a   b   c\\n1      2   3   4\\n2      3   4   5\\n</code>', \"<code>'b'</code>\", \"<code>'c'</code>\", \"<code>df1 = df['a':'b']\\ndf1 = df.ix[:, 'a':'b']\\n</code>\", \"<code>df1 = df.ix[:, ['a', 'b']]</code>\", \"<code>df.ix[:, 'a':'b']</code>\", '<code>.ix</code>', '<code>.loc</code>', '<code>ix</code>', '<code>.ix</code>', '<code>.iloc</code>', '<code>.loc</code>', '<code>__getitem__</code>', \"<code>df1 = df[['a','b']]\\n</code>\", '<code>df1 = df.iloc[:,0:2] # Remember that Python does not slice inclusive of the ending index.\\n</code>', '<code>copy()</code>', '<code>df1 = df.iloc[0,0:2].copy() # To avoid the case where changing df1 also changes df\\n</code>', \"<code>df[['a','b']]</code>\", '<code>ix[]</code>', '<code>ix[]</code>', '<code>ix</code>', '<code>df.ix[0:2, 0:2]</code>', \"<code>df.ix[0, 'Col1':'Col5']</code>\", '<code>Col1</code>', '<code>Col5</code>', '<code>df.columns</code>', '<code>ix</code>', '<code>ix</code>', '<code>ix</code>', '<code>.iloc</code>', '<code>df.columns</code>', \"<code>['index','a','b','c']</code>\", '<code>newdf = df[df.columns[2:4]] # Remember, Python is 0-offset! The \"3rd\" entry is at slot 2.\\n</code>', '<code>df.ix</code>', '<code>.columns</code>', \"<code>'index'</code>\", '<code>DataFrame</code>', '<code>df.index</code>', '<code>Index</code>', \"<code>df['index']</code>\", '<code>df.index</code>', '<code>Index</code>', '<code>Series</code>', '<code>df.columns</code>', '<code>pd.Index</code>', '<code>.ix</code>', '<code>__getitem__</code>', '<code>df.T</code>', '<code>columns[1:3]</code>', '<code>columns</code>', '<code>Series</code>', '<code>columns[2:3]</code>', '<code>[2:4]</code>', '<code>[2:3]</code>', '<code>ix</code>', '<code>df.columns</code>', \"<code>In [39]: df\\nOut[39]: \\n   index  a  b  c\\n0      1  2  3  4\\n1      2  3  4  5\\n\\nIn [40]: df1 = df[['b', 'c']]\\n\\nIn [41]: df1\\nOut[41]: \\n   b  c\\n0  3  4\\n1  4  5\\n</code>\", \"<code>df[['b as foo', 'c as bar']</code>\", '<code>b</code>', '<code>foo</code>', '<code>c</code>', '<code>bar</code>', \"<code>df[['b', 'c']].rename(columns = {'b' : 'foo', 'c' : 'bar'})</code>\", '<code>.loc</code>', \"<code>df.loc[:, 'C':'E']\\n</code>\", '<code>C</code>', '<code>E</code>', \"<code>import pandas as pd\\nimport numpy as np\\nnp.random.seed(5)\\ndf = pd.DataFrame(np.random.randint(100, size=(100, 6)), \\n                  columns=list('ABCDEF'), \\n                  index=['R{}'.format(i) for i in range(100)])\\ndf.head()\\n\\nOut: \\n     A   B   C   D   E   F\\nR0  99  78  61  16  73   8\\nR1  62  27  30  80   7  76\\nR2  15  53  80  27  44  77\\nR3  75  65  47  30  84  86\\nR4  18   9  41  62   1  82\\n</code>\", \"<code>df.loc[:, 'C':'E']\\n\\nOut: \\n      C   D   E\\nR0   61  16  73\\nR1   30  80   7\\nR2   80  27  44\\nR3   47  30  84\\nR4   41  62   1\\nR5    5  58   0\\n...\\n</code>\", \"<code>df.loc['R6':'R10', 'C':'E']\\n\\nOut: \\n      C   D   E\\nR6   51  27  31\\nR7   83  19  18\\nR8   11  67  65\\nR9   78  27  29\\nR10   7  16  94\\n</code>\", '<code>.loc</code>', '<code>True</code>', \"<code>df.columns.isin(list('BCD'))</code>\", '<code>array([False,  True,  True,  True, False, False], dtype=bool)</code>', \"<code>['B', 'C', 'D']</code>\", \"<code>df.loc[:, df.columns.isin(list('BCD'))]\\n\\nOut: \\n      B   C   D\\nR0   78  61  16\\nR1   27  30  80\\nR2   53  80  27\\nR3   65  47  30\\nR4    9  41  62\\nR5   78   5  58\\n...\\n</code>\", \"<code>columns = ['b', 'c']\\ndf1 = pd.DataFrame(df, columns=columns)\\n</code>\", '<code>drop()</code>', \"<code>colsToDrop = ['a']\\ndf.drop(colsToDrop, axis=1)\\n</code>\", '<code>b</code>', '<code>c</code>', '<code>drop</code>', \"<code>df1=pd.DataFrame()\\ndf1=df[['b','c']]\\n</code>\", '<code>df1\\n</code>', \"<code>df['b'][0]</code>\", \"<code>df.ix[0,'b']</code>\"]",
         "title": "Selecting columns in a pandas dataframe",
         "_childDocuments_": [
            {
               "up_vote_count": 645,
               "answer_id": 11287278,
               "last_activity_date": 1508282711,
               "path": "3.stack.answer",
               "body_markdown": "The column names (which are strings) cannot be sliced in the manner you tried.\r\n\r\nHere you have a couple of options. If you know from context which variables you want to slice out, you can just return a view of only those columns by passing a list into the `__getitem__` syntax (the []&#39;s).\r\n\r\n    df1 = df[[&#39;a&#39;,&#39;b&#39;]]\r\n\r\nAlternatively, if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:\r\n\r\n    df1 = df.iloc[:,0:2] # Remember that Python does not slice inclusive of the ending index.\r\n\r\n\r\n\r\n\r\nAdditionally, you should familiarize yourself with the idea of a view into a Pandas object vs. a copy of that object. The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices). \r\n\r\nSometimes, however, there are indexing conventions in Pandas that don&#39;t do this and instead give you a new variable that just refers to the same chunk of memory as the sub-object or slice in the original object. This will happen with the second way of indexing, so you can modify it with the `copy()` function to get a regular copy. When this happens, changing what you think is the sliced object can sometimes alter the original object. Always good to be on the look out for this.\r\n\r\n    df1 = df.iloc[0,0:2].copy() # To avoid the case where changing df1 also changes df",
               "tags": [],
               "creation_date": 1341196982,
               "last_edit_date": 1508282711,
               "is_accepted": true,
               "id": "11287278",
               "down_vote_count": 1,
               "score": 644
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 42,
               "answer_id": 11385335,
               "is_accepted": false,
               "last_activity_date": 1341770112,
               "body_markdown": "    In [39]: df\r\n    Out[39]: \r\n       index  a  b  c\r\n    0      1  2  3  4\r\n    1      2  3  4  5\r\n    \r\n    In [40]: df1 = df[[&#39;b&#39;, &#39;c&#39;]]\r\n    \r\n    In [41]: df1\r\n    Out[41]: \r\n       b  c\r\n    0  3  4\r\n    1  4  5\r\n\r\n",
               "id": "11385335",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1341770112,
               "score": 42
            },
            {
               "up_vote_count": 51,
               "answer_id": 13165753,
               "last_activity_date": 1490808401,
               "path": "3.stack.answer",
               "body_markdown": "Assuming your column names (`df.columns`) are `[&#39;index&#39;,&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]`, then the data you want is in the \r\n3rd &amp; 4th columns. If you don&#39;t know their names when your script runs, you can do this\r\n\r\n    newdf = df[df.columns[2:4]] # Remember, Python is 0-offset! The &quot;3rd&quot; entry is at slot 2.\r\n\r\nAs EMS points out in [his answer](https://stackoverflow.com/a/11287278/623735), `df.ix` slices columns a bit more concisely, but the `.columns` slicing interface might be more natural because it uses the vanilla 1-D python list indexing/slicing syntax.\r\n\r\nWARN: `&#39;index&#39;` is a bad name for a `DataFrame` column. That same label is also used for the real `df.index` attribute, a `Index` array. So your column is returned by `df[&#39;index&#39;]` and the real DataFrame index is returned by `df.index`. An `Index` is a special kind of `Series` optimized for lookup of it&#39;s elements&#39; values. For df.index it&#39;s for looking up rows by their label. That `df.columns` attribute is also a `pd.Index` array, for looking up columns by their labels.",
               "tags": [],
               "creation_date": 1351709853,
               "last_edit_date": 1495541448,
               "is_accepted": false,
               "id": "13165753",
               "down_vote_count": 1,
               "score": 50
            },
            {
               "up_vote_count": 10,
               "answer_id": 25643178,
               "last_activity_date": 1415053019,
               "path": "3.stack.answer",
               "body_markdown": "You could provide a list of columns to be dropped and return back the DataFrame with only the columns needed using the `drop()` function on a Pandas DataFrame.\r\n\r\nJust saying\r\n\r\n    colsToDrop = [&#39;a&#39;]\r\n    df.drop(colsToDrop, axis=1)\r\n\r\nwould return a DataFrame with just the columns `b` and `c`.\r\n\r\nThe `drop` method is documented [here](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html).",
               "tags": [],
               "creation_date": 1409743859,
               "last_edit_date": 1415053019,
               "is_accepted": false,
               "id": "25643178",
               "down_vote_count": 0,
               "score": 10
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 29,
               "answer_id": 35203149,
               "is_accepted": false,
               "last_activity_date": 1454594735,
               "body_markdown": "I realize this question is quite old, but in the latest version of pandas there is an easy way to do exactly this. Column names (which are strings) **can** be sliced in whatever manner you like.\r\n\r\n    columns = [&#39;b&#39;, &#39;c&#39;]\r\n    df1 = pd.DataFrame(df, columns=columns)\r\n",
               "id": "35203149",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1454594735,
               "score": 28
            },
            {
               "up_vote_count": 38,
               "answer_id": 36955053,
               "last_activity_date": 1493482478,
               "path": "3.stack.answer",
               "body_markdown": "As of version 0.11.0, columns *can be* sliced in the manner you tried using the [`.loc`][1] indexer: \r\n\r\n    df.loc[:, &#39;C&#39;:&#39;E&#39;]\r\n\r\nreturns columns `C` through `E`.\r\n\r\n---\r\n\r\nA demo on a randomly generated DataFrame:\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    np.random.seed(5)\r\n    df = pd.DataFrame(np.random.randint(100, size=(100, 6)), \r\n                      columns=list(&#39;ABCDEF&#39;), \r\n                      index=[&#39;R{}&#39;.format(i) for i in range(100)])\r\n    df.head()\r\n\r\n    Out: \r\n         A   B   C   D   E   F\r\n    R0  99  78  61  16  73   8\r\n    R1  62  27  30  80   7  76\r\n    R2  15  53  80  27  44  77\r\n    R3  75  65  47  30  84  86\r\n    R4  18   9  41  62   1  82\r\n\r\nTo get the columns from C to E (note that unlike integer slicing, &#39;E&#39; is included in the columns):\r\n\r\n    df.loc[:, &#39;C&#39;:&#39;E&#39;]\r\n\r\n    Out: \r\n          C   D   E\r\n    R0   61  16  73\r\n    R1   30  80   7\r\n    R2   80  27  44\r\n    R3   47  30  84\r\n    R4   41  62   1\r\n    R5    5  58   0\r\n    ...\r\n\r\nSame works for selecting rows based on labels. Get the rows &#39;R6&#39; to &#39;R10&#39; from those columns:\r\n\r\n    df.loc[&#39;R6&#39;:&#39;R10&#39;, &#39;C&#39;:&#39;E&#39;]\r\n\r\n    Out: \r\n          C   D   E\r\n    R6   51  27  31\r\n    R7   83  19  18\r\n    R8   11  67  65\r\n    R9   78  27  29\r\n    R10   7  16  94\r\n\r\n`.loc` also accepts a boolean array so you can select the columns whose corresponding entry in the array is `True`. For example, `df.columns.isin(list(&#39;BCD&#39;))` returns `array([False,  True,  True,  True, False, False], dtype=bool)` - True if the column name is in the list `[&#39;B&#39;, &#39;C&#39;, &#39;D&#39;]`; False, otherwise.\r\n\r\n    df.loc[:, df.columns.isin(list(&#39;BCD&#39;))]\r\n\r\n    Out: \r\n          B   C   D\r\n    R0   78  61  16\r\n    R1   27  30  80\r\n    R2   53  80  27\r\n    R3   65  47  30\r\n    R4    9  41  62\r\n    R5   78   5  58\r\n    ...\r\n\r\n\r\n\r\n\r\n  [1]: http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#selection-choices",
               "tags": [],
               "creation_date": 1462019948,
               "last_edit_date": 1493482478,
               "is_accepted": false,
               "id": "36955053",
               "down_vote_count": 0,
               "score": 38
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 43734680,
               "is_accepted": false,
               "last_activity_date": 1493718112,
               "body_markdown": "I found this method to be very useful:\r\n\r\n# iloc[row slicing, column slicing]\r\nsurveys_df.iloc [0:3, 1:4]\r\n\r\nMore details can be found [here][1]\r\n\r\n\r\n  [1]: http://www.datacarpentry.org/python-ecology-lesson/02-index-slice-subset/",
               "id": "43734680",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1493718112,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 47219638,
               "is_accepted": false,
               "last_activity_date": 1510306550,
               "body_markdown": "just use:\r\nit will select b and c column.\r\n\r\n    df1=pd.DataFrame()\r\n    df1=df[[&#39;b&#39;,&#39;c&#39;]]\r\nthen u can just call df1:\r\n\r\n    df1",
               "id": "47219638",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1510306550,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 48073336,
               "is_accepted": false,
               "last_activity_date": 1514966167,
               "body_markdown": "If you want to get one element by row index and column name, you can do it just like `df[&#39;b&#39;][0]`. It is as simple as you can image. \r\n\r\nOr you can use `df.ix[0,&#39;b&#39;]`,mixed usage of index and label.\r\n\r\n",
               "id": "48073336",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1514966167,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/11285613/selecting-columns-in-a-pandas-dataframe",
         "id": "858127-2313"
      },
      {
         "up_vote_count": "1521",
         "path": "2.stack",
         "body_markdown": "How do I read every line of a file in Python and store each line as an element in a list? \r\n\r\nI want to read the file line by line and append each line to the end of the list.",
         "view_count": "2324356",
         "answer_count": "31",
         "tags": "['python', 'string', 'file', 'readlines']",
         "creation_date": "1279491901",
         "last_edit_date": "1518406093",
         "code_snippet": "['<code>for line in f:</code>', '<code>with open(fname) as f:\\n    content = f.readlines()\\n# you may also want to remove whitespace characters like `\\\\n` at the end of each line\\ncontent = [x.strip() for x in content] \\n</code>', '<code>list</code>', \"<code>content = [x.strip('\\\\n') for x in f.readlines()]</code>\", '<code>file.readlines()</code>', '<code>for</code>', \"<code>lines = [line.rstrip('\\\\n') for line in file]</code>\", \"<code>with open('filename') as f:\\n    lines = f.readlines()\\n</code>\", \"<code>lines = [line.rstrip('\\\\n') for line in open('filename')]\\n</code>\", '<code>line.strip()</code>', '<code>\\\\n</code>', \"<code>lines = (line.rstrip('\\\\n') for line in open(filename))</code>\", \"<code>lines = [line.rstrip('\\\\n') for line in open(filename)]</code>\", \"<code>with open('filename') as f: lines = [line.rstrip('\\\\n') for line in f]</code>\", '<code>with open(\"file.txt\", \"r\") as ins:\\n    array = []\\n    for line in ins:\\n        array.append(line)\\n</code>', '<code>ins.close()</code>', '<code>ins</code>', '<code>close()</code>', '<code>with open(....) as ins:</code>', '<code>file</code>', \"<code>lines = tuple(open(filename, 'r'))\\n</code>\", '<code>open</code>', '<code>tuple</code>', '<code>lines</code>', \"<code>lines = open(filename).read().split('\\\\n')</code>\", '<code>lines = open(filename).read().splitlines()</code>', '<code>\\\\n</code>', '<code>with open(fname) as f:\\n    content = f.readlines()\\n</code>', '<code>\\\\n</code>', '<code>with open(fname) as f:\\n    content = f.read().splitlines()\\n</code>', \"<code>with open('/your/path/file') as f:\\n    my_lines = f.readlines()\\n</code>\", \"<code>with open('/your/path/file') as f:\\n    for line in f:\\n        process(line)\\n</code>\", \"<code>def process(line):\\n    if 'save the world' in line.lower():\\n         superman.save_the_world()\\n</code>\", '<code>Superman</code>', \"<code>open('file_path', 'r+')</code>\", '<code>lines = open(\\'file.txt\\').read().split(\"\\\\n\")\\n</code>', '<code>fp = open(\\'file.txt\\') # open file on read mode\\nlines = fp.read().split(\"\\\\n\") # create a list containing all lines\\nfp.close() # close file\\n</code>', '<code>with</code>', '<code>with open(\\'file.txt\\') as fp:\\n    lines = fp.read().split(\"\\\\n\")\\n</code>', '<code>with</code>', '<code>fp.close()</code>', '<code>array = []\\nwith open(\"file.txt\", \"r\") as f:\\n  for line in f:\\n    array.append(line)\\n</code>', \"<code>infile = open('my_file.txt', 'r')  # Open the file for reading.\\n\\ndata = infile.read()  # Read the contents of the file.\\n\\ninfile.close()  # Close the file since we're done using it.\\n</code>\", \"<code># Open the file for reading.\\nwith open('my_file.txt', 'r') as infile:\\n\\n    data = infile.read()  # Read the contents of the file into memory.\\n</code>\", '<code># Return a list of the lines, breaking at line boundaries.\\nmy_list = data.splitlines()\\n</code>', \"<code># Open the file for reading.\\nwith open('my_file.txt', 'r') as infile:\\n\\n    data = infile.read()  # Read the contents of the file into memory.\\n\\n# Return a list of the lines, breaking at line boundaries.\\nmy_list = data.splitlines()\\n</code>\", '<code>     A fost odat\u00e3 ca-n povesti,\\n     A fost ca niciodat\u00e3,\\n     Din rude m\u00e3ri \u00eemp\u00e3r\u00e3testi,\\n     O prea frumoas\u00e3 fat\u00e3.\\n</code>', '<code>    print my_list  # Print the list.\\n\\n    # Print each line in the list.\\n    for line in my_list:\\n        print line\\n\\n    # Print the fourth element in this list.\\n    print my_list[3]\\n</code>', \"<code>     ['A fost odat\\\\xc3\\\\xa3 ca-n povesti,', 'A fost ca niciodat\\\\xc3\\\\xa3,',\\n     'Din rude m\\\\xc3\\\\xa3ri \\\\xc3\\\\xaemp\\\\xc3\\\\xa3r\\\\xc3\\\\xa3testi,', 'O prea\\n     frumoas\\\\xc3\\\\xa3 fat\\\\xc3\\\\xa3.']\\n\\n     A fost odat\u00e3 ca-n povesti, A fost ca niciodat\u00e3, Din rude m\u00e3ri\\n     \u00eemp\u00e3r\u00e3testi, O prea frumoas\u00e3 fat\u00e3.\\n\\n     O prea frumoas\u00e3 fat\u00e3.\\n</code>\", '<code>lines = []\\nwith open(\"myfile.txt\") as f:\\n    for line in f:\\n        lines.append(line)\\n</code>', '<code>   line 1\\n   line 2\\n   line 3\\n</code>', '<code>&gt;&gt;&gt; with open(\"myfile.txt\", encoding=\"utf-8\") as file:\\n...     x = [l.strip() for l in file]\\n&gt;&gt;&gt; x\\n[\\'line 1\\',\\'line 2\\',\\'line 3\\']\\n</code>', '<code>x = []\\nwith open(\"myfile.txt\") as file:\\n    for l in file:\\n        x.append(l.strip())\\n</code>', '<code>&gt;&gt;&gt; x = open(\"myfile.txt\").read().splitlines()\\n&gt;&gt;&gt; x\\n[\\'line 1\\',\\'line 2\\',\\'line 3\\']\\n</code>', '<code>&gt;&gt;&gt; y = [x.rstrip() for x in open(\"my_file.txt\")]\\n&gt;&gt;&gt; y\\n[\\'line 1\\',\\'line 2\\',\\'line 3\\']\\n</code>', '<code>from urllib.request import urlopen\\ntesto = urlopen(\"https://www.gutenberg.org/files/11/11.txt\").read()\\ntesto = str(testo).split(\"\\\\\\\\n\")\\nfor l in testo[30:48]:\\n    print(l.replace(\"\\\\\\\\r\",\"\").replace(\"\\\\\\\\\\'\",\"\\\\\\'\").replace(\"b\\'\",\"\"))\\n</code>', '<code>numpy.genfromtxt</code>', '<code>import numpy as np\\ndata = np.genfromtxt(\"yourfile.dat\",delimiter=\"\\\\n\")\\n</code>', '<code>data</code>', \"<code>lines = [line.rstrip() for line in open('file.txt')]\\n</code>\", '<code>rstrip()</code>', '<code>\\\\n</code>', \"<code>.rstrip('\\\\n')</code>\", '<code>fileinput</code>', '<code># reader.py\\nimport fileinput\\n\\ncontent = []\\nfor line in fileinput.input():\\n    content.append(line.strip())\\n\\nfileinput.close()\\n</code>', '<code>$ python reader.py textfile.txt \\n</code>', \"<code>lines = open('C:/path/file.txt').read().splitlines()\\n</code>\", '<code>f = open(\"your_file.txt\",\\'r\\')\\nout = f.readlines() # will append in the list out\\n</code>', '<code>for line in out:\\n    print line\\n</code>', '<code>for line in f:\\n    print line\\n</code>', '<code>lst = list(open(filename))\\n</code>', '<code>open</code>', \"<code>open('afile')   # opens the file named afile in the current working directory\\nopen('adir/afile')            # relative path (relative to the current working directory)\\nopen('C:/users/aname/afile')  # absolute path (windows)\\nopen('/usr/local/afile')      # absolute path (linux)\\n</code>\", '<code>.txt</code>', '<code>.doc</code>', '<code>mode</code>', '<code>r</code>', '<code>mode</code>', \"<code>open(filename)\\nopen(filename, 'r')\\n</code>\", '<code>rb</code>', \"<code>open(filename, 'rb')\\n</code>\", \"<code>'b'</code>\", '<code>open</code>', '<code>close</code>', '<code>f = open(filename)\\n# ... do stuff with f\\nf.close()\\n</code>', '<code>open</code>', '<code>close</code>', '<code>try</code>', '<code>finally</code>', '<code>f = open(filename)\\n# nothing in between!\\ntry:\\n    # do stuff with f\\nfinally:\\n    f.close()\\n</code>', '<code>open</code>', '<code>try</code>', '<code>finally</code>', '<code>with open(filename) as f:\\n    # do stuff with f\\n# The file is always closed after the with-scope ends.\\n</code>', '<code>open</code>', '<code>file</code>', '<code>with open(filename) as f:\\n    for line in f:\\n        print(line)\\n</code>', '<code>\\\\n</code>', '<code>\\\\r\\\\n</code>', '<code>with open(filename) as f:\\n    for line in f:\\n        print(line[:-1])\\n</code>', \"<code>with open(filename) as f:\\n    for line in f:\\n        if line.endswith('\\\\n'):\\n            line = line[:-1]\\n        print(line)\\n</code>\", '<code>\\\\n</code>', '<code>with open(filename) as f:\\n    for line in f:\\n        print(f.rstrip())\\n</code>', '<code>\\\\r\\\\n</code>', '<code>.rstrip()</code>', '<code>\\\\r</code>', '<code>list</code>', '<code>with open(filename) as f:\\n    lst = list(f)\\n</code>', '<code>with open(filename) as f:\\n    lst = [line.rstrip() for line in f]\\n</code>', '<code>.readlines()</code>', '<code>file</code>', '<code>list</code>', '<code>with open(filename) as f:\\n    lst = f.readlines()\\n</code>', '<code>[line.rstrip() for line in f]</code>', '<code>[line.rstrip() for line in f.readlines()]</code>', '<code>read</code>', \"<code>with open(filename) as f:\\n    lst = f.read().split('\\\\n')\\n</code>\", '<code>split</code>', '<code>with open(...) as f</code>', '<code>file</code>', '<code>for line in the_file_object:</code>', '<code>readlines()</code>', '<code>inp = \"file.txt\"\\ndata = open(inp)\\ndat = data.read()\\nlst = dat.splitlines()\\nprint lst\\n# print(lst) # for python 3\\n</code>', '<code>with open(file) as g:\\n    stuff = g.readlines()\\n</code>', '<code>file = raw_input (\"Enter EXACT file name: \")\\nwith open(file) as g:\\n    stuff = g.readlines()\\nprint (stuff)\\nexit = raw_input(\"Press enter when you are done.\")\\n</code>', '<code>buffersize = 2**16\\nwith open(path) as f: \\n    while True:\\n        lines_buffer = f.readlines(buffersize)\\n        if not lines_buffer:\\n            break\\n        for line in lines_buffer:\\n            process(line)\\n</code>', '<code>process(line)</code>', '<code>print(line)</code>', '<code>file1 = open(\"filename\",\"r\")\\n# and for reading use\\nlines = file1.readlines()\\nfile1.close()\\n</code>', '<code>lines[\"linenumber-1\"]</code>', \"<code>array = [] #declaring a list with name '**array**'\\nwith open(PATH,'r') as reader :\\n    for line in reader :\\n        array.append(line)\\n</code>\", '<code>array</code>', '<code>import pandas as pd\\ndata = pd.read_csv(filename) # You can also add parameters such as header, sep, etc.\\narray = data.values\\n</code>', '<code>data</code>', '<code>array.tolist()</code>', '<code>import numpy \\ndata = numpy.loadtxt(filename,delimiter=\"\\\\n\")\\n</code>', '<code>lines = open(filePath).readlines()\\n</code>', '<code>#!/bin/python3\\nimport os\\nimport sys\\nabspath = os.path.abspath(__file__)\\ndname = os.path.dirname(abspath)\\nfilename = dname + sys.argv[1]\\narr = open(filename).read().split(\"\\\\n\") \\nprint(arr)\\n</code>', '<code>python3 somefile.py input_file_name.txt\\n</code>', \"<code>#!/usr/bin/env python3\\n# -*- coding: utf-8 -*-\\n\\n# Define data\\nlines = ['     A first string  ',\\n         'A unicode sample: \u20ac',\\n         'German: \u00e4\u00f6\u00fc\u00df']\\n\\n# Write text file\\nwith open('file.txt', 'w') as fp:\\n    fp.write('\\\\n'.join(lines))\\n\\n# Read text file\\nwith open('file.txt', 'r') as fp:\\n    read_lines = fp.readlines()\\n    read_lines = [line.rstrip('\\\\n') for line in read_lines]\\n\\nprint(lines == read_lines)\\n</code>\", '<code>with</code>', '<code>.strip()</code>', '<code>.rstrip()</code>', '<code>lines</code>', '<code>.txt</code>', '<code>lines = list(open(\"dict.lst\", \"r\"))\\nlinesSanitized = map(lambda each:each.strip(\"\\\\n\"), lines)\\nprint linesSanitized\\n</code>', '<code>lines = list(open(\"dict.lst\", \"r\"))</code>', '<code>, r\"</code>', '<code>map(lambda...)</code>', '<code>list()</code>', '<code>\\\\n</code>', \"<code>linesSanitized = [line.rstrip('\\\\n') for line in lines]</code>\", \"<code>with open(fname) as fo:\\n        data=fo.read().replace('\\\\n', ' ').replace (',', ' ')\\n</code>\", '<code>textFile = open(\"E:\\\\Values.txt\",\"r\")\\ntextFileLines = textFile.readlines()\\n</code>']",
         "title": "How do I read a file line-by-line into a list?",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 190,
               "answer_id": 3277511,
               "is_accepted": false,
               "last_activity_date": 1279492044,
               "body_markdown": "This will yield an &quot;array&quot; of lines from the file.\r\n\r\n    lines = tuple(open(filename, &#39;r&#39;))",
               "id": "3277511",
               "tags": [],
               "down_vote_count": 6,
               "creation_date": 1279492044,
               "score": 184
            },
            {
               "up_vote_count": 327,
               "answer_id": 3277512,
               "last_activity_date": 1420707043,
               "path": "3.stack.answer",
               "body_markdown": "This is more explicit than necessary, but does what you want.\r\n\r\n    with open(&quot;file.txt&quot;, &quot;r&quot;) as ins:\r\n        array = []\r\n        for line in ins:\r\n            array.append(line)",
               "tags": [],
               "creation_date": 1279492046,
               "last_edit_date": 1420707043,
               "is_accepted": false,
               "id": "3277512",
               "down_vote_count": 3,
               "score": 324
            },
            {
               "up_vote_count": 725,
               "answer_id": 3277515,
               "last_activity_date": 1432313076,
               "path": "3.stack.answer",
               "body_markdown": "See [Input and Ouput][1]:\r\n\r\n    with open(&#39;filename&#39;) as f:\r\n        lines = f.readlines()\r\n\r\nor with stripping the newline character:\r\n\r\n    lines = [line.rstrip(&#39;\\n&#39;) for line in open(&#39;filename&#39;)]\r\n\r\n&lt;sup&gt;Editor&#39;s note: This answer&#39;s original whitespace-stripping command, `line.strip()`, as implied by Janus Troelsen&#39;s comment, would remove _all leading and trailing_ whitespace, not just the trailing `\\n`.&lt;/sup&gt;\r\n\r\n  [1]: http://docs.python.org/tutorial/inputoutput.html#reading-and-writing-files",
               "tags": [],
               "creation_date": 1279492090,
               "last_edit_date": 1432313076,
               "is_accepted": false,
               "id": "3277515",
               "down_vote_count": 2,
               "score": 723
            },
            {
               "up_vote_count": 1451,
               "answer_id": 3277516,
               "last_activity_date": 1484144690,
               "path": "3.stack.answer",
               "body_markdown": "    with open(fname) as f:\r\n        content = f.readlines()\r\n    # you may also want to remove whitespace characters like `\\n` at the end of each line\r\n    content = [x.strip() for x in content] \r\n\r\nI&#39;m guessing that you meant [`list`](http://docs.python.org/glossary.html#term-list) and not array.",
               "tags": [],
               "creation_date": 1279492112,
               "last_edit_date": 1484144690,
               "is_accepted": false,
               "id": "3277516",
               "down_vote_count": 2,
               "score": 1449
            },
            {
               "up_vote_count": 21,
               "answer_id": 17166344,
               "last_activity_date": 1488664374,
               "path": "3.stack.answer",
               "body_markdown": "Another option is [`numpy.genfromtxt`][1], for example:\r\n\r\n    import numpy as np\r\n    data = np.genfromtxt(&quot;yourfile.dat&quot;,delimiter=&quot;\\n&quot;)\r\n\r\nThis will make `data` a NumPy array with as many rows as are in your file.\r\n\r\n  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\r\n",
               "tags": [],
               "creation_date": 1371550653,
               "last_edit_date": 1488664374,
               "is_accepted": false,
               "id": "17166344",
               "down_vote_count": 1,
               "score": 20
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 37,
               "answer_id": 19639084,
               "is_accepted": false,
               "last_activity_date": 1382974852,
               "body_markdown": "This should encapsulate the open command. \r\n\r\n    array = []\r\n    with open(&quot;file.txt&quot;, &quot;r&quot;) as f:\r\n      for line in f:\r\n        array.append(line)",
               "id": "19639084",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1382974852,
               "score": 35
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 18,
               "answer_id": 20147869,
               "is_accepted": false,
               "last_activity_date": 1385132268,
               "body_markdown": "If you&#39;d like to read a file from the command line or from stdin, you can also use the `fileinput` module:\r\n\r\n    # reader.py\r\n    import fileinput\r\n    \r\n    content = []\r\n    for line in fileinput.input():\r\n        content.append(line.strip())\r\n\r\n    fileinput.close()\r\n\r\nPass files to it like so:\r\n     \r\n    $ python reader.py textfile.txt \r\n\r\nRead more here: http://docs.python.org/2/library/fileinput.html",
               "id": "20147869",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1385132268,
               "score": 18
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 14,
               "answer_id": 21073824,
               "is_accepted": false,
               "last_activity_date": 1389524284,
               "body_markdown": "    f = open(&quot;your_file.txt&quot;,&#39;r&#39;)\r\n    out = f.readlines() # will append in the list out\r\n\r\nNow variable out is a list (array) of what you want. You could either do:\r\n\r\n    for line in out:\r\n        print line\r\n\r\nor\r\n\r\n    for line in f:\r\n        print line\r\n\r\nyou&#39;ll get the same results.",
               "id": "21073824",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1389524284,
               "score": 14
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 139,
               "answer_id": 22123823,
               "is_accepted": false,
               "last_activity_date": 1393734159,
               "body_markdown": "If you want the `\\n` included:\r\n\r\n    with open(fname) as f:\r\n        content = f.readlines()\r\n\r\nIf you do not want `\\n` included:\r\n\r\n    with open(fname) as f:\r\n        content = f.read().splitlines()\r\n\r\n",
               "id": "22123823",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1393734159,
               "score": 138
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 19,
               "answer_id": 23889306,
               "is_accepted": false,
               "last_activity_date": 1401193261,
               "body_markdown": "Here&#39;s one more option by using list comprehensions on files;\r\n\r\n    lines = [line.rstrip() for line in open(&#39;file.txt&#39;)]\r\n\r\nThis should be more efficient way as the most of the work is done inside the Python interpreter.\r\n\r\n",
               "id": "23889306",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1401193261,
               "score": 19
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 24675187,
               "is_accepted": false,
               "last_activity_date": 1404991164,
               "body_markdown": "    lines = list(open(&quot;dict.lst&quot;, &quot;r&quot;))\r\n    linesSanitized = map(lambda each:each.strip(&quot;\\n&quot;), lines)\r\n    print linesSanitized\r\n",
               "id": "24675187",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1404991164,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 29,
               "answer_id": 27583116,
               "is_accepted": false,
               "last_activity_date": 1419100282,
               "body_markdown": "**Clean and Pythonic Way of Reading the Lines of a File Into a List**\r\n\r\n\r\n----------\r\nFirst and foremost, you should focus on opening your file and reading its contents in an efficient and pythonic way. Here is an example of the way I personally DO NOT prefer:\r\n\r\n&lt;!-- language: lang-python --&gt;\r\n\r\n    infile = open(&#39;my_file.txt&#39;, &#39;r&#39;)  # Open the file for reading.\r\n\r\n    data = infile.read()  # Read the contents of the file.\r\n\r\n    infile.close()  # Close the file since we&#39;re done using it.\r\n\r\n\r\nInstead, I prefer the below method of opening files for both reading and writing as it\r\nis very clean, and does not require an extra step of closing the file\r\nonce you are done using it. In the statement below, we&#39;re opening the file\r\nfor reading, and assigning it to the variable &#39;infile.&#39;  Once the code within\r\nthis statement has finished running, the file will be automatically closed.\r\n\r\n&lt;!-- language: lang-python --&gt;\r\n\r\n    # Open the file for reading.\r\n    with open(&#39;my_file.txt&#39;, &#39;r&#39;) as infile:\r\n\r\n        data = infile.read()  # Read the contents of the file into memory.\r\n\r\nNow we need to focus on bringing this data into a **Python List** because they are iterable, efficient, and flexible.  In your case, the desired goal is to bring each line of the text file into a separate element. To accomplish this, we will use the **splitlines()** method as follows:\r\n\r\n&lt;!-- language: lang-python --&gt;\r\n\r\n    # Return a list of the lines, breaking at line boundaries.\r\n    my_list = data.splitlines()\r\n\r\n\r\n----------\r\n***The Final Product:***\r\n\r\n&lt;!-- language: lang-python --&gt;\r\n\r\n    # Open the file for reading.\r\n    with open(&#39;my_file.txt&#39;, &#39;r&#39;) as infile:\r\n\r\n        data = infile.read()  # Read the contents of the file into memory.\r\n\r\n    # Return a list of the lines, breaking at line boundaries.\r\n    my_list = data.splitlines()\r\n\r\n\r\n\r\n*Testing Our Code:*\r\n\r\n - Contents of the text file:\r\n\r\n&lt;!-- language: lang-none --&gt;\r\n\r\n         A fost odat&#227; ca-n povesti,\r\n         A fost ca niciodat&#227;,\r\n         Din rude m&#227;ri &#238;mp&#227;r&#227;testi,\r\n         O prea frumoas&#227; fat&#227;.\r\n\r\n - Print statements for testing purposes:\r\n\r\n&lt;!-- language: lang-python --&gt;\r\n\r\n        print my_list  # Print the list.\r\n\r\n        # Print each line in the list.\r\n        for line in my_list:\r\n            print line\r\n\r\n        # Print the fourth element in this list.\r\n        print my_list[3]\r\n\r\n - Output (different-looking because of unicode characters):\r\n\r\n&lt;!-- language: lang-none --&gt;\r\n\r\n         [&#39;A fost odat\\xc3\\xa3 ca-n povesti,&#39;, &#39;A fost ca niciodat\\xc3\\xa3,&#39;,\r\n         &#39;Din rude m\\xc3\\xa3ri \\xc3\\xaemp\\xc3\\xa3r\\xc3\\xa3testi,&#39;, &#39;O prea\r\n         frumoas\\xc3\\xa3 fat\\xc3\\xa3.&#39;]\r\n         \r\n         A fost odat&#227; ca-n povesti, A fost ca niciodat&#227;, Din rude m&#227;ri\r\n         &#238;mp&#227;r&#227;testi, O prea frumoas&#227; fat&#227;.\r\n         \r\n         O prea frumoas&#227; fat&#227;.",
               "id": "27583116",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1419100282,
               "score": 28
            },
            {
               "up_vote_count": 16,
               "answer_id": 28358149,
               "last_activity_date": 1423547277,
               "path": "3.stack.answer",
               "body_markdown": "**The simplest way to do it**\r\n\r\nA simple way is to:\r\n\r\n 1. Read the whole file as a string\r\n 2. Split the string line by line\r\n\r\nIn one line, that would give:\r\n\r\n    lines = open(&#39;C:/path/file.txt&#39;).read().splitlines()\r\n\r\n ",
               "tags": [],
               "creation_date": 1423193688,
               "last_edit_date": 1423547277,
               "is_accepted": false,
               "id": "28358149",
               "down_vote_count": 1,
               "score": 15
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 29289118,
               "is_accepted": false,
               "last_activity_date": 1427404987,
               "body_markdown": "    with open(fname) as fo:\r\n            data=fo.read().replace(&#39;\\n&#39;, &#39; &#39;).replace (&#39;,&#39;, &#39; &#39;)\r\n\r\nThis should answer your question. The replace function will act as delimiter to strip the file.",
               "id": "29289118",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1427404987,
               "score": 0
            },
            {
               "up_vote_count": 61,
               "answer_id": 29740172,
               "last_activity_date": 1496540659,
               "path": "3.stack.answer",
               "body_markdown": "if you don&#39;t care about closing the file, this one-liner works:\r\n\r\n    lines = open(&#39;file.txt&#39;).read().split(&quot;\\n&quot;)\r\n\r\nThe *traditional* way:\r\n\r\n    fp = open(&#39;file.txt&#39;) # open file on read mode\r\n    lines = fp.read().split(&quot;\\n&quot;) # create a list containing all lines\r\n    fp.close() # close file\r\n\r\nUsing `with` (recommended):\r\n\r\n    with open(&#39;file.txt&#39;) as fp:\r\n        lines = fp.read().split(&quot;\\n&quot;)\r\n",
               "tags": [],
               "creation_date": 1429509190,
               "last_edit_date": 1496540659,
               "is_accepted": false,
               "id": "29740172",
               "down_vote_count": 2,
               "score": 59
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 31522796,
               "is_accepted": false,
               "last_activity_date": 1437413583,
               "body_markdown": "Could also use the loadtxt command in numpy. This checks for fewer conditions than genfromtxt so it may be faster. \r\n\r\n    import numpy \r\n    data = numpy.loadtxt(filename,delimiter=&quot;\\n&quot;)",
               "id": "31522796",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1437413583,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 35123171,
               "is_accepted": false,
               "last_activity_date": 1454302198,
               "body_markdown": "    textFile = open(&quot;E:\\Values.txt&quot;,&quot;r&quot;)\r\n    textFileLines = textFile.readlines()\r\n\r\n&quot;textFileLines&quot; is the array you wanted",
               "id": "35123171",
               "tags": [],
               "down_vote_count": 4,
               "creation_date": 1454302198,
               "score": -2
            },
            {
               "up_vote_count": 81,
               "answer_id": 35622867,
               "last_activity_date": 1472061888,
               "path": "3.stack.answer",
               "body_markdown": "You could simply do the following, as has been suggested:\r\n\r\n    with open(&#39;/your/path/file&#39;) as f:\r\n        my_lines = f.readlines()\r\n\r\nNote that this approach has 2 downsides:\r\n\r\n1) You store all the lines in memory. In the general case, this is a very bad idea. The file could be very large, and you could run out of memory. Even if it&#39;s not large, it is simply a waste of memory.\r\n\r\n2) This does not allow processing of each line as you read them. So if you process your lines after this, it is not efficient (requires two passes rather than one).\r\n\r\nA better approach for the general case would be the following:\r\n\r\n    with open(&#39;/your/path/file&#39;) as f:\r\n        for line in f:\r\n            process(line)\r\n\r\nWhere you define your process function any way you want. For example:\r\n  \r\n    def process(line):\r\n        if &#39;save the world&#39; in line.lower():\r\n             superman.save_the_world()\r\n\r\n(The implementation of the `Superman` class is left as an exercise for you).\r\n\r\n\r\nThis will work nicely for any file size and you go through your file in just 1 pass. This is typically how generic parsers will work.\r\n \r\n\r\n",
               "tags": [],
               "creation_date": 1456391618,
               "last_edit_date": 1472061888,
               "is_accepted": false,
               "id": "35622867",
               "down_vote_count": 1,
               "score": 80
            },
            {
               "up_vote_count": 7,
               "answer_id": 35837826,
               "last_activity_date": 1488664482,
               "path": "3.stack.answer",
               "body_markdown": "To my knowledge Python doesn&#39;t have a native array data structure. But it does support the list data structure which is much simpler to use than an array.\r\n\r\n    array = [] #declaring a list with name &#39;**array**&#39;\r\n    with open(PATH,&#39;r&#39;) as reader :\r\n        for line in reader :\r\n            array.append(line)\r\n",
               "tags": [],
               "creation_date": 1457333646,
               "last_edit_date": 1488664482,
               "is_accepted": false,
               "id": "35837826",
               "down_vote_count": 0,
               "score": 7
            },
            {
               "up_vote_count": 13,
               "answer_id": 36252891,
               "last_activity_date": 1511683130,
               "path": "3.stack.answer",
               "body_markdown": "A real easy way:\r\n\r\n    with open(file) as g:\r\n        stuff = g.readlines()\r\n\r\nIf you want to make it a fully-fledged program, type this in:\r\n\r\n    file = raw_input (&quot;Enter EXACT file name: &quot;)\r\n    with open(file) as g:\r\n        stuff = g.readlines()\r\n    print (stuff)\r\n    exit = raw_input(&quot;Press enter when you are done.&quot;)\r\n\r\nFor some reason, it doesn&#39;t read .py files properly.\r\n\r\n",
               "tags": [],
               "creation_date": 1459114194,
               "last_edit_date": 1511683130,
               "is_accepted": false,
               "id": "36252891",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 6,
               "answer_id": 36313833,
               "last_activity_date": 1488664596,
               "path": "3.stack.answer",
               "body_markdown": "Use this:\r\n\r\n    import pandas as pd\r\n    data = pd.read_csv(filename) # You can also add parameters such as header, sep, etc.\r\n    array = data.values\r\n\r\n`data` is a dataframe type, and uses values to get ndarray. You can also get a list by using `array.tolist()`.\r\n",
               "tags": [],
               "creation_date": 1459353032,
               "last_edit_date": 1488664596,
               "is_accepted": false,
               "id": "36313833",
               "down_vote_count": 1,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 37019927,
               "is_accepted": false,
               "last_activity_date": 1462342751,
               "body_markdown": "How about:\r\n\r\n    fp = open(&quot;filename&quot;)\r\n    content = fp.read();\r\n    lines = content.split(&quot;\\n&quot;)\r\n\r\n",
               "id": "37019927",
               "tags": [],
               "down_vote_count": 3,
               "creation_date": 1462342751,
               "score": -2
            },
            {
               "up_vote_count": 13,
               "answer_id": 39407957,
               "last_activity_date": 1511682981,
               "path": "3.stack.answer",
               "body_markdown": "Just use the splitlines() functions. Here is an example.\r\n\r\n    inp = &quot;file.txt&quot;\r\n    data = open(inp)\r\n    dat = data.read()\r\n    lst = dat.splitlines()\r\n    print lst\r\n    # print(lst) # for python 3\r\n\r\nIn the output you will have the list of lines.",
               "tags": [],
               "creation_date": 1473412388,
               "last_edit_date": 1511682981,
               "is_accepted": false,
               "id": "39407957",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 25,
               "answer_id": 41066923,
               "is_accepted": false,
               "last_activity_date": 1481309037,
               "body_markdown": "I&#39;d do it like this.\r\n\r\n    lines = []\r\n    with open(&quot;myfile.txt&quot;) as f:\r\n        for line in f:\r\n            lines.append(line)",
               "id": "41066923",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1481309037,
               "score": 25
            },
            {
               "up_vote_count": 9,
               "answer_id": 42733235,
               "last_activity_date": 1489637353,
               "path": "3.stack.answer",
               "body_markdown": "If you want to are faced with a **very large / huge file** and want to **read faster** (imagine you are in a Topcoder/Hackerrank coding competition), you might read a considerably bigger chunk of lines into a memory buffer at one time, rather than just iterate line by line at file level.\r\n\r\n&lt;!-- language: lang-python --&gt;\r\n\r\n    buffersize = 2**16\r\n    with open(path) as f: \r\n        while True:\r\n            lines_buffer = f.readlines(buffersize)\r\n            if not lines_buffer:\r\n                break\r\n            for line in lines_buffer:\r\n                process(line)\r\n\r\n",
               "tags": [],
               "creation_date": 1489222141,
               "last_edit_date": 1489637353,
               "is_accepted": false,
               "id": "42733235",
               "down_vote_count": 0,
               "score": 9
            },
            {
               "up_vote_count": 25,
               "answer_id": 43625375,
               "last_activity_date": 1515785800,
               "path": "3.stack.answer",
               "body_markdown": "Data into list  \r\n[![][1]][1]\r\n\r\nLet&#39;s read data from a text file  \r\n\r\n&lt;h3&gt;Text file content:&lt;/h3&gt;\r\n    \r\n       line 1\r\n       line 2\r\n       line 3\r\n\r\n&lt;ol&gt;\r\n&lt;li&gt; Open the cmd in the same dir (right click the mouse and choose cmd or powershell)\r\n&lt;li&gt; run python and in the interpreter write:\r\n&lt;/ol&gt;\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n&lt;h1&gt; The python script &lt;/h1&gt;\r\n\r\n    &gt;&gt;&gt; with open(&quot;myfile.txt&quot;, encoding=&quot;utf-8&quot;) as file:\r\n    ...     x = [l.strip() for l in file]\r\n    &gt;&gt;&gt; x\r\n    [&#39;line 1&#39;,&#39;line 2&#39;,&#39;line 3&#39;]\r\n\r\n&lt;h1&gt;Using append&lt;/h1&gt;\r\n\r\n    x = []\r\n    with open(&quot;myfile.txt&quot;) as file:\r\n        for l in file:\r\n            x.append(l.strip())\r\n\r\n&lt;h1&gt; or...&lt;/h1&gt;\r\n\r\n    &gt;&gt;&gt; x = open(&quot;myfile.txt&quot;).read().splitlines()\r\n    &gt;&gt;&gt; x\r\n    [&#39;line 1&#39;,&#39;line 2&#39;,&#39;line 3&#39;]\r\n\r\n&lt;h1&gt; or...&lt;/h1&gt;\r\n\r\n    &gt;&gt;&gt; y = [x.rstrip() for x in open(&quot;my_file.txt&quot;)]\r\n    &gt;&gt;&gt; y\r\n    [&#39;line 1&#39;,&#39;line 2&#39;,&#39;line 3&#39;]\r\n\r\n&lt;h1&gt; Getting a text from a web page with python 3&lt;/h1&gt;\r\nHere there&#39;s a pratical example of a text grabbed from the net. The page contains just plain text. We just need to get rid of the \\n \\r and b&#39; thing, in it to keep it clean to be printed. There&#39;s a convertion of bytes data to string data and then the string is splitted in lines or, better, each line is stored into an item of the list. when the print function is called, we pass each item of the list without the \\r and \\&#39; and b&#39; things that make the text less readeable.\r\n\r\n\r\n    from urllib.request import urlopen\r\n    testo = urlopen(&quot;https://www.gutenberg.org/files/11/11.txt&quot;).read()\r\n    testo = str(testo).split(&quot;\\\\n&quot;)\r\n    for l in testo[30:48]:\r\n        print(l.replace(&quot;\\\\r&quot;,&quot;&quot;).replace(&quot;\\\\&#39;&quot;,&quot;\\&#39;&quot;).replace(&quot;b&#39;&quot;,&quot;&quot;))\r\n\r\n    \r\n&gt;&lt;h1&gt;OUTPUT:&lt;/h1&gt;\r\n&gt; \r\n&gt; ALICE&#39;S ADVENTURES IN WONDERLAND\r\n&gt; \r\n&gt; Lewis Carroll\r\n&gt; \r\n&gt; THE MILLENNIUM FULCRUM EDITION 3.0\r\n&gt; \r\n&gt; \r\n&gt; \r\n&gt; \r\n&gt; CHAPTER I. Down the Rabbit-Hole\r\n&gt; \r\n&gt; Alice was beginning to get very tired of sitting by her sister on the\r\n&gt; bank, and of having nothing to do: once or twice she had peeped into\r\n&gt; the book her sister was reading, but it had no pictures or\r\n&gt; conversations in it, &#39;and what is the use of a book,&#39; thought Alice\r\n&gt; &#39;without pictures or conversations?&#39;\r\n&gt;\r\n\r\n  [1]: https://i.stack.imgur.com/9pZBX.png",
               "tags": [],
               "creation_date": 1493182653,
               "last_edit_date": 1515785800,
               "is_accepted": false,
               "id": "43625375",
               "down_vote_count": 1,
               "score": 24
            },
            {
               "up_vote_count": 8,
               "answer_id": 44068160,
               "last_activity_date": 1511683258,
               "path": "3.stack.answer",
               "body_markdown": "You can just open your file for reading using\r\n\r\n    file1 = open(&quot;filename&quot;,&quot;r&quot;)\r\n    # and for reading use\r\n    lines = file1.readlines()\r\n    file1.close()\r\n\r\nThe list lines will contain all your lines as individual elements and you can call a specific element using `lines[&quot;linenumber-1&quot;]` as python starts its counting from 0.\r\n",
               "tags": [],
               "creation_date": 1495191056,
               "last_edit_date": 1511683258,
               "is_accepted": false,
               "id": "44068160",
               "down_vote_count": 0,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 44752149,
               "is_accepted": false,
               "last_activity_date": 1498441386,
               "body_markdown": "You can easily do it by the following piece of code:\r\n\r\n    lines = open(filePath).readlines()\r\n\r\n",
               "id": "44752149",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1498441386,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 45949906,
               "is_accepted": false,
               "last_activity_date": 1504050839,
               "body_markdown": "### Command line version \r\n    #!/bin/python3\r\n    import os\r\n    import sys\r\n    abspath = os.path.abspath(__file__)\r\n    dname = os.path.dirname(abspath)\r\n    filename = dname + sys.argv[1]\r\n    arr = open(filename).read().split(&quot;\\n&quot;) \r\n    print(arr)\r\n\r\n### Run with: \r\n    python3 somefile.py input_file_name.txt\r\n",
               "id": "45949906",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1504050839,
               "score": 2
            },
            {
               "up_vote_count": 1,
               "answer_id": 48288683,
               "last_activity_date": 1516132263,
               "path": "3.stack.answer",
               "body_markdown": "# Read and write text files with Python 2+3; works with unicode\r\n\r\n    #!/usr/bin/env python3\r\n    # -*- coding: utf-8 -*-\r\n\r\n    # Define data\r\n    lines = [&#39;     A first string  &#39;,\r\n             &#39;A unicode sample: \u20ac&#39;,\r\n             &#39;German: &#228;&#246;&#252;&#223;&#39;]\r\n\r\n    # Write text file\r\n    with open(&#39;file.txt&#39;, &#39;w&#39;) as fp:\r\n        fp.write(&#39;\\n&#39;.join(lines))\r\n\r\n    # Read text file\r\n    with open(&#39;file.txt&#39;, &#39;r&#39;) as fp:\r\n        read_lines = fp.readlines()\r\n        read_lines = [line.rstrip(&#39;\\n&#39;) for line in read_lines]\r\n\r\n    print(lines == read_lines)\r\n\r\nThings to notice:\r\n\r\n* `with` is a so called [context manager](https://docs.python.org/2/reference/compound_stmts.html#with). It makes sure that the opened file is closed again.\r\n* All solutions here which simply make `.strip()` or `.rstrip()` will fail to reproduce the `lines` as they also strip the white space.\r\n\r\n\r\n## Common file endings\r\n\r\n`.txt`\r\n\r\n\r\n## More advanced file writing / reading\r\n\r\n* CSV: Super simple format ([read &amp; write](https://stackoverflow.com/a/41585079/562769))\r\n* JSON: Nice for writing human-readable data; VERY commonly used ([read &amp; write](https://stackoverflow.com/a/37795053/562769))\r\n* YAML: YAML is a superset of JSON, but easier to read ([read &amp; write](https://stackoverflow.com/a/42054860/562769), [comparison of JSON and YAML](https://stackoverflow.com/a/1729545/562769))\r\n* pickle: A Python serialization format ([read &amp; write](https://stackoverflow.com/a/33245595/562769))\r\n* [MessagePack](http://msgpack.org/) ([Python package](https://pypi.python.org/pypi/msgpack-python)): More compact representation ([read &amp; write](https://stackoverflow.com/q/43442194/562769))\r\n* [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) ([Python package](http://docs.h5py.org/en/latest/quick.html)): Nice for matrices ([read &amp; write](https://stackoverflow.com/a/41586571/562769))\r\n* XML: exists too \\*sigh\\* ([read](https://stackoverflow.com/a/1912483/562769) &amp; [write](https://stackoverflow.com/a/3605831/562769))\r\n\r\nFor your application, the following might be important:\r\n\r\n* Support by other programming languages\r\n* Reading / writing performance\r\n* Compactness (file size)\r\n\r\nSee also: [Comparison of data serialization formats](https://en.wikipedia.org/wiki/Comparison_of_data_serialization_formats)\r\n\r\nIn case you are rather looking for a way to make configuration files, you might want to read my short article [Configuration files in Python](https://martin-thoma.com/configuration-files-in-python/)",
               "tags": [],
               "creation_date": 1516131730,
               "last_edit_date": 1516132263,
               "is_accepted": false,
               "id": "48288683",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "up_vote_count": 14,
               "answer_id": 48290921,
               "last_activity_date": 1516929087,
               "path": "3.stack.answer",
               "body_markdown": "To read a file into a list you need to do three things:\r\n\r\n- Open the file\r\n- Read the file\r\n- Store the contents as list\r\n\r\nFortunately Python makes it very easy to do these things so the shortest way to read a file into a list is:\r\n\r\n    lst = list(open(filename))\r\n\r\nHowever I&#39;ll add some more explanation.\r\n\r\n## Opening the file\r\n\r\nI assume that you want to open a specific file and you don&#39;t deal directly with a file-handle (or a file-like-handle). The most commonly used function to open a file in Python is [`open`](https://docs.python.org/library/functions.html#open), it takes one mandatory argument and two optional ones in Python 2.7:\r\n\r\n- Filename\r\n- Mode\r\n- Buffering (I&#39;ll ignore this argument in this answer)\r\n\r\nThe filename should be a string that represents the *path to the file*. For example:\r\n\r\n    open(&#39;afile&#39;)   # opens the file named afile in the current working directory\r\n    open(&#39;adir/afile&#39;)            # relative path (relative to the current working directory)\r\n    open(&#39;C:/users/aname/afile&#39;)  # absolute path (windows)\r\n    open(&#39;/usr/local/afile&#39;)      # absolute path (linux)\r\n\r\nNote that the file extension needs to be specified. This is especially important for Windows users because file extensions like `.txt` or `.doc`, etc. are hidden *by default* when viewed in the explorer.\r\n\r\nThe second argument is the `mode`, it&#39;s `r` by default which means &quot;read-only&quot;. That&#39;s exactly what you need in your case. \r\n\r\nBut in case you actually want to create a file and/or write to a file you&#39;ll need a different argument here. [There is an excellent answer if you want an overview](https://stackoverflow.com/a/30566011/5393381).\r\n\r\nFor reading a file you can omit the `mode` or pass it in explicitly:\r\n\r\n    open(filename)\r\n    open(filename, &#39;r&#39;)\r\n\r\nBoth will open the file in read-only mode. In case you want to read in a binary file on Windows you need to use the mode `rb`:\r\n\r\n    open(filename, &#39;rb&#39;)\r\n\r\nOn other platforms the `&#39;b&#39;` (binary mode) is simply ignored.\r\n\r\n---\r\n\r\nNow that I&#39;ve shown how to `open` the file, let&#39;s talk about the fact that you always need to `close` it again. Otherwise it will keep an open file-handle to the file until the process exits (or Python garbages the file-handle). \r\n\r\nWhile you could use:\r\n\r\n    f = open(filename)\r\n    # ... do stuff with f\r\n    f.close()\r\n\r\nThat will fail to close the file when something between `open` and `close` throws an exception. You could avoid that by using a `try` and `finally`:\r\n\r\n    f = open(filename)\r\n    # nothing in between!\r\n    try:\r\n        # do stuff with f\r\n    finally:\r\n        f.close()\r\n\r\nHowever Python provides context managers that have a prettier syntax (but for `open` it&#39;s almost identical to the `try` and `finally` above):\r\n\r\n    with open(filename) as f:\r\n        # do stuff with f\r\n    # The file is always closed after the with-scope ends.\r\n\r\nThe last approach is the **recommended** approach to open a file in Python!\r\n\r\n## Reading the file\r\n\r\nOkay, you&#39;ve opened the file, now how to read it? \r\n\r\nThe `open` function returns a [`file`](https://docs.python.org/2/library/stdtypes.html#bltin-file-objects) object and it supports Pythons iteration protocol. Each iteration will give you a line:\r\n\r\n    with open(filename) as f:\r\n        for line in f:\r\n            print(line)\r\n\r\nThis will print each line of the file. Note however that each line will contain a newline character `\\n` at the end (probably `\\r\\n` on Windows). If you don&#39;t want that you can could simply remove the last character (or the last two characters on Windows):\r\n\r\n    with open(filename) as f:\r\n        for line in f:\r\n            print(line[:-1])\r\n\r\nBut the last line doesn&#39;t necessarily has a trailing newline, so one shouldn&#39;t use that. One could check if it ends with a trailing newline and if so remove it:\r\n\r\n    with open(filename) as f:\r\n        for line in f:\r\n            if line.endswith(&#39;\\n&#39;):\r\n                line = line[:-1]\r\n            print(line)\r\n\r\nBut you could simply remove all whitespaces (including the `\\n` character) from the *end of the string*, this will also remove all other *trailing* whitespaces so you have to be careful if these are important:\r\n\r\n    with open(filename) as f:\r\n        for line in f:\r\n            print(f.rstrip())\r\n\r\nHowever if the lines end with `\\r\\n` (Windows &quot;newlines&quot;) that `.rstrip()` will also take care of the `\\r`!\r\n\r\n## Store the contents as list\r\n\r\nNow that you know how to open the file and read it, it&#39;s time to store the contents in a list. The simplest option would be to use the [`list`](https://docs.python.org/2/library/functions.html#func-list) function:\r\n\r\n    with open(filename) as f:\r\n        lst = list(f)\r\n\r\nOr in case you want to strip the trailing newlines and a list-comprehension:\r\n\r\n    with open(filename) as f:\r\n        lst = [line.rstrip() for line in f]\r\n\r\nOr even simpler: The [`.readlines()`](https://docs.python.org/2/library/stdtypes.html#file.readlines) method of the `file` object by default returns a `list` of the lines:\r\n\r\n    with open(filename) as f:\r\n        lst = f.readlines()\r\n\r\nThis will also include the trailing newline characters, if you don&#39;t want them I would recommend the `[line.rstrip() for line in f]` approach because it avoids keeping two lists (if you used `[line.rstrip() for line in f.readlines()]`) containing all the lines in memory.\r\n\r\nThere&#39;s an additional option to get the desired output, however it&#39;s rather &quot;suboptimal&quot;: [`read`](https://docs.python.org/2/library/stdtypes.html#file.read) the complete file in a string and then split on newlines:\r\n\r\n    with open(filename) as f:\r\n        lst = f.read().split(&#39;\\n&#39;)\r\n\r\nIt takes care of the trailing newlines automatically because the `split` character isn&#39;t included. However it&#39;s suboptimal because you keep the file as string and as a list of lines in memory!\r\n\r\n## Summary\r\n\r\n- Use `with open(...) as f` when opening files because you don&#39;t need to take care of closing the file yourself and it&#39;s exception-proof.\r\n- `file` objects support the iteration protocol so reading a file line-by-line is as simple as `for line in the_file_object:`.\r\n- Always browse the documentation for the available functions/classes. Most of the time there&#39;s a perfect match for the task or at least a few good ones. The obvious choice in this case would be `readlines()` but if you want to process the lines before storing them in the list I would recommend a simple list-comprehension.",
               "tags": [],
               "creation_date": 1516142037,
               "last_edit_date": 1516929087,
               "is_accepted": false,
               "id": "48290921",
               "down_vote_count": 0,
               "score": 14
            }
         ],
         "link": "https://stackoverflow.com/questions/3277503/how-do-i-read-a-file-line-by-line-into-a-list",
         "id": "858127-2314"
      },
      {
         "up_vote_count": "324",
         "path": "2.stack",
         "body_markdown": "I made a function which will look up ages in dictionary and show the matching name:\r\n\r\n    list = {&#39;george&#39;:16,&#39;amber&#39;:19}\r\n    search_age = raw_input(&quot;Provide age&quot;)\r\n    for age in list.values():\r\n        if age == search_age:\r\n            name = list[age]\r\n            print name\r\n\r\nI know how to compare and find the age I just don&#39;t know how to show the name of the person. Additionally, I am getting a `KeyError` because of line 5. I know it&#39;s not correct but I can&#39;t figure out to make it search backwards.\r\n\r\n",
         "view_count": "700291",
         "answer_count": "28",
         "tags": "['python', 'dictionary']",
         "creation_date": "1320527358",
         "last_edit_date": "1457335980",
         "code_snippet": "['<code>list = {\\'george\\':16,\\'amber\\':19}\\nsearch_age = raw_input(\"Provide age\")\\nfor age in list.values():\\n    if age == search_age:\\n        name = list[age]\\n        print name\\n</code>', '<code>KeyError</code>', '<code>list</code>', '<code>dict</code>', '<code>for name, age in list.iteritems():    # for name, age in list.items():  (for Python 3.x)\\n    if age == search_age:\\n        print name\\n</code>', '<code>Python 3.x</code>', '<code>list.items()</code>', '<code>list.iteritems()</code>', '<code>dict</code>', '<code>dict</code>', \"<code>mydict = {'george':16,'amber':19}\\nprint mydict.keys()[mydict.values().index(16)] # Prints george\\n</code>\", \"<code>mydict = {'george':16,'amber':19}\\nprint(list(mydict.keys())[list(mydict.values()).index(16)]) # Prints george\\n</code>\", '<code>keys()</code>', '<code>.values()</code>', '<code>list.keys()</code>', '<code>list.values()</code>', '<code>.items()</code>', '<code>(key, value)</code>', '<code>for name, age in mydict.items():\\n    if age == search_age:\\n        print name\\n</code>', '<code>for</code>', \"<code>{16: 'george', 19: 'amber'}\\n</code>\", '<code>mydict[search_age]\\n</code>', '<code>mydict</code>', '<code>list</code>', '<code>list</code>', '<code>[name for name, age in mydict.items() if age == search_age]\\n</code>', '<code>next((name for name, age in mydict.items() if age == search_age), None)\\n</code>', '<code>None</code>', '<code>dict</code>', '<code>.iteritems()</code>', '<code>.items()</code>', '<code>dict</code>', '<code>dict</code>', '<code>&gt;&gt;&gt; def method1(list,search_age):\\n...     for name,age in list.iteritems():\\n...             if age == search_age:\\n...                     return name\\n... \\n&gt;&gt;&gt; def method2(list,search_age):\\n...     return [name for name,age in list.iteritems() if age == search_age]\\n... \\n&gt;&gt;&gt; def method3(list,search_age):\\n...     return list.keys()[list.values().index(search_age)]\\n</code>', '<code>profile.run()</code>', '<code>&gt;&gt;&gt; profile.run(\"for i in range(0,100000): method1(list,16)\")\\n     200004 function calls in 1.173 seconds\\n</code>', '<code>&gt;&gt;&gt; profile.run(\"for i in range(0,100000): method2(list,16)\")\\n     200004 function calls in 1.222 seconds\\n</code>', '<code>&gt;&gt;&gt; profile.run(\"for i in range(0,100000): method3(list,16)\")\\n     400004 function calls in 2.125 seconds\\n</code>', '<code>&gt;&gt;&gt; profile.run(\"for i in range(0,10000): method1(UIC_CRS,\\'7088380\\')\")\\n     20004 function calls in 2.928 seconds\\n</code>', '<code>&gt;&gt;&gt; profile.run(\"for i in range(0,10000): method2(UIC_CRS,\\'7088380\\')\")\\n     20004 function calls in 3.872 seconds\\n</code>', '<code>&gt;&gt;&gt; profile.run(\"for i in range(0,10000): method3(UIC_CRS,\\'7088380\\')\")\\n     40004 function calls in 1.176 seconds\\n</code>', '<code>p = dict(zip(i.values(),i.keys()))\\n</code>', '<code>lKey = [key for key, value in lDictionary.iteritems() if value == lValue][0]\\n</code>', \"<code>a = {'a':1,'b':2,'c':3}\\n{v:k for k, v in a.items()}[1]\\n</code>\", '<code>{k:v for k, v in a.items() if v == 1}\\n</code>', '<code>#Code without comments.\\n\\nlist1 = {\\'george\\':16,\\'amber\\':19, \\'Garry\\':19}\\nsearch_age = raw_input(\"Provide age: \")\\nprint\\nsearch_age = int(search_age)\\n\\nlistByAge = {}\\n\\nfor name, age in list1.items():\\n    if age == search_age:\\n        age = str(age)\\n        results = name + \" \" +age\\n        print results\\n\\n        age2 = int(age)\\n        listByAge[name] = listByAge.get(name,0)+age2\\n\\nprint\\nprint listByAge\\n</code>', '<code>#Code with comments.\\n#I\\'ve added another name with the same age to the list.\\nlist1 = {\\'george\\':16,\\'amber\\':19, \\'Garry\\':19}\\n#Original code.\\nsearch_age = raw_input(\"Provide age: \")\\nprint\\n#Because raw_input gives a string, we need to convert it to int,\\n#so we can search the dictionary list with it.\\nsearch_age = int(search_age)\\n\\n#Here we define another empty dictionary, to store the results in a more \\n#permanent way.\\nlistByAge = {}\\n\\n#We use double variable iteration, so we get both the name and age \\n#on each run of the loop.\\nfor name, age in list1.items():\\n    #Here we check if the User Defined age = the age parameter \\n    #for this run of the loop.\\n    if age == search_age:\\n        #Here we convert Age back to string, because we will concatenate it \\n        #with the person\\'s name. \\n        age = str(age)\\n        #Here we concatenate.\\n        results = name + \" \" +age\\n        #If you want just the names and ages displayed you can delete\\n        #the code after \"print results\". If you want them stored, don\\'t...\\n        print results\\n\\n        #Here we create a second variable that uses the value of\\n        #the age for the current person in the list.\\n        #For example if \"Anna\" is \"10\", age2 = 10,\\n        #integer value which we can use in addition.\\n        age2 = int(age)\\n        #Here we use the method that checks or creates values in dictionaries.\\n        #We create a new entry for each name that matches the User Defined Age\\n        #with default value of 0, and then we add the value from age2.\\n        listByAge[name] = listByAge.get(name,0)+age2\\n\\n#Here we print the new dictionary with the users with User Defined Age.\\nprint\\nprint listByAge\\n</code>', \"<code>#Results\\nRunning: *\\\\test.py (Thu Jun 06 05:10:02 2013)\\n\\nProvide age: 19\\n\\namber 19\\nGarry 19\\n\\n{'amber': 19, 'Garry': 19}\\n\\nExecution Successful!\\n</code>\", '<code>dict.keys()</code>', '<code>dict.values()</code>', '<code>list.index()</code>', '<code>names_dict = {\\'george\\':16,\\'amber\\':19}\\nsearch_age = int(raw_input(\"Provide age\"))\\nkey = names_dict.keys()[names_dict.values().index(search_age)]\\n</code>', '<code>search_age</code>', '<code>value</code>', '<code>search_age</code>', '<code>type(dict_values)</code>', '<code>for name in mydict.keys():\\n    if mydict[name] == search_age:\\n        print name \\n        #or do something else with it. \\n        #if in a function append to a temporary list, \\n        #then after the loop return the list\\n</code>', '<code>lookup = {value: key for key, value in self.data}\\nlookup[value]\\n</code>', '<code>def get_Value(dic,value):\\n    for name in dic:\\n        if dic[name] == value:\\n            del dic[name]\\n            return name\\n</code>', \"<code>import pandas as pd\\nlist = {'george':16,'amber':19}\\nlookup_list = pd.Series(list)\\n</code>\", '<code>lookup_list[lookup_list.values == 19]\\n</code>', '<code>Out[1]: \\namber    19\\ndtype: int64\\n</code>', '<code>answer = lookup_list[lookup_list.values == 19].index\\nanswer = pd.Index.tolist(answer)\\n</code>', '<code>def recover_key(dicty,value):\\n    for a_key in dicty.keys():\\n        if (dicty[a_key] == value):\\n            return a_key\\n</code>', '<code>def find_key(value, dictionary):\\n    return reduce(lambda x, y: x if x is not None else y,\\n                  map(lambda x: x[0] if x[1] == value else None, \\n                      dictionary.iteritems()))\\n</code>', \"<code>D = {'Ali': 20, 'Marina': 12, 'George':16}\\nage = int(input('enter age:\\\\t'))  \\nfor element in D.keys():\\n    if D[element] == age:\\n        print(element)\\n</code>\", '<code>d = { k1 : v1, k2 : v2, k3 : v1}\\n</code>', \"<code>myList = {'george':16,'amber':19, 'rachel':19, \\n           'david':15 }                         #Setting the dictionary\\nresult=[]                                       #Making ready of the result list\\nsearch_age = int(input('Enter age '))\\n\\nfor keywords in myList.keys():\\n    if myList[keywords] ==search_age:\\n    result.append(keywords)                    #This part, we are making list of results\\n\\nfor res in result:                             #We are now printing the results\\n    print(res)\\n</code>\", \"<code>titleDic = {'\u0424\u0438\u043b\u044c\u043c\u044b':1, '\u041c\u0443\u0437\u044b\u043a\u0430':2}\\n\\ndef categoryTitleForNumber(self, num):\\n    search_title = ''\\n    for title, titleNum in self.titleDic.items():\\n        if int(titleNum) == int(num):\\n            search_title = title\\n    return search_title\\n</code>\", '<code>enum</code>', '<code>enum34</code>', '<code>from enum import Enum\\n\\nclass Color(Enum): \\n    red = 1 \\n    green = 2 \\n    blue = 3\\n\\n&gt;&gt;&gt; print(Color.red) \\nColor.red\\n\\n&gt;&gt;&gt; print(repr(Color.red)) \\n&lt;color.red: 1=\"\"&gt; \\n\\n&gt;&gt;&gt; type(Color.red) \\n&lt;enum \\'color\\'=\"\"&gt; \\n&gt;&gt;&gt; isinstance(Color.green, Color) \\nTrue \\n\\n&gt;&gt;&gt; member = Color.red \\n&gt;&gt;&gt; member.name \\n\\'red\\' \\n&gt;&gt;&gt; member.value \\n1 \\n</code>', '<code>reversedict = dict([(value, key) for key, value in mydict.iteritems()])\\n</code>', '<code>reversedict = {value:key for key, value in mydict.iteritems()}\\n</code>', '<code>reversedict = defaultdict(list)\\n[reversedict[value].append(key) for key, value in mydict.iteritems()]\\n</code>', \"<code>largedict = dict((x,x) for x in range(100000))\\n\\n# Should be slow, has to search 90000 entries before it finds it\\nIn [26]: %timeit largedict.keys()[largedict.values().index(90000)]\\n100 loops, best of 3: 4.81 ms per loop\\n\\n# Should be fast, has to only search 9 entries to find it. \\nIn [27]: %timeit largedict.keys()[largedict.values().index(9)]\\n100 loops, best of 3: 2.94 ms per loop\\n\\n# How about using iterkeys() instead of keys()?\\n# These are faster, because you don't have to create the entire keys array.\\n# You DO have to create the entire values array - more on that later.\\n\\nIn [31]: %timeit islice(largedict.iterkeys(), largedict.values().index(90000))\\n100 loops, best of 3: 3.38 ms per loop\\n\\nIn [32]: %timeit islice(largedict.iterkeys(), largedict.values().index(9))\\n1000 loops, best of 3: 1.48 ms per loop\\n\\nIn [24]: %timeit reversedict = dict([(value, key) for key, value in largedict.iteritems()])\\n10 loops, best of 3: 22.9 ms per loop\\n\\nIn [23]: %%timeit\\n....: reversedict = defaultdict(list)\\n....: [reversedict[value].append(key) for key, value in largedict.iteritems()]\\n....:\\n10 loops, best of 3: 53.6 ms per loop\\n</code>\", '<code>In [72]: %%timeit\\n....: myf = ifilter(lambda x: x[1] == 90000, largedict.iteritems())\\n....: myf.next()[0]\\n....:\\n100 loops, best of 3: 15.1 ms per loop\\n\\nIn [73]: %%timeit\\n....: myf = ifilter(lambda x: x[1] == 9, largedict.iteritems())\\n....: myf.next()[0]\\n....:\\n100000 loops, best of 3: 2.36 us per loop\\n</code>', '<code>[</code>', '<code>;</code>', '<code>list = {\\'george\\': 16, \\'amber\\': 19}\\nsearch_age = raw_input(\"Provide age\")\\nfor age in list:\\n    if list[age] == search_age:\\n        print age\\n</code>', '<code>list = {\\'george\\': 16, \\'amber\\': 19}\\nsearch_age = raw_input(\"Provide age\")\\nfor name in list:\\n    if list[name] == search_age:\\n        print name\\n</code>', '<code>people = {\\'george\\': {\\'age\\': 16}, \\'amber\\': {\\'age\\': 19}}\\nsearch_age = raw_input(\"Provide age\")\\nfor name in people:\\n    if people[name][\\'age\\'] == search_age:\\n        print name\\n</code>', '<code>for key in list:\\n   if list[key] == search_value:\\n       return key\\n</code>', \"<code>d= {'george':16,'amber':19}\\n\\ndict((v,k) for k,v in d.items()).get(16)\\n</code>\", '<code>-&gt; prints george\\n</code>', '<code>dict((v, k) for k, v in list.items())[search_age]\\n</code>', '<code>[search_age]</code>', '<code>def find_name(age, _rev_lookup=dict((v, k) for k, v in ages_by_name.items())):\\n    return _rev_lookup[age]\\n</code>', '<code>def create_name_finder(ages_by_name):\\n    names_by_age = dict((v, k) for k, v in ages_by_name.items())\\n    def find_name(age):\\n      return names_by_age[age]\\n</code>', \"<code>find_teen_by_age = create_name_finder({'george':16,'amber':19})\\n...\\nfind_teen_by_age(search_age)\\n</code>\", '<code>list</code>', '<code>ages_by_name</code>', \"<code>ages = {'george':16,'amber':19}\\nsearch = 16\\nprint([name for (name, age) in ages.items() if age == search])\\n</code>\", '<code>list = {\\'george\\':16,\\'amber\\':19}\\nsearch_age = raw_input(\"Provide age\")\\nfor age in list.values():\\n    name = list[list==search_age].key().tolist()\\n    print name\\n</code>', '<code>get_key = lambda v, d: next(k for k in d if d[k] is v)\\n</code>']",
         "title": "Get key by value in dictionary",
         "_childDocuments_": [
            {
               "up_vote_count": 174,
               "answer_id": 8023329,
               "last_activity_date": 1320527922,
               "path": "3.stack.answer",
               "body_markdown": "If you want both the name _and_ the age, you should be using `.items()` which gives you key `(key, value)` tuples:\r\n\r\n    for name, age in mydict.items():\r\n        if age == search_age:\r\n            print name\r\n\r\nYou can unpack the tuple into two separate variables right in the `for` loop, then match the age.\r\n\r\nYou should also consider reversing the dictionary if you&#39;re generally going to be looking up by age, and no two people have the same age:\r\n\r\n    {16: &#39;george&#39;, 19: &#39;amber&#39;}\r\n\r\nso you can look up the name for an age by just doing\r\n\r\n    mydict[search_age]\r\n\r\nI&#39;ve been calling it `mydict` instead of `list` because `list` is the name of a built-in type, and you shouldn&#39;t use that name for anything else.\r\n\r\nYou can even get a list of all people with a given age in one line:\r\n\r\n    [name for name, age in mydict.items() if age == search_age]\r\n\r\nor if there is only one person with each age:\r\n\r\n    next((name for name, age in mydict.items() if age == search_age), None)\r\n\r\nwhich will just give you `None` if there isn&#39;t anyone with that age.\r\n\r\nFinally, if the `dict` is long and you&#39;re on Python 2, you should consider using `.iteritems()` instead of `.items()` as Cat Plus Plus did in his answer, since it doesn&#39;t need to make a copy of the list.",
               "tags": [],
               "creation_date": 1320527510,
               "last_edit_date": 1320527922,
               "is_accepted": false,
               "id": "8023329",
               "down_vote_count": 1,
               "score": 173
            },
            {
               "up_vote_count": 313,
               "answer_id": 8023337,
               "last_activity_date": 1516014423,
               "path": "3.stack.answer",
               "body_markdown": "There is none. `dict` is not intended to be used this way.\r\n\r\n    for name, age in list.iteritems():    # for name, age in list.items():  (for Python 3.x)\r\n        if age == search_age:\r\n            print name",
               "tags": [],
               "creation_date": 1320527589,
               "last_edit_date": 1516014423,
               "is_accepted": true,
               "id": "8023337",
               "down_vote_count": 14,
               "score": 299
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 11423953,
               "is_accepted": false,
               "last_activity_date": 1341966940,
               "body_markdown": "    lKey = [key for key, value in lDictionary.iteritems() if value == lValue][0]",
               "id": "11423953",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1341966940,
               "score": 16
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 12533610,
               "is_accepted": false,
               "last_activity_date": 1348241501,
               "body_markdown": "    for name in mydict.keys():\r\n        if mydict[name] == search_age:\r\n            print name \r\n            #or do something else with it. \r\n            #if in a function append to a temporary list, \r\n            #then after the loop return the list",
               "id": "12533610",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1348241501,
               "score": 4
            },
            {
               "up_vote_count": 327,
               "answer_id": 13149770,
               "last_activity_date": 1427142088,
               "path": "3.stack.answer",
               "body_markdown": "    mydict = {&#39;george&#39;:16,&#39;amber&#39;:19}\r\n    print mydict.keys()[mydict.values().index(16)] # Prints george\r\n\r\nOr in Python 3.x:\r\n\r\n    mydict = {&#39;george&#39;:16,&#39;amber&#39;:19}\r\n    print(list(mydict.keys())[list(mydict.values()).index(16)]) # Prints george\r\n\r\nBasically, it separates the dictionary&#39;s values in a list, finds the position of the value you have, and gets the key at that position.\r\n\r\nMore about `keys()` and `.values()` in Python 3: https://stackoverflow.com/questions/16228248/python-simplest-way-to-get-list-of-values-from-dict",
               "tags": [],
               "creation_date": 1351644994,
               "last_edit_date": 1495540976,
               "is_accepted": false,
               "id": "13149770",
               "down_vote_count": 0,
               "score": 327
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 50,
               "answer_id": 14624923,
               "is_accepted": false,
               "last_activity_date": 1359632579,
               "body_markdown": "&lt;!-- language: python --&gt;\r\nI thought it would be interesting to point out which methods are the quickest, and in what scenario:\r\n\r\nHere&#39;s some tests I ran (on a 2012 MacBook Pro)\r\n\r\n\r\n\t&gt;&gt;&gt; def method1(list,search_age):\r\n\t...     for name,age in list.iteritems():\r\n\t...             if age == search_age:\r\n\t...                     return name\r\n\t... \r\n\t&gt;&gt;&gt; def method2(list,search_age):\r\n\t...     return [name for name,age in list.iteritems() if age == search_age]\r\n\t... \r\n\t&gt;&gt;&gt; def method3(list,search_age):\r\n\t...     return list.keys()[list.values().index(search_age)]\r\n\r\nResults from `profile.run()` on each method 100000 times:\r\n\r\nMethod 1:\r\n\r\n\t&gt;&gt;&gt; profile.run(&quot;for i in range(0,100000): method1(list,16)&quot;)\r\n         200004 function calls in 1.173 seconds\r\n\r\nMethod 2:\r\n\r\n\t&gt;&gt;&gt; profile.run(&quot;for i in range(0,100000): method2(list,16)&quot;)\r\n         200004 function calls in 1.222 seconds\r\n\r\nMethod 3:\r\n\r\n\t&gt;&gt;&gt; profile.run(&quot;for i in range(0,100000): method3(list,16)&quot;)\r\n         400004 function calls in 2.125 seconds\r\n\r\nSo this shows that for a small dict, method 1 is the quickest. This is most likely because it returns the first match, as opposed to all of the matches like method 2 (see note below).\r\n\r\n-----\r\n\r\nInterestingly, performing the same tests on a dict I have with 2700 entries, I get quite different results (this time run 10000 times):\r\n\r\nMethod 1:\r\n\r\n\t&gt;&gt;&gt; profile.run(&quot;for i in range(0,10000): method1(UIC_CRS,&#39;7088380&#39;)&quot;)\r\n         20004 function calls in 2.928 seconds\r\n\r\nMethod 2:\r\n\r\n\t&gt;&gt;&gt; profile.run(&quot;for i in range(0,10000): method2(UIC_CRS,&#39;7088380&#39;)&quot;)\r\n         20004 function calls in 3.872 seconds\r\n\r\nMethod 3:\r\n\r\n\t&gt;&gt;&gt; profile.run(&quot;for i in range(0,10000): method3(UIC_CRS,&#39;7088380&#39;)&quot;)\r\n         40004 function calls in 1.176 seconds\r\n\r\n\r\nSo here, method 3 is *much* faster. Just goes to show the size of your dict will affect which method you choose.\r\n\r\nNotes:\r\nMethod 2 returns a list of *all* names, whereas methods 1 and 3 return only the first match.\r\nI have not considered memory usage. I&#39;m not sure if method 3 creates 2 extra lists (keys() and values()) and stores them in memory.",
               "id": "14624923",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1359632579,
               "score": 50
            },
            {
               "up_vote_count": 3,
               "answer_id": 16847307,
               "last_activity_date": 1369992111,
               "path": "3.stack.answer",
               "body_markdown": "it&#39;s answered, but it could be done with a fancy &#39;map/reduce&#39; use, e.g.:\r\n\r\n    def find_key(value, dictionary):\r\n        return reduce(lambda x, y: x if x is not None else y,\r\n                      map(lambda x: x[0] if x[1] == value else None, \r\n                          dictionary.iteritems()))\r\n\r\n",
               "tags": [],
               "creation_date": 1369955305,
               "last_edit_date": 1369992111,
               "is_accepted": false,
               "id": "16847307",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 10,
               "answer_id": 16952703,
               "last_activity_date": 1370485596,
               "path": "3.stack.answer",
               "body_markdown": "Here is my take on this problem. :)\r\nI have just started learning Python, so I call this:\r\n\r\n&quot;The Understandable for beginners&quot; solution.\r\n\r\n    #Code without comments.\r\n    \r\n    list1 = {&#39;george&#39;:16,&#39;amber&#39;:19, &#39;Garry&#39;:19}\r\n    search_age = raw_input(&quot;Provide age: &quot;)\r\n    print\r\n    search_age = int(search_age)\r\n    \r\n    listByAge = {}\r\n    \r\n    for name, age in list1.items():\r\n        if age == search_age:\r\n            age = str(age)\r\n            results = name + &quot; &quot; +age\r\n            print results\r\n    \r\n            age2 = int(age)\r\n            listByAge[name] = listByAge.get(name,0)+age2\r\n    \r\n    print\r\n    print listByAge\r\n.\r\n    \r\n    #Code with comments.\r\n    #I&#39;ve added another name with the same age to the list.\r\n    list1 = {&#39;george&#39;:16,&#39;amber&#39;:19, &#39;Garry&#39;:19}\r\n    #Original code.\r\n    search_age = raw_input(&quot;Provide age: &quot;)\r\n    print\r\n    #Because raw_input gives a string, we need to convert it to int,\r\n    #so we can search the dictionary list with it.\r\n    search_age = int(search_age)\r\n    \r\n    #Here we define another empty dictionary, to store the results in a more \r\n    #permanent way.\r\n    listByAge = {}\r\n    \r\n    #We use double variable iteration, so we get both the name and age \r\n    #on each run of the loop.\r\n    for name, age in list1.items():\r\n        #Here we check if the User Defined age = the age parameter \r\n        #for this run of the loop.\r\n        if age == search_age:\r\n            #Here we convert Age back to string, because we will concatenate it \r\n            #with the person&#39;s name. \r\n            age = str(age)\r\n            #Here we concatenate.\r\n            results = name + &quot; &quot; +age\r\n            #If you want just the names and ages displayed you can delete\r\n            #the code after &quot;print results&quot;. If you want them stored, don&#39;t...\r\n            print results\r\n            \r\n            #Here we create a second variable that uses the value of\r\n            #the age for the current person in the list.\r\n            #For example if &quot;Anna&quot; is &quot;10&quot;, age2 = 10,\r\n            #integer value which we can use in addition.\r\n            age2 = int(age)\r\n            #Here we use the method that checks or creates values in dictionaries.\r\n            #We create a new entry for each name that matches the User Defined Age\r\n            #with default value of 0, and then we add the value from age2.\r\n            listByAge[name] = listByAge.get(name,0)+age2\r\n\r\n    #Here we print the new dictionary with the users with User Defined Age.\r\n    print\r\n    print listByAge\r\n\r\n.\r\n\r\n    #Results\r\n    Running: *\\test.py (Thu Jun 06 05:10:02 2013)\r\n\r\n    Provide age: 19\r\n\r\n    amber 19\r\n    Garry 19\r\n\r\n    {&#39;amber&#39;: 19, &#39;Garry&#39;: 19}\r\n\r\n    Execution Successful!\r\n    ",
               "tags": [],
               "creation_date": 1370485055,
               "last_edit_date": 1370485596,
               "is_accepted": false,
               "id": "16952703",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 26,
               "answer_id": 19165996,
               "last_activity_date": 1430140016,
               "path": "3.stack.answer",
               "body_markdown": "one line version: (i is an old dictionary, p is a reversed dictionary)\r\n\r\n\r\n    p = dict(zip(i.values(),i.keys()))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    ",
               "tags": [],
               "creation_date": 1380823264,
               "last_edit_date": 1430140016,
               "is_accepted": false,
               "id": "19165996",
               "down_vote_count": 1,
               "score": 25
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 19166962,
               "is_accepted": false,
               "last_activity_date": 1380826349,
               "body_markdown": "already been answered, but since several people mentioned reversing the dictionary, here&#39;s how you do it in one line (assuming 1:1 mapping) and some various perf data:\r\n\r\npython 2.6:\r\n\r\n    reversedict = dict([(value, key) for key, value in mydict.iteritems()])\r\n\r\n2.7+:\r\n\r\n    reversedict = {value:key for key, value in mydict.iteritems()}\r\n\r\nif you think it&#39;s not 1:1, you can still create a reasonable reverse mapping with a couple lines:\r\n\r\n    reversedict = defaultdict(list)\r\n    [reversedict[value].append(key) for key, value in mydict.iteritems()]\r\n\r\nhow slow is this: slower than a simple search, but not nearly as slow as you&#39;d think - on a &#39;straight&#39; 100000 entry dictionary, a &#39;fast&#39; search (i.e. looking for a value that should be early in the keys) was about 10x faster than reversing the entire dictionary, and a &#39;slow&#39; search (towards the end) about 4-5x faster. So after at most about 10 lookups, it&#39;s paid for itself. \r\n\r\nthe second version (with lists per item) takes about 2.5x as long as the simple version.\r\n\r\n    largedict = dict((x,x) for x in range(100000))\r\n\r\n    # Should be slow, has to search 90000 entries before it finds it\r\n    In [26]: %timeit largedict.keys()[largedict.values().index(90000)]\r\n    100 loops, best of 3: 4.81 ms per loop\r\n\r\n    # Should be fast, has to only search 9 entries to find it. \r\n    In [27]: %timeit largedict.keys()[largedict.values().index(9)]\r\n    100 loops, best of 3: 2.94 ms per loop\r\n\r\n    # How about using iterkeys() instead of keys()?\r\n    # These are faster, because you don&#39;t have to create the entire keys array.\r\n    # You DO have to create the entire values array - more on that later.\r\n\r\n    In [31]: %timeit islice(largedict.iterkeys(), largedict.values().index(90000))\r\n    100 loops, best of 3: 3.38 ms per loop\r\n\r\n    In [32]: %timeit islice(largedict.iterkeys(), largedict.values().index(9))\r\n    1000 loops, best of 3: 1.48 ms per loop\r\n\r\n    In [24]: %timeit reversedict = dict([(value, key) for key, value in largedict.iteritems()])\r\n    10 loops, best of 3: 22.9 ms per loop\r\n\r\n    In [23]: %%timeit\r\n    ....: reversedict = defaultdict(list)\r\n    ....: [reversedict[value].append(key) for key, value in largedict.iteritems()]\r\n    ....:\r\n    10 loops, best of 3: 53.6 ms per loop\r\n\r\n\r\nAlso had some interesting results with ifilter. Theoretically, ifilter should be faster, in that we can use itervalues() and possibly not have to create/go through the entire values list. In practice, the results were... odd...\r\n\r\n    In [72]: %%timeit\r\n    ....: myf = ifilter(lambda x: x[1] == 90000, largedict.iteritems())\r\n    ....: myf.next()[0]\r\n    ....:\r\n    100 loops, best of 3: 15.1 ms per loop\r\n\r\n    In [73]: %%timeit\r\n    ....: myf = ifilter(lambda x: x[1] == 9, largedict.iteritems())\r\n    ....: myf.next()[0]\r\n    ....:\r\n    100000 loops, best of 3: 2.36 us per loop\r\n\r\nSo, for small offsets, it was dramatically faster than any previous version (2.36 *u*S vs. a minimum of 1.48 *m*S for previous cases). However, for large offsets near the end of the list, it was dramatically slower (15.1ms vs. the same 1.48mS). The small savings at the low end is not worth the cost at the high end, imho. ",
               "id": "19166962",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1380826349,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 22699770,
               "is_accepted": false,
               "last_activity_date": 1395956301,
               "body_markdown": "Cat Plus Plus mentioned that this isn&#39;t how a dictionary is intended to be used. Here&#39;s why:\r\n\r\nThe definition of a dictionary is analogous to that of a mapping in mathematics. In this case, a dict is a mapping of K (the set of keys) to V (the values) - but not vice versa. If you dereference a dict, you expect to get exactly one value returned. But, it is perfectly legal for different keys to map onto the same value, e.g.:\r\n\r\n    d = { k1 : v1, k2 : v2, k3 : v1}\r\n\r\nWhen you look up a key by it&#39;s corresponding value, you&#39;re essentially inverting the dictionary. But a mapping isn&#39;t necessarily invertible! In this example, asking for the key corresponding to v1 could yield k1 or k3. Should you return both? Just the first one found? That&#39;s why indexof() is undefined for dictionaries.\r\n\r\nIf you know your data, you could do this. But an API can&#39;t assume that an arbitrary dictionary is invertible, hence the lack of such an operation.",
               "id": "22699770",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1395956301,
               "score": 2
            },
            {
               "up_vote_count": 2,
               "answer_id": 23719934,
               "last_activity_date": 1400402069,
               "path": "3.stack.answer",
               "body_markdown": "here is my take on it. This is good for displaying multiple results just in case you need one. So I added the list as well \r\n\r\n    myList = {&#39;george&#39;:16,&#39;amber&#39;:19, &#39;rachel&#39;:19, \r\n               &#39;david&#39;:15 }                         #Setting the dictionary\r\n    result=[]                                       #Making ready of the result list\r\n    search_age = int(input(&#39;Enter age &#39;))\r\n\r\n    for keywords in myList.keys():\r\n        if myList[keywords] ==search_age:\r\n        result.append(keywords)                    #This part, we are making list of results\r\n\r\n    for res in result:                             #We are now printing the results\r\n        print(res)\r\n\r\nAnd that&#39;s it... \r\n",
               "tags": [],
               "creation_date": 1400401262,
               "last_edit_date": 1400402069,
               "is_accepted": false,
               "id": "23719934",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 0,
               "answer_id": 24012167,
               "last_activity_date": 1408904427,
               "path": "3.stack.answer",
               "body_markdown": "    d= {&#39;george&#39;:16,&#39;amber&#39;:19}\r\n\r\n    dict((v,k) for k,v in d.items()).get(16)\r\n\r\nThe output is as follows:\r\n\r\n    -&gt; prints george\r\n\r\n",
               "tags": [],
               "creation_date": 1401789355,
               "last_edit_date": 1408904427,
               "is_accepted": false,
               "id": "24012167",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 25474781,
               "is_accepted": false,
               "last_activity_date": 1408903992,
               "body_markdown": "Sometimes int() may be needed:\r\n\r\n    titleDic = {&#39;\u0424\u0438\u043b\u044c\u043c\u044b&#39;:1, &#39;\u041c\u0443\u0437\u044b\u043a\u0430&#39;:2}\r\n    \r\n    def categoryTitleForNumber(self, num):\r\n        search_title = &#39;&#39;\r\n        for title, titleNum in self.titleDic.items():\r\n            if int(titleNum) == int(num):\r\n                search_title = title\r\n        return search_title",
               "id": "25474781",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1408903992,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 26430710,
               "is_accepted": false,
               "last_activity_date": 1413568405,
               "body_markdown": "Here is a solution which works both in Python 2 and Python 3:\r\n\r\n    dict((v, k) for k, v in list.items())[search_age]\r\n\r\nThe part until `[search_age]` constructs the reverse dictionary (where values are keys and vice-versa).\r\nYou could create a helper method which will cache this reversed dictionary like so:\r\n\r\n    def find_name(age, _rev_lookup=dict((v, k) for k, v in ages_by_name.items())):\r\n        return _rev_lookup[age]\r\n\r\nor even more generally a factory which would create a by-age name lookup method for one or more of you lists\r\n\r\n    def create_name_finder(ages_by_name):\r\n        names_by_age = dict((v, k) for k, v in ages_by_name.items())\r\n        def find_name(age):\r\n          return names_by_age[age]\r\n\r\nso you would be able to do:\r\n    \r\n    find_teen_by_age = create_name_finder({&#39;george&#39;:16,&#39;amber&#39;:19})\r\n    ...\r\n    find_teen_by_age(search_age)\r\n\r\nNote that I renamed `list` to `ages_by_name` since the former is a predefined type.",
               "id": "26430710",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1413568405,
               "score": 0
            },
            {
               "up_vote_count": 3,
               "answer_id": 26874228,
               "last_activity_date": 1428950673,
               "path": "3.stack.answer",
               "body_markdown": "There is no easy way to find a key in a list by &#39;looking up&#39; the value. However, if you know the value, iterating through the keys, you can look up values in the dictionary by the element. If D[element] where D is a dictionary object, is equal to the key you&#39;re trying to look up, you can execute some code.\r\n\r\n\r\n    D = {&#39;Ali&#39;: 20, &#39;Marina&#39;: 12, &#39;George&#39;:16}\r\n    age = int(input(&#39;enter age:\\t&#39;))  \r\n    for element in D.keys():\r\n        if D[element] == age:\r\n            print(element)\r\n        else:\r\n            continue",
               "tags": [],
               "creation_date": 1415738742,
               "last_edit_date": 1428950673,
               "is_accepted": false,
               "id": "26874228",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 27553439,
               "is_accepted": false,
               "last_activity_date": 1418927850,
               "body_markdown": "If you want to find the key by the value, you can use a dictionary comprehension to create a lookup dictionary and then use that to find the key from the value.\r\n\r\n    lookup = {value: key for key, value in self.data}\r\n    lookup[value]",
               "id": "27553439",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1418927850,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 31173189,
               "is_accepted": false,
               "last_activity_date": 1435792592,
               "body_markdown": "This is how you access the dictionary to do what you want:\r\n\r\n    list = {&#39;george&#39;: 16, &#39;amber&#39;: 19}\r\n    search_age = raw_input(&quot;Provide age&quot;)\r\n    for age in list:\r\n        if list[age] == search_age:\r\n            print age\r\n\r\nof course, your names are so off it looks like it would be printing an age, but it DOES print the name. Since you are accessing by name, it becomes more understandable if you write:\r\n\r\n    list = {&#39;george&#39;: 16, &#39;amber&#39;: 19}\r\n    search_age = raw_input(&quot;Provide age&quot;)\r\n    for name in list:\r\n        if list[name] == search_age:\r\n            print name\r\nBetter yet: \r\n\r\n    people = {&#39;george&#39;: {&#39;age&#39;: 16}, &#39;amber&#39;: {&#39;age&#39;: 19}}\r\n    search_age = raw_input(&quot;Provide age&quot;)\r\n    for name in people:\r\n        if people[name][&#39;age&#39;] == search_age:\r\n            print name\r\n",
               "id": "31173189",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1435792592,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 32426540,
               "is_accepted": false,
               "last_activity_date": 1441562060,
               "body_markdown": "You need to use a dictionary and reverse of that dictionary. It means you need another data structure. If you are in python 3, use `enum` module but if you are using python 2.7 use `enum34` which is back ported for python 2.\r\n\r\nExample:\r\n\r\n    from enum import Enum\r\n    \r\n    class Color(Enum): \r\n        red = 1 \r\n        green = 2 \r\n        blue = 3\r\n    \r\n    &gt;&gt;&gt; print(Color.red) \r\n    Color.red\r\n    \r\n    &gt;&gt;&gt; print(repr(Color.red)) \r\n    &lt;color.red: 1=&quot;&quot;&gt; \r\n    \r\n    &gt;&gt;&gt; type(Color.red) \r\n    &lt;enum &#39;color&#39;=&quot;&quot;&gt; \r\n    &gt;&gt;&gt; isinstance(Color.green, Color) \r\n    True \r\n    \r\n    &gt;&gt;&gt; member = Color.red \r\n    &gt;&gt;&gt; member.name \r\n    &#39;red&#39; \r\n    &gt;&gt;&gt; member.value \r\n    1 ",
               "id": "32426540",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1441562060,
               "score": 2
            },
            {
               "up_vote_count": 5,
               "answer_id": 32676012,
               "last_activity_date": 1511640128,
               "path": "3.stack.answer",
               "body_markdown": "Here, recover_key takes dictionary and value to find in dictionary. We then loop over the keys in dictionary and make a comparison with that of value and return that particular key.\r\n\r\n    def recover_key(dicty,value):\r\n        for a_key in dicty.keys():\r\n            if (dicty[a_key] == value):\r\n                return a_key",
               "tags": [],
               "creation_date": 1442725652,
               "last_edit_date": 1511640128,
               "is_accepted": false,
               "id": "32676012",
               "down_vote_count": 1,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 35837658,
               "is_accepted": false,
               "last_activity_date": 1457332870,
               "body_markdown": "    def get_Value(dic,value):\r\n    \tfor name in dic:\r\n\t    \tif dic[name] == value:\r\n\t    \t\tdel dic[name]\r\n\t\t    \treturn name",
               "id": "35837658",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1457332870,
               "score": 4
            },
            {
               "up_vote_count": 8,
               "answer_id": 35837797,
               "last_activity_date": 1479900204,
               "path": "3.stack.answer",
               "body_markdown": "You can get key by using [`dict.keys()`](https://docs.python.org/2/library/stdtypes.html#dict.keys), [`dict.values()`](https://docs.python.org/2/library/stdtypes.html#dict.values) and [`list.index()`](https://docs.python.org/2/tutorial/datastructures.html#more-on-lists) methods, see code samples below:\r\n\r\n    names_dict = {&#39;george&#39;:16,&#39;amber&#39;:19}\r\n    search_age = int(raw_input(&quot;Provide age&quot;))\r\n    key = names_dict.keys()[names_dict.values().index(search_age)]",
               "tags": [],
               "creation_date": 1457333536,
               "last_edit_date": 1479900204,
               "is_accepted": false,
               "id": "35837797",
               "down_vote_count": 1,
               "score": 7
            },
            {
               "up_vote_count": 13,
               "answer_id": 38587809,
               "last_activity_date": 1492779617,
               "path": "3.stack.answer",
               "body_markdown": "&lt;!-- language: lang-py --&gt;\r\n\r\n    a = {&#39;a&#39;:1,&#39;b&#39;:2,&#39;c&#39;:3}\r\n    {v:k for k, v in a.items()}[1]\r\n\r\nor better\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    {k:v for k, v in a.items() if v == 1}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1469530110,
               "last_edit_date": 1492779617,
               "is_accepted": false,
               "id": "38587809",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 44871076,
               "is_accepted": false,
               "last_activity_date": 1499000361,
               "body_markdown": "I know this is old but you could quite easily find all the people in the list with your search age using list comprehension.\r\n\r\n    ages = {&#39;george&#39;:16,&#39;amber&#39;:19}\r\n    search = 16\r\n    print([name for (name, age) in ages.items() if age == search])",
               "id": "44871076",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1499000361,
               "score": 0
            },
            {
               "up_vote_count": 1,
               "answer_id": 44972365,
               "last_activity_date": 1499436033,
               "path": "3.stack.answer",
               "body_markdown": "I hope this might help...\r\n\r\n    for key in list:\r\n       if list[key] == search_value:\r\n           return key",
               "tags": [],
               "creation_date": 1499435025,
               "last_edit_date": 1499436033,
               "is_accepted": false,
               "id": "44972365",
               "down_vote_count": 0,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 45362184,
               "is_accepted": false,
               "last_activity_date": 1501195423,
               "body_markdown": "A simple way to do this could be:\r\n\r\n    list = {&#39;george&#39;:16,&#39;amber&#39;:19}\r\n    search_age = raw_input(&quot;Provide age&quot;)\r\n    for age in list.values():\r\n        name = list[list==search_age].key().tolist()\r\n        print name\r\nThis will return a list of the keys with value that match search_age. You can also replace &quot;list==search_age&quot; with any other conditions statement if needed.",
               "id": "45362184",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1501195423,
               "score": 0
            },
            {
               "up_vote_count": 4,
               "answer_id": 45761755,
               "last_activity_date": 1503076789,
               "path": "3.stack.answer",
               "body_markdown": "Consider using Pandas. As stated in William McKinney&#39;s &quot;Python for Data Analysis&#39;\r\n\r\n&gt; Another way to think about a Series is as a fixed-length, ordered\r\n&gt; dict, as it is a mapping of index values to data values. It can be\r\n&gt; used in many contexts where you might use a dict.\r\n    \r\n    import pandas as pd\r\n    list = {&#39;george&#39;:16,&#39;amber&#39;:19}\r\n    lookup_list = pd.Series(list)\r\n\r\nTo query your series do the following:\r\n\r\n    lookup_list[lookup_list.values == 19]\r\n\r\nWhich yields:\r\n\r\n    Out[1]: \r\n    amber    19\r\n    dtype: int64\r\n\r\nIf you need to do anything else with the output transforming the \r\nanswer into a list might be useful:\r\n\r\n    answer = lookup_list[lookup_list.values == 19].index\r\n    answer = pd.Index.tolist(answer)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1503076306,
               "last_edit_date": 1503076789,
               "is_accepted": false,
               "id": "45761755",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 47493704,
               "is_accepted": false,
               "last_activity_date": 1511678696,
               "body_markdown": "    get_key = lambda v, d: next(k for k in d if d[k] is v)",
               "id": "47493704",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1511678696,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/8023306/get-key-by-value-in-dictionary",
         "id": "858127-2315"
      },
      {
         "up_vote_count": "217",
         "path": "2.stack",
         "body_markdown": "I am in the process of learning Python and I have reached the section about the `pass` statement. The guide I&#39;m using defines it as being a `Null` statement that is commonly used as a placeholder. \r\n\r\nI still don&#39;t fully understand what that means though. Can someone show me a simple/basic situation where the `pass` statement would be used and why it is needed?",
         "view_count": "138211",
         "answer_count": "12",
         "tags": "['python']",
         "creation_date": "1355518770",
         "last_edit_date": "1462786404",
         "code_snippet": "['<code>pass</code>', '<code>Null</code>', '<code>pass</code>', '<code>pass</code>', '<code>pass</code>', '<code>except</code>', '<code>class MyClass(object):\\n    def meth_a(self):\\n        pass\\n\\n    def meth_b(self):\\n        print \"I\\'m meth_b\"\\n</code>', '<code>pass</code>', '<code>IndentationError: expected an indented block\\n</code>', '<code>pass</code>', '<code>return</code>', '<code>if</code>', '<code>except</code>', '<code>def</code>', '<code>class</code>', '<code>pass</code>', '<code>IndentationError</code>', '<code>Ellipsis</code>', '<code>...</code>', '<code>pass</code>', '<code>Exception</code>', '<code>xml</code>', '<code>try:\\n    self.version = \"Expat %d.%d.%d\" % expat.version_info\\nexcept AttributeError:\\n    pass # unknown\\n</code>', '<code>pandas</code>', '<code>KeyboardInterrupt</code>', '<code>SystemExit</code>', '<code>HardwareIsOnFireError</code>', '<code>try:\\n    os.unlink(filename_larry)\\nexcept:\\n    pass\\n</code>', '<code>except Error:</code>', '<code>except OSError:</code>', '<code>except ...: pass</code>', '<code>scipy</code>', '<code>class CompileError(Exception):\\n    pass\\n</code>', '<code>__init__</code>', '<code>pebl</code>', '<code>class _BaseSubmittingController(_BaseController):\\n    def submit(self, tasks): pass\\n    def retrieve(self, deferred_results): pass\\n</code>', '<code>mpmath</code>', '<code>for x, error in MDNewton(mp, f, (1,-2), verbose=0,\\n                         norm=lambda x: norm(x, inf)):\\n    pass\\n</code>', '<code>pass</code>', '<code>pebl</code>', '<code>class ParsingError(Exception): \\n    \"\"\"Error encountered while parsing an ill-formed datafile.\"\"\"\\n    pass\\n</code>', '<code>pass</code>', '<code>Ellipsis</code>', '<code>...</code>', '<code>def update_agent(agent):\\n    ... \\n</code>', '<code>def update_agent(agent):\\n    pass\\n</code>', '<code>def time_step(agents):\\n    for agent in agents:\\n        update_agent(agent)\\n</code>', '<code>update_agent</code>', '<code>raise NotImplementedError</code>', '<code>pass</code>', '<code>def foo(x,y):\\n    return x+y\\n</code>', '<code>def bar():\\n    pass\\n</code>', '<code>c = a / b\\n</code>', '<code>try:\\n    c = a / b\\nexcept ZeroDivisionError:\\n    pass\\n</code>', '<code>for t in range(25):\\n    do_a_thing(t)\\n    if t == 20:\\n        pass\\n</code>', '<code>def bar(): pass</code>', '<code>None</code>', '<code>pass</code>', '<code>def some_silly_transform(n):\\n    # Even numbers should be divided by 2\\n    if n % 2 == 0:\\n        n /= 2\\n        flag = True\\n    # Negative odd numbers should return their absolute value\\n    elif n &lt; 0:\\n        n = -n\\n        flag = True\\n    # Otherwise, number should remain unchanged\\n    else:\\n        pass\\n</code>', '<code>return</code>', '<code>pass</code>', '<code>flag</code>', '<code>else</code>', '<code>pass</code>', '<code>flag = True</code>', '<code>flag</code>', '<code>if __name__ == \"__main__\":\\n    pass\\n</code>', '<code>pass</code>', '<code>try:\\n    n[i] = 0\\nexcept IndexError:\\n    pass\\n</code>', '<code>flag = False</code>', '<code>if ... elif </code>', '<code>if __name__ == \"__main__\":     pass</code>', '<code>...</code>', '<code>pass</code>', '<code>class Error(Exception):\\n    pass\\n</code>', '<code>Error</code>', '<code>try:\\n   int(someuserinput)\\nexcept ValueError:\\n   pass\\n</code>', '<code>class TestFunctions(unittest.TestCase):\\n\\n   def test_some_feature(self):\\n      pass\\n\\n   def test_some_other_feature(self):\\n      pass\\n</code>', '<code>pass</code>', '<code>   class TestClass: \\n      pass\\n</code>', '<code>   if (something == true):  # used in conditional statement\\n       pass\\n\\n   while (some condition is true):  # user is not sure about the body of the loop\\n       pass\\n</code>', '<code>   def testFunction(args): # programmer wants to implement the body of the function later\\n       pass\\n</code>', '<code>pass</code>', '<code>pass</code>', '<code>#include&lt;stdio.h&gt;\\n\\nvoid main()\\n{\\n    int age = 12;\\n\\n    if( age &lt; 18 )\\n    {\\n         printf(\"You are not adult, so you can\\'t do that task \");\\n    }\\n    else if( age &gt;= 18 &amp;&amp; age &lt; 60)\\n    {\\n        // I will add more code later inside it \\n    }\\n    else\\n    {\\n         printf(\"You are too old to do anything , sorry \");\\n    }\\n}\\n</code>', '<code>age = 12\\n\\nif age &lt; 18:\\n\\n    print \"You are not adult, so you can\\'t do that task\"\\n\\nelif age &gt;= 18 and age &lt; 60:\\n\\nelse:\\n\\n    print \"You are too old to do anything , sorry \"\\n</code>', '<code>age = 12\\n\\nif age &lt; 18:\\n\\n    print \"You are not adult, so you can\\'t do that task\"\\n\\nelif age &gt;= 18 and age &lt; 60:\\n\\n    pass\\n\\nelse:\\n\\n    print \"You are too old to do anything , sorry \"\\n</code>', '<code>&amp;&amp;</code>', '<code># code that does something to to a variable, var\\nif var == 2000:\\n    pass\\nelse:\\n    var += 1\\n</code>', '<code>var == 2000</code>', \"<code>&gt;&gt;&gt; a = ['1', 'env', '2', 'gag', '1.234', 'nef']\\n&gt;&gt;&gt; data = []\\n&gt;&gt;&gt; type(a)\\n&lt;class 'list'&gt;\\n&gt;&gt;&gt; type(a[1])\\n&lt;class 'str'&gt;\\n&gt;&gt;&gt; type(a[0])\\n&lt;class 'str'&gt;\\n</code>\", \"<code>a = ['1', 'env', '2', 'gag', '1.234', 'nef']\\ndata = []\\nfor i in range(0, len(a)):\\n    if a[i].isalpha():\\n        pass\\n    else:\\n        data.append(a[i])\\nprint(data)\\n['1', '2', '1.234']\\n</code>\", '<code>#!/usr/bin/python\\n\\nfor letter in \\'Python\\': \\n   if letter == \\'h\\':\\n      pass\\n      print \\'This is pass block\\'\\n   print \\'Current Letter :\\', letter\\n\\nprint \"Good bye!\"\\n</code>', '<code>Current Letter : P\\nCurrent Letter : y\\nCurrent Letter : t\\nThis is pass block\\nCurrent Letter : h\\nCurrent Letter : o\\nCurrent Letter : n\\nGood bye!\\n</code>']",
         "title": "How To Use The Pass Statement In Python",
         "_childDocuments_": [
            {
               "up_vote_count": 275,
               "answer_id": 13886195,
               "last_activity_date": 1394910617,
               "path": "3.stack.answer",
               "body_markdown": "Suppose you are designing a new class with some methods that you don&#39;t want to implement, yet.\r\n\r\n    class MyClass(object):\r\n        def meth_a(self):\r\n            pass\r\n        \r\n        def meth_b(self):\r\n            print &quot;I&#39;m meth_b&quot;\r\n\r\nIf you would leave out the `pass`, the code wouldn&#39;t run.\r\n\r\nYou would then get an\r\n\r\n    IndentationError: expected an indented block\r\n\r\nTo summarize, the `pass` statement does nothing particular but can act as a placeholder, as shown before.",
               "tags": [],
               "creation_date": 1355518929,
               "last_edit_date": 1394910617,
               "is_accepted": true,
               "id": "13886195",
               "down_vote_count": 5,
               "score": 270
            },
            {
               "up_vote_count": 5,
               "answer_id": 13886204,
               "last_activity_date": 1355519354,
               "path": "3.stack.answer",
               "body_markdown": "if you don&#39;t know what you are going to put in a certain code block\r\n\r\n    try:\r\n       int(someuserinput)\r\n    except ValueError:\r\n       pass\r\n\r\nI like to use it when stubbing out tests too. I often times am aware of what i would liek to test but dont&#39; quite know how to do it.  Testing example looks like what sebastian_oe suggested\r\n\r\n    class TestFunctions(unittest.TestCase):\r\n    \r\n       def test_some_feature(self):\r\n          pass\r\n    \r\n       def test_some_other_feature(self):\r\n          pass\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1355518983,
               "last_edit_date": 1355519354,
               "is_accepted": false,
               "id": "13886204",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 13886226,
               "is_accepted": false,
               "last_activity_date": 1355519106,
               "body_markdown": "as the book said, I only ever use it as a temporary placeholder, ie,\r\n\r\n    # code that does something to to a variable, var\r\n    if var == 2000:\r\n        pass\r\n    else:\r\n        var += 1\r\n\r\nand then later fill in the scenario where `var == 2000`",
               "id": "13886226",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1355519106,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 13886269,
               "is_accepted": false,
               "last_activity_date": 1355519346,
               "body_markdown": "A common use case where it can be used &#39;as is&#39; is to override a class just to create a type (which is otherwise the same as the superclass), e.g.\r\n\r\n    class Error(Exception):\r\n        pass\r\n\r\nSo you can raise and catch `Error` exceptions. What matters here is the type of exception, rather than the content.",
               "id": "13886269",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1355519346,
               "score": 8
            },
            {
               "up_vote_count": 18,
               "answer_id": 17579272,
               "last_activity_date": 1407781038,
               "path": "3.stack.answer",
               "body_markdown": "Besides its use as a placeholder for unimplemented functions, `pass` can be useful in filling out an if-else statement (&quot;Explicit is better than implicit.&quot;)\r\n    \r\n    def some_silly_transform(n):\r\n        # Even numbers should be divided by 2\r\n        if n % 2 == 0:\r\n            n /= 2\r\n            flag = True\r\n        # Negative odd numbers should return their absolute value\r\n        elif n &lt; 0:\r\n            n = -n\r\n            flag = True\r\n        # Otherwise, number should remain unchanged\r\n        else:\r\n            pass\r\n\r\nOf course, in this case, one would probably use `return` instead of assignment, but in cases where mutation is desired, this works best.\r\n\r\nThe use of `pass` here is especially useful to warn future maintainers (including yourself!) not to put redundant steps outside of the conditional statements. In the example above, `flag` is set in the two specifically mentioned cases, but not in the `else`-case. Without using `pass`, a future programmer might move `flag = True` to outside the condition\u2014thus setting `flag` in *all* cases.\r\n\r\n----------\r\n\r\nAnother case is with the boilerplate function often seen at the bottom of a file:\r\n\r\n    if __name__ == &quot;__main__&quot;:\r\n        pass\r\n\r\nIn some files, it might be nice to leave that there with `pass` to allow for easier editing later, and to make explicit that nothing is expected to happen when the file is run on its own.\r\n\r\n----------\r\n\r\nFinally, as mentioned in other answers, it can be useful to do nothing when an exception is caught:\r\n\r\n    try:\r\n        n[i] = 0\r\n    except IndexError:\r\n        pass",
               "tags": [],
               "creation_date": 1373484310,
               "last_edit_date": 1407781038,
               "is_accepted": false,
               "id": "17579272",
               "down_vote_count": 2,
               "score": 16
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 16,
               "answer_id": 21080436,
               "is_accepted": false,
               "last_activity_date": 1389561698,
               "body_markdown": "The best and most accurate way to think of `pass` is as a way to explicitly tell the interpreter to do nothing. In the same way the following code:\r\n\r\n    def foo(x,y):\r\n        return x+y\r\n\r\nmeans &quot;if I call the function foo(x, y), sum the two numbers the labels x and y represent and hand back the result&quot;,\r\n\r\n    def bar():\r\n        pass\r\n\r\nmeans &quot;If I call the function bar(), do absolutely nothing.&quot;\r\n\r\n\r\nThe other answers are quite correct, but it&#39;s also useful for a few things that don&#39;t involve place-holding.\r\n\r\nFor example, in a bit of code I worked on just recently, it was necessary to divide two variables, and it was possible for the divisor to be zero.\r\n\r\n    c = a / b\r\n\r\nwill, obviously, produce a ZeroDivisionError if b is zero. In this particular situation, leaving c as zero was the desired behavior in the case that b was zero, so I used the following code:\r\n\r\n    try:\r\n        c = a / b\r\n    except ZeroDivisionError:\r\n        pass\r\n\r\nAnother, less standard usage is as a handy place to put a breakpoint for your debugger. For example, I wanted a bit of code to break into the debugger on the 20th iteration of a for... in statement. So:\r\n\r\n    for t in range(25):\r\n        do_a_thing(t)\r\n        if t == 20:\r\n            pass\r\n\r\nwith the breakpoint on pass.",
               "id": "21080436",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1389561698,
               "score": 16
            },
            {
               "up_vote_count": 129,
               "answer_id": 22612774,
               "last_activity_date": 1505156939,
               "path": "3.stack.answer",
               "body_markdown": "Python has the syntactical requirement that code blocks (after `if`, `except`, `def`, `class` etc.) cannot be empty. Empty code blocks are however useful in a variety of different contexts, such as in examples below, which are the most frequent use cases I have seen.\r\n\r\nTherefore, if nothing is supposed to happen in a code block, a `pass` is needed for such a block to not produce an `IndentationError`. Alternatively, any statement (including just a term to be evaluated, like the `Ellipsis` literal `...` or a string, most often a docstring) can be used, but the `pass` makes clear that indeed nothing is supposed to happen, and does not need to be actually evaluated and (at least temporarily) stored in memory.\r\n\r\n - Ignoring (all or) a certain type of `Exception` (example from `xml`):\r\n\r\n        try:\r\n            self.version = &quot;Expat %d.%d.%d&quot; % expat.version_info\r\n        except AttributeError:\r\n            pass # unknown\r\n\r\n   **Note:** Ignoring all types of raises, as in the following example from `pandas`, is generally considered bad practice, because it also catches exceptions that should probably be passed on to the caller, e.g. `KeyboardInterrupt` or `SystemExit` (or even `HardwareIsOnFireError` \u2013 How do you  know you aren&#39;t running on a custom box with specific errors defined, which some calling application would want to know about?). \r\n\r\n        try:\r\n            os.unlink(filename_larry)\r\n        except:\r\n            pass\r\n\r\n    Instead using at least `except Error:` or in this case preferably `except OSError:` is considered much better practice. A quick analysis of all python modules I have installed gave me that more than 10% of all `except ...: pass` statements catch all exceptions, so it&#39;s still a frequent pattern in python programming.\r\n\r\n - Deriving an exception class that does not add new behaviour (e.g. in `scipy`):\r\n\r\n        class CompileError(Exception):\r\n            pass\r\n   Similarly, classes intended as abstract base class often have an explicit empty `__init__` or other methods that subclasses are supposed to derive. (e.g. `pebl`)\r\n\r\n        class _BaseSubmittingController(_BaseController):\r\n            def submit(self, tasks): pass\r\n            def retrieve(self, deferred_results): pass\r\n\r\n - Testing that code runs properly for a few test values, without caring about the results (from `mpmath`):\r\n\r\n        for x, error in MDNewton(mp, f, (1,-2), verbose=0,\r\n                                 norm=lambda x: norm(x, inf)):\r\n            pass\r\n\r\n\r\n - In class or function definitions, often a docstring is already in place as the _obligatory statement_ to be executed as the only thing in the block. In such cases, the block may contain `pass` _in addition_ to the docstring in order to say \u201cThis is indeed intended to do nothing.\u201d, for example in `pebl`:\r\n\r\n        class ParsingError(Exception): \r\n            &quot;&quot;&quot;Error encountered while parsing an ill-formed datafile.&quot;&quot;&quot;\r\n            pass\r\n\r\n\r\n\r\n - In some cases, `pass` is used as a placeholder to say \u201cThis method/class/if-block/... has not been implemented yet, but this will be the place to do it\u201d, although I personally prefer the `Ellipsis` literal `...` in order to strictly differentiate between this and the intentional \u201cno-op\u201d in the previous example.\r\n   For example, if I write a model in broad strokes, I might write\r\n\r\n        def update_agent(agent):\r\n            ... \r\n\r\n    where others might have\r\n\r\n        def update_agent(agent):\r\n            pass\r\n\r\n    before\r\n \r\n        def time_step(agents):\r\n            for agent in agents:\r\n                update_agent(agent)\r\n\r\n   as a reminder to fill in the `update_agent` function at a later point, but run some tests already to see if the rest of the code behaves as intended. (A third option for this case is `raise NotImplementedError`. This is useful in particular for two cases: Either _\u201cThis abstract method should be implemented by every subclass, there is no generic way to define it in this base class\u201d_, or _\u201cThis function, with this name, is not yet implemented in this release, but this is what its signature will look like\u201d_)",
               "tags": [],
               "creation_date": 1395672703,
               "last_edit_date": 1505156939,
               "is_accepted": false,
               "id": "22612774",
               "down_vote_count": 0,
               "score": 129
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 30484534,
               "is_accepted": false,
               "last_activity_date": 1432734689,
               "body_markdown": "The pass statement does nothing. It can be used when a statement is required syntactically but the program requires no action.",
               "id": "30484534",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1432734689,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 31571330,
               "is_accepted": false,
               "last_activity_date": 1437590122,
               "body_markdown": "Here&#39;s an example where I was extracting particular data from a list where I had multiple data types (that&#39;s what I&#39;d call it in R-- sorry if it&#39;s the wrong nomenclature) and I wanted to extract only integers/numeric and NOT character data.\r\n\r\nThe data looked like:\r\n\r\n    &gt;&gt;&gt; a = [&#39;1&#39;, &#39;env&#39;, &#39;2&#39;, &#39;gag&#39;, &#39;1.234&#39;, &#39;nef&#39;]\r\n    &gt;&gt;&gt; data = []\r\n    &gt;&gt;&gt; type(a)\r\n    &lt;class &#39;list&#39;&gt;\r\n    &gt;&gt;&gt; type(a[1])\r\n    &lt;class &#39;str&#39;&gt;\r\n    &gt;&gt;&gt; type(a[0])\r\n    &lt;class &#39;str&#39;&gt;\r\n\r\nI wanted to remove all alphabetical characters, so I had the machine do it by subsetting the data, and &quot;passing&quot; over the alphabetical data:\r\n\r\n\r\n    a = [&#39;1&#39;, &#39;env&#39;, &#39;2&#39;, &#39;gag&#39;, &#39;1.234&#39;, &#39;nef&#39;]\r\n    data = []\r\n    for i in range(0, len(a)):\r\n        if a[i].isalpha():\r\n            pass\r\n        else:\r\n            data.append(a[i])\r\n    print(data)\r\n    [&#39;1&#39;, &#39;2&#39;, &#39;1.234&#39;]",
               "id": "31571330",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1437590122,
               "score": 0
            },
            {
               "up_vote_count": 4,
               "answer_id": 34194637,
               "last_activity_date": 1484459774,
               "path": "3.stack.answer",
               "body_markdown": "`pass` in Python basically does nothing, but unlike a comment it is not ignored by interpreter. So you can take advantage of it in a lot of places by making it a place holder:\r\n\r\n\r\n1: Can be used in class\r\n\r\n       class TestClass: \r\n          pass\r\n\r\n2: Can be use in loop and conditional statements:\r\n       \r\n       if (something == true):  # used in conditional statement\r\n           pass\r\n       \r\n       while (some condition is true):  # user is not sure about the body of the loop\r\n           pass\r\n\r\n3: Can be used in function :\r\n\r\n       def testFunction(args): # programmer wants to implement the body of the function later\r\n           pass\r\n\r\n`pass` is mostly used when programmer does not want to give implementation at the moment but still wants to create a certain class/function/conditional statement which can be used later on. Since the Python interpreter does not allow for blank or unimplemented class/function/conditional statement it gives an error:\r\n \r\n&gt; IndentationError: expected an indented block\r\n\r\n`pass` can be used in such scenarios.",
               "tags": [],
               "creation_date": 1449726579,
               "last_edit_date": 1484459774,
               "is_accepted": false,
               "id": "34194637",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 0,
               "answer_id": 37740636,
               "last_activity_date": 1465542297,
               "path": "3.stack.answer",
               "body_markdown": "The pass statement in Python is used when a statement is required syntactically but you do not want any command or code to execute.\r\n\r\nThe pass statement is a null operation; nothing happens when it executes. The pass is also useful in places where your code will eventually go, but has not been written yet (e.g., in stubs for example):\r\n\r\n`Example:\r\n\r\n    #!/usr/bin/python\r\n\r\n    for letter in &#39;Python&#39;: \r\n       if letter == &#39;h&#39;:\r\n          pass\r\n          print &#39;This is pass block&#39;\r\n       print &#39;Current Letter :&#39;, letter\r\n\r\n    print &quot;Good bye!&quot;\r\n\r\nThis will produce following result:\r\n\r\n    Current Letter : P\r\n    Current Letter : y\r\n    Current Letter : t\r\n    This is pass block\r\n    Current Letter : h\r\n    Current Letter : o\r\n    Current Letter : n\r\n    Good bye!\r\n\r\nThe preceding code does not execute any statement or code if the value of letter is &#39;h&#39;. The pass statement is helpful when you have created a code block but it is no longer required.\r\n\r\nYou can then remove the statements inside the block but let the block remain with a pass statement so that it doesn&#39;t interfere with other parts of the code.\r\n",
               "tags": [],
               "creation_date": 1465537074,
               "last_edit_date": 1465542297,
               "is_accepted": false,
               "id": "37740636",
               "down_vote_count": 0,
               "score": 0
            },
            {
               "up_vote_count": 3,
               "answer_id": 43841335,
               "last_activity_date": 1499843146,
               "path": "3.stack.answer",
               "body_markdown": "You can say that **pass** means **NOP** (No Operation) operation. You will get a clear picture after this example :-\r\n\r\n**C Program**\r\n\r\n    #include&lt;stdio.h&gt;\r\n    \r\n    void main()\r\n    {\r\n        int age = 12;\r\n      \r\n        if( age &lt; 18 )\r\n        {\r\n             printf(&quot;You are not adult, so you can&#39;t do that task &quot;);\r\n        }\r\n        else if( age &gt;= 18 &amp;&amp; age &lt; 60)\r\n        {\r\n            // I will add more code later inside it \r\n        }\r\n        else\r\n        {\r\n             printf(&quot;You are too old to do anything , sorry &quot;);\r\n        }\r\n    }\r\n\r\n\r\nNow how you will write that in Python :-\r\n\r\n\r\n    age = 12\r\n    \r\n    if age &lt; 18:\r\n    \r\n        print &quot;You are not adult, so you can&#39;t do that task&quot;\r\n    \r\n    elif age &gt;= 18 and age &lt; 60:\r\n    \r\n    else:\r\n\r\n        print &quot;You are too old to do anything , sorry &quot;\r\n\r\nBut your code will give error because it required an indented block after **elif** . Here is the role of **pass** keyword.\r\n\r\n    age = 12\r\n    \r\n    if age &lt; 18:\r\n    \r\n        print &quot;You are not adult, so you can&#39;t do that task&quot;\r\n    \r\n    elif age &gt;= 18 and age &lt; 60:\r\n        \r\n        pass\r\n    \r\n    else:\r\n\r\n        print &quot;You are too old to do anything , sorry &quot;\r\n\r\nNow I think its clear to you.",
               "tags": [],
               "creation_date": 1494225824,
               "last_edit_date": 1499843146,
               "is_accepted": false,
               "id": "43841335",
               "down_vote_count": 0,
               "score": 3
            }
         ],
         "link": "https://stackoverflow.com/questions/13886168/how-to-use-the-pass-statement-in-python",
         "id": "858127-2316"
      },
      {
         "up_vote_count": "428",
         "path": "2.stack",
         "body_markdown": "Silly question, but is there a built-in method for converting a `date` to a `datetime` in Python, ie. getting the `datetime` for the midnight of the `date`? The opposite conversion is easy - `datetime` has a `.date()` method. Do I really have to manually call `datetime(d.year, d.month, d.day)` ?",
         "view_count": "178043",
         "answer_count": "10",
         "tags": "['python', 'datetime', 'date']",
         "creation_date": "1261355161",
         "last_edit_date": "1351673346",
         "code_snippet": "['<code>date</code>', '<code>datetime</code>', '<code>datetime</code>', '<code>date</code>', '<code>datetime</code>', '<code>.date()</code>', '<code>datetime(d.year, d.month, d.day)</code>', '<code>.datetime()</code>', '<code>datetime.from_date()</code>', '<code>datetime.time</code>', '<code>from datetime import date\\nfrom datetime import datetime\\nd = date.today()\\ndatetime.combine(d, datetime.min.time())\\n</code>', '<code>datetime.combine(d, time())</code>', '<code>time.time</code>', '<code>datetime.time</code>', '<code>&gt;&gt;&gt; t=datetime.date.today()\\n&gt;&gt;&gt; datetime.datetime.fromordinal(t.toordinal())\\ndatetime.datetime(2009, 12, 20, 0, 0)\\n&gt;&gt;&gt; datetime.datetime(t.year, t.month, t.day)\\ndatetime.datetime(2009, 12, 20, 0, 0)\\n&gt;&gt;&gt; datetime.datetime(*t.timetuple()[:-4])\\ndatetime.datetime(2009, 12, 20, 0, 0)\\n</code>', '<code>date</code>', '<code>datetime</code>', '<code>datetime(d.year, d.month, d.day)</code>', '<code>datetime.combine(d, datetime.min.time())</code>', '<code>midnight = datetime.datetime.combine(d, datetime.time.min)</code>', '<code>@</code>', '<code>datetime(d.year, d.month, d.day)</code>', '<code>datetime.time.min</code>', '<code>datetime(d.year, d.mont, d.day)</code>', '<code>datetime.min.time()</code>', '<code>timetuple</code>', '<code>datetime</code>', '<code>from datetime import date, datetime\\ntoday = date.today()\\ntoday_with_time = datetime(\\n    year=today.year, \\n    month=today.month,\\n    day=today.day,\\n)\\n</code>', '<code>d2dt = lambda date: datetime(year=date.year, month=date.month, day=date.day)</code>', '<code>datetime.datetime(*(d.timetuple()[:6]))\\n</code>', '<code>datetime.combine</code>', '<code>datetime.time</code>', '<code>datetime.time(*map(int,\"H:M:S\".split(\":\")))</code>', '<code>import date_converter\\nmy_datetime = date_converter.date_to_datetime(my_date)\\n</code>', '<code>from datetime import date\\nimport pandas as pd\\nd = date.today()\\npd.Timestamp(d)\\n</code>', '<code>from datetime import datetime\\nisinstance(pd.Timestamp(d), datetime)\\n</code>', '<code>pd.Timestamp(d).to_datetime()\\n</code>', \"<code>from datetime import date, datetime\\nd = date.today()\\ndatetime.strptime(d.strftime('%Y%m%d'), '%Y%m%d')\\n</code>\", \"<code>dates = pd.DataFrame(\\n    {'date': pd.DatetimeIndex(start='2017-01-01', end='2017-01-5', freq='D')})\\n&gt;&gt;&gt; dates\\n        date\\n0 2017-01-01\\n1 2017-01-02\\n2 2017-01-03\\n3 2017-01-04\\n4 2017-01-05\\n\\n&gt;&gt;&gt; pd.DatetimeIndex(dates['date']).to_pydatetime().tolist()\\n[datetime.datetime(2017, 1, 1, 0, 0),\\n datetime.datetime(2017, 1, 2, 0, 0),\\n datetime.datetime(2017, 1, 3, 0, 0),\\n datetime.datetime(2017, 1, 4, 0, 0),\\n datetime.datetime(2017, 1, 5, 0, 0)]\\n</code>\", \"<code>dates = pd.DataFrame(\\n        {'date': ['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05']})\\ndates['date'] = pd.DatetimeIndex(dates['date'])\\npd.DatetimeIndex(dates['date']).to_pydatetime().tolist()\\n</code>\", '<code>datetime_object.date()</code>', '<code>date</code>', '<code>datetime</code>', '<code>import sys\\nfrom datetime import datetime\\nfrom time import mktime, strptime\\n\\nuser_date = \\'02/15/1989\\'\\nif user_date is not None:\\n     user_date = datetime.strptime(user_date,\"%m/%d/%Y\")\\nelse:\\n     user_date = datetime.now()\\nprint user_date\\n</code>']",
         "title": "Convert date to datetime in Python",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 90,
               "answer_id": 1937629,
               "is_accepted": false,
               "last_activity_date": 1261355424,
               "body_markdown": "There are several ways, although I do believe the one you mention (and dislike) is the most readable one.\r\n\r\n    &gt;&gt;&gt; t=datetime.date.today()\r\n    &gt;&gt;&gt; datetime.datetime.fromordinal(t.toordinal())\r\n    datetime.datetime(2009, 12, 20, 0, 0)\r\n    &gt;&gt;&gt; datetime.datetime(t.year, t.month, t.day)\r\n    datetime.datetime(2009, 12, 20, 0, 0)\r\n    &gt;&gt;&gt; datetime.datetime(*t.timetuple()[:-4])\r\n    datetime.datetime(2009, 12, 20, 0, 0)\r\n\r\nand so forth -- but basically they all hinge on appropriately extracting info from the `date` object and ploughing it back into the suitable ctor or classfunction for `datetime`.\r\n",
               "id": "1937629",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1261355424,
               "score": 90
            },
            {
               "up_vote_count": 38,
               "answer_id": 1937631,
               "last_activity_date": 1338795199,
               "path": "3.stack.answer",
               "body_markdown": "You can use the timetuple() method and varargs.\r\n\r\n    datetime.datetime(*(d.timetuple()[:6]))",
               "tags": [],
               "creation_date": 1261355490,
               "last_edit_date": 1338795199,
               "is_accepted": false,
               "id": "1937631",
               "down_vote_count": 2,
               "score": 36
            },
            {
               "up_vote_count": 526,
               "answer_id": 1937636,
               "last_activity_date": 1409215556,
               "path": "3.stack.answer",
               "body_markdown": "You can use [datetime.combine][1](date, time); for the time, you create a `datetime.time` object initialized to midnight.\r\n\r\n    from datetime import date\r\n    from datetime import datetime\r\n    d = date.today()\r\n    datetime.combine(d, datetime.min.time())\r\n\r\n\r\n\r\n  [1]: https://docs.python.org/2/library/datetime.html#datetime.datetime.combine",
               "tags": [],
               "creation_date": 1261355620,
               "last_edit_date": 1409215556,
               "is_accepted": true,
               "id": "1937636",
               "down_vote_count": 1,
               "score": 525
            },
            {
               "up_vote_count": 5,
               "answer_id": 4472626,
               "last_activity_date": 1338795303,
               "path": "3.stack.answer",
               "body_markdown": "If you need something quick, `datetime_object.date()` gives you a date of a datetime object.",
               "tags": [],
               "creation_date": 1292603553,
               "last_edit_date": 1338795303,
               "is_accepted": false,
               "id": "4472626",
               "down_vote_count": 10,
               "score": -5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 0,
               "answer_id": 14653296,
               "is_accepted": false,
               "last_activity_date": 1359745498,
               "body_markdown": "I am a newbie to Python. But this code worked for me which converts the specified input I provide to datetime. Here&#39;s the code. Correct me if I&#39;m wrong.\r\n\r\n\r\n    import sys\r\n    from datetime import datetime\r\n    from time import mktime, strptime\r\n    \r\n    user_date = &#39;02/15/1989&#39;\r\n    if user_date is not None:\r\n         user_date = datetime.strptime(user_date,&quot;%m/%d/%Y&quot;)\r\n    else:\r\n    \t user_date = datetime.now()\r\n    print user_date",
               "id": "14653296",
               "tags": [],
               "down_vote_count": 7,
               "creation_date": 1359745498,
               "score": -7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 38,
               "answer_id": 22514625,
               "is_accepted": false,
               "last_activity_date": 1395252742,
               "body_markdown": "The accepted answer is correct, but I would prefer to avoid using `datetime.min.time()` because it&#39;s not obvious to me exactly what it does. If it&#39;s obvious to you, then more power to you. I also feel the same way about the `timetuple` method and the reliance on the ordering.\r\n\r\nIn my opinion, the most readable, explicit way of doing this without relying on the reader to be very familiar with the `datetime` module API is:\r\n\r\n    from datetime import date, datetime\r\n    today = date.today()\r\n    today_with_time = datetime(\r\n        year=today.year, \r\n        month=today.month,\r\n        day=today.day,\r\n    )\r\n\r\nThat&#39;s my take on &quot;explicit is better than implicit.&quot;",
               "id": "22514625",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1395252742,
               "score": 38
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 27760382,
               "is_accepted": false,
               "last_activity_date": 1420325417,
               "body_markdown": "One way to convert from date to datetime that hasn&#39;t been mentioned yet:\r\n\r\n    from datetime import date, datetime\r\n    d = date.today()\r\n    datetime.strptime(d.strftime(&#39;%Y%m%d&#39;), &#39;%Y%m%d&#39;)\r\n",
               "id": "27760382",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1420325417,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 30143983,
               "is_accepted": false,
               "last_activity_date": 1431198709,
               "body_markdown": "You can use [easy_date][1] to make it easy:\r\n\r\n    import date_converter\r\n    my_datetime = date_converter.date_to_datetime(my_date)\r\n\r\n\r\n  [1]: https://github.com/ralphavalon/easy_date",
               "id": "30143983",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1431198709,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 6,
               "answer_id": 34576632,
               "is_accepted": false,
               "last_activity_date": 1451824907,
               "body_markdown": "Today being 2016, I think the cleanest solution is provided by pandas Timestamp:\r\n\r\n    from datetime import date\r\n    import pandas as pd\r\n    d = date.today()\r\n    pd.Timestamp(d)\r\n\r\nTimestamp is the pandas equivalent of datetime and is interchangable with it in most cases. Check:\r\n\r\n    from datetime import datetime\r\n    isinstance(pd.Timestamp(d), datetime)\r\n\r\nBut in case you really want a vanilla datetime, you can still do:\r\n\r\n    pd.Timestamp(d).to_datetime()\r\n\r\nTimestamps are a lot more powerful than datetimes, amongst others when dealing with timezones. Actually, Timestamps are so powerful that it&#39;s a pity they are so poorly documented...",
               "id": "34576632",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1451824907,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 45848455,
               "is_accepted": false,
               "last_activity_date": 1503518809,
               "body_markdown": "Using pandas to convert a series of dates to python datetimes:\r\n\r\n    dates = pd.DataFrame(\r\n        {&#39;date&#39;: pd.DatetimeIndex(start=&#39;2017-01-01&#39;, end=&#39;2017-01-5&#39;, freq=&#39;D&#39;)})\r\n    &gt;&gt;&gt; dates\r\n            date\r\n    0 2017-01-01\r\n    1 2017-01-02\r\n    2 2017-01-03\r\n    3 2017-01-04\r\n    4 2017-01-05\r\n    \r\n    &gt;&gt;&gt; pd.DatetimeIndex(dates[&#39;date&#39;]).to_pydatetime().tolist()\r\n    [datetime.datetime(2017, 1, 1, 0, 0),\r\n     datetime.datetime(2017, 1, 2, 0, 0),\r\n     datetime.datetime(2017, 1, 3, 0, 0),\r\n     datetime.datetime(2017, 1, 4, 0, 0),\r\n     datetime.datetime(2017, 1, 5, 0, 0)]\r\n\r\nOne may first need to convert the dates:\r\n\r\n    dates = pd.DataFrame(\r\n            {&#39;date&#39;: [&#39;2017-01-01&#39;, &#39;2017-01-02&#39;, &#39;2017-01-03&#39;, &#39;2017-01-04&#39;, &#39;2017-01-05&#39;]})\r\n    dates[&#39;date&#39;] = pd.DatetimeIndex(dates[&#39;date&#39;])\r\n    pd.DatetimeIndex(dates[&#39;date&#39;]).to_pydatetime().tolist()",
               "id": "45848455",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1503518809,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/1937622/convert-date-to-datetime-in-python",
         "id": "858127-2317"
      },
      {
         "up_vote_count": "293",
         "path": "2.stack",
         "body_markdown": "I have a dataframe in pandas which I would like to write to a CSV file. I am doing this using:\r\n\r\n    df.to_csv(&#39;out.csv&#39;)\r\n\r\nAnd getting the error:\r\n\r\n    UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\\u03b1&#39; in position 20: ordinal not in range(128)\r\n\r\nIs there any way to get around this easily (i.e. I have unicode characters in my data frame)? And is there a way to write to a tab delimited file instead of a CSV using e.g. a &#39;to-tab&#39; method (that I dont think exists)?",
         "view_count": "442977",
         "answer_count": "6",
         "tags": "['python', 'csv', 'pandas', 'dataframe']",
         "creation_date": "1370364416",
         "last_edit_date": "1440990793",
         "code_snippet": "[\"<code>df.to_csv('out.csv')\\n</code>\", \"<code>UnicodeEncodeError: 'ascii' codec can't encode character u'\\\\u03b1' in position 20: ordinal not in range(128)\\n</code>\", '<code>sep</code>', '<code>to_csv</code>', \"<code>df.to_csv(file_name, sep='\\\\t')\\n</code>\", '<code>encoding</code>', \"<code>df.to_csv(file_name, sep='\\\\t', encoding='utf-8')\\n</code>\", \"<code>encoding='utf-8'</code>\", \"<code>df.toCSV(file_name, sep='\\\\t')</code>\", '<code>enpkg</code>', '<code>DataFrame</code>', '<code>to_csv</code>', '<code>DataFrame</code>', '<code>False</code>', '<code>index</code>', \"<code>df.to_csv(file_name, encoding='utf-8', index=False)\\n</code>\", '<code>  Color  Number\\n0   red     22\\n1  blue     10\\n</code>', '<code>Color,Number\\nred,22\\nblue,10\\n</code>', '<code>True</code>', '<code>,Color,Number\\n0,red,22\\n1,blue,10\\n</code>', \"<code>for column in df.columns:\\n    for idx in df[column].index:\\n        x = df.get_value(idx,column)\\n        try:\\n            x = unicode(x.encode('utf-8','ignore'),errors ='ignore') if type(x) == unicode else unicode(str(x),errors='ignore')\\n            df.set_value(idx,column,x)\\n        except Exception:\\n            print 'encoding error: {0} {1}'.format(idx,column)\\n            df.set_value(idx,column,'')\\n            continue\\n</code>\", '<code>df.to_csv(file_name)\\n</code>', \"<code>for column in df.columns:\\n    print '{0} {1}'.format(str(type(df[column][0])),str(column))\\n</code>\", \"<code>IN: unicode('Regenexx\\\\xae',errors='ignore')\\nOUT: u'Regenexx'\\n</code>\", \"<code>for column in df.columns:\\n    for idx in df[column].index:\\n        x = df.get_value(idx,column)\\n        try:\\n            x = x if type(x) == str else str(x).encode('utf-8','ignore').decode('utf-8','ignore')\\n            df.set_value(idx,column,x)\\n        except Exception:\\n            print('encoding error: {0} {1}'.format(idx,column))\\n            df.set_value(idx,column,'')\\n            continue\\n</code>\", \"<code>df.to_csv('out.csv', sep=',')\\n</code>\", '<code>df</code>', '<code>df.to_dense().to_csv(\"submission.csv\", index = False, sep=\\',\\', encoding=\\'utf-8\\')\\n</code>']",
         "title": "Pandas writing dataframe to CSV file",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 463,
               "answer_id": 16923367,
               "is_accepted": true,
               "last_activity_date": 1370364737,
               "body_markdown": "To delimit by a tab you can use the `sep` argument of [`to_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html):\r\n\r\n    df.to_csv(file_name, sep=&#39;\\t&#39;)\r\n\r\nTo use a specific encoding (e.g. &#39;utf-8&#39;) use the `encoding` argument:\r\n\r\n    df.to_csv(file_name, sep=&#39;\\t&#39;, encoding=&#39;utf-8&#39;)",
               "id": "16923367",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1370364737,
               "score": 463
            },
            {
               "up_vote_count": 5,
               "answer_id": 37324029,
               "last_activity_date": 1463663709,
               "path": "3.stack.answer",
               "body_markdown": "Sometimes you face these problems if you specify UTF-8 encoding also.\r\nI recommend you to specify encoding while reading file and same encoding while writing to file.\r\nThis might solve your problem.",
               "tags": [],
               "creation_date": 1463662966,
               "last_edit_date": 1463663709,
               "is_accepted": false,
               "id": "37324029",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "up_vote_count": 7,
               "answer_id": 41729348,
               "last_activity_date": 1512149917,
               "path": "3.stack.answer",
               "body_markdown": "Something else you can try if you are having issues encoding to &#39;utf-8&#39; and want to go cell by cell you could try the following. \r\n\r\n**Python 2**\r\n\r\n(Where &quot;df&quot; is your DataFrame object.)\r\n\r\n    for column in df.columns:\r\n        for idx in df[column].index:\r\n            x = df.get_value(idx,column)\r\n            try:\r\n                x = unicode(x.encode(&#39;utf-8&#39;,&#39;ignore&#39;),errors =&#39;ignore&#39;) if type(x) == unicode else unicode(str(x),errors=&#39;ignore&#39;)\r\n                df.set_value(idx,column,x)\r\n            except Exception:\r\n                print &#39;encoding error: {0} {1}&#39;.format(idx,column)\r\n                df.set_value(idx,column,&#39;&#39;)\r\n                continue\r\n\r\nThen try:\r\n\r\n    df.to_csv(file_name)\r\n\r\n----\r\n\r\n\r\nYou can check the encoding of the columns by:\r\n\r\n    for column in df.columns:\r\n        print &#39;{0} {1}&#39;.format(str(type(df[column][0])),str(column))\r\n\r\nWarning: errors=&#39;ignore&#39; will just omit the character e.g.\r\n\r\n    IN: unicode(&#39;Regenexx\\xae&#39;,errors=&#39;ignore&#39;)\r\n    OUT: u&#39;Regenexx&#39;\r\n\r\n**Python 3**\r\n\r\n    for column in df.columns:\r\n        for idx in df[column].index:\r\n            x = df.get_value(idx,column)\r\n            try:\r\n                x = x if type(x) == str else str(x).encode(&#39;utf-8&#39;,&#39;ignore&#39;).decode(&#39;utf-8&#39;,&#39;ignore&#39;)\r\n                df.set_value(idx,column,x)\r\n            except Exception:\r\n                print(&#39;encoding error: {0} {1}&#39;.format(idx,column))\r\n                df.set_value(idx,column,&#39;&#39;)\r\n                continue\r\n",
               "tags": [],
               "creation_date": 1484773259,
               "last_edit_date": 1512149917,
               "is_accepted": false,
               "id": "41729348",
               "down_vote_count": 0,
               "score": 7
            },
            {
               "up_vote_count": 3,
               "answer_id": 42564388,
               "last_activity_date": 1512546741,
               "path": "3.stack.answer",
               "body_markdown": "    df.to_csv(&#39;out.csv&#39;, sep=&#39;,&#39;)\r\n\r\nIt will work definitely, change `&quot;df&quot;` to your data frame name and run.**(Use anaconda idle)**",
               "tags": [],
               "creation_date": 1488483322,
               "last_edit_date": 1512546741,
               "is_accepted": false,
               "id": "42564388",
               "down_vote_count": 3,
               "score": 0
            },
            {
               "up_vote_count": 41,
               "answer_id": 45141782,
               "last_activity_date": 1512175249,
               "path": "3.stack.answer",
               "body_markdown": "I&#39;d like to **add something** to what *Andy Hayden* already mentioned in his *precise* answer. \n\nWhen you are storing a `DataFrame` object into a **csv file** using the `to_csv` method, you probably wont be needing to store the **preceding indices** of each **row** of the `DataFrame` object.\r\n\r\nYou can **avoid** that by passing a `False` boolean value to `index` parameter.\r\n\r\nSomewhat like:\r\n\r\n    df.to_csv(file_name, encoding=&#39;utf-8&#39;, index=False)\r\n\r\nSo if your DataFrame object is something like:\r\n\r\n      Color  Number\r\n    0   red     22\r\n    1  blue     10\r\n\r\nThe csv file will store:\r\n\r\n    Color,Number\r\n    red,22\r\n    blue,10\r\n\r\ninstead of (the case when the **default value** `True` was passed)\r\n\r\n    ,Color,Number\r\n    0,red,22\r\n    1,blue,10\r\n\r\nFound it worth sharing, Cheers! :-)",
               "tags": [],
               "creation_date": 1500287238,
               "last_edit_date": 1512175249,
               "is_accepted": false,
               "id": "45141782",
               "down_vote_count": 0,
               "score": 41
            },
            {
               "up_vote_count": 0,
               "answer_id": 48464540,
               "last_activity_date": 1516986252,
               "path": "3.stack.answer",
               "body_markdown": "it could be not the answer for this case, but as I had the same error-message with &lt;i&gt;.to_csv&lt;/i&gt; I tried &lt;i&gt;.toCSV(&#39;name.csv&#39;)&lt;/i&gt; and the error-message was different (&quot;&#39;SparseDataFrame&#39; object has no attribute &#39;toCSV&#39;&quot;). So the problem was solved by turning dataframe to dense dataframe\r\n\r\n    df.to_dense().to_csv(&quot;submission.csv&quot;, index = False, sep=&#39;,&#39;, encoding=&#39;utf-8&#39;)",
               "tags": [],
               "creation_date": 1516980936,
               "last_edit_date": 1516986252,
               "is_accepted": false,
               "id": "48464540",
               "down_vote_count": 0,
               "score": 0
            }
         ],
         "link": "https://stackoverflow.com/questions/16923281/pandas-writing-dataframe-to-csv-file",
         "id": "858127-2318"
      },
      {
         "up_vote_count": "344",
         "path": "2.stack",
         "body_markdown": "I&#39;m looking for a way to test whether or not a given string repeats itself for the entire string or not.\r\n\r\nExamples:\r\n\r\n    [\r\n        &#39;0045662100456621004566210045662100456621&#39;,             # &#39;00456621&#39;\r\n        &#39;0072992700729927007299270072992700729927&#39;,             # &#39;00729927&#39;\r\n        &#39;001443001443001443001443001443001443001443&#39;,           # &#39;001443&#39;\r\n        &#39;037037037037037037037037037037037037037037037&#39;,        # &#39;037&#39;\r\n        &#39;047619047619047619047619047619047619047619&#39;,           # &#39;047619&#39;\r\n        &#39;002457002457002457002457002457002457002457&#39;,           # &#39;002457&#39;\r\n        &#39;001221001221001221001221001221001221001221&#39;,           # &#39;001221&#39;\r\n        &#39;001230012300123001230012300123001230012300123&#39;,        # &#39;00123&#39;\r\n        &#39;0013947001394700139470013947001394700139470013947&#39;,    # &#39;0013947&#39;\r\n        &#39;001001001001001001001001001001001001001001001001001&#39;,  # &#39;001&#39;\r\n        &#39;001406469760900140646976090014064697609&#39;,              # &#39;0014064697609&#39;\r\n    ]\r\n\r\nare strings which repeat themselves, and\r\n\r\n    [\r\n        &#39;004608294930875576036866359447&#39;,\r\n        &#39;00469483568075117370892018779342723&#39;,\r\n        &#39;004739336492890995260663507109&#39;,\r\n        &#39;001508295625942684766214177978883861236802413273&#39;,\r\n        &#39;007518796992481203&#39;,\r\n        &#39;0071942446043165467625899280575539568345323741&#39;,\r\n        &#39;0434782608695652173913&#39;,\r\n        &#39;0344827586206896551724137931&#39;,\r\n        &#39;002481389578163771712158808933&#39;,\r\n        &#39;002932551319648093841642228739&#39;,\r\n        &#39;0035587188612099644128113879&#39;,\r\n        &#39;003484320557491289198606271777&#39;,\r\n        &#39;00115074798619102416570771&#39;,\r\n    ]\r\n\r\nare examples of ones that do not.\r\n\r\nThe repeating sections of the strings I&#39;m given can be quite long, and the strings themselves can be 500 or more characters, so looping through each character trying to build a pattern then checking the pattern vs the rest of the string seems awful slow. Multiply that by potentially hundreds of strings and I can&#39;t see any intuitive solution.\r\n\r\nI&#39;ve looked into regexes a bit and they seem good for when you know what you&#39;re looking for, or at least the length of the pattern you&#39;re looking for. Unfortunately, I know neither.\r\n\r\nHow can I tell if a string is repeating itself and if it is, what the shortest repeating subsequence is?\r\n",
         "view_count": "34868",
         "answer_count": "13",
         "tags": "['python', 'string', 'pattern-matching']",
         "creation_date": "1428361329",
         "last_edit_date": "1429716902",
         "code_snippet": "[\"<code>[\\n    '0045662100456621004566210045662100456621',             # '00456621'\\n    '0072992700729927007299270072992700729927',             # '00729927'\\n    '001443001443001443001443001443001443001443',           # '001443'\\n    '037037037037037037037037037037037037037037037',        # '037'\\n    '047619047619047619047619047619047619047619',           # '047619'\\n    '002457002457002457002457002457002457002457',           # '002457'\\n    '001221001221001221001221001221001221001221',           # '001221'\\n    '001230012300123001230012300123001230012300123',        # '00123'\\n    '0013947001394700139470013947001394700139470013947',    # '0013947'\\n    '001001001001001001001001001001001001001001001001001',  # '001'\\n    '001406469760900140646976090014064697609',              # '0014064697609'\\n]\\n</code>\", \"<code>[\\n    '004608294930875576036866359447',\\n    '00469483568075117370892018779342723',\\n    '004739336492890995260663507109',\\n    '001508295625942684766214177978883861236802413273',\\n    '007518796992481203',\\n    '0071942446043165467625899280575539568345323741',\\n    '0434782608695652173913',\\n    '0344827586206896551724137931',\\n    '002481389578163771712158808933',\\n    '002932551319648093841642228739',\\n    '0035587188612099644128113879',\\n    '003484320557491289198606271777',\\n    '00115074798619102416570771',\\n]\\n</code>\", '<code>def principal_period(s):\\n    i = (s+s).find(s, 1, -1)\\n    return None if i == -1 else s[:i]\\n</code>', '<code>s</code>', '<code>(s+s)[1:-1]</code>', '<code>start</code>', '<code>end</code>', '<code>string.find</code>', '<code>.find()</code>', '<code>.index()</code>', '<code>in</code>', '<code>(s+s).find(s, 1, -1)</code>', '<code>(s+s).find(s, 1, -1)</code>', '<code>(s+s)[1:-1].find(s)</code>', '<code>\"abcd\"</code>', '<code>\"dabc\"</code>', '<code>n</code>', '<code>n</code>', '<code>k</code>', '<code>k</code>', '<code>import re\\n\\nREPEATER = re.compile(r\"(.+?)\\\\1+$\")\\n\\ndef repeated(s):\\n    match = REPEATER.match(s)\\n    return match.group(1) if match else None\\n</code>', '<code>examples = [\\n    \\'0045662100456621004566210045662100456621\\',\\n    \\'0072992700729927007299270072992700729927\\',\\n    \\'001443001443001443001443001443001443001443\\',\\n    \\'037037037037037037037037037037037037037037037\\',\\n    \\'047619047619047619047619047619047619047619\\',\\n    \\'002457002457002457002457002457002457002457\\',\\n    \\'001221001221001221001221001221001221001221\\',\\n    \\'001230012300123001230012300123001230012300123\\',\\n    \\'0013947001394700139470013947001394700139470013947\\',\\n    \\'001001001001001001001001001001001001001001001001001\\',\\n    \\'001406469760900140646976090014064697609\\',\\n    \\'004608294930875576036866359447\\',\\n    \\'00469483568075117370892018779342723\\',\\n    \\'004739336492890995260663507109\\',\\n    \\'001508295625942684766214177978883861236802413273\\',\\n    \\'007518796992481203\\',\\n    \\'0071942446043165467625899280575539568345323741\\',\\n    \\'0434782608695652173913\\',\\n    \\'0344827586206896551724137931\\',\\n    \\'002481389578163771712158808933\\',\\n    \\'002932551319648093841642228739\\',\\n    \\'0035587188612099644128113879\\',\\n    \\'003484320557491289198606271777\\',\\n    \\'00115074798619102416570771\\',\\n]\\n\\nfor e in examples:\\n    sub = repeated(e)\\n    if sub:\\n        print(\"%r: %r\" % (e, sub))\\n    else:\\n        print(\"%r does not repeat.\" % e)\\n</code>', \"<code>'0045662100456621004566210045662100456621': '00456621'\\n'0072992700729927007299270072992700729927': '00729927'\\n'001443001443001443001443001443001443001443': '001443'\\n'037037037037037037037037037037037037037037037': '037'\\n'047619047619047619047619047619047619047619': '047619'\\n'002457002457002457002457002457002457002457': '002457'\\n'001221001221001221001221001221001221001221': '001221'\\n'001230012300123001230012300123001230012300123': '00123'\\n'0013947001394700139470013947001394700139470013947': '0013947'\\n'001001001001001001001001001001001001001001001001001': '001'\\n'001406469760900140646976090014064697609': '0014064697609'\\n'004608294930875576036866359447' does not repeat.\\n'00469483568075117370892018779342723' does not repeat.\\n'004739336492890995260663507109' does not repeat.\\n'001508295625942684766214177978883861236802413273' does not repeat.\\n'007518796992481203' does not repeat.\\n'0071942446043165467625899280575539568345323741' does not repeat.\\n'0434782608695652173913' does not repeat.\\n'0344827586206896551724137931' does not repeat.\\n'002481389578163771712158808933' does not repeat.\\n'002932551319648093841642228739' does not repeat.\\n'0035587188612099644128113879' does not repeat.\\n'003484320557491289198606271777' does not repeat.\\n'00115074798619102416570771' does not repeat.\\n</code>\", '<code>(.+?)\\\\1+$</code>', '<code>(.+?)</code>', '<code>+?</code>', '<code>\\\\1+</code>', '<code>$</code>', '<code>re.match()</code>', '<code>$</code>', '<code>re.fullmatch()</code>', '<code>re.search()</code>', '<code>^(.+?)\\\\1+$</code>', '<code>0</code>', '<code>1</code>', '<code>1</code>', '<code>n / 2</code>', '<code>from math import sqrt, floor\\n\\ndef divquot(n):\\n    if n &gt; 1:\\n        yield 1, n\\n    swapped = []\\n    for d in range(2, int(floor(sqrt(n))) + 1):\\n        q, r = divmod(n, d)\\n        if r == 0:\\n            yield d, q\\n            swapped.append((q, d))\\n    while swapped:\\n        yield swapped.pop()\\n\\ndef repeats(s):\\n    n = len(s)\\n    for d, q in divquot(n):\\n        sl = s[0:d]\\n        if sl * q == s:\\n            return sl\\n    return None\\n</code>', '<code>/</code>', '<code>int</code>', '<code>//</code>', '<code>//</code>', '<code>all</code>', '<code>None</code>', '<code>divmod</code>', '<code>divisors</code>', '<code>n</code>', '<code>n</code>', '<code>(n/2)</code>', '<code>n / 2</code>', '<code>divisors()</code>', '<code>n // 2</code>', '<code>/</code>', '<code>//</code>', '<code>mean performance:\\n 0.0003  david_zhang\\n 0.0009  zero\\n 0.0013  antti\\n 0.0013  tigerhawk_2\\n 0.0015  carpetpython\\n 0.0029  tigerhawk_1\\n 0.0031  davidism\\n 0.0035  saksham\\n 0.0046  shashank\\n 0.0052  riad\\n 0.0056  piotr\\n\\nmedian performance:\\n 0.0003  david_zhang\\n 0.0008  zero\\n 0.0013  antti\\n 0.0013  tigerhawk_2\\n 0.0014  carpetpython\\n 0.0027  tigerhawk_1\\n 0.0031  davidism\\n 0.0038  saksham\\n 0.0044  shashank\\n 0.0054  riad\\n 0.0058  piotr\\n</code>', '<code>mean performance:\\n 0.0006  david_zhang\\n 0.0036  tigerhawk_2\\n 0.0036  antti\\n 0.0037  zero\\n 0.0039  carpetpython\\n 0.0052  shashank\\n 0.0056  piotr\\n 0.0066  davidism\\n 0.0120  tigerhawk_1\\n 0.0177  riad\\n 0.0283  saksham\\n\\nmedian performance:\\n 0.0004  david_zhang\\n 0.0018  zero\\n 0.0022  tigerhawk_2\\n 0.0022  antti\\n 0.0024  carpetpython\\n 0.0043  davidism\\n 0.0049  shashank\\n 0.0055  piotr\\n 0.0061  tigerhawk_1\\n 0.0077  riad\\n 0.0109  saksham\\n</code>', '<code>mean performance:\\n 0.0123  shashank\\n 0.0375  david_zhang\\n 0.0376  piotr\\n 0.0394  carpetpython\\n 0.0479  antti\\n 0.0488  tigerhawk_2\\n 0.2269  tigerhawk_1\\n 0.2336  davidism\\n 0.7239  saksham\\n 3.6265  zero\\n 6.0111  riad\\n\\nmedian performance:\\n 0.0107  tigerhawk_2\\n 0.0108  antti\\n 0.0109  carpetpython\\n 0.0135  david_zhang\\n 0.0137  tigerhawk_1\\n 0.0150  shashank\\n 0.0229  saksham\\n 0.0255  piotr\\n 0.0721  davidism\\n 0.1080  zero\\n 1.8539  riad\\n</code>', '<code>spl</code>', '<code>str.split</code>', '<code>boxplot</code>', \"<code>pandas.DataFrame.plot(kind='boxplot')</code>\", '<code>pandas.DataFrame.boxplot</code>', '<code>def repeat(string):\\n    for i in range(1, len(string)//2+1):\\n        if not len(string)%len(string[0:i]) and string[0:i]*(len(string)//len(string[0:i])) == string:\\n            return string[0:i]\\n</code>', '<code>def repeat(string):\\n    l = len(string)\\n    for i in range(1, len(string)//2+1):\\n        if l%i: continue\\n        s = string[0:i]\\n        if s*(l//i) == string:\\n            return s\\n</code>', \"<code>print(repeat('009009009'))\\nprint(repeat('254725472547'))\\nprint(repeat('abcdeabcdeabcdeabcde'))\\nprint(repeat('abcdefg'))\\nprint(repeat('09099099909999'))\\nprint(repeat('02589675192'))\\n</code>\", '<code>009\\n2547\\nabcde\\nNone\\nNone\\nNone\\n</code>', '<code>len(string[0:i])</code>', '<code>i</code>', '<code>len(string)</code>', '<code>string[0:i]</code>', '<code>length // 2</code>', '<code>def shortest_repeat(orig_value):\\n    if not orig_value:\\n        return None\\n\\n    value = orig_value\\n\\n    while True:\\n        len_half = len(value) // 2\\n        first_half = value[:len_half]\\n\\n        if first_half != value[len_half:]:\\n            break\\n\\n        value = first_half\\n\\n    len_value = len(value)\\n    split = value.split\\n\\n    for i in (i for i in range(1, len_value // 2) if len_value % i == 0):\\n        if not any(split(value[:i])):\\n            return value[:i]\\n\\n    return value if value != orig_value else None\\n</code>', '<code>O(n)</code>', '<code>n</code>', '<code>aaa....aab</code>', '<code>n - 1 = 2 * 3 * 5 * 7 ... *p_n - 1</code>', '<code>a</code>', '<code>def prefix_function(s):\\n    n = len(s)\\n    pi = [0] * n\\n    for i in xrange(1, n):\\n        j = pi[i - 1]\\n        while(j &gt; 0 and s[i] != s[j]):\\n            j = pi[j - 1]\\n        if (s[i] == s[j]):\\n            j += 1\\n        pi[i] = j;\\n    return pi\\n</code>', '<code>k = len(s) - prefix_function(s[-1])\\n</code>', '<code>k != n and n % k == 0</code>', '<code>k != n and n % k == 0</code>', '<code>s[:k]</code>', '<code>def riad(s):\\n    n = len(s)\\n    pi = [0] * n\\n    for i in xrange(1, n):\\n        j = pi[i - 1]\\n        while(j &gt; 0 and s[i] != s[j]):\\n            j = pi[j - 1]\\n        if (s[i] == s[j]):\\n            j += 1\\n        pi[i] = j;\\n    k = n - pi[-1]\\n    return s[:k] if (n != k and n % k == 0) else None\\n</code>', '<code>prefix_function()</code>', '<code>while</code>', '<code>if</code>', '<code>&amp;&amp;</code>', '<code>and</code>', \"<code>UnboundLocalError: local variable 'i' referenced before assignment</code>\", '<code>for i in range(i, n):</code>', '<code>prefix_function()</code>', '<code>None</code>', '<code>*</code>', '<code>def get_shortest_repeat(string):\\n    length = len(string)\\n    for i in range(1, length // 2 + 1):\\n        if length % i:  # skip non-factors early\\n            continue\\n\\n        candidate = string[:i]\\n        if string == candidate * (length // i):\\n            return candidate\\n\\n    return None\\n</code>', '<code>length // 2</code>', '<code>+ 1</code>', '<code>abab</code>', '<code>range</code>', '<code>length//2</code>', '<code>length//2+1</code>', \"<code>'aabaab'</code>\", '<code>s</code>', '<code>len(s)</code>', '<code>substr</code>', '<code>substr</code>', '<code>ratio</code>', '<code>s</code>', '<code>ratio=len(s)/len(substr)</code>', '<code>def check_repeat(s):\\n    for i in range(1, len(s)):\\n        substr = s[:i]\\n        ratio = len(s)/len(substr)\\n        if substr * ratio == s:\\n            print \\'Repeating on \"%s\"\\' % substr\\n            return\\n    print \\'Non repeating\\'\\n\\n&gt;&gt;&gt; check_repeat(\\'254725472547\\')\\nRepeating on \"2547\"\\n&gt;&gt;&gt; check_repeat(\\'abcdeabcdeabcdeabcde\\')\\nRepeating on \"abcde\"\\n</code>', '<code>def repeating(s):\\n    size = len(s)\\n    incr = size % 2 + 1\\n    for n in xrange(1, size//2+1, incr):\\n        if size % n == 0:\\n            if s[:n] * (size//n) == s:\\n                return s[:n]\\n</code>', '<code>xrange</code>', '<code>s[:n]</code>', '<code>statistics</code>', '<code>/</code>', '<code>//</code>', '<code>xrange()</code>', '<code>range()</code>', '<code>xrange()</code>', '<code>//</code>', '<code>/</code>', '<code>/</code>', '<code>range()</code>', '<code>xrange()</code>', '<code>range()</code>', \"<code>def repeats(string):\\n    n = len(string)\\n    tried = set([])\\n    best = None\\n    nums = [i for i in  xrange(2, int(n**0.5) + 1) if n % i == 0]\\n    nums = [n/i for i in nums if n/i!=i] + list(reversed(nums)) + [1]\\n    for s in nums:\\n        if all(t%s for t in tried):\\n            print 'Trying repeating string of length:', s\\n            if string[:s]*(n/s)==string:\\n                best = s\\n            else:\\n                tried.add(s)\\n    if best:\\n        return string[:best]\\n</code>\", \"<code>&gt;&gt;&gt; repeats('12345678')\\nTrying repeating string of length: 4\\nNone\\n\\n# for this one we need only 2 checks \\n&gt;&gt;&gt; repeats('1234567812345678')\\nTrying repeating string of length: 8\\nTrying repeating string of length: 4\\n'12345678'\\n</code>\", \"<code>principal_period('6210045662100456621004566210045662100456621')</code>\", '<code>621</code>', '<code>00456621</code>', \"<code>def principal_period(s):\\n    for j in range(int(len(s)/2)):\\n        idx = (s[j:]+s[j:]).find(s[j:], 1, -1)\\n        if idx != -1:\\n            # Make sure that the first substring is part of pattern\\n            if s[:j] == s[j:][:idx][-j:]:\\n                break\\n\\n    return None if idx == -1 else s[j:][:idx]\\n\\nprincipal_period('6210045662100456621004566210045662100456621')\\n&gt;&gt;&gt; '00456621'\\n</code>\", '<code>print \"Enter a string....\"\\n#mainstring = String given by user\\nmainstring=raw_input(\"&gt;\")\\nif(mainstring==\\'\\'):\\n    print \"Invalid string\"\\n    exit()\\n#charlist = Character list of mainstring\\ncharlist=list(mainstring)\\nstrarr=\\'\\'\\nprint \"Length of your string :\",len(mainstring)\\nfor i in range(0,len(mainstring)):\\n    strarr=strarr+charlist[i]\\n    splitlist=mainstring.split(strarr)\\n    count = 0\\n    for j in splitlist:\\n        if j ==\\'\\':\\n            count+=1\\n    if count == len(splitlist):\\n        break\\nif count == len(splitlist):\\n    if count == 2:\\n        print \"No repeating Sub-String found in string %r\"%(mainstring)\\n\\n    else:\\n        print \"Sub-String %r repeats in string %r\"%(strarr,mainstring)\\nelse :\\n    print \"No repeating Sub-String found in string %r\"%(mainstring)\\n</code>']",
         "title": "How can I tell if a string repeats itself in Python?",
         "_childDocuments_": [
            {
               "up_vote_count": 185,
               "answer_id": 29481228,
               "last_activity_date": 1428535211,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s a solution using regular expressions.\r\n\r\n    import re\r\n\r\n    REPEATER = re.compile(r&quot;(.+?)\\1+$&quot;)\r\n\r\n    def repeated(s):\r\n        match = REPEATER.match(s)\r\n        return match.group(1) if match else None\r\n\r\nIterating over the examples in the question:\r\n\r\n    examples = [\r\n        &#39;0045662100456621004566210045662100456621&#39;,\r\n        &#39;0072992700729927007299270072992700729927&#39;,\r\n        &#39;001443001443001443001443001443001443001443&#39;,\r\n        &#39;037037037037037037037037037037037037037037037&#39;,\r\n        &#39;047619047619047619047619047619047619047619&#39;,\r\n        &#39;002457002457002457002457002457002457002457&#39;,\r\n        &#39;001221001221001221001221001221001221001221&#39;,\r\n        &#39;001230012300123001230012300123001230012300123&#39;,\r\n        &#39;0013947001394700139470013947001394700139470013947&#39;,\r\n        &#39;001001001001001001001001001001001001001001001001001&#39;,\r\n        &#39;001406469760900140646976090014064697609&#39;,\r\n        &#39;004608294930875576036866359447&#39;,\r\n        &#39;00469483568075117370892018779342723&#39;,\r\n        &#39;004739336492890995260663507109&#39;,\r\n        &#39;001508295625942684766214177978883861236802413273&#39;,\r\n        &#39;007518796992481203&#39;,\r\n        &#39;0071942446043165467625899280575539568345323741&#39;,\r\n        &#39;0434782608695652173913&#39;,\r\n        &#39;0344827586206896551724137931&#39;,\r\n        &#39;002481389578163771712158808933&#39;,\r\n        &#39;002932551319648093841642228739&#39;,\r\n        &#39;0035587188612099644128113879&#39;,\r\n        &#39;003484320557491289198606271777&#39;,\r\n        &#39;00115074798619102416570771&#39;,\r\n    ]\r\n\r\n    for e in examples:\r\n        sub = repeated(e)\r\n        if sub:\r\n            print(&quot;%r: %r&quot; % (e, sub))\r\n        else:\r\n            print(&quot;%r does not repeat.&quot; % e)\r\n\r\n... produces this output:\r\n\r\n    &#39;0045662100456621004566210045662100456621&#39;: &#39;00456621&#39;\r\n    &#39;0072992700729927007299270072992700729927&#39;: &#39;00729927&#39;\r\n    &#39;001443001443001443001443001443001443001443&#39;: &#39;001443&#39;\r\n    &#39;037037037037037037037037037037037037037037037&#39;: &#39;037&#39;\r\n    &#39;047619047619047619047619047619047619047619&#39;: &#39;047619&#39;\r\n    &#39;002457002457002457002457002457002457002457&#39;: &#39;002457&#39;\r\n    &#39;001221001221001221001221001221001221001221&#39;: &#39;001221&#39;\r\n    &#39;001230012300123001230012300123001230012300123&#39;: &#39;00123&#39;\r\n    &#39;0013947001394700139470013947001394700139470013947&#39;: &#39;0013947&#39;\r\n    &#39;001001001001001001001001001001001001001001001001001&#39;: &#39;001&#39;\r\n    &#39;001406469760900140646976090014064697609&#39;: &#39;0014064697609&#39;\r\n    &#39;004608294930875576036866359447&#39; does not repeat.\r\n    &#39;00469483568075117370892018779342723&#39; does not repeat.\r\n    &#39;004739336492890995260663507109&#39; does not repeat.\r\n    &#39;001508295625942684766214177978883861236802413273&#39; does not repeat.\r\n    &#39;007518796992481203&#39; does not repeat.\r\n    &#39;0071942446043165467625899280575539568345323741&#39; does not repeat.\r\n    &#39;0434782608695652173913&#39; does not repeat.\r\n    &#39;0344827586206896551724137931&#39; does not repeat.\r\n    &#39;002481389578163771712158808933&#39; does not repeat.\r\n    &#39;002932551319648093841642228739&#39; does not repeat.\r\n    &#39;0035587188612099644128113879&#39; does not repeat.\r\n    &#39;003484320557491289198606271777&#39; does not repeat.\r\n    &#39;00115074798619102416570771&#39; does not repeat.\r\n\r\nThe regular expression `(.+?)\\1+$` is divided into three parts:\r\n\r\n1. `(.+?)` is a matching group containing at least one (but as few as possible) of any character (because [`+?` is non-greedy][1]).\r\n\r\n2. `\\1+` checks for at least one repetition of the matching group in the first part.\r\n\r\n3. `$` checks for the end of the string, to ensure that there&#39;s no extra, non-repeating content after the repeated substrings (and using [`re.match()`][2] ensures that there&#39;s no non-repeating text *before* the repeated substrings).\r\n\r\nIn Python 3.4 and later, you could drop the `$` and use [`re.fullmatch()`][3] instead, or (in any Python at least as far back as 2.3) go the other way and use [`re.search()`][4] with the regex `^(.+?)\\1+$`, all of which are more down to personal taste than anything else.\r\n\r\n  [1]: https://docs.python.org/3/library/re.html#regular-expression-syntax\r\n  [2]: https://docs.python.org/3/library/re.html#re.match\r\n  [3]: https://docs.python.org/3/library/re.html#re.fullmatch\r\n  [4]: https://docs.python.org/3/library/re.html#re.search\r\n  [5]: http://stackoverflow.com/a/29482936",
               "tags": [],
               "creation_date": 1428362296,
               "last_edit_date": 1428535211,
               "is_accepted": false,
               "id": "29481228",
               "down_vote_count": 5,
               "score": 180
            },
            {
               "up_vote_count": 39,
               "answer_id": 29481244,
               "last_activity_date": 1428525832,
               "path": "3.stack.answer",
               "body_markdown": "Non-regex solution:\r\n\r\n    def repeat(string):\r\n        for i in range(1, len(string)//2+1):\r\n            if not len(string)%len(string[0:i]) and string[0:i]*(len(string)//len(string[0:i])) == string:\r\n                return string[0:i]\r\n\r\nFaster non-regex solution, thanks to @ThatWeirdo (see comments):\r\n\r\n    def repeat(string):\r\n        l = len(string)\r\n        for i in range(1, len(string)//2+1):\r\n            if l%i: continue\r\n            s = string[0:i]\r\n            if s*(l//i) == string:\r\n                return s\r\n\r\nThe above solution is very rarely slower than the original by a few percent, but it&#39;s usually a good bit faster - sometimes a whole lot faster. It&#39;s still not faster than davidism&#39;s for longer strings, and zero&#39;s regex solution is superior for short strings. It comes out to the fastest (according to davidism&#39;s test on github - see his answer) with strings of about 1000-1500 characters. Regardless, it&#39;s reliably second-fastest (or better) in all cases I tested. Thanks, ThatWeirdo.\r\n\r\nTest:\r\n\r\n    print(repeat(&#39;009009009&#39;))\r\n    print(repeat(&#39;254725472547&#39;))\r\n    print(repeat(&#39;abcdeabcdeabcdeabcde&#39;))\r\n    print(repeat(&#39;abcdefg&#39;))\r\n    print(repeat(&#39;09099099909999&#39;))\r\n    print(repeat(&#39;02589675192&#39;))\r\n\r\nResults:\r\n\r\n    009\r\n    2547\r\n    abcde\r\n    None\r\n    None\r\n    None",
               "tags": [],
               "creation_date": 1428362438,
               "last_edit_date": 1428525832,
               "is_accepted": false,
               "id": "29481244",
               "down_vote_count": 2,
               "score": 37
            },
            {
               "up_vote_count": 90,
               "answer_id": 29481262,
               "last_activity_date": 1433645731,
               "path": "3.stack.answer",
               "body_markdown": "You can make the observation that for a string to be considered repeating, its length must be divisible by the length of its repeated sequence. Given that, here is a solution that generates divisors of the length from `1` to `n / 2` inclusive, divides the original string into substrings with the length of the divisors, and tests the equality of the result set:\r\n\r\n    from math import sqrt, floor\r\n\r\n    def divquot(n):\r\n        if n &gt; 1:\r\n            yield 1, n\r\n        swapped = []\r\n        for d in range(2, int(floor(sqrt(n))) + 1):\r\n            q, r = divmod(n, d)\r\n            if r == 0:\r\n                yield d, q\r\n                swapped.append((q, d))\r\n        while swapped:\r\n            yield swapped.pop()\r\n\r\n    def repeats(s):\r\n        n = len(s)\r\n        for d, q in divquot(n):\r\n            sl = s[0:d]\r\n            if sl * q == s:\r\n                return sl\r\n        return None\r\n\r\n**EDIT:** In Python 3, the `/` operator has changed to do float division by default. To get the `int` division from Python 2, you can use the `//` operator instead. Thank you to @TigerhawkT3 for bringing this to my attention.\r\n\r\nThe `//` operator performs integer division in both Python 2 and Python 3, so I&#39;ve updated the answer to support both versions. The part where we test to see if all the substrings are equal is now a short-circuiting operation using `all` and a generator expression.\r\n\r\n**UPDATE:** In response to a change in the original question, the code has now been updated to return the smallest repeating substring if it exists and `None` if it does not. @godlygeek has suggested using `divmod` to reduce the number of iterations on the `divisors` generator, and the code has been updated to match that as well. It now returns all positive divisors of `n` in ascending order, exclusive of `n` itself.\r\n\r\n**Further update for high performance:** After multiple tests, I&#39;ve come to the conclusion that  simply testing for string equality has the best performance out of any slicing or iterator solution in Python. Thus, I&#39;ve taken a leaf out of @TigerhawkT3 &#39;s book and updated my solution. It&#39;s now over 6x as fast as before, noticably faster than Tigerhawk&#39;s solution but slower than David&#39;s.",
               "tags": [],
               "creation_date": 1428362571,
               "last_edit_date": 1433645731,
               "is_accepted": false,
               "id": "29481262",
               "down_vote_count": 0,
               "score": 90
            },
            {
               "up_vote_count": 15,
               "answer_id": 29482465,
               "last_activity_date": 1428388793,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s a straight forward solution, without regexes. \r\n\r\nFor substrings of `s` starting from zeroth index, of lengths 1 through `len(s)`, check if that substring, `substr` is the repeated pattern. This check can be performed by concatenating `substr` with itself `ratio` times, such that the length of the string thus formed is equal to the length of `s`. Hence `ratio=len(s)/len(substr)`. \r\n\r\nReturn when first such substring is found. This would provide the smallest possible substring, if one exists.\r\n\r\n    def check_repeat(s):\r\n        for i in range(1, len(s)):\r\n            substr = s[:i]\r\n            ratio = len(s)/len(substr)\r\n            if substr * ratio == s:\r\n                print &#39;Repeating on &quot;%s&quot;&#39; % substr\r\n                return\r\n        print &#39;Non repeating&#39;\r\n    \r\n    &gt;&gt;&gt; check_repeat(&#39;254725472547&#39;)\r\n    Repeating on &quot;2547&quot;\r\n    &gt;&gt;&gt; check_repeat(&#39;abcdeabcdeabcdeabcde&#39;)\r\n    Repeating on &quot;abcde&quot;\r\n\r\n",
               "tags": [],
               "creation_date": 1428371731,
               "last_edit_date": 1428388793,
               "is_accepted": false,
               "id": "29482465",
               "down_vote_count": 0,
               "score": 15
            },
            {
               "up_vote_count": 24,
               "answer_id": 29482830,
               "last_activity_date": 1428498786,
               "path": "3.stack.answer",
               "body_markdown": "First, halve the string as long as it&#39;s a &quot;2 part&quot; duplicate.  This reduces the search space if there are an even number of repeats.  Then, working forwards to find the smallest repeating string, check if splitting the full string by increasingly larger sub-string results in only empty values.  Only sub-strings up to `length // 2` need to be tested since anything over that would have no repeats.\r\n\r\n    def shortest_repeat(orig_value):\r\n        if not orig_value:\r\n            return None\r\n\r\n        value = orig_value\r\n    \r\n        while True:\r\n            len_half = len(value) // 2\r\n            first_half = value[:len_half]\r\n    \r\n            if first_half != value[len_half:]:\r\n                break\r\n    \r\n            value = first_half\r\n    \r\n        len_value = len(value)\r\n        split = value.split\r\n    \r\n        for i in (i for i in range(1, len_value // 2) if len_value % i == 0):\r\n            if not any(split(value[:i])):\r\n                return value[:i]\r\n    \r\n        return value if value != orig_value else None\r\n\r\nThis returns the shortest match or None if there is no match.",
               "tags": [],
               "creation_date": 1428374520,
               "last_edit_date": 1428498786,
               "is_accepted": false,
               "id": "29482830",
               "down_vote_count": 1,
               "score": 23
            },
            {
               "up_vote_count": 86,
               "answer_id": 29482936,
               "last_activity_date": 1429899942,
               "path": "3.stack.answer",
               "body_markdown": "Here are some benchmarks for the various answers to this question. There were some surprising results, including wildly different performance depending on the string being tested.\r\n\r\nSome functions were modified to work with Python 3 (mainly by replacing `/` with `//` to ensure integer division). If you see something wrong, want to add your function, or want to add another test string, ping @ZeroPiraeus in the [Python chatroom][1].\r\n\r\nIn summary: there&#39;s about a 50x difference between the best- and worst-performing solutions for the large set of example data supplied by OP [here][2] (via [this][3] comment). [David Zhang&#39;s solution][4] is the clear winner, outperforming all others by around 5x for the large example set.\r\n\r\nA couple of the answers are *very* slow in extremely large &quot;no match&quot; cases. Otherwise, the functions seem to be equally matched or clear winners depending on the test.\r\n\r\nHere are the results, including plots made using matplotlib and seaborn to show the different distributions:\r\n\r\n---\r\n\r\n**Corpus 1 (supplied examples - small set)**\r\n\r\n    mean performance:\r\n     0.0003  david_zhang\r\n     0.0009  zero\r\n     0.0013  antti\r\n     0.0013  tigerhawk_2\r\n     0.0015  carpetpython\r\n     0.0029  tigerhawk_1\r\n     0.0031  davidism\r\n     0.0035  saksham\r\n     0.0046  shashank\r\n     0.0052  riad\r\n     0.0056  piotr\r\n    \r\n    median performance:\r\n     0.0003  david_zhang\r\n     0.0008  zero\r\n     0.0013  antti\r\n     0.0013  tigerhawk_2\r\n     0.0014  carpetpython\r\n     0.0027  tigerhawk_1\r\n     0.0031  davidism\r\n     0.0038  saksham\r\n     0.0044  shashank\r\n     0.0054  riad\r\n     0.0058  piotr\r\n\r\n[![Corpus 1 graph][5]][5]\r\n\r\n---\r\n\r\n**Corpus 2 (supplied examples - large set)**\r\n\r\n    mean performance:\r\n     0.0006  david_zhang\r\n     0.0036  tigerhawk_2\r\n     0.0036  antti\r\n     0.0037  zero\r\n     0.0039  carpetpython\r\n     0.0052  shashank\r\n     0.0056  piotr\r\n     0.0066  davidism\r\n     0.0120  tigerhawk_1\r\n     0.0177  riad\r\n     0.0283  saksham\r\n    \r\n    median performance:\r\n     0.0004  david_zhang\r\n     0.0018  zero\r\n     0.0022  tigerhawk_2\r\n     0.0022  antti\r\n     0.0024  carpetpython\r\n     0.0043  davidism\r\n     0.0049  shashank\r\n     0.0055  piotr\r\n     0.0061  tigerhawk_1\r\n     0.0077  riad\r\n     0.0109  saksham\r\n\r\n[![Corpus 1 graph][6]][6]\r\n\r\n---\r\n\r\n**Corpus 3 (edge cases)**\r\n\r\n    mean performance:\r\n     0.0123  shashank\r\n     0.0375  david_zhang\r\n     0.0376  piotr\r\n     0.0394  carpetpython\r\n     0.0479  antti\r\n     0.0488  tigerhawk_2\r\n     0.2269  tigerhawk_1\r\n     0.2336  davidism\r\n     0.7239  saksham\r\n     3.6265  zero\r\n     6.0111  riad\r\n    \r\n    median performance:\r\n     0.0107  tigerhawk_2\r\n     0.0108  antti\r\n     0.0109  carpetpython\r\n     0.0135  david_zhang\r\n     0.0137  tigerhawk_1\r\n     0.0150  shashank\r\n     0.0229  saksham\r\n     0.0255  piotr\r\n     0.0721  davidism\r\n     0.1080  zero\r\n     1.8539  riad\r\n\r\n[![Corpus 3 graph][7]][7]\r\n\r\n---\r\n\r\nThe tests and raw results are available [here][8].\r\n\r\n  [1]: http://chat.stackoverflow.com/rooms/6/python\r\n  [2]: http://paste.ubuntu.com/10765231/\r\n  [3]: https://stackoverflow.com/questions/29481088/how-can-i-tell-if-a-string-repeats-itself-in-python#comment47156601_29481088\r\n  [4]: https://stackoverflow.com/a/29489919\r\n  [5]: http://i.stack.imgur.com/Xx34F.png\r\n  [6]: http://i.stack.imgur.com/KZgxr.png\r\n  [7]: http://i.stack.imgur.com/w0qqT.png\r\n  [8]: https://bitbucket.org/snippets/schesis/nMnR/benchmarking-answers-to-http",
               "tags": [],
               "creation_date": 1428375324,
               "last_edit_date": 1495535492,
               "is_accepted": false,
               "id": "29482936",
               "down_vote_count": 3,
               "score": 83
            },
            {
               "up_vote_count": 17,
               "answer_id": 29484962,
               "last_activity_date": 1428565882,
               "path": "3.stack.answer",
               "body_markdown": "This version tries only those candidate sequence lengths that are factors of the string length; and uses the `*` operator to build a full-length string from the candidate sequence:\r\n\r\n    def get_shortest_repeat(string):\r\n        length = len(string)\r\n        for i in range(1, length // 2 + 1):\r\n            if length % i:  # skip non-factors early\r\n                continue\r\n\r\n            candidate = string[:i]\r\n            if string == candidate * (length // i):\r\n                return candidate\r\n    \r\n        return None\r\n\r\nThanks to TigerhawkT3 for noticing that `length // 2` without `+ 1` would fail to match the `abab` case.",
               "tags": [],
               "creation_date": 1428387215,
               "last_edit_date": 1428565882,
               "is_accepted": false,
               "id": "29484962",
               "down_vote_count": 1,
               "score": 16
            },
            {
               "up_vote_count": 555,
               "answer_id": 29489919,
               "last_activity_date": 1428801135,
               "path": "3.stack.answer",
               "body_markdown": "Here&#39;s a concise solution which avoids regular expressions and slow in-Python loops:\r\n\r\n    def principal_period(s):\r\n        i = (s+s).find(s, 1, -1)\r\n        return None if i == -1 else s[:i]\r\n\r\nSee the [Community Wiki answer](https://stackoverflow.com/a/29482936/2447250) started by @davidism for benchmark results. In summary,\r\n\r\n&gt; David Zhang&#39;s solution is the clear winner, outperforming all others by at least 5x for the large example set.\r\n\r\n(That answer&#39;s words, not mine.)\r\n\r\nThis is based on the observation that a string is periodic if and only if it is equal to a nontrivial rotation of itself. Kudos to @AleksiTorhamo for realizing that we can then recover the principal period from the index of the first occurrence of `s` in `(s+s)[1:-1]`, and for informing me of the optional `start` and `end` arguments of Python&#39;s `string.find`.",
               "tags": [],
               "creation_date": 1428404290,
               "last_edit_date": 1495535492,
               "is_accepted": true,
               "id": "29489919",
               "down_vote_count": 1,
               "score": 554
            },
            {
               "up_vote_count": 16,
               "answer_id": 29519746,
               "last_activity_date": 1428515082,
               "path": "3.stack.answer",
               "body_markdown": "The problem may also be solved in `O(n)` in worst case with prefix function.\r\n\r\nNote, it may be slower in general case(UPD: and is much slower) than other solutions which depend on number of divisors of `n`, but usually find fails sooner, I think one of bad cases for them will be `aaa....aab`, where there are `n - 1 = 2 * 3 * 5 * 7 ... *p_n - 1` `a`&#39;s\r\n\r\nFirst of all you need to calculate prefix function\r\n\r\n    def prefix_function(s):\r\n        n = len(s)\r\n        pi = [0] * n\r\n        for i in xrange(1, n):\r\n            j = pi[i - 1]\r\n            while(j &gt; 0 and s[i] != s[j]):\r\n                j = pi[j - 1]\r\n            if (s[i] == s[j]):\r\n                j += 1\r\n            pi[i] = j;\r\n        return pi\r\n\r\nthen either there&#39;s no answer or the shortest period is\r\n\r\n    k = len(s) - prefix_function(s[-1])\r\n\r\nand you just have to check if `k != n and n % k == 0` (if `k != n and n % k == 0` then answer is `s[:k]`, else there&#39;s no answer\r\n\r\nYou may check the proof [here](http://e-maxx.ru/algo/prefix_function#header_12) (in Russian, but online translator will probably do the trick)\r\n\r\n\r\n    def riad(s):\r\n        n = len(s)\r\n        pi = [0] * n\r\n        for i in xrange(1, n):\r\n            j = pi[i - 1]\r\n            while(j &gt; 0 and s[i] != s[j]):\r\n                j = pi[j - 1]\r\n            if (s[i] == s[j]):\r\n                j += 1\r\n            pi[i] = j;\r\n        k = n - pi[-1]\r\n        return s[:k] if (n != k and n % k == 0) else None\r\n\r\n",
               "tags": [],
               "creation_date": 1428509078,
               "last_edit_date": 1428515082,
               "is_accepted": false,
               "id": "29519746",
               "down_vote_count": 0,
               "score": 16
            },
            {
               "up_vote_count": 10,
               "answer_id": 29543743,
               "last_activity_date": 1428596489,
               "path": "3.stack.answer",
               "body_markdown": "I started with more than eight solutions to this problem. Some were bases on regex (match, findall, split), some of string slicing and testing, and some with string methods (find, count, split). Each had benefits in code clarity, code size, speed and memory consumption. I was going to post my answer here when I noticed that execution speed was ranked as important, so I did more testing and improvement to arrive at this:\r\n\r\n&lt;!-- language-all: lang-python --&gt;\r\n\r\n    def repeating(s):\r\n        size = len(s)\r\n        incr = size % 2 + 1\r\n        for n in xrange(1, size//2+1, incr):\r\n            if size % n == 0:\r\n                if s[:n] * (size//n) == s:\r\n                    return s[:n]\r\n\r\nThis answer seems similar to a few other answers here, but it has a few speed optimisations others have not used:\r\n\r\n- `xrange` is a little faster in this application,\r\n- if an input string is an odd length, do not check any even length substrings,\r\n- by using `s[:n]` directly, we avoid creating a variable in each loop.\r\n\r\nI would be interested to see how this performs in the standard tests with common hardware. I believe it will be well short of David Zhang&#39;s excellent algorithm in most tests, but should be quite fast otherwise.\r\n\r\nI found this problem to be very counter-intuitive. The solutions I thought would be fast were slow. The solutions that looked slow were fast! It seems that Python&#39;s string creation with the multiply operator and string comparisons are highly optimised.\r\n\r\n",
               "tags": [],
               "creation_date": 1428596101,
               "last_edit_date": 1428596489,
               "is_accepted": false,
               "id": "29543743",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "up_vote_count": 2,
               "answer_id": 29785400,
               "last_activity_date": 1429912646,
               "path": "3.stack.answer",
               "body_markdown": "This function runs very quickly (tested and it&#39;s over 3 times faster than fastest solution here on strings with over 100k characters and the difference gets bigger the longer the repeating pattern is). It tries to minimise the number of comparisons needed to get the answer:\r\n\r\n    def repeats(string):\r\n        n = len(string)\r\n        tried = set([])\r\n        best = None\r\n        nums = [i for i in  xrange(2, int(n**0.5) + 1) if n % i == 0]\r\n        nums = [n/i for i in nums if n/i!=i] + list(reversed(nums)) + [1]\r\n        for s in nums:\r\n            if all(t%s for t in tried):\r\n                print &#39;Trying repeating string of length:&#39;, s\r\n                if string[:s]*(n/s)==string:\r\n                    best = s\r\n                else:\r\n                    tried.add(s)\r\n        if best:\r\n            return string[:best]\r\n\r\nNote that for example for string of length 8 it checks only fragment of size 4 and it does not have to test further because pattern of length 1 or 2 would result in repeating pattern of length 4:\r\n\r\n    &gt;&gt;&gt; repeats(&#39;12345678&#39;)\r\n    Trying repeating string of length: 4\r\n    None\r\n\r\n    # for this one we need only 2 checks \r\n    &gt;&gt;&gt; repeats(&#39;1234567812345678&#39;)\r\n    Trying repeating string of length: 8\r\n    Trying repeating string of length: 4\r\n    &#39;12345678&#39;",
               "tags": [],
               "creation_date": 1429661441,
               "last_edit_date": 1429912646,
               "is_accepted": false,
               "id": "29785400",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 1,
               "answer_id": 29880583,
               "last_activity_date": 1430112387,
               "path": "3.stack.answer",
               "body_markdown": "*Here is the code in python that checks for repetition of sub string in the main string given by the user*.\r\n\r\n\r\n    print &quot;Enter a string....&quot;\r\n    #mainstring = String given by user\r\n    mainstring=raw_input(&quot;&gt;&quot;)\r\n    if(mainstring==&#39;&#39;):\r\n\t    print &quot;Invalid string&quot;\r\n\t    exit()\r\n    #charlist = Character list of mainstring\r\n    charlist=list(mainstring)\r\n    strarr=&#39;&#39;\r\n    print &quot;Length of your string :&quot;,len(mainstring)\r\n    for i in range(0,len(mainstring)):\r\n\t    strarr=strarr+charlist[i]\r\n\t    splitlist=mainstring.split(strarr)\r\n\t    count = 0\r\n\t    for j in splitlist:\r\n\t\t    if j ==&#39;&#39;:\r\n\t\t\t    count+=1\r\n\t    if count == len(splitlist):\r\n\t\t    break\r\n    if count == len(splitlist):\r\n\t    if count == 2:\r\n\t\t    print &quot;No repeating Sub-String found in string %r&quot;%(mainstring)\r\n\r\n\t    else:\r\n\t\t    print &quot;Sub-String %r repeats in string %r&quot;%(strarr,mainstring)\r\n    else :\r\n\t    print &quot;No repeating Sub-String found in string %r&quot;%(mainstring)\r\n\r\n*Input*:  \r\n&gt; 0045662100456621004566210045662100456621  \r\n  \r\n*Output* :\r\n&gt;Length of your string : 40    \r\n  \r\n&gt;Sub-String &#39;00456621&#39; repeats in string &#39;0045662100456621004566210045662100456621&#39; \r\n  \r\n*Input* :\r\n&gt;004608294930875576036866359447\r\n  \r\n*Output*:  \r\n&gt;Length of your string : 30\r\n  \r\n&gt;No repeating Sub-String found in string &#39;004608294930875576036866359447&#39;",
               "tags": [],
               "creation_date": 1430067340,
               "last_edit_date": 1430112387,
               "is_accepted": false,
               "id": "29880583",
               "down_vote_count": 2,
               "score": -1
            },
            {
               "up_vote_count": 1,
               "answer_id": 41294035,
               "last_activity_date": 1482456971,
               "path": "3.stack.answer",
               "body_markdown": "In David Zhang&#39;s answer if we have some sort of circular buffer this will not work: `principal_period(&#39;6210045662100456621004566210045662100456621&#39;)` due to the starting `621`, where I would have liked it to spit out: `00456621`.\r\n\r\nExtending his solution we can use the following: \r\n\r\n    def principal_period(s):\r\n        for j in range(int(len(s)/2)):\r\n            idx = (s[j:]+s[j:]).find(s[j:], 1, -1)\r\n            if idx != -1:\r\n                # Make sure that the first substring is part of pattern\r\n                if s[:j] == s[j:][:idx][-j:]:\r\n                    break\r\n        \r\n        return None if idx == -1 else s[j:][:idx]\r\n    \r\n    principal_period(&#39;6210045662100456621004566210045662100456621&#39;)\r\n    &gt;&gt;&gt; &#39;00456621&#39;",
               "tags": [],
               "creation_date": 1482456652,
               "last_edit_date": 1482456971,
               "is_accepted": false,
               "id": "41294035",
               "down_vote_count": 0,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/29481088/how-can-i-tell-if-a-string-repeats-itself-in-python",
         "id": "858127-2319"
      },
      {
         "up_vote_count": "461",
         "path": "2.stack",
         "body_markdown": "I have the following indexed DataFrame with named columns and rows not- continuous numbers:\r\n\r\n              a         b         c         d\r\n    2  0.671399  0.101208 -0.181532  0.241273\r\n    3  0.446172 -0.243316  0.051767  1.577318\r\n    5  0.614758  0.075793 -0.451460 -0.012493\r\n\r\n\r\nI would like to add a new column, `&#39;e&#39;`, to the existing data frame and do not want to change anything in the data frame (i.e., the new column always has the same length as the DataFrame). \r\n\r\n    0   -0.335485\r\n    1   -1.166658\r\n    2   -0.385571\r\n    dtype: float64\r\n\r\nI tried different versions of `join`, `append`, `merge`, but I did not get the result I wanted, only errors at most. How can I add column `e` to the above example? ",
         "view_count": "816084",
         "answer_count": "20",
         "tags": "['python', 'pandas', 'dataframe', 'chained-assignment']",
         "creation_date": "1348426801",
         "last_edit_date": "1512147126",
         "code_snippet": "['<code>          a         b         c         d\\n2  0.671399  0.101208 -0.181532  0.241273\\n3  0.446172 -0.243316  0.051767  1.577318\\n5  0.614758  0.075793 -0.451460 -0.012493\\n</code>', \"<code>'e'</code>\", '<code>0   -0.335485\\n1   -1.166658\\n2   -0.385571\\ndtype: float64\\n</code>', '<code>join</code>', '<code>append</code>', '<code>merge</code>', '<code>e</code>', \"<code>df1['e'] = Series(np.random.randn(sLength), index=df1.index)\\n</code>\", '<code>SettingWithCopyWarning</code>', \"<code>&gt;&gt;&gt; sLength = len(df1['a'])\\n&gt;&gt;&gt; df1\\n          a         b         c         d\\n6 -0.269221 -0.026476  0.997517  1.294385\\n8  0.917438  0.847941  0.034235 -0.448948\\n\\n&gt;&gt;&gt; df1['e'] = p.Series(np.random.randn(sLength), index=df1.index)\\n&gt;&gt;&gt; df1\\n          a         b         c         d         e\\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167\\n8  0.917438  0.847941  0.034235 -0.448948  2.228131\\n\\n&gt;&gt;&gt; p.version.short_version\\n'0.16.1'\\n</code>\", '<code>SettingWithCopyWarning</code>', \"<code>&gt;&gt;&gt; df1.loc[:,'f'] = p.Series(np.random.randn(sLength), index=df1.index)\\n&gt;&gt;&gt; df1\\n          a         b         c         d         e         f\\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927\\n8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109\\n&gt;&gt;&gt; \\n</code>\", '<code>assign</code>', '<code>df1 = df1.assign(e=p.Series(np.random.randn(sLength)).values)\\n</code>', '<code>SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_index,col_indexer] = value instead</code>', '<code>df1.assign(e = Series(np.random.randn(sLength), index=df1.index))</code>', \"<code>df['e'] = e</code>\", '<code>e</code>', '<code>Series(np.random.randn(sLength))</code>', \"<code>my_dataframe = pd.DataFrame(columns=('foo', 'bar'))</code>\", '<code>e</code>', '<code>df1</code>', '<code>e</code>', '<code>e</code>', \"<code>df['e'] = e.values\\n</code>\", '<code>assign</code>', '<code>df1 = df1.assign(e=e.values)\\n</code>', '<code>assign</code>', \"<code>df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\\n&gt;&gt;&gt; df.assign(mean_a=df.a.mean(), mean_b=df.b.mean())\\n   a  b  mean_a  mean_b\\n0  1  3     1.5     3.5\\n1  2  4     1.5     3.5\\n</code>\", \"<code>np.random.seed(0)\\ndf1 = pd.DataFrame(np.random.randn(10, 4), columns=['a', 'b', 'c', 'd'])\\nmask = df1.applymap(lambda x: x &lt;-0.7)\\ndf1 = df1[-mask.any(axis=1)]\\nsLength = len(df1['a'])\\ne = pd.Series(np.random.randn(sLength))\\n\\n&gt;&gt;&gt; df1\\n          a         b         c         d\\n0  1.764052  0.400157  0.978738  2.240893\\n2 -0.103219  0.410599  0.144044  1.454274\\n3  0.761038  0.121675  0.443863  0.333674\\n7  1.532779  1.469359  0.154947  0.378163\\n9  1.230291  1.202380 -0.387327 -0.302303\\n\\n&gt;&gt;&gt; e\\n0   -1.048553\\n1   -1.420018\\n2   -1.706270\\n3    1.950775\\n4   -0.509652\\ndtype: float64\\n\\ndf1 = df1.assign(e=e.values)\\n\\n&gt;&gt;&gt; df1\\n          a         b         c         d         e\\n0  1.764052  0.400157  0.978738  2.240893 -1.048553\\n2 -0.103219  0.410599  0.144044  1.454274 -1.420018\\n3  0.761038  0.121675  0.443863  0.333674 -1.706270\\n7  1.532779  1.469359  0.154947  0.378163  1.950775\\n9  1.230291  1.202380 -0.387327 -0.302303 -0.509652\\n</code>\", \"<code>df['e'] = e.values</code>\", '<code>df.assign</code>', '<code>assign</code>', \"<code>df.assign(**df.mean().add_prefix('mean_'))</code>\", \"<code>df1['e'] = np.random.randn(sLength)\\n</code>\", '<code>map</code>', \"<code>df1['e'] = df1['a'].map(lambda x: np.random.random())\\n</code>\", '<code>.map</code>', '<code>lambda</code>', \"<code>df1['e'] = df1['a'].map(lambda x: e)</code>\", \"<code>df1['e'] = df1['a'].map(e)</code>\", '<code>e</code>', '<code>map</code>', \"<code>df['e']=e</code>\", '<code>df1 = df1.assign(e=np.random.randn(sLength))</code>', '<code>SettingWithCopyWarning</code>', \"<code>df.insert(len(df.columns), 'e', pd.Series(np.random.randn(sLength),  index=df.index))\\n</code>\", '<code>__getitem__</code>', '<code>[]</code>', '<code>__setitem__</code>', '<code>[] =</code>', '<code>[]</code>', \"<code>    size      name color\\n0    big      rose   red\\n1  small    violet  blue\\n2  small     tulip   red\\n3  small  harebell  blue\\n\\ndf['protected'] = ['no', 'no', 'no', 'yes']\\n\\n    size      name color protected\\n0    big      rose   red        no\\n1  small    violet  blue        no\\n2  small     tulip   red        no\\n3  small  harebell  blue       yes\\n</code>\", \"<code>df.index = [3,2,1,0]\\ndf['protected'] = ['no', 'no', 'no', 'yes']\\n    size      name color protected\\n3    big      rose   red        no\\n2  small    violet  blue        no\\n1  small     tulip   red        no\\n0  small  harebell  blue       yes\\n</code>\", '<code>pd.Series</code>', \"<code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes'])\\n    size      name color protected\\n3    big      rose   red       yes\\n2  small    violet  blue        no\\n1  small     tulip   red        no\\n0  small  harebell  blue        no\\n</code>\", '<code>pd.Series</code>', '<code>[] =</code>', '<code>[] =</code>', \"<code>df['column'] = series</code>\", '<code>[]=</code>', '<code>[]=</code>', '<code>pd.Series</code>', '<code>pd.Series</code>', '<code>np.ndarray</code>', '<code>list</code>', \"<code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes']).values\\n</code>\", \"<code>df['protected'] = list(pd.Series(['no', 'no', 'no', 'yes']))\\n</code>\", '<code>pd.Series</code>', '<code>df</code>', \"<code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes'], index=df.index)\\n</code>\", '<code>pd.Series</code>', \"<code>protected_series = pd.Series(['no', 'no', 'no', 'yes'])\\nprotected_series.index = df.index\\n\\n3     no\\n2     no\\n1     no\\n0    yes\\n</code>\", \"<code>df['protected'] = protected_series\\n\\n    size      name color protected\\n3    big      rose   red        no\\n2  small    violet  blue        no\\n1  small     tulip   red        no\\n0  small  harebell  blue       yes\\n</code>\", '<code>df.reset_index()</code>', \"<code>df.reset_index(drop=True)\\nprotected_series.reset_index(drop=True)\\ndf['protected'] = protected_series\\n\\n    size      name color protected\\n0    big      rose   red        no\\n1  small    violet  blue        no\\n2  small     tulip   red        no\\n3  small  harebell  blue       yes\\n</code>\", '<code>df.assign</code>', '<code>df.assign</code>', '<code>[]=</code>', \"<code>df.assign(protected=pd.Series(['no', 'no', 'no', 'yes']))\\n    size      name color protected\\n3    big      rose   red       yes\\n2  small    violet  blue        no\\n1  small     tulip   red        no\\n0  small  harebell  blue        no\\n</code>\", '<code>df.assign</code>', '<code>self</code>', '<code>df.assign</code>', \"<code>df.assign(self=pd.Series(['no', 'no', 'no', 'yes'])\\nTypeError: assign() got multiple values for keyword argument 'self'\\n</code>\", '<code>self</code>', '<code>[] =</code>', '<code>[]=</code>', '<code>None</code>', \"<code>df1['e'] = None</code>\", '<code>.loc</code>', '<code>SettingWithCopyWarning</code>', '<code>df.insert()</code>', \"<code>dict['a']['e']</code>\", \"<code>'e'</code>\", \"<code>dict['a']</code>\", '<code>pd.options.mode.chained_assignment = None</code>', \"<code>df.loc[:, 'NewCol'] = 'New_Val'\\n</code>\", \"<code>df = pd.DataFrame(data=np.random.randn(20, 4), columns=['A', 'B', 'C', 'D'])\\n\\ndf\\n\\n           A         B         C         D\\n0  -0.761269  0.477348  1.170614  0.752714\\n1   1.217250 -0.930860 -0.769324 -0.408642\\n2  -0.619679 -1.227659 -0.259135  1.700294\\n3  -0.147354  0.778707  0.479145  2.284143\\n4  -0.529529  0.000571  0.913779  1.395894\\n5   2.592400  0.637253  1.441096 -0.631468\\n6   0.757178  0.240012 -0.553820  1.177202\\n7  -0.986128 -1.313843  0.788589 -0.707836\\n8   0.606985 -2.232903 -1.358107 -2.855494\\n9  -0.692013  0.671866  1.179466 -1.180351\\n10 -1.093707 -0.530600  0.182926 -1.296494\\n11 -0.143273 -0.503199 -1.328728  0.610552\\n12 -0.923110 -1.365890 -1.366202 -1.185999\\n13 -2.026832  0.273593 -0.440426 -0.627423\\n14 -0.054503 -0.788866 -0.228088 -0.404783\\n15  0.955298 -1.430019  1.434071 -0.088215\\n16 -0.227946  0.047462  0.373573 -0.111675\\n17  1.627912  0.043611  1.743403 -0.012714\\n18  0.693458  0.144327  0.329500 -0.655045\\n19  0.104425  0.037412  0.450598 -0.923387\\n\\n\\ndf.drop([3, 5, 8, 10, 18], inplace=True)\\n\\ndf\\n\\n           A         B         C         D\\n0  -0.761269  0.477348  1.170614  0.752714\\n1   1.217250 -0.930860 -0.769324 -0.408642\\n2  -0.619679 -1.227659 -0.259135  1.700294\\n4  -0.529529  0.000571  0.913779  1.395894\\n6   0.757178  0.240012 -0.553820  1.177202\\n7  -0.986128 -1.313843  0.788589 -0.707836\\n9  -0.692013  0.671866  1.179466 -1.180351\\n11 -0.143273 -0.503199 -1.328728  0.610552\\n12 -0.923110 -1.365890 -1.366202 -1.185999\\n13 -2.026832  0.273593 -0.440426 -0.627423\\n14 -0.054503 -0.788866 -0.228088 -0.404783\\n15  0.955298 -1.430019  1.434071 -0.088215\\n16 -0.227946  0.047462  0.373573 -0.111675\\n17  1.627912  0.043611  1.743403 -0.012714\\n19  0.104425  0.037412  0.450598 -0.923387\\n\\ndf.loc[:, 'NewCol'] = 0\\n\\ndf\\n           A         B         C         D  NewCol\\n0  -0.761269  0.477348  1.170614  0.752714       0\\n1   1.217250 -0.930860 -0.769324 -0.408642       0\\n2  -0.619679 -1.227659 -0.259135  1.700294       0\\n4  -0.529529  0.000571  0.913779  1.395894       0\\n6   0.757178  0.240012 -0.553820  1.177202       0\\n7  -0.986128 -1.313843  0.788589 -0.707836       0\\n9  -0.692013  0.671866  1.179466 -1.180351       0\\n11 -0.143273 -0.503199 -1.328728  0.610552       0\\n12 -0.923110 -1.365890 -1.366202 -1.185999       0\\n13 -2.026832  0.273593 -0.440426 -0.627423       0\\n14 -0.054503 -0.788866 -0.228088 -0.404783       0\\n15  0.955298 -1.430019  1.434071 -0.088215       0\\n16 -0.227946  0.047462  0.373573 -0.111675       0\\n17  1.627912  0.043611  1.743403 -0.012714       0\\n19  0.104425  0.037412  0.450598 -0.923387       0\\n</code>\", '<code>data.set_index([\\'index_column\\'], inplace=True)\\n\"if index is unsorted, assignment of a new column will fail\"        \\ndata.sort_index(inplace = True)\\ndata.loc[\\'index_value1\\', \\'column_y\\'] = np.random.randn(data.loc[\\'index_value1\\', \\'column_x\\'].shape[0])\\n</code>', \"<code>df1['e'] = Series(np.random.randn(sLength), index=df1.index)\\n</code>\", \"<code>data = pd.DataFrame(index=all_possible_values)\\ndf1['e'] = Series(np.random.randn(sLength), index=df1.index)\\n</code>\", \"<code>df = pd.DataFrame([[1, 2], [3, 4], [5,6]], columns=list('AB'))\\n\\nnewCol = [3,5,7]\\nnewName = 'C'\\n\\nvalues = np.insert(df.values,df.shape[1],newCol,axis=1)\\nheader = df.columns.values.tolist()\\nheader.append(newName)\\n\\ndf = pd.DataFrame(values,columns=header)\\n</code>\", '<code>SettingWithCopyWarning</code>', \"<code>df = df.copy()\\ndf['col_name'] = values\\n</code>\", \"<code> df1.loc[:,'e'] = Series(np.random.randn(sLength))\\n</code>\", '<code>In [44]: e\\nOut[44]:\\n0    1.225506\\n1   -1.033944\\n2   -0.498953\\n3   -0.373332\\n4    0.615030\\n5   -0.622436\\ndtype: float64\\n\\nIn [45]: df1\\nOut[45]:\\n          a         b         c         d\\n0 -0.634222 -0.103264  0.745069  0.801288\\n4  0.782387 -0.090279  0.757662 -0.602408\\n5 -0.117456  2.124496  1.057301  0.765466\\n7  0.767532  0.104304 -0.586850  1.051297\\n8 -0.103272  0.958334  1.163092  1.182315\\n9 -0.616254  0.296678 -0.112027  0.679112\\n</code>', '<code>In [46]: df1.eval(\"e = @e.values\", inplace=True)\\n\\nIn [47]: df1\\nOut[47]:\\n          a         b         c         d         e\\n0 -0.634222 -0.103264  0.745069  0.801288  1.225506\\n4  0.782387 -0.090279  0.757662 -0.602408 -1.033944\\n5 -0.117456  2.124496  1.057301  0.765466 -0.498953\\n7  0.767532  0.104304 -0.586850  1.051297 -0.373332\\n8 -0.103272  0.958334  1.163092  1.182315  0.615030\\n9 -0.616254  0.296678 -0.112027  0.679112 -0.622436\\n</code>', '<code>pandas.concat</code>', '<code>import pandas as pd\\ndf\\n#          a            b           c           d\\n#0  0.671399     0.101208   -0.181532    0.241273\\n#1  0.446172    -0.243316    0.051767    1.577318\\n#2  0.614758     0.075793   -0.451460   -0.012493\\n\\ne = pd.Series([-0.335485, -1.166658, -0.385571])    \\ne\\n#0   -0.335485\\n#1   -1.166658\\n#2   -0.385571\\n#dtype: float64\\n\\n# here we need to give the series object a name which converts to the new  column name \\n# in the result\\ndf = pd.concat([df, e.rename(\"e\")], axis=1)\\ndf\\n\\n#          a            b           c           d           e\\n#0  0.671399     0.101208   -0.181532    0.241273   -0.335485\\n#1  0.446172    -0.243316    0.051767    1.577318   -1.166658\\n#2  0.614758     0.075793   -0.451460   -0.012493   -0.385571\\n</code>', '<code>e.index = df.index\\ndf = pd.concat([df, e.rename(\"e\")], axis=1)\\n</code>', '<code>numpy.nan</code>', '<code>SettingWithCopyWarning</code>', '<code>numpy</code>', \"<code>col = 'column_name'\\ndf = df.assign(**{col:numpy.full(len(df), numpy.nan)})\\n</code>\", '<code>df[\"new_columns_name\"]=series_variable_name #this will do it for you\\n</code>']",
         "title": "Adding new column to existing DataFrame in Python pandas",
         "_childDocuments_": [
            {
               "up_vote_count": 33,
               "answer_id": 12555491,
               "last_activity_date": 1445346301,
               "path": "3.stack.answer",
               "body_markdown": "Doing this directly via [NumPy][1] will be the most efficient:\r\n\r\n    df1[&#39;e&#39;] = np.random.randn(sLength)\r\n\r\n---\r\n\r\nNote my original (very old) suggestion was to use `map` (which is much slower):\r\n\r\n    df1[&#39;e&#39;] = df1[&#39;a&#39;].map(lambda x: np.random.random())\r\n\r\n  [1]: http://en.wikipedia.org/wiki/NumPy\r\n\r\n",
               "tags": [],
               "creation_date": 1348428147,
               "last_edit_date": 1445346301,
               "is_accepted": false,
               "id": "12555491",
               "down_vote_count": 2,
               "score": 31
            },
            {
               "up_vote_count": 542,
               "answer_id": 12555510,
               "last_activity_date": 1483790879,
               "path": "3.stack.answer",
               "body_markdown": "Use the original df1 indexes to create the series:\r\n\r\n    df1[&#39;e&#39;] = Series(np.random.randn(sLength), index=df1.index)\r\n\r\n\r\n  \r\n\r\n  \r\n----------\r\n\r\n\r\n----------\r\n\r\n\r\n**Edit 2015**  \r\nSome reported to get the `SettingWithCopyWarning` with this code.  \r\nHowever, the code still runs perfect with the current pandas version 0.16.1.\r\n\r\n    &gt;&gt;&gt; sLength = len(df1[&#39;a&#39;])\r\n    &gt;&gt;&gt; df1\r\n              a         b         c         d\r\n    6 -0.269221 -0.026476  0.997517  1.294385\r\n    8  0.917438  0.847941  0.034235 -0.448948\r\n    \r\n    &gt;&gt;&gt; df1[&#39;e&#39;] = p.Series(np.random.randn(sLength), index=df1.index)\r\n    &gt;&gt;&gt; df1\r\n              a         b         c         d         e\r\n    6 -0.269221 -0.026476  0.997517  1.294385  1.757167\r\n    8  0.917438  0.847941  0.034235 -0.448948  2.228131\r\n\r\n    &gt;&gt;&gt; p.version.short_version\r\n    &#39;0.16.1&#39;\r\n\r\nThe `SettingWithCopyWarning` aims to inform of a possibly invalid assignment on a copy of the Dataframe. It doesn&#39;t necessarily say you did it wrong (it can trigger false positives) but from 0.13.0 it let you know there are more adequate methods for the same purpose. Then, if you get the warning, just follow its advise: *Try using .loc[row_index,col_indexer] = value instead*\r\n\r\n    &gt;&gt;&gt; df1.loc[:,&#39;f&#39;] = p.Series(np.random.randn(sLength), index=df1.index)\r\n    &gt;&gt;&gt; df1\r\n              a         b         c         d         e         f\r\n    6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927\r\n    8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109\r\n    &gt;&gt;&gt; \r\n\r\nIn fact, this is currently the more efficient method as [described in pandas docs][1]\r\n\r\n\r\n----------\r\n\r\n\r\n----------\r\n\r\n**Edit 2017**\r\n\r\nAs indicated in the comments and by @Alexander, currently the best method to add the values of a Series as a new column of a DataFrame could be using `assign`:\r\n\r\n    df1 = df1.assign(e=p.Series(np.random.randn(sLength)).values)\r\n\r\n\r\n\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\r\n",
               "tags": [],
               "creation_date": 1348428285,
               "last_edit_date": 1483790879,
               "is_accepted": true,
               "id": "12555510",
               "down_vote_count": 2,
               "score": 540
            },
            {
               "up_vote_count": 138,
               "answer_id": 13843741,
               "last_activity_date": 1481352838,
               "path": "3.stack.answer",
               "body_markdown": "This is the simple way of adding a new column: `df[&#39;e&#39;] = e`",
               "tags": [],
               "creation_date": 1355328271,
               "last_edit_date": 1481352838,
               "is_accepted": false,
               "id": "13843741",
               "down_vote_count": 25,
               "score": 113
            },
            {
               "up_vote_count": 4,
               "answer_id": 28634878,
               "last_activity_date": 1445346350,
               "path": "3.stack.answer",
               "body_markdown": "One thing to note, though, is that if you do\r\n\r\n    df1[&#39;e&#39;] = Series(np.random.randn(sLength), index=df1.index)\r\n\r\nthis will effectively be a **left** join on the df1.index. So if you want to have an **outer** join effect, my probably imperfect solution is to create a dataframe with index values covering the universe of your data, and then use the code above. For example,\r\n\r\n    data = pd.DataFrame(index=all_possible_values)\r\n    df1[&#39;e&#39;] = Series(np.random.randn(sLength), index=df1.index)\r\n",
               "tags": [],
               "creation_date": 1424453539,
               "last_edit_date": 1445346350,
               "is_accepted": false,
               "id": "28634878",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 13,
               "answer_id": 30777185,
               "last_activity_date": 1477049563,
               "path": "3.stack.answer",
               "body_markdown": "I got the dreaded `SettingWithCopyWarning`, and it wasn&#39;t fixed by using the iloc syntax. My DataFrame was created by read_sql from an ODBC source. Using a suggestion by lowtech above, the following worked for me:\r\n\r\n    df.insert(len(df.columns), &#39;e&#39;, pd.Series(np.random.randn(sLength),  index=df.index))\r\n\r\nThis worked fine to insert the column at the end. I don&#39;t know if it is the most efficient, but I don&#39;t like warning messages. I think there is a better solution, but I can&#39;t find it, and I think it depends on some aspect of the index.   \r\n*Note*. That this only works once and will give an error message if trying to overwrite and existing column.   \r\n**Note** As above and from 0.16.0 assign is the best solution. See documentation http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html#pandas.DataFrame.assign \r\nWorks well for data flow type where you don&#39;t overwrite your intermediate values.\r\n",
               "tags": [],
               "creation_date": 1434015904,
               "last_edit_date": 1477049563,
               "is_accepted": false,
               "id": "30777185",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 4,
               "answer_id": 30835776,
               "last_activity_date": 1434486435,
               "path": "3.stack.answer",
               "body_markdown": "Before assigning a new column, if you have indexed data, you need to sort the index. At least in my case I had to:\r\n\r\n    data.set_index([&#39;index_column&#39;], inplace=True)\r\n    &quot;if index is unsorted, assignment of a new column will fail&quot;        \r\n    data.sort_index(inplace = True)\r\n    data.loc[&#39;index_value1&#39;, &#39;column_y&#39;] = np.random.randn(data.loc[&#39;index_value1&#39;, &#39;column_x&#39;].shape[0])",
               "tags": [],
               "creation_date": 1434326256,
               "last_edit_date": 1434486435,
               "is_accepted": false,
               "id": "30835776",
               "down_vote_count": 0,
               "score": 4
            },
            {
               "up_vote_count": 5,
               "answer_id": 32960263,
               "last_activity_date": 1445346467,
               "path": "3.stack.answer",
               "body_markdown": "The following is what I did... But I&#39;m pretty new to pandas and really Python in general, so no promises.\r\n\r\n    df = pd.DataFrame([[1, 2], [3, 4], [5,6]], columns=list(&#39;AB&#39;))\r\n\r\n    newCol = [3,5,7]\r\n    newName = &#39;C&#39;\r\n\r\n    values = np.insert(df.values,df.shape[1],newCol,axis=1)\r\n    header = df.columns.values.tolist()\r\n    header.append(newName)\r\n\r\n    df = pd.DataFrame(values,columns=header)\r\n",
               "tags": [],
               "creation_date": 1444094332,
               "last_edit_date": 1445346467,
               "is_accepted": false,
               "id": "32960263",
               "down_vote_count": 1,
               "score": 4
            },
            {
               "up_vote_count": 5,
               "answer_id": 33283360,
               "last_activity_date": 1445523705,
               "path": "3.stack.answer",
               "body_markdown": "Let me just add that, just like for [hum3](https://stackoverflow.com/users/3649456/hum3), `.loc` didn&#39;t solve the `SettingWithCopyWarning` and I had to resort to `df.insert()`. In my case false positive was generated by &quot;fake&quot; chain indexing  `dict[&#39;a&#39;][&#39;e&#39;]`, where `&#39;e&#39;` is the new column, and `dict[&#39;a&#39;]` is a DataFrame coming from dictionary.\r\n\r\nAlso note that if you know what you are doing, you can switch of the warning using\r\n`pd.options.mode.chained_assignment = None`\r\nand than use one of the other solutions given here.",
               "tags": [],
               "creation_date": 1445523705,
               "last_edit_date": 1495540056,
               "is_accepted": false,
               "id": "33283360",
               "down_vote_count": 0,
               "score": 5
            },
            {
               "up_vote_count": 72,
               "answer_id": 35387129,
               "last_activity_date": 1494530244,
               "path": "3.stack.answer",
               "body_markdown": "&gt; I would like to add a new column, &#39;e&#39;, to the existing data frame and do not change anything in the data frame. (The series always got the same length as a dataframe.) \r\n\r\n\r\nI assume that the index values in `e` match those in `df1`.\r\n\r\nThe easiest way to initiate a new column named `e`, and assign it the values from your series `e`:\r\n\r\n    df[&#39;e&#39;] = e.values\r\n\r\n\r\n**assign (Pandas 0.16.0+)**\r\n\r\n\r\nAs of Pandas 0.16.0, you can also use [`assign`][1], which assigns new columns to a DataFrame and returns a new object (a copy) with all the original columns in addition to the new ones.\r\n\r\n    df1 = df1.assign(e=e.values)\r\n\r\nAs per [this example][2] (which also includes the source code of the `assign` function), you can also include more than one column:\r\n\r\n    df = pd.DataFrame({&#39;a&#39;: [1, 2], &#39;b&#39;: [3, 4]})\r\n    &gt;&gt;&gt; df.assign(mean_a=df.a.mean(), mean_b=df.b.mean())\r\n       a  b  mean_a  mean_b\r\n    0  1  3     1.5     3.5\r\n    1  2  4     1.5     3.5\r\n\r\nIn context with your example: \r\n\r\n    np.random.seed(0)\r\n    df1 = pd.DataFrame(np.random.randn(10, 4), columns=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])\r\n    mask = df1.applymap(lambda x: x &lt;-0.7)\r\n    df1 = df1[-mask.any(axis=1)]\r\n    sLength = len(df1[&#39;a&#39;])\r\n    e = pd.Series(np.random.randn(sLength))\r\n    \r\n    &gt;&gt;&gt; df1\r\n              a         b         c         d\r\n    0  1.764052  0.400157  0.978738  2.240893\r\n    2 -0.103219  0.410599  0.144044  1.454274\r\n    3  0.761038  0.121675  0.443863  0.333674\r\n    7  1.532779  1.469359  0.154947  0.378163\r\n    9  1.230291  1.202380 -0.387327 -0.302303\r\n    \r\n    &gt;&gt;&gt; e\r\n    0   -1.048553\r\n    1   -1.420018\r\n    2   -1.706270\r\n    3    1.950775\r\n    4   -0.509652\r\n    dtype: float64\r\n    \r\n    df1 = df1.assign(e=e.values)\r\n    \r\n    &gt;&gt;&gt; df1\r\n              a         b         c         d         e\r\n    0  1.764052  0.400157  0.978738  2.240893 -1.048553\r\n    2 -0.103219  0.410599  0.144044  1.454274 -1.420018\r\n    3  0.761038  0.121675  0.443863  0.333674 -1.706270\r\n    7  1.532779  1.469359  0.154947  0.378163  1.950775\r\n    9  1.230291  1.202380 -0.387327 -0.302303 -0.509652\r\n    \r\nThe description of this new feature when it was first introduced can be found [here][3].\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html\r\n  [2]: https://stackoverflow.com/questions/42101382/pandas-dataframe-assign-arguments\r\n  [3]: http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#dataframe-assign",
               "tags": [],
               "creation_date": 1455410998,
               "last_edit_date": 1495541912,
               "is_accepted": false,
               "id": "35387129",
               "down_vote_count": 0,
               "score": 72
            },
            {
               "up_vote_count": 8,
               "answer_id": 35835705,
               "last_activity_date": 1457323233,
               "path": "3.stack.answer",
               "body_markdown": "If you get the `SettingWithCopyWarning`, an easy fix is to copy the DataFrame you are trying to add a column to.\r\n\r\n    df = df.copy()\r\n    df[&#39;col_name&#39;] = values\r\n",
               "tags": [],
               "creation_date": 1457321334,
               "last_edit_date": 1457323233,
               "is_accepted": false,
               "id": "35835705",
               "down_vote_count": 4,
               "score": 4
            },
            {
               "up_vote_count": 24,
               "answer_id": 38510820,
               "last_activity_date": 1493332228,
               "path": "3.stack.answer",
               "body_markdown": "It seems that in recent Pandas versions the way to go is to use [df.assign](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#assigning-new-columns-in-method-chains):\r\n\r\n`df1 = df1.assign(e=np.random.randn(sLength))`\r\n\r\nIt doesn&#39;t produce SettingWithCopyWarning.",
               "tags": [],
               "creation_date": 1469122537,
               "last_edit_date": 1493332228,
               "is_accepted": false,
               "id": "38510820",
               "down_vote_count": 0,
               "score": 24
            },
            {
               "up_vote_count": 3,
               "answer_id": 40480710,
               "last_activity_date": 1480888229,
               "path": "3.stack.answer",
               "body_markdown": "To add a new column, &#39;e&#39;, to the existing data frame \r\n\r\n     df1.loc[:,&#39;e&#39;] = Series(np.random.randn(sLength))",
               "tags": [],
               "creation_date": 1478588137,
               "last_edit_date": 1480888229,
               "is_accepted": false,
               "id": "40480710",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "up_vote_count": 3,
               "answer_id": 41641283,
               "last_activity_date": 1484332494,
               "path": "3.stack.answer",
               "body_markdown": "I was looking for a general way of adding a column of `numpy.nan`s to a dataframe without getting the dumb `SettingWithCopyWarning`.\r\n\r\nFrom the following:\r\n\r\n* the answers here\r\n* [this question][1] about passing a variable as a keyword argument\r\n* [this method][2] for generating a `numpy` array of NaNs in-line\r\n\r\nI came up with this:\r\n\r\n    col = &#39;column_name&#39;\r\n    df = df.assign(**{col:numpy.full(len(df), numpy.nan)})\r\n\r\n\r\n  [1]: https://stackoverflow.com/q/6976658/943773\r\n  [2]: https://stackoverflow.com/a/22414429/943773",
               "tags": [],
               "creation_date": 1484332494,
               "last_edit_date": 1495542402,
               "is_accepted": false,
               "id": "41641283",
               "down_vote_count": 1,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 42797207,
               "is_accepted": false,
               "last_activity_date": 1489528184,
               "body_markdown": "For the sake of completeness - yet another solution using [DataFrame.eval()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.eval.html) method:\r\n\r\nData:\r\n\r\n    In [44]: e\r\n    Out[44]:\r\n    0    1.225506\r\n    1   -1.033944\r\n    2   -0.498953\r\n    3   -0.373332\r\n    4    0.615030\r\n    5   -0.622436\r\n    dtype: float64\r\n    \r\n    In [45]: df1\r\n    Out[45]:\r\n              a         b         c         d\r\n    0 -0.634222 -0.103264  0.745069  0.801288\r\n    4  0.782387 -0.090279  0.757662 -0.602408\r\n    5 -0.117456  2.124496  1.057301  0.765466\r\n    7  0.767532  0.104304 -0.586850  1.051297\r\n    8 -0.103272  0.958334  1.163092  1.182315\r\n    9 -0.616254  0.296678 -0.112027  0.679112\r\n\r\nSolution:\r\n    \r\n    In [46]: df1.eval(&quot;e = @e.values&quot;, inplace=True)\r\n    \r\n    In [47]: df1\r\n    Out[47]:\r\n              a         b         c         d         e\r\n    0 -0.634222 -0.103264  0.745069  0.801288  1.225506\r\n    4  0.782387 -0.090279  0.757662 -0.602408 -1.033944\r\n    5 -0.117456  2.124496  1.057301  0.765466 -0.498953\r\n    7  0.767532  0.104304 -0.586850  1.051297 -0.373332\r\n    8 -0.103272  0.958334  1.163092  1.182315  0.615030\r\n    9 -0.616254  0.296678 -0.112027  0.679112 -0.622436\r\n\r\n",
               "id": "42797207",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1489528184,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 43180437,
               "is_accepted": false,
               "last_activity_date": 1491209962,
               "body_markdown": "#Super simple column assignment\r\nA pandas dataframe is implemented as an ordered dict of columns.\r\n\r\nThis means that the `__getitem__` `[]` can not only be used to get a certain column, but `__setitem__` `[] =` can be used to assign a new column.\r\n\r\nFor example, this dataframe can have a column added to it by simply using the `[]` accessor\r\n\r\n        size      name color\r\n    0    big      rose   red\r\n    1  small    violet  blue\r\n    2  small     tulip   red\r\n    3  small  harebell  blue\r\n\r\n    df[&#39;protected&#39;] = [&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;]\r\n\r\n        size      name color protected\r\n    0    big      rose   red        no\r\n    1  small    violet  blue        no\r\n    2  small     tulip   red        no\r\n    3  small  harebell  blue       yes\r\n\r\nNote that this works even if the index of the dataframe is off.\r\n\r\n    df.index = [3,2,1,0]\r\n    df[&#39;protected&#39;] = [&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;]\r\n        size      name color protected\r\n    3    big      rose   red        no\r\n    2  small    violet  blue        no\r\n    1  small     tulip   red        no\r\n    0  small  harebell  blue       yes\r\n\r\n###[]= is the way to go, but watch out!\r\nHowever, if you have a `pd.Series` and try to assign it to a dataframe where the indexes are off, you will run in to trouble. See example:\r\n\r\n    df[&#39;protected&#39;] = pd.Series([&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;])\r\n        size      name color protected\r\n    3    big      rose   red       yes\r\n    2  small    violet  blue        no\r\n    1  small     tulip   red        no\r\n    0  small  harebell  blue        no\r\n\r\nThis is because a `pd.Series` by default has an index enumerated from 0 to n. And the pandas `[] =` method **tries** *to be &quot;smart&quot;*\r\n\r\n##What actually is going on.\r\nWhen you use the `[] =` method pandas is quietly performing an outer join or outer merge using the index of the left hand dataframe and the index of the right hand series. `df[&#39;column&#39;] = series`\r\n\r\n###Side note\r\nThis quickly causes cognitive dissonance, since the `[]=` method is trying to do a lot of different things depending on the input, and the outcome cannot be predicted unless you *just know* how pandas works. I would therefore advice against the `[]=` in code bases, but when exploring data in a notebook, it is fine.\r\n\r\n##Going around the problem\r\nIf you have a `pd.Series` and want it assigned from top to bottom, or if you are coding productive code and you are not sure of the index order, it is worth it to safeguard for this kind of issue.\r\n\r\nYou could downcast the `pd.Series` to a `np.ndarray` or a `list`, this will do the trick.\r\n\r\n    df[&#39;protected&#39;] = pd.Series([&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;]).values\r\n\r\nor\r\n\r\n    df[&#39;protected&#39;] = list(pd.Series([&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;]))\r\n\r\n**But this is not very explicit.**\r\n\r\nSome coder may come along and say &quot;Hey, this looks redundant, I&#39;ll just optimize this away&quot;.\r\n\r\n###Explicit way\r\nSetting the index of the `pd.Series` to be the index of the `df` is explicit.\r\n\r\n    df[&#39;protected&#39;] = pd.Series([&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;], index=df.index)\r\n\r\nOr more realistically, you probably have a `pd.Series` already available.\r\n\r\n    protected_series = pd.Series([&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;])\r\n    protected_series.index = df.index\r\n\r\n    3     no\r\n    2     no\r\n    1     no\r\n    0    yes\r\n\r\nCan now be assigned\r\n\r\n    df[&#39;protected&#39;] = protected_series\r\n\r\n        size      name color protected\r\n    3    big      rose   red        no\r\n    2  small    violet  blue        no\r\n    1  small     tulip   red        no\r\n    0  small  harebell  blue       yes\r\n\r\n##Alternative way with `df.reset_index()`\r\n\r\nSince the index dissonance is the problem, if you feel that the index of the dataframe *should* not dictate things, you can simply drop the index, this should be faster, but it is not very clean, since your function now *probably* does two things.\r\n\r\n    df.reset_index(drop=True)\r\n    protected_series.reset_index(drop=True)\r\n    df[&#39;protected&#39;] = protected_series\r\n\r\n        size      name color protected\r\n    0    big      rose   red        no\r\n    1  small    violet  blue        no\r\n    2  small     tulip   red        no\r\n    3  small  harebell  blue       yes\r\n\r\n## Note on `df.assign`\r\n\r\nWhile `df.assign` make it more explicit what you are doing, it actually has all the same problems as the above `[]=`\r\n\r\n    df.assign(protected=pd.Series([&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;]))\r\n        size      name color protected\r\n    3    big      rose   red       yes\r\n    2  small    violet  blue        no\r\n    1  small     tulip   red        no\r\n    0  small  harebell  blue        no\r\n\r\nJust watch out with `df.assign` that your column is not called `self`. It will cause errors. This makes `df.assign` **smelly**, since there are these kind of artifacts in the function.\r\n\r\n    df.assign(self=pd.Series([&#39;no&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;])\r\n    TypeError: assign() got multiple values for keyword argument &#39;self&#39;\r\n\r\nYou may say, &quot;Well, I&#39;ll just not use `self` then&quot;. But who knows how this function changes in the future to support new arguments. Maybe your column name will be an argument in a new update of pandas, causing problems with upgrading.",
               "id": "43180437",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1491209962,
               "score": 9
            },
            {
               "up_vote_count": 3,
               "answer_id": 43268324,
               "last_activity_date": 1491529568,
               "path": "3.stack.answer",
               "body_markdown": "If the data frame and Series object have ***the same index***, `pandas.concat` also works here:\r\n    \r\n    import pandas as pd\r\n    df\r\n    #          a\t        b\t        c\t        d\r\n    #0\t0.671399\t 0.101208\t-0.181532\t 0.241273\r\n    #1\t0.446172\t-0.243316\t 0.051767\t 1.577318\r\n    #2\t0.614758\t 0.075793\t-0.451460\t-0.012493\r\n    \r\n    e = pd.Series([-0.335485, -1.166658, -0.385571])    \r\n    e\r\n    #0   -0.335485\r\n    #1   -1.166658\r\n    #2   -0.385571\r\n    #dtype: float64\r\n    \r\n    # here we need to give the series object a name which converts to the new  column name \r\n    # in the result\r\n    df = pd.concat([df, e.rename(&quot;e&quot;)], axis=1)\r\n    df\r\n\r\n    #          a\t        b\t        c\t        d\t        e\r\n    #0\t0.671399\t 0.101208\t-0.181532\t 0.241273\t-0.335485\r\n    #1\t0.446172\t-0.243316\t 0.051767\t 1.577318\t-1.166658\r\n    #2\t0.614758\t 0.075793\t-0.451460\t-0.012493\t-0.385571\r\n\r\nIn case they don&#39;t have the same index:\r\n\r\n    e.index = df.index\r\n    df = pd.concat([df, e.rename(&quot;e&quot;)], axis=1)\r\n\r\n",
               "tags": [],
               "creation_date": 1491529117,
               "last_edit_date": 1491529568,
               "is_accepted": false,
               "id": "43268324",
               "down_vote_count": 0,
               "score": 3
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 43368149,
               "is_accepted": false,
               "last_activity_date": 1491996123,
               "body_markdown": "**Foolproof:**\r\n\r\n    df.loc[:, &#39;NewCol&#39;] = &#39;New_Val&#39;\r\n\r\nExample:\r\n\r\n    df = pd.DataFrame(data=np.random.randn(20, 4), columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;])\r\n\r\n    df\r\n\r\n               A         B         C         D\r\n    0  -0.761269  0.477348  1.170614  0.752714\r\n    1   1.217250 -0.930860 -0.769324 -0.408642\r\n    2  -0.619679 -1.227659 -0.259135  1.700294\r\n    3  -0.147354  0.778707  0.479145  2.284143\r\n    4  -0.529529  0.000571  0.913779  1.395894\r\n    5   2.592400  0.637253  1.441096 -0.631468\r\n    6   0.757178  0.240012 -0.553820  1.177202\r\n    7  -0.986128 -1.313843  0.788589 -0.707836\r\n    8   0.606985 -2.232903 -1.358107 -2.855494\r\n    9  -0.692013  0.671866  1.179466 -1.180351\r\n    10 -1.093707 -0.530600  0.182926 -1.296494\r\n    11 -0.143273 -0.503199 -1.328728  0.610552\r\n    12 -0.923110 -1.365890 -1.366202 -1.185999\r\n    13 -2.026832  0.273593 -0.440426 -0.627423\r\n    14 -0.054503 -0.788866 -0.228088 -0.404783\r\n    15  0.955298 -1.430019  1.434071 -0.088215\r\n    16 -0.227946  0.047462  0.373573 -0.111675\r\n    17  1.627912  0.043611  1.743403 -0.012714\r\n    18  0.693458  0.144327  0.329500 -0.655045\r\n    19  0.104425  0.037412  0.450598 -0.923387\r\n\r\n\r\n    df.drop([3, 5, 8, 10, 18], inplace=True)\r\n\r\n    df\r\n     \r\n               A         B         C         D\r\n    0  -0.761269  0.477348  1.170614  0.752714\r\n    1   1.217250 -0.930860 -0.769324 -0.408642\r\n    2  -0.619679 -1.227659 -0.259135  1.700294\r\n    4  -0.529529  0.000571  0.913779  1.395894\r\n    6   0.757178  0.240012 -0.553820  1.177202\r\n    7  -0.986128 -1.313843  0.788589 -0.707836\r\n    9  -0.692013  0.671866  1.179466 -1.180351\r\n    11 -0.143273 -0.503199 -1.328728  0.610552\r\n    12 -0.923110 -1.365890 -1.366202 -1.185999\r\n    13 -2.026832  0.273593 -0.440426 -0.627423\r\n    14 -0.054503 -0.788866 -0.228088 -0.404783\r\n    15  0.955298 -1.430019  1.434071 -0.088215\r\n    16 -0.227946  0.047462  0.373573 -0.111675\r\n    17  1.627912  0.043611  1.743403 -0.012714\r\n    19  0.104425  0.037412  0.450598 -0.923387\r\n\r\n    df.loc[:, &#39;NewCol&#39;] = 0\r\n\r\n    df\r\n               A         B         C         D  NewCol\r\n    0  -0.761269  0.477348  1.170614  0.752714       0\r\n    1   1.217250 -0.930860 -0.769324 -0.408642       0\r\n    2  -0.619679 -1.227659 -0.259135  1.700294       0\r\n    4  -0.529529  0.000571  0.913779  1.395894       0\r\n    6   0.757178  0.240012 -0.553820  1.177202       0\r\n    7  -0.986128 -1.313843  0.788589 -0.707836       0\r\n    9  -0.692013  0.671866  1.179466 -1.180351       0\r\n    11 -0.143273 -0.503199 -1.328728  0.610552       0\r\n    12 -0.923110 -1.365890 -1.366202 -1.185999       0\r\n    13 -2.026832  0.273593 -0.440426 -0.627423       0\r\n    14 -0.054503 -0.788866 -0.228088 -0.404783       0\r\n    15  0.955298 -1.430019  1.434071 -0.088215       0\r\n    16 -0.227946  0.047462  0.373573 -0.111675       0\r\n    17  1.627912  0.043611  1.743403 -0.012714       0\r\n    19  0.104425  0.037412  0.450598 -0.923387       0",
               "id": "43368149",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1491996123,
               "score": 5
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 44360325,
               "is_accepted": false,
               "last_activity_date": 1496624000,
               "body_markdown": "1. First create a python&#39;s list_of_e that has relevant data. \r\n2. Use this: \r\n        df[&#39;e&#39;] = list_of_e\r\n",
               "id": "44360325",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1496624000,
               "score": 3
            },
            {
               "up_vote_count": 6,
               "answer_id": 46734631,
               "last_activity_date": 1513630260,
               "path": "3.stack.answer",
               "body_markdown": "If you want to set the whole new column to an initial base value (e.g. `None`), you can do this: `df1[&#39;e&#39;] = None`\r\n\r\nThis actually would assign &quot;object&quot; type to the cell. So later you&#39;re free to put complex data types, like list, into individual cells.",
               "tags": [],
               "creation_date": 1507913598,
               "last_edit_date": 1513630260,
               "is_accepted": false,
               "id": "46734631",
               "down_vote_count": 0,
               "score": 6
            },
            {
               "up_vote_count": 2,
               "answer_id": 47093066,
               "last_activity_date": 1509705851,
               "path": "3.stack.answer",
               "body_markdown": "If the column you are trying to add is a series variable then just :\r\n\r\n    df[&quot;new_columns_name&quot;]=series_variable_name #this will do it for you\r\n\r\nThis works well even if you are replacing an existing column.just type the new_columns_name same as the column you want to replace.It will just overwrite the existing column data with the new series data.",
               "tags": [],
               "creation_date": 1509703558,
               "last_edit_date": 1509705851,
               "is_accepted": false,
               "id": "47093066",
               "down_vote_count": 0,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/12555323/adding-new-column-to-existing-dataframe-in-python-pandas",
         "id": "858127-2320"
      },
      {
         "up_vote_count": "181",
         "path": "2.stack",
         "body_markdown": "I have a dataframe as below\r\n\r\n          itm Date                  Amount \r\n    67    420 2012-09-30 00:00:00   65211\r\n    68    421 2012-09-09 00:00:00   29424\r\n    69    421 2012-09-16 00:00:00   29877\r\n    70    421 2012-09-23 00:00:00   30990\r\n    71    421 2012-09-30 00:00:00   61303\r\n    72    485 2012-09-09 00:00:00   71781\r\n    73    485 2012-09-16 00:00:00     NaN\r\n    74    485 2012-09-23 00:00:00   11072\r\n    75    485 2012-09-30 00:00:00  113702\r\n    76    489 2012-09-09 00:00:00   64731\r\n    77    489 2012-09-16 00:00:00     NaN\r\n\r\nwhen I try to .apply a function to the Amount column I get the following error.\r\n\r\n    ValueError: cannot convert float NaN to integer\r\n\r\nI have tried applying a function using .isnan from the Math Module\r\nI have tried the pandas .replace attribute\r\nI tried the .sparse data attribute from pandas 0.9\r\nI have also tried if NaN == NaN statement in a function.\r\nI have also looked at this article https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-r whilst looking at some other articles. \r\nAll the methods I have tried have not worked or do not recognise NaN.\r\nAny Hints or solutions would be appreciated. ",
         "view_count": "248712",
         "answer_count": "7",
         "tags": "['python', 'pandas']",
         "creation_date": "1352400639",
         "last_edit_date": "1495539204",
         "code_snippet": "['<code>      itm Date                  Amount \\n67    420 2012-09-30 00:00:00   65211\\n68    421 2012-09-09 00:00:00   29424\\n69    421 2012-09-16 00:00:00   29877\\n70    421 2012-09-23 00:00:00   30990\\n71    421 2012-09-30 00:00:00   61303\\n72    485 2012-09-09 00:00:00   71781\\n73    485 2012-09-16 00:00:00     NaN\\n74    485 2012-09-23 00:00:00   11072\\n75    485 2012-09-30 00:00:00  113702\\n76    489 2012-09-09 00:00:00   64731\\n77    489 2012-09-16 00:00:00     NaN\\n</code>', '<code>ValueError: cannot convert float NaN to integer\\n</code>', '<code>DataFrame.fillna()</code>', '<code>In [7]: df\\nOut[7]: \\n          0         1\\n0       NaN       NaN\\n1 -0.494375  0.570994\\n2       NaN       NaN\\n3  1.876360 -0.229738\\n4       NaN       NaN\\n\\nIn [8]: df.fillna(0)\\nOut[8]: \\n          0         1\\n0  0.000000  0.000000\\n1 -0.494375  0.570994\\n2  0.000000  0.000000\\n3  1.876360 -0.229738\\n4  0.000000  0.000000\\n</code>', '<code>In [12]: df[1].fillna(0, inplace=True)\\nOut[12]: \\n0    0.000000\\n1    0.570994\\n2    0.000000\\n3   -0.229738\\n4    0.000000\\nName: 1\\n\\nIn [13]: df\\nOut[13]: \\n          0         1\\n0       NaN  0.000000\\n1 -0.494375  0.570994\\n2       NaN  0.000000\\n3  1.876360 -0.229738\\n4       NaN  0.000000\\n</code>', '<code>df[1]</code>', \"<code>df['column']=df['column'].fillna(value)\\n</code>\", '<code>idx = pd.IndexSlice\\ndf.loc[idx[:,mask_1],idx[mask_2,:]].fillna(value=0,inplace=True)\\n</code>', '<code>df.update(df.loc[idx[:,mask_1],idx[[mask_2],:]].fillna(value=0))\\n</code>', \"<code>import pandas\\n\\ndf = pandas.read_csv('somefile.txt')\\n\\ndf = df.fillna(0)\\n</code>\", '<code>replace</code>', '<code>NaN</code>', '<code>0</code>', \"<code>import pandas as pd\\nimport numpy as np\\n\\n# for column\\ndf['column'] = df['column'].replace(np.nan, 0)\\n\\n# for whole dataframe\\ndf = df.replace(np.nan, 0)\\n\\n# inplace\\ndf.replace(np.nan, 0, inplace=True)\\n</code>\", '<code>df = df.fillna(value_to_replace_null)\\n</code>', '<code>#fill all Nan value with zero\\ndf = df.fillna(0)\\n</code>', '<code>df.fillna(0, inplace=True)\\n</code>']",
         "title": "How can I replace all the NaN values with Zero&#39;s in a column of a pandas dataframe",
         "_childDocuments_": [
            {
               "up_vote_count": 324,
               "answer_id": 13295801,
               "last_activity_date": 1466702963,
               "path": "3.stack.answer",
               "body_markdown": "I believe `DataFrame.fillna()` will do this for you.\r\n\r\nLink to Docs for [a dataframe](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) and for [a Series](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html). \r\n\r\nExample: \r\n\r\n    In [7]: df\r\n    Out[7]: \r\n              0         1\r\n    0       NaN       NaN\r\n    1 -0.494375  0.570994\r\n    2       NaN       NaN\r\n    3  1.876360 -0.229738\r\n    4       NaN       NaN\r\n\r\n    In [8]: df.fillna(0)\r\n    Out[8]: \r\n              0         1\r\n    0  0.000000  0.000000\r\n    1 -0.494375  0.570994\r\n    2  0.000000  0.000000\r\n    3  1.876360 -0.229738\r\n    4  0.000000  0.000000\r\n\r\nTo fill the NaNs in only one column, select just that column. in this case I&#39;m using inplace=True to actually change the contents of df. \r\n\r\n    In [12]: df[1].fillna(0, inplace=True)\r\n    Out[12]: \r\n    0    0.000000\r\n    1    0.570994\r\n    2    0.000000\r\n    3   -0.229738\r\n    4    0.000000\r\n    Name: 1\r\n    \r\n    In [13]: df\r\n    Out[13]: \r\n              0         1\r\n    0       NaN  0.000000\r\n    1 -0.494375  0.570994\r\n    2       NaN  0.000000\r\n    3  1.876360 -0.229738\r\n    4       NaN  0.000000\r\n\r\n",
               "tags": [],
               "creation_date": 1352400867,
               "last_edit_date": 1466702963,
               "is_accepted": true,
               "id": "13295801",
               "down_vote_count": 0,
               "score": 324
            },
            {
               "up_vote_count": 16,
               "answer_id": 30587837,
               "last_activity_date": 1450290543,
               "path": "3.stack.answer",
               "body_markdown": "I just wanted to provide a bit of an update/special case since it looks like people still come here. If you&#39;re using a multi-index or otherwise using an index-slicer the inplace=True option may not be enough to update the slice you&#39;ve chosen. For example in a 2x2 level multi-index this will not change any values (as of pandas 0.15):\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    idx = pd.IndexSlice\r\n    df.loc[idx[:,mask_1],idx[mask_2,:]].fillna(value=0,inplace=True)\r\n\r\nThe &quot;problem&quot; is that the chaining breaks the fillna ability to update the original dataframe. I put &quot;problem&quot; in quotes because there are good reasons for the design decisions that led to not interpreting through these chains in certain situations. Also, this is a complex example (though I really ran into it), but the same may apply to fewer levels of indexes depending on how you slice.\r\n\r\nThe solution is DataFrame.update:\r\n\r\n&lt;!-- language: lang-py --&gt;\r\n\r\n    df.update(df.loc[idx[:,mask_1],idx[[mask_2],:]].fillna(value=0))\r\n\r\nIt&#39;s one line, reads reasonably well (sort of) and eliminates any unnecessary messing with intermediate variables or loops while allowing you to apply fillna to any multi-level slice you like!\r\n\r\nIf anybody can find places this doesn&#39;t work please post in the comments, I&#39;ve been messing with it and looking at the source and it seems to solve at least my multi-index slice problems.\r\n",
               "tags": [],
               "creation_date": 1433222014,
               "last_edit_date": 1450290543,
               "is_accepted": false,
               "id": "30587837",
               "down_vote_count": 0,
               "score": 16
            },
            {
               "up_vote_count": 12,
               "answer_id": 39478896,
               "last_activity_date": 1473801217,
               "path": "3.stack.answer",
               "body_markdown": "The below code worked for me.\r\n\r\n    import pandas\r\n    \r\n    df = pandas.read_csv(&#39;somefile.txt&#39;)\r\n    \r\n    df = df.fillna(0)",
               "tags": [],
               "creation_date": 1473800371,
               "last_edit_date": 1473801217,
               "is_accepted": false,
               "id": "39478896",
               "down_vote_count": 0,
               "score": 12
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 31,
               "answer_id": 39891994,
               "is_accepted": false,
               "last_activity_date": 1475745008,
               "body_markdown": "It is not guaranteed that the slicing returns a view or a copy. You can do\r\n\r\n    df[&#39;column&#39;]=df[&#39;column&#39;].fillna(value)",
               "id": "39891994",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1475745008,
               "score": 31
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 44559011,
               "is_accepted": false,
               "last_activity_date": 1497503480,
               "body_markdown": "You could use [`replace`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html) to change `NaN` to `0`:\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n\r\n    # for column\r\n    df[&#39;column&#39;] = df[&#39;column&#39;].replace(np.nan, 0)\r\n   \r\n    # for whole dataframe\r\n    df = df.replace(np.nan, 0)\r\n\r\n    # inplace\r\n    df.replace(np.nan, 0, inplace=True)",
               "id": "44559011",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1497503480,
               "score": 8
            },
            {
               "up_vote_count": 2,
               "answer_id": 45380821,
               "last_activity_date": 1501271323,
               "path": "3.stack.answer",
               "body_markdown": "***fillna()*** is the best way to do it. Code -\r\n\r\n    #fill all Nan value with zero\r\n    df = df.fillna(0)\r\n\r\nYou can also use ***inplace*** if you don&#39;t want to use  **&#39;*df = df.fillna(value)*&#39;** . Code -\r\n\r\n    df.fillna(0, inplace=True)",
               "tags": [],
               "creation_date": 1501269132,
               "last_edit_date": 1501271323,
               "is_accepted": false,
               "id": "45380821",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 45381288,
               "is_accepted": false,
               "last_activity_date": 1501271066,
               "body_markdown": "You should use ***fillna()*** . It works for me. \r\n\r\n    df = df.fillna(value_to_replace_null)",
               "id": "45381288",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1501271066,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/13295735/how-can-i-replace-all-the-nan-values-with-zeros-in-a-column-of-a-pandas-datafra",
         "id": "858127-2321"
      },
      {
         "up_vote_count": "919",
         "path": "2.stack",
         "body_markdown": "I have a DataFrame using pandas and column labels that I need to edit to replace the original column labels. \r\n\r\nI&#39;d like to change the column names in a DataFrame `A` where the original column names are:\r\n\r\n    [&#39;$a&#39;, &#39;$b&#39;, &#39;$c&#39;, &#39;$d&#39;, &#39;$e&#39;] \r\n\r\nto \r\n\r\n    [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;].\r\n\r\nI have the edited column names stored it in a list, but I don&#39;t know how to replace the column names.\r\n",
         "view_count": "1079088",
         "answer_count": "22",
         "tags": "['python', 'pandas', 'replace', 'dataframe', 'rename']",
         "creation_date": "1341498075",
         "last_edit_date": "1513104920",
         "code_snippet": "['<code>A</code>', \"<code>['$a', '$b', '$c', '$d', '$e'] \\n</code>\", \"<code>['a', 'b', 'c', 'd', 'e'].\\n</code>\", '<code>.rename</code>', '<code>.columns</code>', \"<code>&gt;&gt;&gt; df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\\n&gt;&gt;&gt; df.columns = ['a', 'b']\\n&gt;&gt;&gt; df\\n   a   b\\n0  1  10\\n1  2  20\\n</code>\", '<code>new_columns = df.columns.values; </code>', \"<code>new_columns[0] = 'XX';</code>\", '<code>df.columns  = new_columns</code>', \"<code>df.rename(columns = {'$b':'B'}, inplace = True)</code>\", '<code>df.rename()</code>', \"<code>df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\\n# Or rename the existing DataFrame (rather than creating a copy) \\ndf.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\\n</code>\", '<code>code</code>', '<code>code</code>', '<code>SettingWithCopyWarning:</code>', \"<code>df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})</code>\", \"<code>df['newName1']</code>\", '<code>inplace=True</code>', '<code>rename</code>', \"<code>In [11]: df.columns\\nOut[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)\\n\\nIn [12]: df.rename(columns=lambda x: x[1:], inplace=True)\\n\\nIn [13]: df.columns\\nOut[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)\\n</code>\", '<code>df.rename(columns=lambda x: x.lstrip(), inplace=True)</code>', \"<code>t.columns = t.columns.str.replace(r'[^\\\\x00-\\\\x7F]+','')</code>\", \"<code>df.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)</code>\", '<code>df.Column_1_Name</code>', \"<code>df.loc[:, 'Column 1 Name']</code>\", \"<code>df = df.rename(columns=lambda x: x.replace('$', ''))\\n</code>\", \"<code>df.rename(columns=lambda x: x.replace('$', ''), inplace=True)\\n</code>\", \"<code>df.columns = df.columns.str.replace('$','')\\n</code>\", \"<code>df.columns = ['a', 'b', 'c', 'd', 'e']\\n</code>\", \"<code>df.columns.values[2] = 'c'    #renames the 2nd column to 'c'\\n</code>\", '<code>rename</code>', '<code>axis</code>', '<code>columns</code>', '<code>1</code>', '<code>index</code>', '<code>columns</code>', '<code>set_axis</code>', '<code>inplace</code>', '<code>False</code>', \"<code>df = pd.DataFrame({'$a':[1,2], '$b': [3,4], \\n                   '$c':[5,6], '$d':[7,8], \\n                   '$e':[9,10]})\\n\\n   $a  $b  $c  $d  $e\\n0   1   3   5   7   9\\n1   2   4   6   8  10\\n</code>\", '<code>rename</code>', \"<code>axis='columns'</code>\", '<code>axis=1</code>', \"<code>df.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis='columns')\\n</code>\", \"<code>df.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis=1)\\n</code>\", '<code>   a  b  c  d   e\\n0  1  3  5  7   9\\n1  2  4  6  8  10\\n</code>', \"<code>df.rename(columns={'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'})\\n</code>\", '<code>rename</code>', \"<code>df.rename(lambda x: x[1:], axis='columns')\\n</code>\", '<code>df.rename(lambda x: x[1:], axis=1)\\n</code>', '<code>set_axis</code>', '<code>inplace=False</code>', '<code>set_axis</code>', '<code>inplace</code>', '<code>True</code>', '<code>inplace</code>', '<code>False</code>', \"<code>df.set_axis(['a', 'b', 'c', 'd', 'e'], axis='columns', inplace=False)\\n</code>\", \"<code>df.set_axis(['a', 'b', 'c', 'd', 'e'], axis=1, inplace=False)\\n</code>\", \"<code>df.columns = ['a', 'b', 'c', 'd', 'e']</code>\", '<code>set_axis</code>', '<code># new for pandas 0.21+\\ndf.some_method1()\\n  .some_method2()\\n  .set_axis()\\n  .some_method3()\\n\\n# old way\\ndf1 = df.some_method1()\\n        .some_method2()\\ndf1.columns = columns\\ndf1.some_method3()\\n</code>', '<code>Pandas 0.21+ answer</code>', \"<code>old_names = ['$a', '$b', '$c', '$d', '$e'] \\nnew_names = ['a', 'b', 'c', 'd', 'e']\\ndf.rename(columns=dict(zip(old_names, new_names)), inplace=True)\\n</code>\", '<code>new_names</code>', \"<code>df.columns = ['a', 'b', 'c', 'd', 'e']</code>\", '<code>df.columns.values</code>', '<code>myList = list(df)  myList[10:20]</code>', '<code>numpy.array</code>', '<code>numpy.array</code>', '<code>.name</code>', '<code>df.columns</code>', '<code>list</code>', '<code>Series</code>', '<code>.name</code>', '<code>Series</code>', \"<code>df.columns = ['column_one', 'column_two']\\ndf.columns.names = ['name of the list of columns']\\ndf.index.names = ['name of the index']\\n\\nname of the list of columns     column_one  column_two\\nname of the index       \\n0                                    4           1\\n1                                    5           2\\n2                                    6           3\\n</code>\", '<code>.name</code>', \"<code>df.columns = ['one', 'two']</code>\", '<code>df.one.name</code>', \"<code>'one'</code>\", \"<code>df.one.name = 'three'</code>\", '<code>df.columns</code>', \"<code>['one', 'two']</code>\", '<code>df.one.name</code>', \"<code>'three'</code>\", '<code>pd.DataFrame(df.one)</code>', '<code>    three\\n0       1\\n1       2\\n2       3\\n</code>', '<code>.name</code>', '<code>Series</code>', '<code>    |one            |\\n    |one      |two  |\\n0   |  4      |  1  |\\n1   |  5      |  2  |\\n2   |  6      |  3  |\\n</code>', \"<code>df.columns = [['one', 'one'], ['one', 'two']]\\n</code>\", \"<code>'$'</code>\", '<code>columns</code>', '<code>df.columns = new</code>', '<code>new</code>', '<code>columns</code>', '<code>df</code>', \"<code>df = pd.DataFrame({'Jack': [1, 2], 'Mahesh': [3, 4], 'Xin': [5, 6]})\\nnew = ['x098', 'y765', 'z432']\\n\\ndf\\n\\n   Jack  Mahesh  Xin\\n0     1       3    5\\n1     2       4    6\\n</code>\", '<code>pd.DataFrame.rename</code>', '<code>pd.DataFrame.rename</code>', \"<code>d = {'Jack': 'x098', 'Mahesh': 'y765', 'Xin': 'z432'}\\ndf.rename(columns=d)\\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>\", '<code>rename</code>', '<code>df</code>', '<code># given just a list of new column names\\ndf.rename(columns=dict(zip(df, new)))\\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', \"<code>df = pd.DataFrame(\\n    [[1, 3, 5], [2, 4, 6]],\\n    columns=['Mahesh', 'Mahesh', 'Xin']\\n)\\nnew = ['x098', 'y765', 'z432']\\n\\ndf\\n\\n   Mahesh  Mahesh  Xin\\n0       1       3    5\\n1       2       4    6\\n</code>\", '<code>pd.concat</code>', '<code>keys</code>', '<code>df.rename(columns=dict(zip(df, new)))\\n\\n   y765  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', '<code>new</code>', '<code>y765</code>', '<code>keys</code>', '<code>pd.concat</code>', '<code>df</code>', '<code>pd.concat([c for _, c in df.items()], axis=1, keys=new) \\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', '<code>dtype</code>', '<code>dtype</code>', '<code>object</code>', '<code>dtype</code>', '<code>pd.DataFrame(df.values, df.index, new)\\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', '<code>dtype</code>', '<code>pd.DataFrame(df.values, df.index, new).astype(dict(zip(new, df.dtypes)))\\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', '<code>transpose</code>', '<code>set_index</code>', '<code>pd.DataFrame.set_index</code>', '<code>set_columns</code>', '<code>set_index</code>', '<code>dtype</code>', '<code>dtype</code>', '<code>dtype</code>', '<code>df.T.set_index(np.asarray(new)).T\\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', '<code>dtype</code>', '<code>df.T.set_index(np.asarray(new)).T.astype(dict(zip(new, df.dtypes)))\\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', '<code>lambda</code>', '<code>pd.DataFrame.rename</code>', '<code>new</code>', '<code>x</code>', '<code>y</code>', '<code>x</code>', '<code>df.rename(columns=lambda x, y=iter(new): next(y))\\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', '<code>*</code>', '<code>x</code>', '<code>y</code>', '<code>y</code>', '<code>df.rename(columns=lambda x, *, y=iter(new): next(y))\\n\\n   x098  y765  z432\\n0     1     3     5\\n1     2     4     6\\n</code>', '<code>columns = df.columns\\ncolumns = [row.replace(\"$\",\"\") for row in columns]\\ndf.rename(columns=dict(zip(columns, things)), inplace=True)\\ndf.head() #to validate the output\\n</code>', '<code>import pandas as pd\\nimport cProfile, pstats, re\\n\\nold_names = [\\'$a\\', \\'$b\\', \\'$c\\', \\'$d\\', \\'$e\\']\\nnew_names = [\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\']\\ncol_dict = {\\'$a\\': \\'a\\', \\'$b\\': \\'b\\',\\'$c\\':\\'c\\',\\'$d\\':\\'d\\',\\'$e\\':\\'e\\'}\\n\\ndf = pd.DataFrame({\\'$a\\':[1,2], \\'$b\\': [10,20],\\'$c\\':[\\'bleep\\',\\'blorp\\'],\\'$d\\':[1,2],\\'$e\\':[\\'texa$\\',\\'\\']})\\n\\ndf.head()\\n\\ndef eumiro(df,nn):\\n    df.columns = nn\\n    #This direct renaming approach is duplicated in methodology in several other answers: \\n    return df\\n\\ndef lexual1(df):\\n    return df.rename(columns=col_dict)\\n\\ndef lexual2(df,col_dict):\\n    return df.rename(columns=col_dict, inplace=True)\\n\\ndef Panda_Master_Hayden(df):\\n    return df.rename(columns=lambda x: x[1:], inplace=True)\\n\\ndef paulo1(df):\\n    return df.rename(columns=lambda x: x.replace(\\'$\\', \\'\\'))\\n\\ndef paulo2(df):\\n    return df.rename(columns=lambda x: x.replace(\\'$\\', \\'\\'), inplace=True)\\n\\ndef migloo(df,on,nn):\\n    return df.rename(columns=dict(zip(on, nn)), inplace=True)\\n\\ndef kadee(df):\\n    return df.columns.str.replace(\\'$\\',\\'\\')\\n\\ndef awo(df):\\n    columns = df.columns\\n    columns = [row.replace(\"$\",\"\") for row in columns]\\n    return df.rename(columns=dict(zip(columns, \\'\\')), inplace=True)\\n\\ndef kaitlyn(df):\\n    df.columns = [col.strip(\\'$\\') for col in df.columns]\\n    return df\\n\\nprint \\'eumiro\\'\\ncProfile.run(\\'eumiro(df,new_names)\\')\\nprint \\'lexual1\\'\\ncProfile.run(\\'lexual1(df)\\')\\nprint \\'lexual2\\'\\ncProfile.run(\\'lexual2(df,col_dict)\\')\\nprint \\'andy hayden\\'\\ncProfile.run(\\'Panda_Master_Hayden(df)\\')\\nprint \\'paulo1\\'\\ncProfile.run(\\'paulo1(df)\\')\\nprint \\'paulo2\\'\\ncProfile.run(\\'paulo2(df)\\')\\nprint \\'migloo\\'\\ncProfile.run(\\'migloo(df,old_names,new_names)\\')\\nprint \\'kadee\\'\\ncProfile.run(\\'kadee(df)\\')\\nprint \\'awo\\'\\ncProfile.run(\\'awo(df)\\')\\nprint \\'kaitlyn\\'\\ncProfile.run(\\'kaitlyn(df)\\')\\n</code>', \"<code>df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]})\\n</code>\", \"<code>new_cols = ['a', 'b', 'c', 'd', 'e']\\ndf.columns = new_cols\\n&gt;&gt;&gt; df\\n   a  b  c  d  e\\n0  1  1  1  1  1\\n</code>\", \"<code>d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}\\ndf.columns = df.columns.map(lambda col: d[col])  # Or `.map(d.get)` as pointed out by @PiRSquared.\\n&gt;&gt;&gt; df\\n   a  b  c  d  e\\n0  1  1  1  1  1\\n</code>\", '<code>$</code>', \"<code>df.columns = [col[1:] if col[0] == '$' else col for col in df]\\n</code>\", '<code>lambda col: d[col]</code>', '<code>d.get</code>', '<code>df.columns.map(d.get)</code>', \"<code>df.rename(columns = {'Old Name':'New Name'})\\n</code>\", \"<code>df.columns = [col.strip('$') for col in df.columns]\\n</code>\", '<code>strip</code>', \"<code>df.columns = ['Name1', 'Name2', 'Name3'...]\\n</code>\", '<code>str.slice</code>', '<code>df.columns = df.columns.str.slice(1)\\n</code>', '<code>delimiters=</code>', \"<code>import pandas as pd\\nimport re\\n\\n\\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})\\n\\ndelimiters = '$'\\nmatchPattern = '|'.join(map(re.escape, delimiters))\\ndf.columns = [re.split(matchPattern, i)[1] for i in df.columns ]\\n</code>\", '<code>&gt;&gt;&gt; df\\n   $a  $b  $c  $d  $e\\n0   1   3   5   7   9\\n1   2   4   6   8  10\\n\\n&gt;&gt;&gt; df\\n   a  b  c  d   e\\n0  1  3  5  7   9\\n1  2  4  6  8  10\\n</code>', \"<code>&gt;&gt;&gt; df = pd.DataFrame({('$a','$x'):[1,2], ('$b','$y'): [3,4], ('e','f'):[5,6]})\\n&gt;&gt;&gt; df\\n   $a $b  e\\n   $x $y  f\\n0  1  3  5\\n1  2  4  6\\n&gt;&gt;&gt; rename = {('$a','$x'):('a','x'), ('$b','$y'):('b','y')}\\n&gt;&gt;&gt; df.columns = pandas.MultiIndex.from_tuples([\\n        rename.get(item, item) for item in df.columns.tolist()])\\n&gt;&gt;&gt; df\\n   a  b  e\\n   x  y  f\\n0  1  3  5\\n1  2  4  6\\n</code>\", \"<code>dict=dict(zip(df.columns,df.columns.str.replace('(:S$|:C1$|:L$|:D$|\\\\.Serial:L$)','')))\\ndict['brand_timeseries:C1']='BTS'\\ndict['respid:L']='RespID'\\ndict['country:C1']='CountryID\\ndict['pim1:D']='pim_actual'\\ndf.rename(columns=dict, inplace=True)\\n</code>\", '<code>df.rename(index=str, columns={\"$a\": \"a\", \"$b\": \"b\", \"$c\" : \"c\", \"$d\" : \"d\", \"$e\" : \"e\"})</code>', \"<code>df = df.rename(columns=lambda n: n.replace('$', ''))</code>\", \"<code>df.columns = ['a', 'b',index=False]</code>\"]",
         "title": "Renaming columns in pandas",
         "_childDocuments_": [
            {
               "path": "3.stack.answer",
               "up_vote_count": 978,
               "answer_id": 11346337,
               "is_accepted": true,
               "last_activity_date": 1341498207,
               "body_markdown": "Just assign it to the `.columns` attribute:\r\n\r\n    &gt;&gt;&gt; df = pd.DataFrame({&#39;$a&#39;:[1,2], &#39;$b&#39;: [10,20]})\r\n    &gt;&gt;&gt; df.columns = [&#39;a&#39;, &#39;b&#39;]\r\n    &gt;&gt;&gt; df\r\n       a   b\r\n    0  1  10\r\n    1  2  20\r\n\r\n",
               "id": "11346337",
               "tags": [],
               "down_vote_count": 19,
               "creation_date": 1341498207,
               "score": 959
            },
            {
               "up_vote_count": 1627,
               "answer_id": 11354850,
               "last_activity_date": 1510940382,
               "path": "3.stack.answer",
               "body_markdown": "Use the [``df.rename()``][1] function and refer the columns to be renamed. Not all the columns have to be renamed:\r\n\r\n    df = df.rename(columns={&#39;oldName1&#39;: &#39;newName1&#39;, &#39;oldName2&#39;: &#39;newName2&#39;})\r\n    # Or rename the existing DataFrame (rather than creating a copy) \r\n    df.rename(columns={&#39;oldName1&#39;: &#39;newName1&#39;, &#39;oldName2&#39;: &#39;newName2&#39;}, inplace=True)\r\n\r\n\r\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html",
               "tags": [],
               "creation_date": 1341539295,
               "last_edit_date": 1510940382,
               "is_accepted": false,
               "id": "11354850",
               "down_vote_count": 1,
               "score": 1626
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 253,
               "answer_id": 16667215,
               "is_accepted": false,
               "last_activity_date": 1369130339,
               "body_markdown": "The [`rename`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html) method can take a function, for example:\r\n\r\n    In [11]: df.columns\r\n    Out[11]: Index([u&#39;$a&#39;, u&#39;$b&#39;, u&#39;$c&#39;, u&#39;$d&#39;, u&#39;$e&#39;], dtype=object)\r\n\r\n    In [12]: df.rename(columns=lambda x: x[1:], inplace=True)\r\n\r\n    In [13]: df.columns\r\n    Out[13]: Index([u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;, u&#39;d&#39;, u&#39;e&#39;], dtype=object)",
               "id": "16667215",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1369130339,
               "score": 253
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 101,
               "answer_id": 22657894,
               "is_accepted": false,
               "last_activity_date": 1395829245,
               "body_markdown": "Since you only want to remove the $ sign in all column names, you could just do:\r\n    \r\n    df = df.rename(columns=lambda x: x.replace(&#39;$&#39;, &#39;&#39;))\r\nOR\r\n\r\n    df.rename(columns=lambda x: x.replace(&#39;$&#39;, &#39;&#39;), inplace=True)",
               "id": "22657894",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1395829245,
               "score": 101
            },
            {
               "up_vote_count": 43,
               "answer_id": 30380922,
               "last_activity_date": 1432230865,
               "path": "3.stack.answer",
               "body_markdown": "    old_names = [&#39;$a&#39;, &#39;$b&#39;, &#39;$c&#39;, &#39;$d&#39;, &#39;$e&#39;] \r\n    new_names = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]\r\n    df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\r\n\r\nThis way you can manually edit the `new_names` as you wish.\r\nWorks great when you need to rename only a few columns to correct mispellings, accents, remove special characters etc.",
               "tags": [],
               "creation_date": 1432230513,
               "last_edit_date": 1432230865,
               "is_accepted": false,
               "id": "30380922",
               "down_vote_count": 0,
               "score": 43
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 96,
               "answer_id": 30546734,
               "is_accepted": false,
               "last_activity_date": 1432992245,
               "body_markdown": "As documented in http://pandas.pydata.org/pandas-docs/stable/text.html:\r\n\r\n    df.columns = df.columns.str.replace(&#39;$&#39;,&#39;&#39;)",
               "id": "30546734",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1432992245,
               "score": 96
            },
            {
               "up_vote_count": 13,
               "answer_id": 32322596,
               "last_activity_date": 1473215059,
               "path": "3.stack.answer",
               "body_markdown": "If you&#39;ve got the dataframe, df.columns dumps everything into a list you can manipulate and then reassign into your dataframe as the names of columns...\r\n\r\n    columns = df.columns\r\n    columns = [row.replace(&quot;$&quot;,&quot;&quot;) for row in columns]\r\n    df.rename(columns=dict(zip(columns, things)), inplace=True)\r\n    df.head() #to validate the output\r\n\r\nBest way? IDK. A way - yes.\r\n\r\nA better way of evaluating all the main techniques put forward in the answers to the question is below using cProfile to gage memory &amp; execution time. @kadee, @kaitlyn, &amp; @eumiro had the functions with the fastest execution times - though these functions are so fast we&#39;re comparing the rounding of .000 and .001 seconds for all the answers. Moral: my answer above likely isn&#39;t the &#39;Best&#39; way.\r\n\r\n    import pandas as pd\r\n    import cProfile, pstats, re\r\n    \r\n    old_names = [&#39;$a&#39;, &#39;$b&#39;, &#39;$c&#39;, &#39;$d&#39;, &#39;$e&#39;]\r\n    new_names = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]\r\n    col_dict = {&#39;$a&#39;: &#39;a&#39;, &#39;$b&#39;: &#39;b&#39;,&#39;$c&#39;:&#39;c&#39;,&#39;$d&#39;:&#39;d&#39;,&#39;$e&#39;:&#39;e&#39;}\r\n    \r\n    df = pd.DataFrame({&#39;$a&#39;:[1,2], &#39;$b&#39;: [10,20],&#39;$c&#39;:[&#39;bleep&#39;,&#39;blorp&#39;],&#39;$d&#39;:[1,2],&#39;$e&#39;:[&#39;texa$&#39;,&#39;&#39;]})\r\n    \r\n    df.head()\r\n    \r\n    def eumiro(df,nn):\r\n        df.columns = nn\r\n        #This direct renaming approach is duplicated in methodology in several other answers: \r\n        return df\r\n        \r\n    def lexual1(df):\r\n        return df.rename(columns=col_dict)\r\n    \r\n    def lexual2(df,col_dict):\r\n        return df.rename(columns=col_dict, inplace=True)\r\n        \r\n    def Panda_Master_Hayden(df):\r\n        return df.rename(columns=lambda x: x[1:], inplace=True)\r\n        \r\n    def paulo1(df):\r\n        return df.rename(columns=lambda x: x.replace(&#39;$&#39;, &#39;&#39;))\r\n    \r\n    def paulo2(df):\r\n        return df.rename(columns=lambda x: x.replace(&#39;$&#39;, &#39;&#39;), inplace=True)\r\n        \r\n    def migloo(df,on,nn):\r\n        return df.rename(columns=dict(zip(on, nn)), inplace=True)\r\n        \r\n    def kadee(df):\r\n        return df.columns.str.replace(&#39;$&#39;,&#39;&#39;)\r\n    \r\n    def awo(df):\r\n        columns = df.columns\r\n        columns = [row.replace(&quot;$&quot;,&quot;&quot;) for row in columns]\r\n        return df.rename(columns=dict(zip(columns, &#39;&#39;)), inplace=True)\r\n        \r\n    def kaitlyn(df):\r\n        df.columns = [col.strip(&#39;$&#39;) for col in df.columns]\r\n        return df\r\n    \r\n    print &#39;eumiro&#39;\r\n    cProfile.run(&#39;eumiro(df,new_names)&#39;)\r\n    print &#39;lexual1&#39;\r\n    cProfile.run(&#39;lexual1(df)&#39;)\r\n    print &#39;lexual2&#39;\r\n    cProfile.run(&#39;lexual2(df,col_dict)&#39;)\r\n    print &#39;andy hayden&#39;\r\n    cProfile.run(&#39;Panda_Master_Hayden(df)&#39;)\r\n    print &#39;paulo1&#39;\r\n    cProfile.run(&#39;paulo1(df)&#39;)\r\n    print &#39;paulo2&#39;\r\n    cProfile.run(&#39;paulo2(df)&#39;)\r\n    print &#39;migloo&#39;\r\n    cProfile.run(&#39;migloo(df,old_names,new_names)&#39;)\r\n    print &#39;kadee&#39;\r\n    cProfile.run(&#39;kadee(df)&#39;)\r\n    print &#39;awo&#39;\r\n    cProfile.run(&#39;awo(df)&#39;)\r\n    print &#39;kaitlyn&#39;\r\n    cProfile.run(&#39;kaitlyn(df)&#39;)",
               "tags": [],
               "creation_date": 1441074257,
               "last_edit_date": 1473215059,
               "is_accepted": false,
               "id": "32322596",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 10,
               "answer_id": 33872824,
               "last_activity_date": 1499260775,
               "path": "3.stack.answer",
               "body_markdown": "Another way we could replace the original column labels is by stripping the unwanted characters (here &#39;$&#39;) from the original column labels.\r\n\r\nThis could have been done by running a for loop over df.columns and appending the stripped columns to df.columns.\r\n\r\nInstead , we can do this neatly in a single statement by using list comprehension like below:\r\n\r\n**df.columns = [col.strip(&#39;$&#39;) for col in df.columns]**\r\n\r\n*(&#39;strip&#39; method in Python strips the given character from beginning and end of the string.)*",
               "tags": [],
               "creation_date": 1448286970,
               "last_edit_date": 1499260775,
               "is_accepted": false,
               "id": "33872824",
               "down_vote_count": 1,
               "score": 9
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 33986975,
               "is_accepted": false,
               "last_activity_date": 1448824967,
               "body_markdown": "Real simple just use \r\n\r\n    df.columns = [&#39;Name1&#39;, &#39;Name2&#39;, &#39;Name3&#39;...]\r\nand it will assign the column names by the order you put them",
               "id": "33986975",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1448824967,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 8,
               "answer_id": 35068123,
               "is_accepted": false,
               "last_activity_date": 1454002299,
               "body_markdown": "You could use [`str.slice`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.slice.html) for that:\r\n\r\n    df.columns = df.columns.str.slice(1)",
               "id": "35068123",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1454002299,
               "score": 8
            },
            {
               "up_vote_count": 13,
               "answer_id": 35387028,
               "last_activity_date": 1505305471,
               "path": "3.stack.answer",
               "body_markdown": "    df = pd.DataFrame({&#39;$a&#39;: [1], &#39;$b&#39;: [1], &#39;$c&#39;: [1], &#39;$d&#39;: [1], &#39;$e&#39;: [1]})\r\n\r\n\r\nIf your new list of columns is in the same order as the existing columns, the assignment is simple:\r\n\r\n    new_cols = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]\r\n    df.columns = new_cols\r\n    &gt;&gt;&gt; df\r\n       a  b  c  d  e\r\n    0  1  1  1  1  1\r\n\r\nIf you had a dictionary keyed on old column names to new column names, you could do the following:\r\n\r\n    d = {&#39;$a&#39;: &#39;a&#39;, &#39;$b&#39;: &#39;b&#39;, &#39;$c&#39;: &#39;c&#39;, &#39;$d&#39;: &#39;d&#39;, &#39;$e&#39;: &#39;e&#39;}\r\n    df.columns = df.columns.map(lambda col: d[col])  # Or `.map(d.get)` as pointed out by @PiRSquared.\r\n    &gt;&gt;&gt; df\r\n       a  b  c  d  e\r\n    0  1  1  1  1  1\r\n\r\nIf you don&#39;t have a list or dictionary mapping, you could strip the leading `$` symbol via a list comprehension:\r\n\r\n    df.columns = [col[1:] if col[0] == &#39;$&#39; else col for col in df]\r\n",
               "tags": [],
               "creation_date": 1455409913,
               "last_edit_date": 1505305471,
               "is_accepted": false,
               "id": "35387028",
               "down_vote_count": 0,
               "score": 13
            },
            {
               "up_vote_count": 54,
               "answer_id": 36149967,
               "last_activity_date": 1481524152,
               "path": "3.stack.answer",
               "body_markdown": "    \r\n    df.columns = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]\r\n\r\nIt will replace the existing names with the names you provide, in the order you provide.\r\n\r\nYou can also assign them by index like this:\r\n\r\n    df.columns.values[2] = &#39;c&#39;    #renames the 2nd column to &#39;c&#39;",
               "tags": [],
               "creation_date": 1458637152,
               "last_edit_date": 1481524152,
               "is_accepted": false,
               "id": "36149967",
               "down_vote_count": 1,
               "score": 53
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 7,
               "answer_id": 38776854,
               "is_accepted": false,
               "last_activity_date": 1470342410,
               "body_markdown": "I know this question and answer has been chewed to death. But I referred to it for inspiration for one of the problem I was having . I was able to solve it using bits and pieces from different answers hence providing my response in case anyone needs it.\r\n\r\nMy method is generic wherein you can add additional delimiters by comma separating `delimiters=` variable and future-proof it.\r\n\r\n**Working Code:**\r\n\r\n    import pandas as pd\r\n    import re\r\n    \r\n    \r\n    df = pd.DataFrame({&#39;$a&#39;:[1,2], &#39;$b&#39;: [3,4],&#39;$c&#39;:[5,6], &#39;$d&#39;: [7,8], &#39;$e&#39;: [9,10]})\r\n    \r\n    delimiters = &#39;$&#39;\r\n    matchPattern = &#39;|&#39;.join(map(re.escape, delimiters))\r\n    df.columns = [re.split(matchPattern, i)[1] for i in df.columns ]\r\n\r\n**Output:**\r\n\r\n    &gt;&gt;&gt; df\r\n       $a  $b  $c  $d  $e\r\n    0   1   3   5   7   9\r\n    1   2   4   6   8  10\r\n\r\n    &gt;&gt;&gt; df\r\n       a  b  c  d   e\r\n    0  1  3  5  7   9\r\n    1  2  4  6  8  10",
               "id": "38776854",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1470342410,
               "score": 7
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 5,
               "answer_id": 39215492,
               "is_accepted": false,
               "last_activity_date": 1472506040,
               "body_markdown": "Note that these approach do not work for a MultiIndex. For a MultiIndex, you need to do something like the following:\r\n  \r\n    &gt;&gt;&gt; df = pd.DataFrame({(&#39;$a&#39;,&#39;$x&#39;):[1,2], (&#39;$b&#39;,&#39;$y&#39;): [3,4], (&#39;e&#39;,&#39;f&#39;):[5,6]})\r\n    &gt;&gt;&gt; df\r\n       $a $b  e\r\n       $x $y  f\r\n    0  1  3  5\r\n    1  2  4  6\r\n    &gt;&gt;&gt; rename = {(&#39;$a&#39;,&#39;$x&#39;):(&#39;a&#39;,&#39;x&#39;), (&#39;$b&#39;,&#39;$y&#39;):(&#39;b&#39;,&#39;y&#39;)}\r\n    &gt;&gt;&gt; df.columns = pandas.MultiIndex.from_tuples([\r\n            rename.get(item, item) for item in df.columns.tolist()])\r\n    &gt;&gt;&gt; df\r\n       a  b  e\r\n       x  y  f\r\n    0  1  3  5\r\n    1  2  4  6",
               "id": "39215492",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1472506040,
               "score": 4
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 25,
               "answer_id": 39770407,
               "is_accepted": false,
               "last_activity_date": 1475152240,
               "body_markdown": "#Column names vs Names of Series\r\n\r\nI would like to explain a bit what happens behind the scenes.\r\n\r\nDataframes are a set of Series.\r\n\r\nSeries in turn are an extension of a `numpy.array`\r\n\r\n`numpy.array`s have a property `.name`\r\n\r\nThis is the name of the series. It is seldom that pandas respects this attribute, but it lingers in places and can be used to hack some pandas behaviors.\r\n\r\n#Naming the list of columns\r\n\r\nA lot of answers here talks about the `df.columns` attribute being a `list` when in fact it is a `Series`. This means it has a `.name` attribute.\r\n\r\nThis is what happens if you decide to fill in the name of the columns `Series`:\r\n\r\n    df.columns = [&#39;column_one&#39;, &#39;column_two&#39;]\r\n    df.columns.names = [&#39;name of the list of columns&#39;]\r\n    df.index.names = [&#39;name of the index&#39;]\r\n\r\n    name of the list of columns \tcolumn_one \tcolumn_two\r\n    name of the index \t\t\r\n    0 \t                                 4 \t         1\r\n    1 \t                                 5 \t         2\r\n    2 \t                                 6 \t         3\r\n\r\nNote that the name of the index always comes one column lower.\r\n\r\n##Artifacts that linger\r\n\r\nThe `.name` attribute lingers on sometimes. If you set `df.columns = [&#39;one&#39;, &#39;two&#39;]` then the `df.one.name` will be `&#39;one&#39;`.\r\n\r\nIf you set `df.one.name = &#39;three&#39;` then `df.columns` will still give you `[&#39;one&#39;, &#39;two&#39;]`, and `df.one.name` will give you `&#39;three&#39;`\r\n\r\n###BUT\r\n\r\n`pd.DataFrame(df.one)` will return\r\n\r\n        three\r\n    0       1\r\n    1       2\r\n    2       3\r\n\r\nBecause pandas reuses the `.name` of the already defined `Series`.\r\n\r\n#Multi level column names\r\n\r\nPandas has ways of doing multi layered column names. There is not so much magic involved but I wanted to cover this in my answer too since I don&#39;t see anyone picking up on this here.\r\n\r\n     \t|one            |\r\n    \t|one \t  |two  |\r\n    0 \t|  4 \t  |  1  |\r\n    1 \t|  5 \t  |  2  |\r\n    2 \t|  6 \t  |  3  |\r\n\r\nThis is easily achievable by setting columns to lists, like this:\r\n\r\n    df.columns = [[&#39;one&#39;, &#39;one&#39;], [&#39;one&#39;, &#39;two&#39;]]\r\n",
               "id": "39770407",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1475152240,
               "score": 25
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 44584447,
               "is_accepted": false,
               "last_activity_date": 1497601657,
               "body_markdown": "If you have to deal with loads of columns named by the providing system out of your control, I came up with the following approach that is a combination of a general approach and specific replacments in one go.\r\n\r\nI first create a dictionary from the dataframe column names using regex expressions in order to throw away certain appendixes of column names \r\nand then I add specific replacements to the dictionary to name core columns as expected later in the receiving database.\r\n\r\nThis is then applied to the dataframe in one go.\r\n\r\n    dict=dict(zip(df.columns,df.columns.str.replace(&#39;(:S$|:C1$|:L$|:D$|\\.Serial:L$)&#39;,&#39;&#39;)))\r\n    dict[&#39;brand_timeseries:C1&#39;]=&#39;BTS&#39;\r\n    dict[&#39;respid:L&#39;]=&#39;RespID&#39;\r\n    dict[&#39;country:C1&#39;]=&#39;CountryID\r\n    dict[&#39;pim1:D&#39;]=&#39;pim_actual&#39;\r\n    df.rename(columns=dict, inplace=True)",
               "id": "44584447",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1497601657,
               "score": 2
            },
            {
               "up_vote_count": 20,
               "answer_id": 46192213,
               "last_activity_date": 1505293815,
               "path": "3.stack.answer",
               "body_markdown": "# One line or Pipeline solutions\r\n\r\nI&#39;ll focus on two things:\r\n\r\n1. OP clearly states\r\n&gt; I have the edited column names stored it in a list, but I don&#39;t know how to replace the column names.  \r\n\r\n    I do not want to solve the problem of how to replace `&#39;$&#39;` or strip the first character off of each column header.  OP has already done this step.  Instead I want to focus on replacing the existing `columns` object with a new one given a list of replacement column names.\r\n\r\n2. `df.columns = new` where `new` is the list of new columns names is as simple as it gets.  The drawback of this approach is that it requires editing the existing dataframe&#39;s `columns` attribute and it isn&#39;t done inline.  I&#39;ll show a few ways to perform this via pipelining without editing the existing dataframe.\r\n\r\n___\r\n\r\n**Setup 1**  \r\nTo focus on the need to rename of replace column names with a pre-existing list, I&#39;ll create a new sample dataframe `df` with initial column names and unrelated new column names.\r\n\r\n    df = pd.DataFrame({&#39;Jack&#39;: [1, 2], &#39;Mahesh&#39;: [3, 4], &#39;Xin&#39;: [5, 6]})\r\n    new = [&#39;x098&#39;, &#39;y765&#39;, &#39;z432&#39;]\r\n    \r\n    df\r\n\r\n       Jack  Mahesh  Xin\r\n    0     1       3    5\r\n    1     2       4    6\r\n\r\n___\r\n**Solution 1**  \r\n[**`pd.DataFrame.rename`**][1]  \r\n\r\nIt has been said already that **if** you had a dictionary mapping the old column names to new column names, you could use `pd.DataFrame.rename`.\r\n\r\n    d = {&#39;Jack&#39;: &#39;x098&#39;, &#39;Mahesh&#39;: &#39;y765&#39;, &#39;Xin&#39;: &#39;z432&#39;}\r\n    df.rename(columns=d)\r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\nHowever, you can easily create that dictionary and include it in the call to `rename`.  The following takes advantage of the fact that when iterating over `df`, we iterate over each column name.\r\n\r\n    # given just a list of new column names\r\n    df.rename(columns=dict(zip(df, new)))\r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\nThis works great if your original column names are unique.  But if they are not, then this breaks down.\r\n\r\n___\r\n**Setup 2**  \r\nnon-unique columns  \r\n\r\n    df = pd.DataFrame(\r\n        [[1, 3, 5], [2, 4, 6]],\r\n        columns=[&#39;Mahesh&#39;, &#39;Mahesh&#39;, &#39;Xin&#39;]\r\n    )\r\n    new = [&#39;x098&#39;, &#39;y765&#39;, &#39;z432&#39;]\r\n    \r\n    df\r\n\r\n       Mahesh  Mahesh  Xin\r\n    0       1       3    5\r\n    1       2       4    6\r\n\r\n___\r\n**Solution 2**  \r\n[**`pd.concat`**][2] using the `keys` argument  \r\n\r\nFirst, notice what happens when we attempt to use solution 1:\r\n\r\n    df.rename(columns=dict(zip(df, new)))\r\n\r\n       y765  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\nWe didn&#39;t map the `new` list as the column names.  We ended up repeating `y765`.  Instead, we can use the `keys` argument of the `pd.concat` function while iterating through the columns of `df`.\r\n\r\n    pd.concat([c for _, c in df.items()], axis=1, keys=new) \r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\n___\r\n**Solution 3**  \r\nReconstruct.  This should only be used if you have a single `dtype` for all columns.  Otherwise, you&#39;ll end up with `dtype` `object` for all columns and converting them back requires more dictionary work.\r\n\r\n*Single `dtype`*  \r\n\r\n    pd.DataFrame(df.values, df.index, new)\r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\n*Mixed `dtype`*  \r\n\r\n    pd.DataFrame(df.values, df.index, new).astype(dict(zip(new, df.dtypes)))\r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\n___\r\n\r\n**Solution 4**  \r\nThis is a gimmicky trick with `transpose` and `set_index`.  [**`pd.DataFrame.set_index`**][3] allows us to set an index inline but there is no corresponding `set_columns`.  So we can transpose, then `set_index`, and transpose back.  However, the same single `dtype` versus mixed `dtype` caveat from solution 3 applies here.\r\n\r\n*Single `dtype`*  \r\n\r\n    df.T.set_index(np.asarray(new)).T\r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\n*Mixed `dtype`*  \r\n\r\n    df.T.set_index(np.asarray(new)).T.astype(dict(zip(new, df.dtypes)))\r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\n___\r\n**Solution 5**  \r\nUse a `lambda` in `pd.DataFrame.rename` that cycles through each element of `new`  \r\nIn this solution, we pass a lambda that takes `x` but then ignores it.  It also takes a `y` but doesn&#39;t expect it.  Instead, an iterator is given as a default value and I can then use that to cycle through one at a time without regard to what the value of `x` is.\r\n\r\n    df.rename(columns=lambda x, y=iter(new): next(y))\r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\nAnd as pointed out to me by the folks in *sopython* chat, if I add a `*` in between `x` and `y`, I can protect my `y` variable.  Though, in this context I don&#39;t believe it needs protecting.  It is still worth mentioning.\r\n\r\n    df.rename(columns=lambda x, *, y=iter(new): next(y))\r\n\r\n       x098  y765  z432\r\n    0     1     3     5\r\n    1     2     4     6\r\n\r\n  [1]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html\r\n  [2]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html\r\n  [3]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html",
               "tags": [],
               "creation_date": 1505290163,
               "last_edit_date": 1505293815,
               "is_accepted": false,
               "id": "46192213",
               "down_vote_count": 0,
               "score": 20
            },
            {
               "up_vote_count": 13,
               "answer_id": 46737673,
               "last_activity_date": 1508084703,
               "path": "3.stack.answer",
               "body_markdown": "#DataFrame -- df.rename() will work.\r\n\r\n    df.rename(columns = {&#39;Old Name&#39;:&#39;New Name&#39;})\r\n\r\n&gt; df is the DataFrame you have, and the **Old Name** is the column name you\r\n&gt; want to change, then the **New Name** is the new name you change to. This DataFrame built-in method makes things very easier.\r\n\r\n",
               "tags": [],
               "creation_date": 1507927033,
               "last_edit_date": 1508084703,
               "is_accepted": false,
               "id": "46737673",
               "down_vote_count": 2,
               "score": 11
            },
            {
               "up_vote_count": 51,
               "answer_id": 46912050,
               "last_activity_date": 1510947117,
               "path": "3.stack.answer",
               "body_markdown": "# Pandas 0.21+ Answer\r\nThere have been some significant updates to column renaming in version 0.21. \r\n\r\n* The [`rename` method](http://pandas.pydata.org/pandas-docs/version/0.21/whatsnew.html#rename-reindex-now-also-accept-axis-keyword) has added the `axis` parameter which may be set to `columns` or `1`. This update makes this method match the rest of the pandas API. It still has the `index` and `columns` parameters but you are no longer forced to use them. \r\n* The [`set_axis` method](http://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.set_axis.html#pandas.DataFrame.set_axis) with the `inplace` set to `False` enables you to rename all the index or column labels with a list.\r\n\r\n## Examples for Pandas 0.21+\r\n\r\nConstruct sample DataFrame:\r\n\r\n    df = pd.DataFrame({&#39;$a&#39;:[1,2], &#39;$b&#39;: [3,4], \r\n                       &#39;$c&#39;:[5,6], &#39;$d&#39;:[7,8], \r\n                       &#39;$e&#39;:[9,10]})\r\n\r\n       $a  $b  $c  $d  $e\r\n    0   1   3   5   7   9\r\n    1   2   4   6   8  10\r\n\r\n### Using `rename` with `axis=&#39;columns&#39;` or `axis=1`\r\n\r\n    df.rename({&#39;$a&#39;:&#39;a&#39;, &#39;$b&#39;:&#39;b&#39;, &#39;$c&#39;:&#39;c&#39;, &#39;$d&#39;:&#39;d&#39;, &#39;$e&#39;:&#39;e&#39;}, axis=&#39;columns&#39;)\r\n\r\nor \r\n\r\n    df.rename({&#39;$a&#39;:&#39;a&#39;, &#39;$b&#39;:&#39;b&#39;, &#39;$c&#39;:&#39;c&#39;, &#39;$d&#39;:&#39;d&#39;, &#39;$e&#39;:&#39;e&#39;}, axis=1)\r\n\r\nBoth result in the following:\r\n\r\n       a  b  c  d   e\r\n    0  1  3  5  7   9\r\n    1  2  4  6  8  10\r\n\r\n\r\nIt is still possible to use the old method signature:\r\n\r\n    df.rename(columns={&#39;$a&#39;:&#39;a&#39;, &#39;$b&#39;:&#39;b&#39;, &#39;$c&#39;:&#39;c&#39;, &#39;$d&#39;:&#39;d&#39;, &#39;$e&#39;:&#39;e&#39;})\r\n\r\nThe `rename` function also accepts functions that will be applied to each column name.\r\n\r\n    df.rename(lambda x: x[1:], axis=&#39;columns&#39;)\r\n\r\nor\r\n\r\n    df.rename(lambda x: x[1:], axis=1)\r\n\r\n_____\r\n### Using `set_axis` with a list and `inplace=False`\r\n\r\nYou can supply a list to the `set_axis` method that is equal in length to the number of columns (or index). Currently, `inplace` defaults to `True`, but `inplace` will be defaulted to `False` in future releases.\r\n\r\n    df.set_axis([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], axis=&#39;columns&#39;, inplace=False)\r\n\r\nor\r\n\r\n    df.set_axis([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], axis=1, inplace=False)\r\n____\r\n\r\n### Why not use `df.columns = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]`?\r\n\r\nThere is nothing wrong with assigning columns directly like this. It is a perfectly good solution. \r\n\r\nThe advantage of using `set_axis` is that it can be used as part of a method chain and that it returns a new copy of the DataFrame. Without it, you would have to store your intermediate steps of the chain to another variable before reassigning the columns.\r\n\r\n    # new for pandas 0.21+\r\n    df.some_method1()\r\n      .some_method2()\r\n      .set_axis()\r\n      .some_method3()\r\n    \r\n    # old way\r\n    df1 = df.some_method1()\r\n            .some_method2()\r\n    df1.columns = columns\r\n    df1.some_method3()",
               "tags": [],
               "creation_date": 1508852355,
               "last_edit_date": 1510947117,
               "is_accepted": false,
               "id": "46912050",
               "down_vote_count": 0,
               "score": 51
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 2,
               "answer_id": 47068084,
               "is_accepted": false,
               "last_activity_date": 1509599858,
               "body_markdown": "In case you don&#39;t want the row names `df.columns = [&#39;a&#39;, &#39;b&#39;,index=False]`",
               "id": "47068084",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1509599858,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 48700719,
               "is_accepted": false,
               "last_activity_date": 1518161307,
               "body_markdown": "Try this. It works for me\r\n\r\n\r\n`df.rename(index=str, columns={&quot;$a&quot;: &quot;a&quot;, &quot;$b&quot;: &quot;b&quot;, &quot;$c&quot; : &quot;c&quot;, &quot;$d&quot; : &quot;d&quot;, &quot;$e&quot; : &quot;e&quot;})`\r\n",
               "id": "48700719",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1518161307,
               "score": 1
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 48956687,
               "is_accepted": false,
               "last_activity_date": 1519422438,
               "body_markdown": "`df = df.rename(columns=lambda n: n.replace(&#39;$&#39;, &#39;&#39;))`\r\nis a functional way of solving this",
               "id": "48956687",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1519422438,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas",
         "id": "858127-2322"
      },
      {
         "up_vote_count": "2514",
         "path": "2.stack",
         "body_markdown": "I&#39;m looking for a `string.contains` or `string.indexof` method in Python.\r\n\r\nI want to do:\r\n\r\n    if not somestring.contains(&quot;blah&quot;):\r\n       continue\r\n",
         "view_count": "2230529",
         "answer_count": "12",
         "tags": "['python', 'string', 'substring', 'contains']",
         "creation_date": "1281322370",
         "last_edit_date": "1495821772",
         "code_snippet": "['<code>string.contains</code>', '<code>string.indexof</code>', '<code>if not somestring.contains(\"blah\"):\\n   continue\\n</code>', '<code>in</code>', '<code>if \"blah\" not in somestring: \\n    continue\\n</code>', '<code>string.find(\"substring\")</code>', '<code>find</code>', '<code>index</code>', '<code>in</code>', '<code>s = \"This be a string\"\\nif s.find(\"is\") == -1:\\n    print \"No \\'is\\' here!\"\\nelse:\\n    print \"Found \\'is\\' in the string.\"\\n</code>', \"<code>Found 'is' in the string.</code>\", '<code>if \"is\" in s:</code>', '<code>True</code>', \"<code>if ' is ' in s:</code>\", '<code>False</code>', '<code>\\\\bis\\\\b</code>', \"<code>' is '</code>\", \"<code>This is, a comma'</code>\", \"<code>'It is.'</code>\", '<code>s.split(string.punctuation + string.whitespace)</code>', '<code>split</code>', '<code>strip</code>', '<code>rstrip</code>', '<code>lstrip</code>', \"<code>r'\\\\bis\\\\b'</code>\", \"<code>'is' not in (w.lower() for w in s.translate(string.maketrans(' ' * len(string.punctuation + string.whitespace), string.punctuation + string.whitespace)).split()</code>\", '<code>if needle in haystack:</code>', '<code>in</code>', '<code>key=</code>', \"<code>'haystack'.__contains__</code>\", '<code>if</code>', '<code>in</code>', '<code>in</code>', '<code>True</code>', '<code>False</code>', '<code>&gt;&gt;&gt; \"King\" in \"King\\'s landing\"\\nTrue\\n\\n&gt;&gt;&gt; \"Jon Snow\" in \"King\\'s landing\"\\nFalse\\n</code>', '<code>str.find()</code>', '<code>str.find()</code>', '<code>.find()</code>', '<code>&gt;&gt;&gt; some_string = \"valar morghulis\"\\n\\n&gt;&gt;&gt; some_string.find(\"morghulis\")\\n6\\n\\n&gt;&gt;&gt; some_string.find(\"dohaeris\")\\n-1\\n</code>', '<code>in</code>', \"<code>&gt;&gt;&gt; 'foo' in '**foo**'\\nTrue\\n</code>\", '<code>not in</code>', \"<code>&gt;&gt;&gt; 'foo' not in '**foo**' # returns False\\nFalse\\n</code>\", \"<code>not 'foo' in '**foo**'</code>\", '<code>__contains__</code>', '<code>find</code>', '<code>index</code>', '<code>contains</code>', \"<code>str.__contains__('**foo**', 'foo')\\n</code>\", '<code>True</code>', \"<code>'**foo**'.__contains__('foo')\\n</code>\", '<code>in</code>', '<code>not in</code>', '<code>str</code>', '<code>class NoisyString(str):\\n    def __contains__(self, other):\\n        print(\\'testing if \"{0}\" in \"{1}\"\\'.format(other, self))\\n        return super(NoisyString, self).__contains__(other)\\n\\nns = NoisyString(\\'a string with a substring inside\\')\\n</code>', '<code>&gt;&gt;&gt; \\'substring\\' in ns\\ntesting if \"substring\" in \"a string with a substring inside\"\\nTrue\\n</code>', '<code>&gt;&gt;&gt; \\'**foo**\\'.index(\\'foo\\')\\n2\\n&gt;&gt;&gt; \\'**foo**\\'.find(\\'foo\\')\\n2\\n\\n&gt;&gt;&gt; \\'**oo**\\'.find(\\'foo\\')\\n-1\\n&gt;&gt;&gt; \\'**oo**\\'.index(\\'foo\\')\\n\\nTraceback (most recent call last):\\n  File \"&lt;pyshell#40&gt;\", line 1, in &lt;module&gt;\\n    \\'**oo**\\'.index(\\'foo\\')\\nValueError: substring not found\\n</code>', '<code>in</code>', \"<code>import timeit\\n\\ndef in_(s, other):\\n    return other in s\\n\\ndef contains(s, other):\\n    return s.__contains__(other)\\n\\ndef find(s, other):\\n    return s.find(other) != -1\\n\\ndef index(s, other):\\n    try:\\n        s.index(other)\\n    except ValueError:\\n        return False\\n    else:\\n        return True\\n\\n\\n\\nperf_dict = {\\n'in:True': min(timeit.repeat(lambda: in_('superstring', 'str'))),\\n'in:False': min(timeit.repeat(lambda: in_('superstring', 'not'))),\\n'__contains__:True': min(timeit.repeat(lambda: contains('superstring', 'str'))),\\n'__contains__:False': min(timeit.repeat(lambda: contains('superstring', 'not'))),\\n'find:True': min(timeit.repeat(lambda: find('superstring', 'str'))),\\n'find:False': min(timeit.repeat(lambda: find('superstring', 'not'))),\\n'index:True': min(timeit.repeat(lambda: index('superstring', 'str'))),\\n'index:False': min(timeit.repeat(lambda: index('superstring', 'not'))),\\n}\\n</code>\", '<code>in</code>', \"<code>&gt;&gt;&gt; perf_dict\\n{'in:True': 0.16450627865128808,\\n 'in:False': 0.1609668098178645,\\n '__contains__:True': 0.24355481654697542,\\n '__contains__:False': 0.24382793854783813,\\n 'find:True': 0.3067379407923454,\\n 'find:False': 0.29860888058124146,\\n 'index:True': 0.29647137792585454,\\n 'index:False': 0.5502287584545229}\\n</code>\", '<code>str.index</code>', '<code>str.find</code>', '<code>s.find(ss) != -1</code>', '<code>ss in s</code>', '<code>re</code>', '<code>string.contains(str)</code>', '<code>in</code>', '<code>if substring in someString:\\n    print \"It\\'s there!!!\"\\n</code>', \"<code># Print all files with dot in home directory\\nimport commands\\n(st, output) = commands.getstatusoutput('ls -a ~')\\nprint [f for f in output.split('\\\\n') if '.' in f ]\\n</code>\", '<code>in</code>', '<code>in</code>', '<code>\"foo\" in \"foobar\"\\nTrue\\n\\n\"foo\" in \"Foobar\"\\nFalse\\n\\n\"foo\" in \"Foobar\".lower()\\nTrue\\n\\n\"foo\".capitalize() in \"Foobar\"\\nTrue\\n\\n\"foo\" in [\"bar\", \"foo\", \"foobar\"]\\nTrue\\n\\n\"foo\" in [\"fo\", \"o\", \"foobar\"]\\nFalse\\n</code>', '<code>in</code>', '<code>[\"bar\", \"foo\", \"foobar\"] in \"foof\"</code>', \"<code>names = ['bob', 'john', 'mike']\\nany(st in 'bob and john' for st in names) \\n&gt;&gt; True\\n\\nany(st in 'mary and jane' for st in names) \\n&gt;&gt; False\\n</code>\", '<code>any()</code>', '<code>all()</code>', \"<code>any([st in 'bob and john' for st in names]) &gt;&gt;&gt; True</code>\", '<code>any</code>', '<code>True</code>', '<code>str1 = \"This be a string\"\\nfind_this = \"tr\"\\nif find_this in str1:\\n    print find_this, \" is been found in \", str1\\nelse:\\n    print find_this, \" is not found in \", str1\\n</code>', '<code>find_that = \"ng\"</code>', '<code>if (find_this or find_that) in str1:     print \"I found find_this or find_that\"</code>', '<code>in</code>', '<code>True</code>', '<code>False</code>', '<code>example_string = \"This is an example string\"\\nsubstring = \"example\"\\nprint(substring in example_string)\\n</code>', '<code>True\\n</code>', '<code>find</code>', \"<code>if example_string.find(substring) != -1:\\n    print('Substring found!')\\nelse:\\n    print('Substring not found!')\\n</code>\", '<code>Substring found!\\n</code>', '<code>if \"insert_char_or_string_here\" in \"insert_string_to_search_here\":\\n    //DOSTUFF\\n</code>', '<code>if not \"insert_char_or_string_here\" in \"insert_string_to_search_here\":\\n    //DOSTUFF\\n</code>', '<code>if \"insert_char_or_string_here\" not in \"insert_string_to_search_here\":\\n    //DOSTUFF\\n</code>', \"<code>s = 'This is my text example'\\nif 'is' not in (w.lower() for w in s.split(string.punctuation + string.whitespace)):\\n     # do something\\n</code>\"]",
         "title": "Does Python have a string &#39;contains&#39; substring method?",
         "_childDocuments_": [
            {
               "up_vote_count": 387,
               "answer_id": 3437068,
               "last_activity_date": 1495821823,
               "path": "3.stack.answer",
               "body_markdown": "If it&#39;s just a substring search you can use `string.find(&quot;substring&quot;)`.\r\n\r\nYou do have to be a little careful with `find`, `index`, and `in` though, as they are substring searches. In other words, this:\r\n\r\n    s = &quot;This be a string&quot;\r\n    if s.find(&quot;is&quot;) == -1:\r\n        print &quot;No &#39;is&#39; here!&quot;\r\n    else:\r\n        print &quot;Found &#39;is&#39; in the string.&quot;\r\n\r\nIt would print `Found &#39;is&#39; in the string.` Similarly, `if &quot;is&quot; in s:` would evaluate to `True`. This may or may not be what you want.\r\n",
               "tags": [],
               "creation_date": 1281322504,
               "last_edit_date": 1495821823,
               "is_accepted": false,
               "id": "3437068",
               "down_vote_count": 3,
               "score": 384
            },
            {
               "up_vote_count": 3756,
               "answer_id": 3437070,
               "last_activity_date": 1447284617,
               "path": "3.stack.answer",
               "body_markdown": "You can use the [`in` operator][1]:\r\n\r\n    if &quot;blah&quot; not in somestring: \r\n        continue\r\n    \r\n\r\n\r\n  [1]: https://docs.python.org/reference/expressions.html#membership-test-details",
               "tags": [],
               "creation_date": 1281322581,
               "last_edit_date": 1447284617,
               "is_accepted": true,
               "id": "3437070",
               "down_vote_count": 1,
               "score": 3755
            },
            {
               "up_vote_count": 110,
               "answer_id": 3437130,
               "last_activity_date": 1434773237,
               "path": "3.stack.answer",
               "body_markdown": "`if needle in haystack:` is the normal use, as @Michael says -- it relies on the [`in`][1] operator, more readable and faster than a method call.\r\n\r\nIf you truly need a method instead of an operator (e.g. to do some weird `key=` for a very peculiar sort...?), that would be [`&#39;haystack&#39;.__contains__`][2].  But since your example is for use in an `if`, I guess you don&#39;t really mean what you say;-).  It&#39;s not good form (nor readable, nor efficient) to use special methods directly -- they&#39;re meant to be used, instead, through the operators and builtins that delegate to them.\r\n\r\n\r\n  [1]: https://docs.python.org/reference/expressions.html#membership-test-details\r\n  [2]: https://docs.python.org/reference/datamodel.html#object.__contains__",
               "tags": [],
               "creation_date": 1281323949,
               "last_edit_date": 1434773237,
               "is_accepted": false,
               "id": "3437130",
               "down_vote_count": 0,
               "score": 110
            },
            {
               "up_vote_count": 20,
               "answer_id": 6859010,
               "last_activity_date": 1495821899,
               "path": "3.stack.answer",
               "body_markdown": "Another way to find whether a string contains a few characters or not with the Boolean return value (i.e. `True` or `False):\r\n\r\n    str1 = &quot;This be a string&quot;\r\n    find_this = &quot;tr&quot;\r\n    if find_this in str1:\r\n        print find_this, &quot; is been found in &quot;, str1\r\n    else:\r\n        print find_this, &quot; is not found in &quot;, str1\r\n\r\n",
               "tags": [],
               "creation_date": 1311856324,
               "last_edit_date": 1495821899,
               "is_accepted": false,
               "id": "6859010",
               "down_vote_count": 7,
               "score": 13
            },
            {
               "up_vote_count": 55,
               "answer_id": 19101749,
               "last_activity_date": 1495821950,
               "path": "3.stack.answer",
               "body_markdown": "No, there isn&#39;t any `string.contains(str)` method, but there is the `in` operator:\r\n\r\n    if substring in someString:\r\n        print &quot;It&#39;s there!!!&quot;\r\n\r\nHere is a more complex working example:\r\n\r\n    # Print all files with dot in home directory\r\n    import commands\r\n    (st, output) = commands.getstatusoutput(&#39;ls -a ~&#39;)\r\n    print [f for f in output.split(&#39;\\n&#39;) if &#39;.&#39; in f ]\r\n",
               "tags": [],
               "creation_date": 1380567586,
               "last_edit_date": 1495821950,
               "is_accepted": false,
               "id": "19101749",
               "down_vote_count": 0,
               "score": 55
            },
            {
               "up_vote_count": 60,
               "answer_id": 27138045,
               "last_activity_date": 1509639888,
               "path": "3.stack.answer",
               "body_markdown": "&gt; ## Does Python have a string contains substring method?\r\n\r\nYes, but Python has a comparison operator that you should use instead, because the language intends its usage, and other programmers will expect you to use it. That keyword is `in`, which is used as a comparison operator:\r\n\r\n    &gt;&gt;&gt; &#39;foo&#39; in &#39;**foo**&#39;\r\n    True\r\n\r\nThe opposite (complement), which the original question asks for, is `not in`:\r\n\r\n    &gt;&gt;&gt; &#39;foo&#39; not in &#39;**foo**&#39; # returns False\r\n    False\r\n\r\nThis is semantically the same as `not &#39;foo&#39; in &#39;**foo**&#39;` but it&#39;s much more readable and explicitly provided for in the language as a readability improvement.\r\n\r\n\r\n## Avoid using `__contains__`, `find`, and `index`\r\n\r\nAs promised, here&#39;s the `contains` method:\r\n\r\n    str.__contains__(&#39;**foo**&#39;, &#39;foo&#39;)\r\n\r\nreturns `True`. You could also call this function from the instance of the superstring:\r\n\r\n    &#39;**foo**&#39;.__contains__(&#39;foo&#39;)\r\n\r\nBut don&#39;t. Methods that start with underscores are considered semantically private. The only reason to use this is when extending the `in` and `not in` functionality (e.g. if subclassing `str`): \r\n\r\n    class NoisyString(str):\r\n        def __contains__(self, other):\r\n            print(&#39;testing if &quot;{0}&quot; in &quot;{1}&quot;&#39;.format(other, self))\r\n            return super(NoisyString, self).__contains__(other)\r\n\r\n    ns = NoisyString(&#39;a string with a substring inside&#39;)\r\n\r\nand now:\r\n\r\n    &gt;&gt;&gt; &#39;substring&#39; in ns\r\n    testing if &quot;substring&quot; in &quot;a string with a substring inside&quot;\r\n    True\r\n\r\nAlso, avoid the following string methods:\r\n\r\n    &gt;&gt;&gt; &#39;**foo**&#39;.index(&#39;foo&#39;)\r\n    2\r\n    &gt;&gt;&gt; &#39;**foo**&#39;.find(&#39;foo&#39;)\r\n    2\r\n\r\n    &gt;&gt;&gt; &#39;**oo**&#39;.find(&#39;foo&#39;)\r\n    -1\r\n    &gt;&gt;&gt; &#39;**oo**&#39;.index(&#39;foo&#39;)\r\n    \r\n    Traceback (most recent call last):\r\n      File &quot;&lt;pyshell#40&gt;&quot;, line 1, in &lt;module&gt;\r\n        &#39;**oo**&#39;.index(&#39;foo&#39;)\r\n    ValueError: substring not found\r\n\r\nOther languages may have no methods to directly test for substrings, and so you would have to use these types of methods, but with Python, it is much more efficient to use the `in` comparison operator.\r\n\r\n## Performance comparisons\r\n\r\nWe can compare various ways of accomplishing the same goal.\r\n\r\n\r\n    import timeit\r\n\r\n    def in_(s, other):\r\n        return other in s\r\n    \r\n    def contains(s, other):\r\n        return s.__contains__(other)\r\n    \r\n    def find(s, other):\r\n        return s.find(other) != -1\r\n    \r\n    def index(s, other):\r\n        try:\r\n            s.index(other)\r\n        except ValueError:\r\n            return False\r\n        else:\r\n            return True\r\n    \r\n\r\n\r\n    perf_dict = {\r\n    &#39;in:True&#39;: min(timeit.repeat(lambda: in_(&#39;superstring&#39;, &#39;str&#39;))),\r\n    &#39;in:False&#39;: min(timeit.repeat(lambda: in_(&#39;superstring&#39;, &#39;not&#39;))),\r\n    &#39;__contains__:True&#39;: min(timeit.repeat(lambda: contains(&#39;superstring&#39;, &#39;str&#39;))),\r\n    &#39;__contains__:False&#39;: min(timeit.repeat(lambda: contains(&#39;superstring&#39;, &#39;not&#39;))),\r\n    &#39;find:True&#39;: min(timeit.repeat(lambda: find(&#39;superstring&#39;, &#39;str&#39;))),\r\n    &#39;find:False&#39;: min(timeit.repeat(lambda: find(&#39;superstring&#39;, &#39;not&#39;))),\r\n    &#39;index:True&#39;: min(timeit.repeat(lambda: index(&#39;superstring&#39;, &#39;str&#39;))),\r\n    &#39;index:False&#39;: min(timeit.repeat(lambda: index(&#39;superstring&#39;, &#39;not&#39;))),\r\n    }\r\n\r\nAnd now we see that using `in` is much faster than the others.\r\nLess time to do an equivalent operation is better:\r\n\r\n    &gt;&gt;&gt; perf_dict\r\n    {&#39;in:True&#39;: 0.16450627865128808,\r\n     &#39;in:False&#39;: 0.1609668098178645,\r\n     &#39;__contains__:True&#39;: 0.24355481654697542,\r\n     &#39;__contains__:False&#39;: 0.24382793854783813,\r\n     &#39;find:True&#39;: 0.3067379407923454,\r\n     &#39;find:False&#39;: 0.29860888058124146,\r\n     &#39;index:True&#39;: 0.29647137792585454,\r\n     &#39;index:False&#39;: 0.5502287584545229}\r\n\r\n&lt;!--\r\n\r\nEveryone likes a good dataviz. Here&#39;s one using pandas and matplotlib:\r\n\r\n    import pandas\r\n    import matplotlib.pyplot as plt\r\n    s = pandas.Series(list(perf_dict.values()), index=list(perf_dict.keys()))\r\n    s.plot(&#39;bar&#39;)    \r\n    plt.show()\r\n\r\n--&gt;",
               "tags": [],
               "creation_date": 1416954828,
               "last_edit_date": 1509639888,
               "is_accepted": false,
               "id": "27138045",
               "down_vote_count": 1,
               "score": 59
            },
            {
               "up_vote_count": 8,
               "answer_id": 30446999,
               "last_activity_date": 1440493664,
               "path": "3.stack.answer",
               "body_markdown": "Here is your answer:\r\n \r\n    if &quot;insert_char_or_string_here&quot; in &quot;insert_string_to_search_here&quot;:\r\n        //DOSTUFF\r\n\r\nFor checking if it is false:\r\n\r\n    if not &quot;insert_char_or_string_here&quot; in &quot;insert_string_to_search_here&quot;:\r\n        //DOSTUFF\r\nOR:\r\n\r\n    if &quot;insert_char_or_string_here&quot; not in &quot;insert_string_to_search_here&quot;:\r\n        //DOSTUFF\r\n",
               "tags": [],
               "creation_date": 1432594259,
               "last_edit_date": 1440493664,
               "is_accepted": false,
               "id": "30446999",
               "down_vote_count": 3,
               "score": 5
            },
            {
               "up_vote_count": 76,
               "answer_id": 30465415,
               "last_activity_date": 1495822145,
               "path": "3.stack.answer",
               "body_markdown": "Basically, you want to find a substring in a string in python. There are two ways to search for a substring in a string in Python.\r\n\r\n**Method 1: `in` operator**\r\n\r\nYou can use the Python&#39;s `in` operator to check for a substring. It&#39;s quite simple and intuitive. It will return `True` if the substring was found in the string else `False`.\r\n\r\n    &gt;&gt;&gt; &quot;King&quot; in &quot;King&#39;s landing&quot;\r\n    True\r\n\r\n    &gt;&gt;&gt; &quot;Jon Snow&quot; in &quot;King&#39;s landing&quot;\r\n    False\r\n\r\n**Method 2: `str.find()` method**\r\n\r\nThe second method is to use the `str.find()` method. Here, we call the `.find()` method on the string in which substring is to found. We pass the substring to the find() method and check its return value. If its value is other than -1, the substring was found in the string, otherwise not. The value returned is the index where substring was found.\r\n\r\n    &gt;&gt;&gt; some_string = &quot;valar morghulis&quot;\r\n\r\n    &gt;&gt;&gt; some_string.find(&quot;morghulis&quot;)\r\n    6\r\n\r\n    &gt;&gt;&gt; some_string.find(&quot;dohaeris&quot;)\r\n    -1\r\n\r\nI would recommend you to use the first method as it is more Pythonic and intuitive.\r\n\r\n",
               "tags": [],
               "creation_date": 1432662373,
               "last_edit_date": 1495822145,
               "is_accepted": false,
               "id": "30465415",
               "down_vote_count": 0,
               "score": 76
            },
            {
               "up_vote_count": 16,
               "answer_id": 31476788,
               "last_activity_date": 1459932562,
               "path": "3.stack.answer",
               "body_markdown": "So apparently there is nothing similar for vector-wise comparison. An obvious Python way to do so would be:\r\n\r\n\r\n    names = [&#39;bob&#39;, &#39;john&#39;, &#39;mike&#39;]\r\n    any(st in &#39;bob and john&#39; for st in names) \r\n    &gt;&gt; True\r\n\r\n    any(st in &#39;mary and jane&#39; for st in names) \r\n    &gt;&gt; False",
               "tags": [],
               "creation_date": 1437139176,
               "last_edit_date": 1459932562,
               "is_accepted": false,
               "id": "31476788",
               "down_vote_count": 0,
               "score": 16
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 9,
               "answer_id": 40708107,
               "is_accepted": false,
               "last_activity_date": 1479668336,
               "body_markdown": "In Python there are two simple ways you can achieve this:\r\n\r\n**The Pythonic way: Using Python&#39;s &#39;in&#39; Keyword-**\r\n\r\n`in` takes two &quot;arguments&quot;, one on the left(*substring*) and one on the right, and returns `True` if the left argument is contained within the rightside argument and if not,it returns `False`.\r\n\r\n    example_string = &quot;This is an example string&quot;\r\n    substring = &quot;example&quot;\r\n    print(substring in example_string)\r\n\r\nOutput:\r\n\r\n    True\r\n**The non-Pythonic way: Using Python&#39;s str.find:**\r\n\r\nThe `find` method returns the position of the string within the string or -1 if it&#39;s not found. But simply check if the position is not -1.\r\n\r\n    if example_string.find(substring) != -1:\r\n        print(&#39;Substring found!&#39;)\r\n    else:\r\n        print(&#39;Substring not found!&#39;)\r\n\r\nOutput:\r\n\r\n    Substring found!\r\n\r\n",
               "id": "40708107",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1479668336,
               "score": 7
            },
            {
               "up_vote_count": 25,
               "answer_id": 43687082,
               "last_activity_date": 1495822198,
               "path": "3.stack.answer",
               "body_markdown": "#`in` Python strings and lists\r\n\r\nHere are a few useful examples that speak for themselves concerning the `in` method:\r\n\r\n    &quot;foo&quot; in &quot;foobar&quot;\r\n    True\r\n\r\n    &quot;foo&quot; in &quot;Foobar&quot;\r\n    False\r\n\r\n    &quot;foo&quot; in &quot;Foobar&quot;.lower()\r\n    True\r\n\r\n    &quot;foo&quot;.capitalize() in &quot;Foobar&quot;\r\n    True\r\n\r\n    &quot;foo&quot; in [&quot;bar&quot;, &quot;foo&quot;, &quot;foobar&quot;]\r\n    True\r\n\r\n    &quot;foo&quot; in [&quot;fo&quot;, &quot;o&quot;, &quot;foobar&quot;]\r\n    False\r\n\r\nCaveat. Lists are iterables, and the `in` method acts on iterables, not just strings.\r\n\r\n\r\n",
               "tags": [],
               "creation_date": 1493405536,
               "last_edit_date": 1495822198,
               "is_accepted": false,
               "id": "43687082",
               "down_vote_count": 0,
               "score": 25
            },
            {
               "up_vote_count": 2,
               "answer_id": 48241340,
               "last_activity_date": 1516116596,
               "path": "3.stack.answer",
               "body_markdown": "If you&#39;re looking for case-insensitive search for whole words, rather than a substring contained within another word:\r\n    \r\n    s = &#39;This is my text example&#39;\r\n    if &#39;is&#39; not in (w.lower() for w in s.split(string.punctuation + string.whitespace)):\r\n         # do something\r\n",
               "tags": [],
               "creation_date": 1515857556,
               "last_edit_date": 1516116596,
               "is_accepted": false,
               "id": "48241340",
               "down_vote_count": 0,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/3437059/does-python-have-a-string-contains-substring-method",
         "id": "858127-2323"
      },
      {
         "up_vote_count": "553",
         "path": "2.stack",
         "body_markdown": "I have JSON data stored in the variable `data`.\r\n\r\nI want to write this to a text file for testing so I don&#39;t have to grab the data from the server each time.\r\n\r\nCurrently, I am trying this:\r\n\r\n    obj = open(&#39;data.txt&#39;, &#39;wb&#39;)\r\n    obj.write(data)\r\n    obj.close\r\n\r\nAnd am receiving the error: \r\n\r\n`TypeError: must be string or buffer, not dict`\r\n\r\nHow to fix this?",
         "view_count": "736325",
         "answer_count": "10",
         "tags": "['python', 'json']",
         "creation_date": "1346970081",
         "last_edit_date": "1501430144",
         "code_snippet": "['<code>data</code>', \"<code>obj = open('data.txt', 'wb')\\nobj.write(data)\\nobj.close\\n</code>\", '<code>TypeError: must be string or buffer, not dict</code>', '<code>data</code>', \"<code>import json\\nwith open('data.txt', 'w') as outfile:\\n    json.dump(data, outfile)\\n</code>\", '<code>json.dump</code>', '<code>json.dumps</code>', '<code>json.dump</code>', '<code>TypeError</code>', '<code>wb</code>', '<code>w</code>', '<code>wb</code>', '<code>json.dump</code>', '<code>ensure_ascii=False</code>', \"<code>import io, json\\nwith io.open('data.txt', 'w', encoding='utf-8') as f:\\n  f.write(json.dumps(data, ensure_ascii=False))\\n</code>\", \"<code>import json\\nwith open('data.txt', 'w') as f:\\n  json.dump(data, f, ensure_ascii=False)\\n</code>\", \"<code>encoding='utf-8'</code>\", '<code>open</code>', '<code>dumps</code>', \"<code>import json, codecs\\nwith open('data.txt', 'wb') as f:\\n    json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)\\n</code>\", '<code>codecs.getwriter</code>', '<code>ensure_acsii=False</code>', '<code>&gt;&gt;&gt; json.dumps({\\'price\\': \\'\u20ac10\\'})\\n\\'{\"price\": \"\\\\\\\\u20ac10\"}\\'\\n&gt;&gt;&gt; json.dumps({\\'price\\': \\'\u20ac10\\'}, ensure_ascii=False)\\n\\'{\"price\": \"\u20ac10\"}\\'\\n\\n&gt;&gt;&gt; len(json.dumps({\\'\u0430\u0431\u0432\u0433\u0434\\': 1}))\\n37\\n&gt;&gt;&gt; len(json.dumps({\\'\u0430\u0431\u0432\u0433\u0434\\': 1}, ensure_ascii=False).encode(\\'utf8\\'))\\n17\\n</code>', '<code>indent=4, sort_keys=True</code>', '<code>dump</code>', '<code>dumps</code>', '<code>unicode</code>', '<code>json.dumps</code>', \"<code>type(json.dumps('a'))</code>\", \"<code>&lt;type 'str'&gt;</code>\", \"<code>type(json.dumps('a', encoding='utf8'))</code>\", \"<code>&lt;type 'str'&gt;</code>\", '<code>utf8</code>', \"<code>'ascii' codec can't decode byte 0xf1 in position 506755: ordinal not in range(128)</code>\", '<code>sort_keys</code>', '<code>True</code>', '<code>indent</code>', \"<code>with open('data.txt', 'w') as outfile:\\n     json.dump(jsonData, outfile, sort_keys = True, indent = 4,\\n               ensure_ascii = False)\\n</code>\", \"<code>UnicodeEncodeError: 'ascii' codec can't encode character u'\\\\xfc' </code>\", '<code># -*- coding: utf-8 -*-</code>', '<code>UnicodeEncodeError</code>', \"<code># -*- coding: utf-8 -*-\\nimport json\\n\\n# Make it work for Python 2+3 and with Unicode\\nimport io\\ntry:\\n    to_unicode = unicode\\nexcept NameError:\\n    to_unicode = str\\n\\n# Define data\\ndata = {'a list': [1, 42, 3.141, 1337, 'help', u'\u20ac'],\\n        'a string': 'bla',\\n        'another dict': {'foo': 'bar',\\n                         'key': 'value',\\n                         'the answer': 42}}\\n\\n# Write JSON file\\nwith io.open('data.json', 'w', encoding='utf8') as outfile:\\n    str_ = json.dumps(data,\\n                      indent=4, sort_keys=True,\\n                      separators=(',', ': '), ensure_ascii=False)\\n    outfile.write(to_unicode(str_))\\n\\n# Read JSON file\\nwith open('data.json') as data_file:\\n    data_loaded = json.load(data_file)\\n\\nprint(data == data_loaded)\\n</code>\", '<code>json.dump</code>', '<code>indent</code>', '<code>sort_keys</code>', '<code>separators</code>', '<code>{\\n    \"a list\":[\\n        1,\\n        42,\\n        3.141,\\n        1337,\\n        \"help\",\\n        \"\u20ac\"\\n    ],\\n    \"a string\":\"bla\",\\n    \"another dict\":{\\n        \"foo\":\"bar\",\\n        \"key\":\"value\",\\n        \"the answer\":42\\n    }\\n}\\n</code>', '<code>.json</code>', '<code>force_ascii</code>', '<code>True</code>', '<code>\"\\\\u20ac\"</code>', '<code>\u20ac</code>', '<code>open</code>', '<code>io.open</code>', '<code>io.open</code>', \"<code>import codecs, json\\nwith codecs.open('data.json', 'w', 'utf8') as f:\\n     f.write(json.dumps(data, sort_keys = True, ensure_ascii=False))\\n</code>\", '<code>open</code>', '<code>io.open</code>', '<code>codecs.open</code>', '<code>codecs.open</code>', '<code>codecs.open</code>', '<code>json.dumps</code>', '<code>str</code>', '<code>unicode</code>', '<code>json.dump()</code>', \"<code>encoding = 'utf-8'</code>\", '<code>json.dumps()</code>', '<code>TypeError: must be unicode, not str</code>', '<code>data</code>', \"<code>import json\\ndata = {u'\\\\u0430\\\\u0431\\\\u0432\\\\u0433\\\\u0434': 1} #{u'\u0430\u0431\u0432\u0433\u0434': 1}\\nwith open('data.txt', 'w') as outfile:\\n    json.dump(data, outfile)\\n</code>\", \"<code>data = {'asdf': 1}</code>\", '<code>TypeError</code>', '<code>ensure_ascii</code>', '<code>unicode()</code>', '<code>io</code>', '<code>write()</code>', '<code>unicode</code>', '<code>str</code>', \"<code>import json\\ndata = [1,2,3,4,5]\\nwith open('no.txt', 'w') as txtfile:\\n    json.dump(data, txtfile)\\n</code>\", \"<code>json.dump(data, open('data.txt', 'wb'))\\n</code>\", \"<code>f = open('1.txt', 'w'); f.write('a'); input()</code>\", '<code>Ctrl-Z</code>', '<code>kill %1</code>', '<code>Ctrl-Break</code>', '<code>1.txt</code>', '<code>with</code>', \"<code>destination='filepath'\\nsaveFile = open(destination, 'w')\\nsaveFile.write(df.to_json())\\nsaveFile.close()\\n</code>\", '<code>from json import dump, load\\nfrom time import sleep\\nfrom random import random\\n\\ndef json_file(path, data = None, delay = 0.1):\\n    while True:\\n        try:\\n            if data == None:\\n                with open(path, \"r\", encoding = \"utf-8\") as f:\\n                    return load(f)\\n            else:\\n                with open(path, \"w\", encoding = \"utf-8\") as f:\\n                    return dump(data, f)\\n        except:\\n            sleep(random()*delay) # concurrency\\n</code>']",
         "title": "How do I write JSON data to a file?",
         "_childDocuments_": [
            {
               "up_vote_count": 1090,
               "answer_id": 12309296,
               "last_activity_date": 1462964850,
               "path": "3.stack.answer",
               "body_markdown": "You forgot the actual JSON part - `data` is a dictionary and not yet JSON-encoded. Write it like this:\r\n\r\n    import json\r\n    with open(&#39;data.txt&#39;, &#39;w&#39;) as outfile:\r\n        json.dump(data, outfile)\r\n\r\nNote: Works on both 3.x and 2.x .",
               "tags": [],
               "creation_date": 1346970194,
               "last_edit_date": 1462964850,
               "is_accepted": true,
               "id": "12309296",
               "down_vote_count": 3,
               "score": 1087
            },
            {
               "up_vote_count": 176,
               "answer_id": 14870531,
               "last_activity_date": 1498984479,
               "path": "3.stack.answer",
               "body_markdown": "To get ***utf8**-encoded* file as opposed to ***ascii**-encoded* in the accepted answer for Python 2 use:\r\n\r\n    import io, json\r\n    with io.open(&#39;data.txt&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f:\r\n      f.write(json.dumps(data, ensure_ascii=False))\r\n\r\nThe code is simpler in Python 3:\r\n\r\n    import json\r\n    with open(&#39;data.txt&#39;, &#39;w&#39;) as f:\r\n      json.dump(data, f, ensure_ascii=False)\r\n\r\n*On Windows, the `encoding=&#39;utf-8&#39;` argument to `open` is still necessary.*\r\n\r\nTo avoid storing an encoded copy of the data in memory (result of `dumps`) and to output *utf8-encoded* bytestrings in both Python 2 and 3, use:\r\n\r\n    import json, codecs\r\n    with open(&#39;data.txt&#39;, &#39;wb&#39;) as f:\r\n        json.dump(data, codecs.getwriter(&#39;utf-8&#39;)(f), ensure_ascii=False)\r\n\r\n*The `codecs.getwriter` call is redundant in Python 3 but required for Python 2*\r\n\r\n----------\r\n**Readability and size:**\r\n\r\nThe use of `ensure_acsii=False` gives better readability and smaller size:\r\n\r\n    &gt;&gt;&gt; json.dumps({&#39;price&#39;: &#39;\u20ac10&#39;})\r\n    &#39;{&quot;price&quot;: &quot;\\\\u20ac10&quot;}&#39;\r\n    &gt;&gt;&gt; json.dumps({&#39;price&#39;: &#39;\u20ac10&#39;}, ensure_ascii=False)\r\n    &#39;{&quot;price&quot;: &quot;\u20ac10&quot;}&#39;\r\n\r\n    &gt;&gt;&gt; len(json.dumps({&#39;\u0430\u0431\u0432\u0433\u0434&#39;: 1}))\r\n    37\r\n    &gt;&gt;&gt; len(json.dumps({&#39;\u0430\u0431\u0432\u0433\u0434&#39;: 1}, ensure_ascii=False).encode(&#39;utf8&#39;))\r\n    17\r\n\r\nFurther improve readability by adding flags `indent=4, sort_keys=True` (as suggested by [dinos66][1]) to arguments of `dump` or `dumps`. This way you&#39;ll get a nicely indented sorted structure in the json file at the cost of a slightly larger file size.\r\n\r\n\r\n  [1]: https://stackoverflow.com/a/31343739/237105",
               "tags": [],
               "creation_date": 1360830156,
               "last_edit_date": 1498984479,
               "is_accepted": false,
               "id": "14870531",
               "down_vote_count": 1,
               "score": 175
            },
            {
               "up_vote_count": 119,
               "answer_id": 20776329,
               "last_activity_date": 1486722726,
               "path": "3.stack.answer",
               "body_markdown": "I would answer with slight modification with aforementioned answers and that is to write a prettified JSON file which human eyes can read better. For this, pass `sort_keys` as `True` and `indent` with 4 space characters and you are good to go. Also take care of ensuring that the ascii codes will not be written in your JSON file:\r\n\r\n    with open(&#39;data.txt&#39;, &#39;w&#39;) as outfile:\r\n         json.dump(jsonData, outfile, sort_keys = True, indent = 4,\r\n                   ensure_ascii = False)\r\n",
               "tags": [],
               "creation_date": 1388001864,
               "last_edit_date": 1486722726,
               "is_accepted": false,
               "id": "20776329",
               "down_vote_count": 0,
               "score": 119
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 15,
               "answer_id": 31343739,
               "is_accepted": false,
               "last_activity_date": 1436539529,
               "body_markdown": "For those of you who are trying to dump greek or other &quot;exotic&quot; languages such as me but are also having problems (unicode errors) with weird characters such as the peace symbol (\\u262E) or others which are often contained in json formated data such as Twitter&#39;s, the solution could be as follows (sort_keys is obviously optional):\r\n\r\n    import codecs, json\r\n    with codecs.open(&#39;data.json&#39;, &#39;w&#39;, &#39;utf8&#39;) as f:\r\n         f.write(json.dumps(data, sort_keys = True, ensure_ascii=False))",
               "id": "31343739",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1436539529,
               "score": 15
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 4,
               "answer_id": 34451699,
               "is_accepted": false,
               "last_activity_date": 1450954701,
               "body_markdown": "Write a data in file using JSON use **json.dump()** or **json.dumps()** used.\r\nwrite like this to store data in file.\r\n\r\n    import json\r\n    data = [1,2,3,4,5]\r\n    with open(&#39;no.txt&#39;, &#39;w&#39;) as txtfile:\r\n        json.dump(data, txtfile)\r\n\r\nthis example in list is store to a file.",
               "id": "34451699",
               "tags": [],
               "down_vote_count": 2,
               "creation_date": 1450954701,
               "score": 2
            },
            {
               "up_vote_count": 9,
               "answer_id": 34574105,
               "last_activity_date": 1488445670,
               "path": "3.stack.answer",
               "body_markdown": "I don&#39;t have enough reputation to add in comments, so I just write some of my findings of this annoying TypeError here:\r\n\r\nBasically, I think it&#39;s a bug in the `json.dump()` function in Python **2** only - It can&#39;t dump a Python (dictionary / list) data containing non-ASCII characters, *even* you open the file with the `encoding = &#39;utf-8&#39;` parameter. (i.e. No matter what you do). But, `json.dumps()` works on both Python 2 and 3.\r\n\r\nTo illustrate this, following up phihag&#39;s answer: the code in his answer breaks in Python 2 with exception `TypeError: must be unicode, not str`, if `data` contains non-ASCII characters. (Python 2.7.6, Debian):\r\n\r\n    import json\r\n    data = {u&#39;\\u0430\\u0431\\u0432\\u0433\\u0434&#39;: 1} #{u&#39;\u0430\u0431\u0432\u0433\u0434&#39;: 1}\r\n    with open(&#39;data.txt&#39;, &#39;w&#39;) as outfile:\r\n        json.dump(data, outfile)\r\n\r\nIt however works fine in Python 3.",
               "tags": [],
               "creation_date": 1451803956,
               "last_edit_date": 1488445670,
               "is_accepted": false,
               "id": "34574105",
               "down_vote_count": 1,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 35708803,
               "is_accepted": false,
               "last_activity_date": 1456777011,
               "body_markdown": "    json.dump(data, open(&#39;data.txt&#39;, &#39;wb&#39;))",
               "id": "35708803",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1456777011,
               "score": 2
            },
            {
               "up_vote_count": 64,
               "answer_id": 37795053,
               "last_activity_date": 1494931877,
               "path": "3.stack.answer",
               "body_markdown": "## Read and write JSON files with Python 2+3; works with unicode\r\n\r\n    # -*- coding: utf-8 -*-\r\n    import json\r\n\r\n    # Make it work for Python 2+3 and with Unicode\r\n    import io\r\n    try:\r\n        to_unicode = unicode\r\n    except NameError:\r\n        to_unicode = str\r\n\r\n    # Define data\r\n    data = {&#39;a list&#39;: [1, 42, 3.141, 1337, &#39;help&#39;, u&#39;\u20ac&#39;],\r\n            &#39;a string&#39;: &#39;bla&#39;,\r\n            &#39;another dict&#39;: {&#39;foo&#39;: &#39;bar&#39;,\r\n                             &#39;key&#39;: &#39;value&#39;,\r\n                             &#39;the answer&#39;: 42}}\r\n\r\n    # Write JSON file\r\n    with io.open(&#39;data.json&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) as outfile:\r\n        str_ = json.dumps(data,\r\n                          indent=4, sort_keys=True,\r\n                          separators=(&#39;,&#39;, &#39;: &#39;), ensure_ascii=False)\r\n        outfile.write(to_unicode(str_))\r\n\r\n    # Read JSON file\r\n    with open(&#39;data.json&#39;) as data_file:\r\n        data_loaded = json.load(data_file)\r\n\r\n    print(data == data_loaded)\r\n\r\n\r\nExplanation of the parameters of [`json.dump`](https://docs.python.org/2/library/json.html#basic-usage):\r\n\r\n* `indent`: Use 4 spaces to indent each entry, e.g. when a new dict is started (otherwise all will be in one line),\r\n* `sort_keys`: sort the keys of dictionaries. This is useful if you want to compare json files with a diff tool / put them under version control.\r\n* `separators`: To prevent Python from adding trailing whitespaces\r\n\r\n\r\n## Created JSON file\r\n\r\n    {\r\n        &quot;a list&quot;:[\r\n            1,\r\n            42,\r\n            3.141,\r\n            1337,\r\n            &quot;help&quot;,\r\n            &quot;\u20ac&quot;\r\n        ],\r\n        &quot;a string&quot;:&quot;bla&quot;,\r\n        &quot;another dict&quot;:{\r\n            &quot;foo&quot;:&quot;bar&quot;,\r\n            &quot;key&quot;:&quot;value&quot;,\r\n            &quot;the answer&quot;:42\r\n        }\r\n    }\r\n\r\n\r\n## Common file endings\r\n\r\n`.json`\r\n\r\n\r\n## Alternatives\r\n\r\n* CSV: Super simple format ([read &amp; write](https://stackoverflow.com/a/41585079/562769))\r\n* JSON: Nice for writing human-readable data; VERY commonly used ([read &amp; write](https://stackoverflow.com/a/37795053/562769))\r\n* YAML: YAML is a superset of JSON, but easier to read ([read &amp; write](https://stackoverflow.com/a/42054860/562769), [comparison of JSON and YAML](https://stackoverflow.com/a/1729545/562769))\r\n* pickle: A Python serialization format ([read &amp; write](https://stackoverflow.com/a/33245595/562769))\r\n* [MessagePack](http://msgpack.org/) ([Python package](https://pypi.python.org/pypi/msgpack-python)): More compact representation ([read &amp; write](https://stackoverflow.com/q/43442194/562769))\r\n* [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) ([Python package](http://docs.h5py.org/en/latest/quick.html)): Nice for matrices ([read &amp; write](https://stackoverflow.com/a/41586571/562769))\r\n* XML: exists too \\*sigh\\* ([read](https://stackoverflow.com/a/1912483/562769) &amp; [write](https://stackoverflow.com/a/3605831/562769))\r\n\r\nFor your application, the following might be important:\r\n\r\n* Support by other programming languages\r\n* Reading / writing performance\r\n* Compactness (file size)\r\n\r\nSee also: [Comparison of data serialization formats](https://en.wikipedia.org/wiki/Comparison_of_data_serialization_formats)\r\n\r\nIn case you are rather looking for a way to make configuration files, you might want to read my short article [Configuration files in Python](https://martin-thoma.com/configuration-files-in-python/)",
               "tags": [],
               "creation_date": 1465836213,
               "last_edit_date": 1495541448,
               "is_accepted": false,
               "id": "37795053",
               "down_vote_count": 0,
               "score": 64
            },
            {
               "up_vote_count": 1,
               "answer_id": 41825637,
               "last_activity_date": 1485258532,
               "path": "3.stack.answer",
               "body_markdown": "Here is a useful structure to both read and write a file in Python 3.\r\n\r\n    from json import dump, load\r\n    from time import sleep\r\n    from random import random\r\n\r\n    def json_file(path, data = None, delay = 0.1):\r\n        while True:\r\n            try:\r\n                if data == None:\r\n                    with open(path, &quot;r&quot;, encoding = &quot;utf-8&quot;) as f:\r\n                        return load(f)\r\n                else:\r\n                    with open(path, &quot;w&quot;, encoding = &quot;utf-8&quot;) as f:\r\n                        return dump(data, f)\r\n            except:\r\n                sleep(random()*delay) # concurrency",
               "tags": [],
               "creation_date": 1485253369,
               "last_edit_date": 1485258532,
               "is_accepted": false,
               "id": "41825637",
               "down_vote_count": 1,
               "score": 0
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 1,
               "answer_id": 44341699,
               "is_accepted": false,
               "last_activity_date": 1496476110,
               "body_markdown": "if you are trying to write a pandas dataframe into a file using a json format i&#39;d recommend this\r\n\r\n    destination=&#39;filepath&#39;\r\n    saveFile = open(destination, &#39;w&#39;)\r\n    saveFile.write(df.to_json())\r\n    saveFile.close()\r\n\r\n",
               "id": "44341699",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1496476110,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/12309269/how-do-i-write-json-data-to-a-file",
         "id": "858127-2324"
      },
      {
         "up_vote_count": "330",
         "path": "2.stack",
         "body_markdown": "How to set the current working directory in Python?",
         "view_count": "375385",
         "answer_count": "6",
         "tags": "['python', 'current-working-directory']",
         "creation_date": "1259358828",
         "last_edit_date": "1508968546",
         "code_snippet": "['<code>os.chdir</code>', '<code>os.chdir(path)\\n</code>', '<code>os.getcwd()</code>', '<code>os.chdir()</code>', '<code>import os\\nos.chdir(default_path)\\n</code>', '<code>import os;\\nprint os.getcwd(); # Prints the working directory\\n</code>', \"<code>os.chdir('c:\\\\\\\\Users\\\\uname\\\\desktop\\\\python') # Provide the path here\\n</code>\", '<code>\\\\u</code>', '<code>\\\\\\\\u</code>', '<code>getcwd</code>', \"<code>import os\\nimport pandas as pd\\n\\ntar = os.chdir('&lt;dir path only&gt;') # do not mention file name here\\nprint os.getcwd()# to print the path name in CLI\\n</code>\", \"<code>dataset(*just a variable) = pd.read_csv('new.csv')\\n</code>\", '<code>os.chdir</code>', '<code>import os\\npath=\"/Users/HOME/Desktop/Addl Work/TimeSeries-Done\"\\nos.chdir(path)\\n</code>', '<code>os.getcwd()\\n</code>', '<code>Python</code>', '<code>os</code>', '<code>chdir()</code>', \"<code>''</code>\", \"<code>import os\\n\\nos.chdir('default_path')\\n</code>\"]",
         "title": "How to set the current working directory?",
         "_childDocuments_": [
            {
               "up_vote_count": 474,
               "answer_id": 1810760,
               "last_activity_date": 1360752215,
               "path": "3.stack.answer",
               "body_markdown": "Try [`os.chdir`][1]\r\n\r\n&gt;     os.chdir(path)\r\n&gt; &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Change the current working directory to path. Availability: Unix, Windows.\r\n\r\n[1]: http://docs.python.org/library/os.html#os.chdir",
               "tags": [],
               "creation_date": 1259359199,
               "last_edit_date": 1360752215,
               "is_accepted": true,
               "id": "1810760",
               "down_vote_count": 0,
               "score": 474
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 103,
               "answer_id": 1810762,
               "is_accepted": false,
               "last_activity_date": 1259359209,
               "body_markdown": "Perhaps this is what you are looking for:\r\n\r\n    import os\r\n    os.chdir(default_path)",
               "id": "1810762",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1259359209,
               "score": 103
            },
            {
               "up_vote_count": 4,
               "answer_id": 33550959,
               "last_activity_date": 1446747211,
               "path": "3.stack.answer",
               "body_markdown": "You need to import the `os` module and then you can use the `chdir()` method, but don&#39;t forget to use the quotations (`&#39;&#39;`) inside the parenthesis:\r\n\r\n    import os\r\n    \r\n    os.chdir(&#39;default_path&#39;)",
               "tags": [],
               "creation_date": 1446744281,
               "last_edit_date": 1446747211,
               "is_accepted": false,
               "id": "33550959",
               "down_vote_count": 12,
               "score": -8
            },
            {
               "up_vote_count": 35,
               "answer_id": 34971949,
               "last_activity_date": 1453608570,
               "path": "3.stack.answer",
               "body_markdown": "\r\n    import os;\r\n    print os.getcwd(); # Prints the working directory\r\n\r\nTo set the working directory:\r\n\r\n    os.chdir(&#39;c:\\\\Users\\uname\\desktop\\python&#39;) # Provide the path here\r\n",
               "tags": [],
               "creation_date": 1453606881,
               "last_edit_date": 1453608570,
               "is_accepted": false,
               "id": "34971949",
               "down_vote_count": 20,
               "score": 15
            },
            {
               "up_vote_count": 3,
               "answer_id": 41276796,
               "last_activity_date": 1482388865,
               "path": "3.stack.answer",
               "body_markdown": "people using pandas package\r\n\r\n    import os\r\n    import pandas as pd\r\n    \r\n    tar = os.chdir(&#39;&lt;dir path only&gt;&#39;) # do not mention file name here\r\n    print os.getcwd()# to print the path name in CLI\r\n\r\nthe following syntax to be used to import the file in python CLI\r\n\r\n    dataset(*just a variable) = pd.read_csv(&#39;new.csv&#39;)",
               "tags": [],
               "creation_date": 1482387176,
               "last_edit_date": 1482388865,
               "is_accepted": false,
               "id": "41276796",
               "down_vote_count": 1,
               "score": 2
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 45975442,
               "is_accepted": false,
               "last_activity_date": 1504163669,
               "body_markdown": "It work for **Mac** also \r\n\r\n    import os\r\n    path=&quot;/Users/HOME/Desktop/Addl Work/TimeSeries-Done&quot;\r\n    os.chdir(path)\r\n\r\nTo check working directory\r\n\r\n    os.getcwd()",
               "id": "45975442",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1504163669,
               "score": 2
            }
         ],
         "link": "https://stackoverflow.com/questions/1810743/how-to-set-the-current-working-directory",
         "id": "858127-2325"
      },
      {
         "up_vote_count": "291",
         "path": "2.stack",
         "body_markdown": "Is there a way to dump a NumPy array into a CSV file? I have a 2D NumPy array and need to dump it in human-readable format.",
         "view_count": "254818",
         "answer_count": "7",
         "tags": "['python', 'arrays', 'csv', 'numpy']",
         "creation_date": "1305972076",
         "last_edit_date": "1338642097",
         "code_snippet": "['<code>numpy.savetxt</code>', '<code>import numpy\\na = numpy.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\\nnumpy.savetxt(\"foo.csv\", a, delimiter=\",\")\\n</code>', '<code>numpy.array</code>', '<code>numpy.array</code>', '<code>import pandas as pd \\ndf = pd.DataFrame(np_array)\\ndf.to_csv(\"file_path.csv\")\\n</code>', '<code>df.to_csv(\"file_path.csv\", header=None)</code>', '<code>header=None, index=None</code>', '<code>tofile</code>', \"<code>import numpy as np\\na = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\\na.tofile('foo.csv',sep=',',format='%10.5f')\\n</code>\", \"<code>import numpy as np\\n\\n# Write an example CSV file with headers on first line\\nwith open('example.csv', 'w') as fp:\\n    fp.write('''\\\\\\ncol1,col2,col3\\n1,100.1,string1\\n2,222.2,second string\\n''')\\n\\n# Read it as a Numpy record array\\nar = np.recfromcsv('example.csv')\\nprint(repr(ar))\\n# rec.array([(1, 100.1, 'string1'), (2, 222.2, 'second string')], \\n#           dtype=[('col1', '&lt;i4'), ('col2', '&lt;f8'), ('col3', 'S13')])\\n\\n# Write as a CSV file with headers on first line\\nwith open('out.csv', 'w') as fp:\\n    fp.write(','.join(ar.dtype.names) + '\\\\n')\\n    np.savetxt(fp, ar, '%s', ',')\\n</code>\", '<code>csv</code>', \"<code>import csv\\n\\nwith open('out2.csv', 'wb') as fp:\\n    writer = csv.writer(fp, quoting=csv.QUOTE_NONNUMERIC)\\n    writer.writerow(ar.dtype.names)\\n    writer.writerows(ar.tolist())\\n</code>\", '<code>    for x in np.nditer(a.T, order=\\'C\\'): \\n            file.write(str(x))\\n            file.write(\"\\\\n\")\\n</code>', \"<code>    writer= csv.writer(file, delimiter=',')\\n    for x in np.nditer(a.T, order='C'): \\n            row.append(str(x))\\n    writer.writerow(row)\\n</code>\", '<code>your_array = np.array([[1,2],[3,4]])</code>', '<code>your_array.tolist()</code>', \"<code>delimiter=';'</code>\", '<code>[[1, 2], [2, 4]]</code>', '<code>your_array = np.array(ast.literal_eval(cell_string))</code>', '<code># format as a block of csv text to do whatever you want\\ncsv_rows = [\"{},{}\".format(i, j) for i, j in array]\\ncsv_text = \"\\\\n\".join(csv_rows)\\n\\n# write it to a file\\nwith open(\\'file.csv\\', \\'w\\') as f:\\n    f.write(csv_text)\\n</code>']",
         "title": "Dump a NumPy array into a csv file",
         "_childDocuments_": [
            {
               "up_vote_count": 469,
               "answer_id": 6081043,
               "last_activity_date": 1503725984,
               "path": "3.stack.answer",
               "body_markdown": "[`numpy.savetxt`][1] saves an array to a text file.\r\n\r\n    import numpy\r\n    a = numpy.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\r\n    numpy.savetxt(&quot;foo.csv&quot;, a, delimiter=&quot;,&quot;)\r\n\r\n  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.savetxt.html",
               "tags": [],
               "creation_date": 1305972615,
               "last_edit_date": 1503725984,
               "is_accepted": true,
               "id": "6081043",
               "down_vote_count": 0,
               "score": 469
            },
            {
               "up_vote_count": 27,
               "answer_id": 30189734,
               "last_activity_date": 1515347865,
               "path": "3.stack.answer",
               "body_markdown": "[`tofile`][1] is a convenient function to do this:\r\n\r\n\r\n    import numpy as np\r\n    a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\r\n    a.tofile(&#39;foo.csv&#39;,sep=&#39;,&#39;,format=&#39;%10.5f&#39;)\r\n\r\nThe man page has some useful notes:\r\n\r\n&gt; This is a convenience function for quick storage of array data.\r\n&gt; Information on endianness and precision is lost, so this method is not\r\n&gt; a good choice for files intended to archive data or transport data\r\n&gt; between machines with different endianness. Some of these problems can\r\n&gt; be overcome by outputting the data as text files, at the expense of\r\n&gt; speed and file size.\r\n\r\n\r\nNote. This function does not produce multi-line csv files, it saves everything to one line.\r\n\r\n  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html",
               "tags": [],
               "creation_date": 1431430664,
               "last_edit_date": 1515347865,
               "is_accepted": false,
               "id": "30189734",
               "down_vote_count": 1,
               "score": 26
            },
            {
               "up_vote_count": 2,
               "answer_id": 40091714,
               "last_activity_date": 1476725017,
               "path": "3.stack.answer",
               "body_markdown": "If you want to save your numpy array (e.g. `your_array = np.array([[1,2],[3,4]])`) to one cell, you could convert it first with `your_array.tolist()`.\r\n\r\nThen save it the normal way to one cell, with `delimiter=&#39;;&#39;`\r\nand the cell in the csv-file will look like this `[[1, 2], [2, 4]]`\r\n\r\nThen you could restore your array like this:\r\n`your_array = np.array(ast.literal_eval(cell_string))`",
               "tags": [],
               "creation_date": 1476723044,
               "last_edit_date": 1476725017,
               "is_accepted": false,
               "id": "40091714",
               "down_vote_count": 0,
               "score": 2
            },
            {
               "up_vote_count": 8,
               "answer_id": 41009026,
               "last_activity_date": 1502158158,
               "path": "3.stack.answer",
               "body_markdown": "Writing record arrays as CSV files with headers requires a bit more work.\r\n\r\nThis example reads a CSV file with the header on the first line, then writes the same file.\r\n\r\n    import numpy as np\r\n    \r\n    # Write an example CSV file with headers on first line\r\n    with open(&#39;example.csv&#39;, &#39;w&#39;) as fp:\r\n        fp.write(&#39;&#39;&#39;\\\r\n    col1,col2,col3\r\n    1,100.1,string1\r\n    2,222.2,second string\r\n    &#39;&#39;&#39;)\r\n    \r\n    # Read it as a Numpy record array\r\n    ar = np.recfromcsv(&#39;example.csv&#39;)\r\n    print(repr(ar))\r\n    # rec.array([(1, 100.1, &#39;string1&#39;), (2, 222.2, &#39;second string&#39;)], \r\n    #           dtype=[(&#39;col1&#39;, &#39;&lt;i4&#39;), (&#39;col2&#39;, &#39;&lt;f8&#39;), (&#39;col3&#39;, &#39;S13&#39;)])\r\n    \r\n    # Write as a CSV file with headers on first line\r\n    with open(&#39;out.csv&#39;, &#39;w&#39;) as fp:\r\n        fp.write(&#39;,&#39;.join(ar.dtype.names) + &#39;\\n&#39;)\r\n        np.savetxt(fp, ar, &#39;%s&#39;, &#39;,&#39;)\r\n\r\nNote that this example does not consider strings with commas. To consider quotes for non-numeric data, use the [`csv`](https://docs.python.org/2/library/csv.html) package:\r\n\r\n    import csv\r\n\r\n    with open(&#39;out2.csv&#39;, &#39;wb&#39;) as fp:\r\n        writer = csv.writer(fp, quoting=csv.QUOTE_NONNUMERIC)\r\n        writer.writerow(ar.dtype.names)\r\n        writer.writerows(ar.tolist())",
               "tags": [],
               "creation_date": 1481082130,
               "last_edit_date": 1502158158,
               "is_accepted": false,
               "id": "41009026",
               "down_vote_count": 0,
               "score": 8
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 36,
               "answer_id": 41096943,
               "is_accepted": false,
               "last_activity_date": 1481531929,
               "body_markdown": "It&#39;s easy and fast with pandas\r\n\r\n    import pandas as pd \r\n    df = pd.DataFrame(np_array)\r\n    df.to_csv(&quot;file_path.csv&quot;)",
               "id": "41096943",
               "tags": [],
               "down_vote_count": 1,
               "creation_date": 1481531929,
               "score": 35
            },
            {
               "path": "3.stack.answer",
               "up_vote_count": 3,
               "answer_id": 42646272,
               "is_accepted": false,
               "last_activity_date": 1488883798,
               "body_markdown": "**if you want to write in column:**\r\n\r\n        for x in np.nditer(a.T, order=&#39;C&#39;):\t\r\n\t\t        file.write(str(x))\r\n                file.write(&quot;\\n&quot;)\r\n\r\n\r\nHere &#39;a&#39; is the name of numpy array and &#39;file&#39; is the variable to write in a file.\r\n\r\n**If you want to write in row:**\r\n\r\n        writer= csv.writer(file, delimiter=&#39;,&#39;)\r\n        for x in np.nditer(a.T, order=&#39;C&#39;):\t\r\n\t\t        row.append(str(x))\r\n        writer.writerow(row)",
               "id": "42646272",
               "tags": [],
               "down_vote_count": 0,
               "creation_date": 1488883798,
               "score": 3
            },
            {
               "up_vote_count": 1,
               "answer_id": 46089936,
               "last_activity_date": 1506649179,
               "path": "3.stack.answer",
               "body_markdown": "You can also do it with pure python without using any modules.\r\n\r\n    # format as a block of csv text to do whatever you want\r\n    csv_rows = [&quot;{},{}&quot;.format(i, j) for i, j in array]\r\n    csv_text = &quot;\\n&quot;.join(csv_rows)\r\n\r\n    # write it to a file\r\n    with open(&#39;file.csv&#39;, &#39;w&#39;) as f:\r\n        f.write(csv_text)",
               "tags": [],
               "creation_date": 1504767945,
               "last_edit_date": 1506649179,
               "is_accepted": false,
               "id": "46089936",
               "down_vote_count": 0,
               "score": 1
            }
         ],
         "link": "https://stackoverflow.com/questions/6081008/dump-a-numpy-array-into-a-csv-file",
         "id": "858127-2326"
      }
   ],
   "github_id": "858127",
   "repo_status": "",
   "homepage_content": "\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n  \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /><script type=\"text/javascript\">\n\n      var _gaq = _gaq || [];\n      _gaq.push(['_setAccount', 'UA-27880019-2']);\n      _gaq.push(['_trackPageview']);\n\n      (function() \n        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n      )();\n    </script>\n    <title>Python Data Analysis Library &#8212; pandas: Python Data Analysis Library</title>\n    <link rel=\"stylesheet\" href=\"_static/pydata.css\" type=\"text/css\" />\n    <link rel=\"stylesheet\" href=\"_static/pygments.css\" type=\"text/css\" />\n    <script type=\"text/javascript\" src=\"_static/documentation_options.js\"></script>\n    <script type=\"text/javascript\" src=\"_static/jquery.js\"></script>\n    <script type=\"text/javascript\" src=\"_static/underscore.js\"></script>\n    <script type=\"text/javascript\" src=\"_static/doctools.js\"></script>\n    <link rel=\"author\" title=\"About these documents\" href=\"about.html\" />\n    <link rel=\"index\" title=\"Index\" href=\"genindex.html\" />\n    <link rel=\"search\" title=\"Search\" href=\"search.html\" />\n    <link rel=\"next\" title=\"The pandas project\" href=\"about.html\" /> \n  </head><body>\n<div class=\"header-wrapper\">\n\t<div class=\"header\">\n\t\t<div class=\"logo\">\n\t\t\t<a href=\"index.html\"><img class=\"logo\" src=\"_static/pandas_logo.png\" alt=\"Logo\"/></a>\n\t\t</div>\n\n        <div class=\"rel\">\n\t\t\t<a href=\"index.html\">home</a> //\n\t\t\t<a href=\"about.html\">about</a> //\n\t\t\t<a href=\"getpandas.html\">get pandas</a> //\n\t\t\t<a href=\"http://pandas.pydata.org/pandas-docs/stable\">documentation</a> //\n\t\t\t<a href=\"community.html\">community</a> //\n\t\t\t<a href=\"talks.html\">talks</a> //\n\t\t\t<a href=\"donate.html\">donate</a>\n\t\t</div>\n\n\t</div>\n</div>\n\n<div class=\"content-wrapper\">\n<div class=\"content\">\n<div class=\"sphinxsidebar\">\n<div class=\"sphinxsidebarwrapper\">\n\n<div class=\"sidebarblock\">\n<h3>Versions</h3>\n\n\n\n\t<div class=\"tile\">\n\t\t\t\t<h4>Release</h4>\n\t\t\t\t0.22.0 - December 2017 <br/>\n\t\t\t\t<a href=\"https://pypi.org/project/pandas/#files\">download</a>          //\n\t\t\t\t<a href=\"http://pandas.pydata.org/pandas-docs/stable/\">docs</a> //\n\t\t\t\t<a href=\"http://pandas.pydata.org/pandas-docs/stable/pandas.pdf\">pdf</a>\n\t</div>\n\n\t<div class=\"tile\">\n\t\t<h4>Development</h4>\n        0.23.0 - 2018<br/>\n        <a href=\"https://github.com/pydata/pandas\">github</a> //\n        <a href=\"http://pandas-docs.github.io/pandas-docs-travis/\">docs</a>\n    </div>\n\n    <div class=\"tile\">\n      <h4>Previous Releases</h4>\n      \n          <div>\n              0.21.1 -\n              <a href=\"https://pypi.org/project/pandas/0.21.1/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.21/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.21/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.21.0 -\n              <a href=\"https://pypi.org/project/pandas/0.21.0/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.21/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.21/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.20.3 -\n              <a href=\"https://pypi.org/project/pandas/0.20.3/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.20/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.20/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.19.2 -\n              <a href=\"https://pypi.org/project/pandas/0.19.2/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.19/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.19/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.18.1 -\n              <a href=\"https://pypi.org/project/pandas/0.18.1/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.18/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.18/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.17.1 -\n              <a href=\"https://pypi.org/project/pandas/0.17.1/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.17/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.17/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.16.2 -\n              <a href=\"https://pypi.org/project/pandas/0.16.2/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.16/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.16/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.15.2 -\n              <a href=\"https://pypi.org/project/pandas/0.15.2/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.15/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.15/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.14.1 -\n              <a href=\"https://pypi.org/project/pandas/0.14.1/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.14/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.14/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.13.1 -\n              <a href=\"https://pypi.org/project/pandas/0.13.1/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.13/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.13/pandas.pdf\">pdf</a>\n          </div>\n\n      \n          <div>\n              0.12.0 -\n              <a href=\"https://pypi.org/project/pandas/0.12.0/#files\">download</a>          //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.12/\">docs</a> //\n              <a href=\"http://pandas.pydata.org/pandas-docs/version/0.12/pandas.pdf\">pdf</a>\n          </div>\n\n      \n    </div>\n\n\n</div>\n\n<div class=\"sidebarblock\">\n\t<h3>About pandas</h3>\n  \t<ul>\n    \t<li><a href=\"http://pandas.pydata.org/pandas-docs/dev/whatsnew.html\">What's New</a></li>\n\t\t<li><a href=\"http://pandas.pydata.org/pandas-docs/stable/install.html\">Getting Started</a></li>\n\t\t<li><a href=\"https://github.com/pydata/pandas/issues\">Issue Tracker</a></li>\n    \t<li><a href=\"http://pandas.pydata.org/pandas-docs/stable/overview.html#license\">License</a></li>\n\t\t<li><a href=\"http://pandas.pydata.org/pandas-docs/stable/contributing.html\">Contributing Guidelines</a></li>\n  \t</ul>\n</div>\n\n<div class=\"sidebarblock\">\n<h3>Get the book</h3>\n   <a href=\"http://www.kqzyfj.com/click-7040302-11260198?url=http%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920050896.do&cjsku=0636920023784\"><img src=\"_static/pydata_cover.jpg\" alt=\"Python for Data Analysis\", width=180, height=240></a>\n</div>\n\n<div class=\"sidebarblock\">\n\t<h3>Related Tools</h3>\n\t<ul>\n    \t<li><a href=\"http://www.scipy.org/\">SciPy</a></li>\n    \t<li><a href=\"http://www.numpy.org/\">NumPy</a></li>\n    \t<li><a href=\"http://statsmodels.sourceforge.net/\">StatsModels</a></li>\n    \t<li><a href=\"http://scikit-learn.org\">scikit-learn</a></li>\n\t\t<li><a href=\"http://jupyter.org/\">Jupyter</a></li>\n\t\t<li><a href=\"http://matplotlib.org/\">matplotlib</a></li>\n  \t</ul>\n</div>\n\n</div>\n</div>\n\n\n    <div class=\"document\">\n      <div class=\"documentwrapper\">\n        <div class=\"bodywrapper\">\n          <div class=\"body\" role=\"main\">\n            \n  <div class=\"section\" id=\"python-data-analysis-library\">\n<h1>Python Data Analysis Library<a class=\"headerlink\" href=\"#python-data-analysis-library\" title=\"Permalink to this headline\">\u00c2\u00b6</a></h1>\n<p><em>pandas</em> is an open source, BSD-licensed library providing high-performance,\neasy-to-use data structures and data analysis tools for the <a class=\"reference external\" href=\"https://www.python.org/\">Python</a> programming language.</p>\n<p><em>pandas</em> is a <a class=\"reference external\" href=\"https://www.numfocus.org/open-source-projects.html\">NumFOCUS</a> sponsored project. This will help ensure the success of development of <em>pandas</em> as a world-class open-source project, and makes it possible to <a class=\"reference internal\" href=\"donate.html\"><span class=\"doc\">donate</span></a> to the project.</p>\n<a class=\"reference external image-reference\" href=\"https://www.numfocus.org/open-source-projects.html\"><img alt=\"NumFOCUS Logo\" src=\"_images/SponsoredProjectStamp_300px.png\" /></a>\n<div class=\"section\" id=\"v0-22-0-final-december-29-2017\">\n<h2>v0.22.0 Final (December 29, 2017)<a class=\"headerlink\" href=\"#v0-22-0-final-december-29-2017\" title=\"Permalink to this headline\">\u00c2\u00b6</a></h2>\n<p>This is a major release from 0.21.1 and includes a single, API-breaking change.\nWe recommend that all users upgrade to this version after carefully reading the\nrelease note.</p>\n<p>The only changes are:</p>\n<ul class=\"simple\">\n<li>The sum of an empty or all-<em>NA</em> <code class=\"docutils literal notranslate\"><span class=\"pre\">Series</span></code> is now <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code></li>\n<li>The product of an empty or all-<em>NA</em> <code class=\"docutils literal notranslate\"><span class=\"pre\">Series</span></code> is now <code class=\"docutils literal notranslate\"><span class=\"pre\">1</span></code></li>\n<li>We\u00e2\u0080\u0099ve added a <code class=\"docutils literal notranslate\"><span class=\"pre\">min_count</span></code> parameter to <code class=\"docutils literal notranslate\"><span class=\"pre\">.sum()</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">.prod()</span></code> controlling\nthe minimum number of valid values for the result to be valid. If fewer than\n<code class=\"docutils literal notranslate\"><span class=\"pre\">min_count</span></code> non-<em>NA</em> values are present, the result is <em>NA</em>. The default is\n<code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code>. To return <code class=\"docutils literal notranslate\"><span class=\"pre\">NaN</span></code>, the 0.21 behavior, use <code class=\"docutils literal notranslate\"><span class=\"pre\">min_count=1</span></code>.</li>\n</ul>\n<p>See the <a class=\"reference external\" href=\"https://pandas.pydata.org/pandas-docs/version/0.22.0/whatsnew.html#whatsnew-0220\">pandas 0.22.0 whatsnew</a>\noverview for further explanation of all the places in the library this affects.</p>\n</div>\n<div class=\"section\" id=\"v0-21-1-final-december-12-2017\">\n<h2>v0.21.1 Final (December 12, 2017)<a class=\"headerlink\" href=\"#v0-21-1-final-december-12-2017\" title=\"Permalink to this headline\">\u00c2\u00b6</a></h2>\n<p>This is a minor bug-fix release in the 0.21.x series and includes some\nsmall regression fixes, bug fixes and performance improvements. We\nrecommend that all users upgrade to this version.</p>\n<p>Highlights include:</p>\n<ul class=\"simple\">\n<li>Temporarily restore matplotlib datetime plotting functionality. This\nshould resolve issues for users who relied implicitly on pandas to\nplot datetimes with matplotlib. See\n<a class=\"reference external\" href=\"https://pandas.pydata.org/pandas-docs/version/0.21.1/whatsnew.html#restore-matplotlib-datetime-converter-registration\">here</a>.</li>\n<li>Improvements to the Parquet IO functions introduced in 0.21.0. See\n<a class=\"reference external\" href=\"https://pandas.pydata.org/pandas-docs/version/0.21.1/whatsnew.html#improvements-to-the-parquet-io-functionality\">here</a>.</li>\n</ul>\n<p>See the <a class=\"reference external\" href=\"https://pandas.pydata.org/pandas-docs/version/0.21.1/whatsnew.html#improvements-to-the-parquet-io-functionality\">v0.21.1\nWhatsnew</a>\noverview for an extensive list of all the changes for 0.21.1.</p>\n</div>\n<div class=\"section\" id=\"best-way-to-install\">\n<h2>Best way to Install<a class=\"headerlink\" href=\"#best-way-to-install\" title=\"Permalink to this headline\">\u00c2\u00b6</a></h2>\n<p>The best way to get pandas is via <a class=\"reference external\" href=\"http://pandas.pydata.org/pandas-docs/stable/install.html#installing-pandas-with-anaconda\">conda</a></p>\n<p><code class=\"docutils literal notranslate\"><span class=\"pre\">conda</span> <span class=\"pre\">install</span> <span class=\"pre\">pandas</span></code></p>\n<p>Packages are available for <a class=\"reference external\" href=\"http://pandas.pydata.org/pandas-docs/stable/install.html#python-version-support\">all supported python versions</a>\non Windows, Linux, and MacOS.</p>\n<p>Wheels are also uploaded to <a class=\"reference external\" href=\"https://pypi.org/project/pandas/\">PyPI</a> and can be installed with</p>\n<div class=\"highlight-shell notranslate\"><div class=\"highlight\"><pre><span></span>pip install pandas\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"quick-vignette\">\n<h2>Quick vignette<a class=\"headerlink\" href=\"#quick-vignette\" title=\"Permalink to this headline\">\u00c2\u00b6</a></h2>\n<iframe src=\"https://player.vimeo.com/video/59324550\" width=\"500\"\nheight=\"309\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen\nallowFullScreen></iframe> <p><a href=\"https://vimeo.com/59324550\">10-minute\ntour of pandas</a> from <a href=\"https://vimeo.com/user10077863\">Wes\nMcKinney</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p></div>\n<div class=\"section\" id=\"what-problem-does-pandas-solve\">\n<h2>What problem does <em>pandas</em> solve?<a class=\"headerlink\" href=\"#what-problem-does-pandas-solve\" title=\"Permalink to this headline\">\u00c2\u00b6</a></h2>\n<p>Python has long been great for data munging and preparation, but less so for\ndata analysis and modeling. <em>pandas</em> helps fill this gap, enabling you to\ncarry out your entire data analysis workflow in Python without having to\nswitch to a more domain specific language like R.</p>\n<p>Combined with the excellent <a class=\"reference external\" href=\"https://ipython.org/\">IPython</a> toolkit and\nother libraries, the environment for doing data analysis in Python excels in\nperformance, productivity, and the ability to collaborate.</p>\n<p><em>pandas</em> does not implement significant modeling functionality outside of\nlinear and panel regression; for this, look to <a class=\"reference external\" href=\"http://statsmodels.sf.net\">statsmodels</a> and <a class=\"reference external\" href=\"http://scikit-learn.org\">scikit-learn</a>. More work is still needed to make Python a first\nclass statistical modeling environment, but we are well on our way toward that\ngoal.</p>\n</div>\n<div class=\"section\" id=\"what-do-our-users-have-to-say\">\n<h2>What do our users have to say?<a class=\"headerlink\" href=\"#what-do-our-users-have-to-say\" title=\"Permalink to this headline\">\u00c2\u00b6</a></h2>\n<blockquote>\n<div><a class=\"reference external image-reference\" href=\"https://www.aqr.com/\"><img alt=\"AQR Capital Management Logo\" class=\"align-right\" src=\"_images/aqr_capital_management_logo.png\" /></a>\n</div></blockquote>\n<div class=\"line-block\">\n<div class=\"line\"><strong>Roni Israelov, PhD</strong></div>\n<div class=\"line\">Portfolio Manager</div>\n<div class=\"line\"><a class=\"reference external\" href=\"https://www.aqr.com/\">AQR Capital Management</a></div>\n</div>\n<blockquote>\n<div><p>\u00e2\u0080\u009c<em>pandas</em> allows us to focus more on research and less on programming. We\nhave found <em>pandas</em> easy to learn, easy to use, and easy to maintain.\nThe bottom line is that it has increased our productivity.\u00e2\u0080\u009d</p>\n<a class=\"reference external image-reference\" href=\"https://www.appnexus.com/\"><img alt=\"AppNexus Logo\" class=\"align-right\" src=\"_images/appnexus_logo.png\" /></a>\n</div></blockquote>\n<div class=\"line-block\">\n<div class=\"line\"><strong>David Himrod</strong></div>\n<div class=\"line\">Director of Optimization &amp; Analytics</div>\n<div class=\"line\"><a class=\"reference external\" href=\"https://www.appnexus.com/\">AppNexus</a></div>\n</div>\n<blockquote>\n<div><p>\u00e2\u0080\u009c<em>pandas</em> is the perfect tool for bridging the gap between rapid\niterations of ad-hoc analysis and production quality code. If you\nwant one tool to be used across a multi-disciplined organization of\nengineers, mathematicians and analysts, look no further.\u00e2\u0080\u009d</p>\n<a class=\"reference external image-reference\" href=\"https://www.datadoghq.com/\"><img alt=\"Datadog Logo\" class=\"align-right\" src=\"_images/datadog_logo.png\" /></a>\n</div></blockquote>\n<div class=\"line-block\">\n<div class=\"line\"><strong>Olivier Pomel</strong></div>\n<div class=\"line\">CEO</div>\n<div class=\"line\"><a class=\"reference external\" href=\"https://www.datadoghq.com/\">Datadog</a></div>\n</div>\n<blockquote>\n<div>\u00e2\u0080\u009cWe use <em>pandas</em> to process time series data on our production servers.\nThe simplicity and elegance of its API, and its high level\nof performance for high-volume datasets, made it a perfect choice for\nus.\u00e2\u0080\u009d</div></blockquote>\n<div class=\"toctree-wrapper compound\">\n</div>\n</div>\n<div class=\"section\" id=\"library-highlights\">\n<h2>Library Highlights<a class=\"headerlink\" href=\"#library-highlights\" title=\"Permalink to this headline\">\u00c2\u00b6</a></h2>\n<ul class=\"simple\">\n<li>A fast and efficient <strong>DataFrame</strong> object for data manipulation with\nintegrated indexing;</li>\n<li>Tools for <strong>reading and writing data</strong> between in-memory data structures and\ndifferent formats: CSV and text files, Microsoft Excel, SQL databases, and\nthe fast HDF5 format;</li>\n<li>Intelligent <strong>data alignment</strong> and integrated handling of <strong>missing data</strong>:\ngain automatic label-based alignment in computations and easily manipulate\nmessy data into an orderly form;</li>\n<li>Flexible <strong>reshaping</strong> and pivoting of data sets;</li>\n<li>Intelligent label-based <strong>slicing</strong>, <strong>fancy indexing</strong>, and <strong>subsetting</strong>\nof large data sets;</li>\n<li>Columns can be inserted and deleted from data structures for <strong>size\nmutability</strong>;</li>\n<li>Aggregating or transforming data with a powerful <strong>group by</strong> engine\nallowing split-apply-combine operations on data sets;</li>\n<li>High performance <strong>merging and joining</strong> of data sets;</li>\n<li><strong>Hierarchical axis indexing</strong> provides an intuitive way of working with\nhigh-dimensional data in a lower-dimensional data structure;</li>\n<li><strong>Time series</strong>-functionality: date range generation and frequency\nconversion, moving window statistics, moving window linear regressions, date\nshifting and lagging. Even create domain-specific time offsets and join time\nseries without losing data;</li>\n<li>Highly <strong>optimized for performance</strong>, with critical code paths written in\n<a class=\"reference external\" href=\"http://www.cython.org/\">Cython</a> or C.</li>\n<li>Python with <em>pandas</em> is in use in a wide variety of <strong>academic and\ncommercial</strong> domains, including Finance, Neuroscience, Economics,\nStatistics, Advertising, Web Analytics, and more.</li>\n</ul>\n</div>\n</div>\n\n\n          </div>\n        </div>\n      </div>\n      <div class=\"clearer\"></div>\n    </div>\n<div class=\"footer-wrapper\">\n</div>\n\n<a href=\"http://github.com/pydata/pandas\"><img style=\"position: fixed; top: 0; right: 0; border: 0;\"\n   src=\"_static/forkme_right_gray_6d6d6d.png\" alt=\"Fork me on GitHub\"></a></div>\n\n</body>\n</html>\n\n  </body>\n</html>",
   "pm_keywords": ""
}]